---
title: "Association, partitionnement et ordination"
author: "Serge-√âtienne Parent"
date: "`r format(Sys.Date())`"
output: github_document
---

# Association, partitionnement et ordination {#chapitre-ordination}

```{r}
library("tidyverse")
```


## Espaces d'analyse

### Abondance et occurence

L'abondance est le d√©compte d'esp√®ces observ√©es, tandis que l'occurence est la pr√©sence ou l'absence d'une esp√®ce. Le tableau suivant contient des donn√©es d'abondance.

```{r}
abundance <- tibble('Bruant familier' = c(1, 0, 0, 3),
                        'Citelle √† poitrine rousse' = c(1, 0, 0, 0),
                        'Colibri √† gorge rubis' = c(0, 1, 0, 0),
                        'Geai bleu' = c(3, 2, 0, 0),
                        'Bruant chanteur' = c(1, 0, 5, 2),
                        'Chardonneret' = c(0, 9, 6, 0),
                        'Bruant √† gorge blanche' = c(1, 0, 0, 0),
                        'M√©sange √† t√™te noire' = c(20, 1, 1, 0),
                        'Jaseur bor√©al' = c(66, 0, 0, 0))
```

Ce tableau peut √™tre rapidement transform√© en donn√©es d'occurence, qui ne comprennent que l'information bool√©enne de pr√©sence (not√© 1) et d'absence (not√© 0).

```{r}
occurence <- abundance %>%
  transmute_all(funs(if_else(. > 0, 1, 0)))
occurence
```

L'**espace des esp√®ces** (ou des variables ou descripteurs) est celui o√π les esp√®ces forment les axes et o√π les sites sont positionn√©s dans cet espace. Il s'agit d'une perspective en *mode R*, qui permet principalement d'identifier quels esp√®ces se retrouvent plus courrament ensemble.


```{r}
library("scatterplot3d")
species <- c("Bruant chanteur", "Chardonneret", "M√©sange √† t√™te noire")
x <- abundance %>% pull(species[1])
y <- abundance %>% pull(species[2])
z <- abundance %>% pull(species[3])
scatterplot3d(x, y, z, angle = 20, asp = 0.3,
              xlab = species[1], ylab = species[2], zlab = species[3])
```

Dans l'**espace des sites** (ou les √©chantillons ou objets), on transpose la matrice d'abondance. On passe ici en *mode Q*, o√π chaque point est une esp√®ce, et o√π l'on peut observer quels √©chantillons sont similaires.

```{r}
site1 <- t(abundance)[, 1]
site2 <- t(abundance)[, 2]
site3 <- t(abundance)[, 3]
scatterplot3d(site1, site2, site3, angle = 20, asp = 10,
              xlab = "Site 1", ylab = "Site 2", zlab = "Site 3")
```

### Environnement

L'**espace de l'environnement** comprend souvent un autre tableau contenant l'information sur l'environnement o√π se trouve les esp√®ces: les coordonn√©es et l'√©l√©vation, la pente, le pH du sol, la pluviom√©trie, etc.

## Analyse d'association

Nous utiliserons le terme *association* come une **mesure pour quantifier la ressemblance ou la diff√©rence entre deux objets (√©chantillons) ou variables (descripteurs)**.

Alors que la corr√©lation et la covariance sont des mesures d'association entre des variables (analyse en *mode R*), la **similarit√©** et la **distance** sont deux types de une mesure d'association entre des objets (analyse en *mode Q*). Une distance de 0 est mesur√© chez deux objets identiques. La distance augmente au fur et √† mesure que les objets sont dissoci√©s. Une similarit√© ayant une valeur de 0 indique aucune association, tandis qu'une valeur de 1 indique une association parfaite. √Ä l'oppos√©, la dissimilarit√© est √©gale √† 1-similarit√©.

La distance peut √™tre li√©e √† la similarit√© par la relation:

$$distance=\sqrt{1-similarit√©}$$

ou

$$distance=\sqrt{dissimilarit√©}$$

La racine carr√©e permet, pour certains indices de similarit√©, d'obtenir des propri√©t√©s eucl√©diennes. Pour plus de d√©tails, voyez le tableau 7.2 de [Legendre et Legendre (2012)](https://www.elsevier.com/books/numerical-ecology/legendre/978-0-444-53868-0).

Les matrices d'association sont g√©n√©ralement pr√©sent√©es comme des matrices carr√©es, dont les dimensions sont √©gales au nombre d'objets (*mode Q*) ou de vrariables (*mode R*) dans le tableau. Chaque √©l√©ment ("cellule") de la matrice est un indice d'association entre un objet (ou une variable) et un autre. Ainsi, la diagonale de la matrice est un vecteur nul (distance ou dissimilarit√©) ou unitaire (similarit√©), car elle correspond √† l'association entre un objet et lui-m√™me. 

Puisque l'association entre A et B est la m√™me qu'entre B et A, et puisque la diagonale retourne une valeur convenue, il est possible d'exprimer une matrice d'association en mode "compact", sous forme de vecteur. Le vecteur d'association entre des objets A, B et C contiendra toute l'information n√©cessaire en un vecteur de trois chiffres, `[AB, AC, BC]`, plut√¥t qu'une matrice de dimension $3 \times 3$. L'impact sur la m√©moire vive peut √™tre consid√©rable pour les calculs comprenant de nombreuses dimensions.

En R, les calculs de similarit√© et de distances peuvent √™tre effectu√©s avec le module `vegan`. La fonction `vegdist` permet de calculer les indices d'association en forme carr√©e.

Nous verons plus tard les m√©thodes de mesure de similarit√© et de distance plus loin. Pour l'instant, utilisons la m√©thode de *Jaccard* pour une d√©monstration sur des donn√©es d'occurence.

```{r}
library("vegan")
vegdist(occurence, method = "jaccard",
        diag = TRUE, upper = TRUE)
```

Remarquez que `vegdist` retourne une matrice dont la diagonale est de 0 (on l'affiche en sp√©cifiant `diag = TRUE`). La diagonale est l'association d'un objet avec lui-m√™me. Or la similarit√© d'un objet avec lui-m√™me devrait √™tre de 1! En fait, par convention `vegdist` retourne des dissimilarit√©s, non pas des similarit√©s. La matrice de distance serait donc calcul√©e en extrayant la racine carr√©e des √©l√©ments de la matrice de dissimilarit√©:

```{r}
dissimilarity <- vegdist(occurence, method = "jaccard",
                        diag = TRUE, upper = TRUE)
distance <- sqrt(dissimilarity)
distance
```

Dans le chapitre sur l'analyse compositionnelle, nous avons abord√© les significations diff√©rentes que peuvent prendre le z√©ro. L'information fournie par un z√©ro peut √™tre diff√©rente selon les circonstances. Dans le cas d'une variable continue, un z√©ro signifie g√©n√©ralement une mesure sous le seuil de d√©tection. Deux tissus dont la concentration en cuivre est nulle ont une afinit√© sous la perspective de la concentration en cuivre. Dans le cas de mesures d'abondance (d√©compte) ou d'occurence (pr√©sence-absence), on pourra d√©crire comme similaires deux niches √©cologiques o√π l'on retrouve une esp√®ce en particulier. Mais deux sites o√π l'on de retouve pas d'ours polaires ne correspondent pas n√©cessairement √† des niches similaires! En effet, il peut exister de nombreuses raisons √©cologiques et m√©thodologiques pour lesquelles l'esp√®ces ou les esp√®ces n'ont pas √©t√© observ√©es. C'est le probl√®me des **double-z√©ros** (esp√®ces non observ√©es √† deux sites), probl√®me qui est amplifi√© avec les grilles comprenant des esp√®ces rares.

La ressemblance entre des objets comprenant des donn√©es continues devrait √™tre calcul√©e gr√¢ce √† des indicateurs *sym√©triques*. Inversement, les affinit√©s entre les objets d√©crits par des donn√©es d'abondance ou d'occurence susceptibles de g√©n√©rer des probl√®mes de double-z√©ros devraient √™tre √©valu√©es gr√¢ce √† des indicateurs *asym√©triques*. Un d√©fi suppl√©mentaire arrive lorsque les donn√©es sont de type mixte.

Nous utiliserons la convention de `scipy` et nous calculerons la dissimilarit√©, non pas la similarit√©. Les mesures de dissimilarit√© sont calcul√©es sur des donn√©es d'abondance ou des donn√©es d'occurence. Notons qu'il existe beaucoup de confusion dans la litt√©rature sur la mani√®re de nommer les dissimilarit√©s (ce qui n'est pas le cas des distances, dont les noms sont reconnus). Dans les sections suivantes, nous noterons la dissimilarit√© avec un $d$ minuscule et la distance avec un $D$ majuscule.

### Association entre objets (mode Q)

#### Objets: Abondance

La **dissimilarit√© de Bray-Curtis** est asym√©trique. Elle est aussi appel√©e l'indice de Steinhaus, de Czekanowski ou de S√∏rensen. Il est important de s'assurer de bien s'entendre la m√©thode √† laquelle on fait r√©f√©rence. L'√©quation enl√®ve toute ambiguit√©. La dissimilarit√© de Bray-Curtis entre les points A et B est calcul√©e comme suit.

$$d_{AB} =  \frac {\sum \left| A_{i} - B_{i} \right| }{\sum \left(A_{i}+B_{i}\right)}$$

Utilisons `vegdist` pour g√©n√©rer les matrices d'association. Le format "liste" de R est pratique pour enregistrer la collection d'objets, dont les matrice d'association que nous allons cr√©er dans cette section.

```{r}
associations_abund <- list()
associations_abund[['BrayCurtis']] <- vegdist(abundance, method = "bray")
associations_abund[['BrayCurtis']]
```

La dissimilarit√© de Bray-Curtis est souvent utilis√©e dans la litt√©rature. Toutefois, la version originale de Bray-Curtis n'est pas tout √† fait m√©trique (semim√©trique). Cons√©quemment, la **dissimilarit√© de Ruzicka** (une variante de la dissimilarit√© de Jaccard pour les donn√©es d'abondance) est m√©trique, et devrait probablement √™tre pr√©f√©r√© √† Bary-Curtis ([Oksanen, 2006](http://ocw.um.es/ciencias/geobotanica/otros-recursos-1/documentos/vegantutorial.pdf)).

$$d_{AB, Ruzicka} =  \frac { 2 \times d_{AB, Bray-Curtis} }{1 + d_{AB, Bray-Curtis}}$$

```{r}
associations_abund[['Ruzicka']] <- associations_abund[['BrayCurtis']] * 2 / (1 + associations_abund[['BrayCurtis']])
```

La **dissimilarit√© de Kulczynski** (aussi √©crit Kulsinski) est asym√©trique et semim√©trique, tout comme celle de Bray-Curtis. Elle est calcul√©e comme suit.

$$d_{AB} = 1-\frac{1}{2} \times \left[ \frac{\sum min(A_i, B_i)}{\sum A_i} + \frac{\sum min(A_i, B_i)}{\sum B_i} \right]$$

```{r}
associations_abund[['Kulczynski']] <- vegdist(abundance, method = "kulczynski")
```

Une approche commune pour mesurer l'association entre sites d√©crits par des donn√©es d'abondance est la **distance de Hellinger**. Notez qu'il s'agit ici d'une distance, non pas d'une dissimilarit√©. Pour l'obtenir, on doit d'abord diviser chaque donn√©e d'abondance par l'abondance totale pour chaque site pour obtenir les esp√®ces en tant que proportions, puis on extrait la racine carr√©e de chaque √©l√©ment. Enfin, on calcule la distance euclidienne entre les proportions de chaque site. Pour rappel, une distance euclidienne est la g√©n√©ralisation en plusieurs dimensions du th√©or√®me de Pythagore, $c = \sqrt{a^2 + b^2}$.

$$D_{AB} = \sqrt {\sum \left( \frac{A_i}{\sum A_i} - \frac{B_i}{\sum B_i} \right)^2}$$

------------------ -----------------------------------------------
üò±\ **Attention**   La distance d'Hellinger h√©rite des biais li√©es aux donn√©es compositionnelles. Elle peut √™tre substiti√©e par une matrice de distances d'Aitchison.

------------------------------------------------------------------

```{r}
associations_abund[['Hellinger']] <- dist(decostand(abundance, method="hellinger"))
```

Toute comme la distance d'Hellinger, la **distance de chord** est calcul√©e par une distance euclidienne sur des donn√©es d'abondance transform√©es de sorte que chaque ligne ait une longueur (norme) de 1.

```{r}
associations_abund[['Chord']] <- dist(decostand(abundance, method="normalize"))
```

La **m√©trique du chi-carr√©**, ou $\chi$-carr√©, ou chi-square, donne davantage de poids aux esp√®ces rares qu'aux esp√®ces communes. Son utilisation est recommand√©e lorsque les esp√®ces rares sont de bons indicateurs de conditions √©cologiques particuli√®res ([Legendre et Legendre, 2012](https://www.elsevier.com/books/numerical-ecology/legendre/978-0-444-53868-0), p. 308).

$$  d_{AB} = \sqrt{\sum _j \frac{1}{\sum y_j} \left( \frac{A_j}{\sum A} - \frac{B_j}{\sum B} \right)^2 }  $$

La m√©trique peut √™tre transform√©e en distance en la multipliant par la racine carr√©e de la somme totale des esp√®ces dans la matric d'abondance ($X$).

$$ D_{AB} = \sqrt{\sum X} \times d_{AB} $$


```{r}
associations_abund[['ChiSquare']] <- dist(decostand(abundance, method="chi.square"))
```

Une manni√®re visuellement plus int√©ressante de pr√©senter une matrice d'association est un graphique de type *heatmap*.

```{r}
associations_abund_df <- list()

for (i in 1:length(associations_abund)) {
  associations_abund_df[[i]] <- data.frame(as.matrix(associations_abund[[i]]))
  colnames(associations_abund_df[[i]]) <- rownames(associations_abund_df[[i]])
  associations_abund_df[[i]]$row <- rownames(associations_abund_df[[i]])
  associations_abund_df[[i]] <- associations_abund_df[[i]] %>% gather(key=row)
  associations_abund_df[[i]]$column = rep(1:4, 4)
  associations_abund_df[[i]]$dist <- names(associations_abund)[i]
}
associations_abund_df <- do.call(rbind, associations_abund_df)

ggplot(associations_abund_df, aes(x=row, y=column)) +
  facet_wrap(. ~ dist, nrow = 2) +
  geom_tile(aes(fill = value)) +
  geom_text(aes(label = round(value, 2))) +
  scale_fill_gradient2(low = "#00ccff", mid = "#aad400", high = "#ff0066", midpoint = 2) +
  labs(x="Site", y="Site")
```

Peu importe le type d'association utilis√©e, les *heatmaps* montrent les m√™mes tendances. Les assocaitions de dissimilarit√© (Bray-Curtis, Kulczynski et Ruzicka) s'√©talent de 0 √† 1, tandis que les distances (Chi-Square, Chord et Hellinger) partent de z√©ro, mais n'ont pas de limite sup√©rieure. On note les plus grandes diff√©rences entre les sites 2 et 4, tandis que les sites 2 et 3 sont les plus semblables pour toutes les mesures d'association √† l'exception de la dissimilarit√© de Kulczynski.

#### Objets: Occurence (pr√©sence-absence)

Des indices d'association diff√©rents devraient √™tre utilis√©s lorsque des donn√©es sont compil√©es sous forme bool√©enne. En g√©n√©ral, les tableaux de donn√©es d'occurence seront compil√©s avec des 1 (pr√©sence) et des 0 (absence).

La **similarit√© de Jaccard** entre le site A et le site B est la proportion de double 1 (pr√©sences de 1 dans A et B) parmi les esp√®ces. La dissimilari√© est la proportion compl√©mentaire (comprenant [1, 0], [0, 1] et [0, 0]). La distance de Jaccard est la racine carr√©e de la dissimilarit√©.

```{r}
associations_occ <- list()
associations_occ[['Jaccard']] <- vegdist(occurence, method = "jaccard")
```

Les **distances d'Hellinger, de chord et de chi-carr√©** sont aussi appropri√©es pour les calculs de distances sur des tableaux d'occurence.


```{r}
associations_occ[['Hellinger']] <- dist(decostand(occurence, method="hellinger"))
associations_occ[['Chord']] <- dist(decostand(occurence, method="normalize"))
associations_occ[['ChiSquare']] <- dist(decostand(occurence, method="chi.square"))
```

Graphiquement,

```{r}
associations_occ_df <- list()

for (i in 1:length(associations_occ)) {
  associations_occ_df[[i]] <- data.frame(as.matrix(associations_occ[[i]]))
  colnames(associations_occ_df[[i]]) <- rownames(associations_occ_df[[i]])
  associations_occ_df[[i]]$row <- rownames(associations_occ_df[[i]])
  associations_occ_df[[i]] <- associations_occ_df[[i]] %>% gather(key=row)
  associations_occ_df[[i]]$column = rep(1:4, 4)
  associations_occ_df[[i]]$dist <- names(associations_occ)[i]
}
associations_occ_df <- do.call(rbind, associations_occ_df)

ggplot(associations_occ_df, aes(x=row, y=column)) +
  facet_wrap(. ~ dist) +
  geom_tile(aes(fill = value)) +
  geom_text(aes(label = round(value, 2))) +
  scale_fill_gradient2(low = "#00ccff", mid = "#aad400", high = "#ff0066", midpoint = 1) +
  labs(x="Site", y="Site")

```

Il est attendu que les matrices d'association sur l'occurence sont semblables √† celles sur l'abondance. Dans ce cas-ci, la distance d'Hellinger donne des r√©sultats semblables √† la dissimilarit√© de Jaccard.

#### Objets: Donn√©es quantitatives

Les donn√©es quantitative en √©cologie peuvent d√©crire l'√©tat de l'environnement: le climat, l'hydrologie, l'hydrog√©ochimie, la p√©dologie, etc. En r√®gle g√©n√©rale, les coordonn√©es des sites ne sot pas des variables environnementales, √† que l'on soup√ßonne la coordonn√©e elle-m√™me d'√™tre responsable d'effets sur notre syst√®me: mais il s'agira la plupart du temps d'effets confondants (par exemple, on peut mesurer un effet de lattitude sur le rendement des agrumes, mais il s'agira probablement avant tout d'effets dus aux conditions climatiques, qui elles changent en fonction de la lattitude). D'autre types de donn√©es quantitative pouvant √™tre appr√©hend√©es par des distances sont les traits ph√©nologiques, les ionomes, les g√©nomes, etc.

La **distance euclidienne** est la racine carr√©e de la somme des carr√©s des distances sur tous les axes. Il s'agit d'une application multidimensionnelle du th√©or√®me de Pythagore. La **distance d'Aitchison**, couverte dans le chapitre 6, est une distance euclidienne calcul√©e sur des donn√©es compositionnelles pr√©alablement transform√©es. La distance euclidienne est sensible aux unit√©s utilis√©s: utiliser des milim√®tres plut√¥t que des m√®tres enflera la distance euclidienne. Il est recommand√© de porter une attention particuli√®re aux unit√©s, et de standardiser les donn√©es au besoin (par exemple, en centrant la moyenne √† z√©ro et en fixant l'√©cart-type √† 1).

On pourrait, par exemple, mesurer la distance entre des observations des dimensions de diff√©rentes esp√®ces d'iris. Ce tableau est inclu dans R par d√©faut.

```{r}
data(iris)
iris %>% sample_n(5)
```

Les mesures du tableau sont en centim√®tres. Pour √©viter de donner davantage de poids aux longueur des s√©pales et en m√™me temps de n√©gliger la largeur des p√©tales, nous allons standardiser le tableau.

```{r}
iris_sc <- iris %>%
  select(-Species) %>% 
  scale(.) %>% 
  as_tibble(.)
iris_sc
```

Pour les comparaisons des dimensions, prenons la moyenne des dimensions (mises √† l'√©chelle) par esp√®ce.

```{r}
iris_means <- iris_sc %>%
  group_by(Species) %>%
  summarise_all(mean) %>%
  select(-Species)
iris_means
```

Nous pouvons utiliser la distance euclidienne, commune en g√©om√©trie, pour comparer les esp√®ces. La distance euclidienne est calcul√©e comme suit.


$$ \mathcal{E} = \sqrt{\Sigma_i \left( A_i - B_i \right) ^2 } $$

```{r}
associations_cont = list()
associations_cont[['Euclidean']] <- dist(iris_sc, method="euclidean")
```

La **distance de Mahalanobis** est semblable √† la distance euclidienne, mais qui tient compte de la covariance de la matrice des objets. Cette covariance peut √™tre utilis√©e pour d√©crire la structure d'un nuage de points. La figure suivante montre deux points verts qui se trouvent aux extr√™mes d'un nuage de point. Ces points ont des distances euclidiennes par rapport au centre diff√©rentes: les lignes d'√©quidistance eucl√©dienne sont trac√©es en rose. Toutefois, les deux points ont un distance de Mahalanobis √©gale √† partir du centre.

<img src="images/07_eucl-maha.png" width=400>
<p style="text-align: center">Source: [Parent et al. (2012)](https://www.intechopen.com/books/soil-fertility/nutrient-balance-as-paradigm-of-plant-and-soil-chemometricsnutrient-balance-as-paradigm-of-soil-and-).</p>

La diastance de Mahalanobis se calcule comme suit.

$$\mathcal{M} = \sqrt{(A - B)^T S^{-1} (A-B)}$$

Notez qu'il s'agit d'une g√©n√©ralisation de la distance euclidienne, qui √©quivaut √† une distance de Mahalanobis dont la matrice de covariance est une matrice identit√©.

La distance de Mahalanobis permet de repr√©senter des distances dans un espace fortement corr√©l√©. Elle est courramment utilis√©e pour d√©tecter les valeurs aberrantes selon des crit√®res de distance √† partir du centre d'un jeu de donn√©es multivari√©es.

```{r}
associations_cont[['Mahalanobis']] <- vegdist(iris_sc, 'mahalanobis')
```

La **distance de Manhattan** porte aussi le nom de distance de cityblock ou de taxi. C'est la distance que vous devrez parcourir pour vous rendre du point A au point B √† Manhattan, c'est-√†-dire selon une s√©quence de tron√ßons perpendiculaires.

$$ D_{AB} = \sum _i \left| A_i - B_i \right| $$

La distance de Manhattan est appropri√©e lorsque les gradients (changements d'un √©tat √† l'autre ou d'une r√©gion √† l'autre) ne permettent pas des changements simultan√©s. Mieux vaut standardiser les variables pour √©viter qu'une dimension soit pr√©pond√©rante.

```{r}
associations_cont[['Manhattan']] <- vegdist(iris_sc, 'manhattan')
```

Graphiquement

```{r}
associations_cont_df <- list()

for (i in 1:length(associations_cont)) {
  associations_cont_df[[i]] <- data.frame(as.matrix(associations_cont[[i]]))
  colnames(associations_cont_df[[i]]) <- rownames(associations_cont_df[[i]])
  associations_cont_df[[i]]$row <- rownames(associations_cont_df[[i]])
  associations_cont_df[[i]] <- associations_cont_df[[i]] %>% gather(key=row)
  associations_cont_df[[i]]$column = rep(1:nrow(iris), nrow(iris))
  associations_cont_df[[i]]$dist <- names(associations_cont)[i]
}
associations_cont_df <- do.call(rbind, associations_cont_df)

ggplot(associations_cont_df, aes(x=row, y=column)) +
  facet_wrap(. ~ dist) +
  geom_tile(aes(fill = value), colour = NA) +
  #geom_text(aes(label = round(value, 2))) +
  scale_fill_gradient2(low = "#00ccff", mid = "#aad400", high = "#ff0066", midpoint = 5) +
  labs(x="Site", y="Site")
```


Le tableau `iris` est ordonn√© par esp√®ce. Les distances euclidienne et de Manhattan permettent ais√©ment de distinguer les esp√®ces selon les dimensions des p√©tales et des s√©pales. Toutefois, l'utilsation de la covariance avec la distance de Mahalanobis cr√©e des distinction moins tranch√©es.

#### Objets: Donn√©es mixtes

Les donn√©es cat√©gorielles ordinales peuvent √™tre transform√©es en donn√©es continues par gradations lin√©aires ou quadratiques. Les donn√©es cat√©gorielles nominales, quant √† elles, peuvent √™tre *dummyfi√©es* en donn√©es similaires √† des occurences. Attention toutefois: contrairement √† la r√©gression lin√©aire qui demande d'exclure une cat√©gorie, la *dummyfication* doit inclure toutes les cat√©gories. Le comportement par d√©faut de la fonction `pandas.get_dummies` est de garder toutes les cat√©gories. La **similarit√© de Gower** a √©t√© d√©velopp√©e pour mesurer des associations entre des objets dont les donn√©es sont mixtes: bool√©ennes, cat√©gorielles et continues. La similarit√© de Gower est calcul√©e en additionnant les distances calcul√©es par colonne, individuellement. Si la colonne est bool√©enne, on utilise les distances de Jaccard (qui exclue les double-z√©ro) de mani√®re univari√©e: une variable √† la fois. Pour les variables continues, on utilise la distance de Manhattan divis√©e par la plage de valeurs de la variable (pour fin de standardisation). Puisqu'elle h√©rite de la particularit√© de la distance de Manhattan et de la similarit√© de Jaccard univari√©e, la **similarit√© de Gower** reste une combinaison lin√©aire de distances univari√©es.


```{r}
X <- tibble(ID = 1:8,
            age = c(21, 21, 19, 30, 21, 21, 19, 30),
            gender = c('M','M','N','M','F','F','F','F'),
            civil_status = c('MARRIED','SINGLE','SINGLE','SINGLE','MARRIED','SINGLE','WIDOW','DIVORCED'),
            salary = c(3000.0,1200.0 ,32000.0,1800.0 ,2900.0 ,1100.0 ,10000.0,1500.0),
            children = c(TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE),
            available_credit = c(2200,100,22000,1100,2000,100,6000,2200))
X
```

Il faut pr√©alablement *dummifier* les variables cat√©gorielles nominales.

```{r}
X_dum <- model.matrix(~ 0 + ., X[, -1])
X_dum
```

Calculons la dissimilarit√© de Gower (cette fois le graphique est fait avec `pheatmap`).

```{r}
library("pheatmap")
d_gow <- as.matrix(vegdist(X_dum, 'gower'))
colnames(d_gow) <- rownames(d_gow) <- X$ID
pheatmap(d_gow)
```

Les dendrogrammes apparaissants sur les axes du graphique sont issus d'un processus de partitionnement bas√© sur la distance, que nous verrons plus loin dans ce chapiter. Les profils des clients 4 et 7, ainsi que ceux des clients 3 et 7 diff√®rent le plus. Les profils 3 et 4 sont n√©anmoins plut√¥t diff√©rents.

### Associations entre variables (mode R)

Il existe de nombreuses approches pour mesurer les associations entre variables. La plus connue est la corr√©lation. Mais les donn√©es d'abondance et d'occurence demandent des approches diff√©rentes.

#### Variables: Abondance

La distance du chi-carr√© est sugg√©r√©e par [Borcard et al. (2011)](http://www.springer.com/us/book/9781441979759).

```{r}
abundance_r <- t(abundance)
D_chisq_R <- as.matrix(dist(decostand(abundance_r, method="chi.square")))
pheatmap(D_chisq_R, display_numbers = round(D_chisq_R, 2))
```

Des coabondances sont notables pour la m√©sange √† t√™te noire, le jaseur bor√©al, la citelle √† poitrine rousse et le bruant √† gorge blanche (tache bleu au centre).

#### Variables: Occurence

La dissimilarit√© de Jaccard peut √™tre utilis√©e.

```{r}
occurence_r <- t(occurence)
D_jacc_R <- as.matrix(vegdist(occurence_r, method = "jaccard"))
pheatmap(D_jacc_R, display_numbers = round(D_jacc_R, 2))
```

Des cooccurences sont notables pour le jaseur bor√©al, la citelle √† poitrine rousse et le bruant √† gorge blanche (tache bleu au centre).

#### Variables: Quantit√©s

La matrice des corr√©lations de Pearson peut √™tre utilis√©e pour les donn√©es continues. Quant aux variables ordinales, elles devraient id√©alement √™tre li√©es lin√©airement ou quadratiquement. Si ce n'est pas le cas, c'est-√†-dire que les cat√©gories sont ordonn√©es par rang seulement, vous pourrez avoir recours aux coefficients de corr√©lation de Spearman ou de Kendall.

```{r}
iris_cor <- iris %>%
  select(-Species) %>%
  cor(.)
pheatmap(iris_cor, cluster_rows = FALSE, cluster_cols = FALSE,
         display_numbers = round(iris_cor, 2))
```

### Conclusion sur les associations

Il n'existe pas de r√®gle claire pour d√©terminer quelle technique d'association utiliser. Cela d√©pend en premier lieu de vos donn√©es. Vous s√©lectionnerez votre m√©thode d'association selon le type de donn√©es que vous abordez, la question √† laquelle vous d√©sirez r√©pondre ainsi l'exp√©rience dans la litt√©rature comme celle de vos coll√®gues scientifiques. S'il n'existe pas de r√®gle clair, c'est qu'il existe des dizaines de m√©thodes diff√©rentes, et la plupart d'entre elles vous donneront une perspective juste et valide. Il faut n√©anmoins faire attention pour √©viter de s√©lectionner les m√©thodes qui ne sont pas appropri√©es. 

## Partitionnement

Les donn√©es suivantes ont √©t√© g√©n√©r√©es par [Leland McInnes](https://github.com/scikit-learn-contrib/hdbscan/blob/master/notebooks/clusterable_data.npy) (Tutte institute of mathematics, Ottawa). √ätes-vous en mesure d'identifier des groupes? Combien en trouvez-vous?

```{r fig.height = 5, fig.width = 5}
df_mcinnes <- read_csv("data/clusterable_data.csv", col_names = c("x", "y"), skip = 1)
plot(y~x, df_mcinnes, pch=16, cex=0.4)
```

En 2D, l'oeil humain peut facilement d√©tecter les groupes. En 3D, c'est toujours possible, mais au-del√† de 3D, le partitionnement cognitive devient rapidement maladroite. Les algorithmes sont alors d'une aide pr√©cieuse. Mais ils transportent en pratique tout un baggage de limitations. Quel est le crit√®re d'association entre les groupes? Combien de groupe devrions-nous cr√©er? Comment distinguer une donn√©e trop bruit√©e pour √™tre classifi√©e?

Le partitionnement de donn√©es (*clustering* en anglais), et inversement leur regroupement, permet de cr√©er des ensembles selon des crit√®res d'association. On suppose donc que Le partitionnement permet de cr√©er des groupes selon l'information que l'on fait √©merger des donn√©es. Il est cons√©quemment entendu que les donn√©es ne sont pas cat√©goris√©es √† priori: **il ne s'agit pas de pr√©dire la cat√©gorie d'un objet, mais bien de cr√©er des cat√©gories √† partir des objets** par exemple selon leurs dimensions, leurs couleurs, leurs signature chimique, leurs comportements, leurs g√®nes, etc. 

Plusieurs m√©thodes sont aujourd'hui offertes aux analystes pour partitionner leurs donn√©es. Dans le cadre de ce manuel, nous couvrirons ici deux grandes tendances dans les algorithmes.

1. *M√©thodes hi√©rarchique et non hi√©rarchiques*. Dans un partitionnement hi√©rarchique, l'ensemble des objets forme un groupe, comprenant des sous-regroupements, des sous-sous-regroupements, etc., dont les objets forment l'ultime partitionnement. On pourra alors identifier comment se d√©cline un partitionnement. √Ä l'inverse, un partitionnement non-hi√©rarchique des algorhitmes permettent de cr√©er les groupes non hi√©rarchis√©s les plus diff√©rents que possible.

2. *Membership exclusif ou flou*. Certaines techniques attribuent √† chaque une classe unique: l'appartenance sera indiqu√©e par un 1 et la non appartenance par un 0. D'autres techniques vont attribuer un membership flou o√π le degr√© d'appartenance est une variable continue de 0 √† 1. Parmi les m√©thodes floues, on retrouve les m√©thodes probabilistes.

### √âvaluation d'un partitionnement

Le choix d'une technique de partitionnement parmi de [nombreuses disponibles](http://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods), ainsi que le choix des param√®tres gouvernant chacune d'entre elles, est avant tout bas√© sur ce que l'on d√©sire d√©finir comme √©tant un groupe, ainsi que la mani√®re d'interpr√©ter les groupes. En outre, **le nombre de groupe √† d√©partager est *toujours* une d√©cision de l'analyste**. N√©anmoins, on peut se fier [des indicateurs de performance de partitionnement](http://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation). Parmis ceux-ci, retenons le score [silouhette](http://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient) ainsi que l'[indice de Calinski-Harabaz](http://scikit-learn.org/stable/modules/clustering.html#calinski-harabaz-index).

#### Score silouhette

En anglais, le *h* dans silouhette se trouve apr√®s le *l*: on parle donc de *silhouette coefficient* pour d√©signer le score de chacun des objets dans le partitionnement. Pour chaque objet, on calcule la distance moyenne qui le s√©pare des autres points de son groupe ($a$) ainsi que la distance moyenne  qui le s√©pare des points du groupe le plus rapproch√©.

$$s = \frac{b-a}{max \left(a, b \right)}$$

Un coefficient de -1 indique le pire classement, tandis qu'un coefficient de 1 indique le meilleur classement. La moyenne des coefficients silouhette est le score silouhette.

#### Indice de Calinski-Harabaz

L'indice de Calinski-Harabaz est proportionnel au ratio des dispersions intra-groupe et la moyenne des dispersions inter-groupes. Plus l'indice est √©lev√©, mieux les groupes sont d√©finis. La math√©matique est d√©crite [dans la documentation de scikit-learn](http://scikit-learn.org/stable/modules/clustering.html#calinski-harabaz-index).

**Note**. Les coefficients silouhette et l'indice de Calinski-Harabaz sont plus appropri√©s pour les formes de groupes convexes (cercles, sph√®res, hypersph√®res) que pour les formes irr√©guli√®res (notamment celles obtenues par la DBSCAN, discut√©e ci-desssous).

### Partitionnement non hi√©rarchique

Il peut arriver que vous n'ayez pas besoin de comprendre la structure d'agglom√©ration des objets (ou variables).  Plusieurs techniques de partitionnement non hi√©rarchique [sont disponibles dans le module scikit-learn](http://scikit-learn.org/stable/modules/clustering.html). On s'int√©ressera en particulier √† celles-ci.

**Kmeans** (`sklearn.cluster.Kmeans`). L'objectif des kmeans est de minimiser la distance eucl√©dienne entre un nombre pr√©d√©fini de *k* groupes exclusifs.

1. L'algorhitme commence par placer une nombre *k* de centroides au hasard dans l'espace d'un nombre *p* de variables (vous devez fixer *k*, et *p* est le nombre de colonnes de vos donn√©es).
2. Ensuite, chaque objet est √©tiquett√© comme appartenant au groupe du centroid le plus pr√®s.
3. La position du centroide est d√©plac√©e √† la moyenne de chaque groupe.
4. Recommencer √† partir de l'√©tape 2 jusqu'√† ce que l'assignation des objets aux groupes ne change plus.

![](https://media.giphy.com/media/12vVAGkaqHUqCQ/giphy.gif)
<center>Source: [David Sheehan](https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/)</center>

La technique des kmeans suppose que les groupes ont des distributions multinormales - repr√©sent√©es par des cercles en 2D, des sph√®res en 3D, des hypersph√®res en plus de 3D. Cette limitation est probl√©matique lorsque les groupes se pr√©sentent sous des formes irr√©guli√®res, comme celles du nuage de points de Leland McInnes, pr√©sent√© plus haut. De plus, la technique classique des kmeans est bas√©e sur des distances euclidiennes: l'utilisation des kmeans n'est appropri√©e pour les donn√©es comprenant beaucoup de z√©ros, comme les donn√©es d'abondance, qui devraient pr√©alablement √™tre transform√©es en variables centr√©es et r√©duites ([Legendre et Legendre, 2012](https://www.elsevier.com/books/numerical-ecology/legendre/978-0-444-53868-0)). La technique des **mixtures gaussiennes** (*gaussian mixtures*, `sklearn.mixture.GaussianMixture`) est une g√©n√©ralisation des kmeans permettant d'int√©grer la covariance des groupes. Les groupes ne sont plus des hyper-sph√®res, mais des hyper-ellipso√Ødes.

**DBSCAN**. La technique DBSCAN (* **D**ensity-**B**ased **S**patial **C**lustering of **A**pplications with **N**oise*) sousentend que les groupes sont compos√©s de zones o√π l'on retrouve plus de points (zones denses) s√©par√©es par des zones de faible densit√©. Pour lancer l'algorithme, nous devons sp√©cifier une mesure d'association critique (distance ou dissimilarit√©) *d* ainsi qu'un nombre de point critique *k* dans le voisinage de cette distance.

1. L'algorithme comme √©tiqueter chaque point selon l'une de ces cat√©gories:

    - *Noyau*: le point a au moins *k* points dans son voisinage, c'est-√†-dire √† une distance inf√©rieure ou √©gale √† *d*.
    - *Bordure*: le point a moins de *k* points dans son voisinage, mais l'un de des points voisins est un *noyau*.
    - *Bruit*: le cas √©ch√©ant. Ces points sont consid√©r√©s comme des outliers.

    <img src="images/dbscan_1.svg" width=600>

2. Les noyaux distanc√©s de *d* ou moins sont connect√©s entre eux en englobant les bordures.

    <img src="images/dbscan_2.svg" width=600>

Le nombre de groupes est prescrit par l'algorithme DBSCAN, qui permet du coup de d√©tecter des donn√©es trop bruit√©es pour √™tre class√©es.

[Damiani et al. (2014)](https://doi.org/10.1145/2666310.2666417) a d√©velopp√© une approche utilisant la technique DBSCAN pour partitionner des zones d'escale pour les flux de populations migratoires.

#### Application

-----

Dans scikit-learn, on d√©finit d'abord le mod√®le (par exemple `Kmeans(...)`), puis on l'applique √† nos donn√©es (`fit(...)`), enfin on applique le mod√®le sur des donn√©es (`predict(...)`). Certaines fonctions utilisent toutefois le raccourcis `fit_predict`. Chaque algorithme doit √™tre ajust√© avec les param√®tres qui convient. De nombreux param√®tres par d√©faut sont utilis√©s dans les ex√©cutions ci-dessous. Lors de travaux de recherche, l'utilsation d'un argument ou d'un autre dans une fonction doit √™tre justifi√©: qu'un param√®tre soit utilis√© par d√©faut dans une fonction n'est a priori pas une justification convainquante.
