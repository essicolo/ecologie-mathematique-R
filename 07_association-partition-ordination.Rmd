---
title: "√âcologie num√©rique: association, partitionnement et ordination"
author: "Serge-√âtienne Parent"
date: "`r format(Sys.Date())`"
output: github_document
---

# Association, partitionnement et ordination {#chapitre-ordination}

***
Ô∏è\ **Objectifs sp√©cifiques**:

√Ä la fin de ce chapitre, vous

- serez en mesure d'effectuer des calculs permettant de mesurer des diff√©rence entre des observations, des groupes d'observation ou des variables observ√©es
- serez en mesure d'effection des analyses de partitionnement hi√©rarchiques et non-hi√©rarchiques
- serez en mesure d'effectuer des calculs d'ordination √† l'aide des techniques de r√©duction d'axe communes: analyse en composante principale, analyse discriminante lin√©aire, l'analyse de correspondance, l'analyse factorielle, l'analyse en coordonn√©es principales et l'analyse de redondance.

***

Les donn√©es √©cologiques incluent g√©n√©ralement plusieurs variables qui doivent √™tre analys√©es conjointement. Les techniques pour l'analyse multivari√©e de donn√©es √©cologiques ont grandi en nombre et en complexit√©, laissant √©merger l'√©cologie num√©rique, un nouveau domaine d'√©tude scientifique initi√© par Pierre Legendre et Louis Legendre dont l'ouvrage *Numerical Ecology*, aujourd'hui √† sa troisi√®me √©dition, reste un incontournable pour qui s'int√©resse aux math√©matiques sous-jacentes au domaine. Pour la r√©daction de ces notes, c'est toutefois le livre *Numerical ecology with R*, √©crit par Borcard et al. (2011) pour offrir un guide √† qui voudrait une approche plus appliqu√©e.

L'√©cologie num√©rique sera effleur√©e dans ce chapitre, qui introduit √† trois concepts.

1. Les **associations** permettent de quantifier la ressemblance ou la diff√©rence entre deux observation (√©chantillons) ou variables (descripteurs). Lorsque l'on a plus de deux variables ou plus de deux site, nous obtenons des matrices d'association.
2. Le **partitionnement** permet de regrouper des observations ou des variables selon des m√©triques d'association.
3. L'**ordination** vise par l'interm√©diaire de techniques de r√©duction d'axe √† mettre de l'ordre dans des donn√©es dont le nombre √©lev√© de variables peut amener √† des difficult√©s d'appr√©ciation et d'interpr√©taion. 

```{r}
library("tidyverse")
```


## Espaces d'analyse

### Abondance et occurence

L'abondance est le d√©compte d'esp√®ces observ√©es, tandis que l'occurence est la pr√©sence ou l'absence d'une esp√®ce. Le tableau suivant contient des donn√©es d'abondance.

```{r}
abundance <- tibble('Bruant familier' = c(1, 0, 0, 3),
                    'Citelle √† poitrine rousse' = c(1, 0, 0, 0),
                    'Colibri √† gorge rubis' = c(0, 1, 0, 0),
                    'Geai bleu' = c(3, 2, 0, 0),
                    'Bruant chanteur' = c(1, 0, 5, 2),
                    'Chardonneret' = c(0, 9, 6, 0),
                    'Bruant √† gorge blanche' = c(1, 0, 0, 0),
                    'M√©sange √† t√™te noire' = c(20, 1, 1, 0),
                    'Jaseur bor√©al' = c(66, 0, 0, 0))
```

Ce tableau peut √™tre rapidement transform√© en donn√©es d'occurence, qui ne comprennent que l'information bool√©enne de pr√©sence (not√© 1) et d'absence (not√© 0).

```{r}
occurence <- abundance %>%
  transmute_all(funs(if_else(. > 0, 1, 0)))
```

L'**espace des esp√®ces** (ou des variables ou descripteurs) est celui o√π les esp√®ces forment les axes et o√π les sites sont positionn√©s dans cet espace. Il s'agit d'une perspective en *mode R*, qui permet principalement d'identifier quels esp√®ces se retrouvent plus courrament ensemble.


```{r}
abundance %>% 
  select("Bruant chanteur", "Chardonneret", "M√©sange √† t√™te noire")
```

Dans l'**espace des sites** (ou les √©chantillons ou objets), on transpose la matrice d'abondance. On passe ici en *mode Q*, o√π chaque point est une esp√®ce, et o√π l'on peut observer quels √©chantillons sont similaires.

```{r}
abundance %>% 
  .[c(1, 2, 3), ] %>% 
  t(.)
```

### Environnement

L'**espace de l'environnement** comprend souvent un autre tableau contenant l'information sur l'environnement o√π se trouve les esp√®ces: les coordonn√©es et l'√©l√©vation, la pente, le pH du sol, la pluviom√©trie, etc.

## Analyse d'association

Nous utiliserons le terme *association* come une **mesure pour quantifier la ressemblance ou la diff√©rence entre deux objets (√©chantillons) ou variables (descripteurs)**.

Alors que la corr√©lation et la covariance sont des mesures d'association entre des variables (analyse en *mode R*), la **similarit√©** et la **distance** sont deux types de une mesure d'association entre des objets (analyse en *mode Q*). Une distance de 0 est mesur√©e chez deux objets identiques. La distance augmente au fur et √† mesure que les objets sont dissoci√©s. Une similarit√© ayant une valeur de 0 indique aucune association, tandis qu'une valeur de 1 indique une association parfaite. √Ä l'oppos√©, la dissimilarit√© est √©gale √† 1-similarit√©.

La distance peut √™tre li√©e √† la similarit√© par la relation:

$$distance=\sqrt{1-similarit√©}$$

ou

$$distance=\sqrt{dissimilarit√©}$$

La racine carr√©e permet, pour certains indices de similarit√©, d'obtenir des propri√©t√©s eucl√©diennes. Pour plus de d√©tails, voyez le tableau 7.2 de [Legendre et Legendre (2012)](https://www.elsevier.com/books/numerical-ecology/legendre/978-0-444-53868-0).

Les matrices d'association sont g√©n√©ralement pr√©sent√©es comme des matrices carr√©es, dont les dimensions sont √©gales au nombre d'objets (*mode Q*) ou de vrariables (*mode R*) dans le tableau. Chaque √©l√©ment ("cellule") de la matrice est un indice d'association entre un objet (ou une variable) et un autre. Ainsi, la diagonale de la matrice est un vecteur nul (distance ou dissimilarit√©) ou unitaire (similarit√©), car elle correspond √† l'association entre un objet et lui-m√™me. 

Puisque l'association entre A et B est la m√™me qu'entre B et A, et puisque la diagonale retourne une valeur convenue, il est possible d'exprimer une matrice d'association en mode "compact", sous forme de vecteur. Le vecteur d'association entre des objets A, B et C contiendra toute l'information n√©cessaire en un vecteur de trois chiffres, `[AB, AC, BC]`, plut√¥t qu'une matrice de dimension $3 \times 3$. L'impact sur la m√©moire vive peut √™tre consid√©rable pour les calculs comprenant de nombreuses dimensions.

En R, les calculs de similarit√© et de distances peuvent √™tre effectu√©s avec le module vegan. La fonction `vegdist` permet de calculer les indices d'association en forme carr√©e.

Nous verons plus tard les m√©thodes de mesure de similarit√© et de distance plus loin. Pour l'instant, utilisons la m√©thode de *Jaccard* pour une d√©monstration sur des donn√©es d'occurence.

```{r}
library("vegan")
vegdist(occurence, method = "jaccard",
        diag = TRUE, upper = TRUE)
```

Remarquez que `vegdist` retourne une matrice dont la diagonale est de 0 (on l'affiche en sp√©cifiant `diag = TRUE`). La diagonale est l'association d'un objet avec lui-m√™me. Or la similarit√© d'un objet avec lui-m√™me devrait √™tre de 1! En fait, par convention `vegdist` retourne des dissimilarit√©s, non pas des similarit√©s. La matrice de distance serait donc calcul√©e en extrayant la racine carr√©e des √©l√©ments de la matrice de dissimilarit√©:

```{r}
dissimilarity <- vegdist(occurence, method = "jaccard",
                         diag = TRUE, upper = TRUE)
distance <- sqrt(dissimilarity)
distance
```

Dans le chapitre sur l'analyse compositionnelle, nous avons abord√© les significations diff√©rentes que peuvent prendre le z√©ro. L'information fournie par un z√©ro peut √™tre diff√©rente selon les circonstances. Dans le cas d'une variable continue, un z√©ro signifie g√©n√©ralement une mesure sous le seuil de d√©tection. Deux tissus dont la concentration en cuivre est nulle ont une afinit√© sous la perspective de la concentration en cuivre. Dans le cas de mesures d'abondance (d√©compte) ou d'occurence (pr√©sence-absence), on pourra d√©crire comme similaires deux niches √©cologiques o√π l'on retrouve une esp√®ce en particulier. Mais deux sites o√π l'on de retouve pas d'ours polaires ne correspondent pas n√©cessairement √† des niches similaires! En effet, il peut exister de nombreuses raisons √©cologiques et m√©thodologiques pour lesquelles l'esp√®ces ou les esp√®ces n'ont pas √©t√© observ√©es. C'est le probl√®me des **double-z√©ros** (esp√®ces non observ√©es √† deux sites), probl√®me qui est amplifi√© avec les grilles comprenant des esp√®ces rares.

La ressemblance entre des objets comprenant des donn√©es continues devrait √™tre calcul√©e gr√¢ce √† des indicateurs *sym√©triques*. Inversement, les affinit√©s entre les objets d√©crits par des donn√©es d'abondance ou d'occurence susceptibles de g√©n√©rer des probl√®mes de double-z√©ros devraient √™tre √©valu√©es gr√¢ce √† des indicateurs *asym√©triques*. Un d√©fi suppl√©mentaire arrive lorsque les donn√©es sont de type mixte.

Nous utiliserons la convention de `scipy` et nous calculerons la dissimilarit√©, non pas la similarit√©. Les mesures de dissimilarit√© sont calcul√©es sur des donn√©es d'abondance ou des donn√©es d'occurence. Notons qu'il existe beaucoup de confusion dans la litt√©rature sur la mani√®re de nommer les dissimilarit√©s (ce qui n'est pas le cas des distances, dont les noms sont reconnus). Dans les sections suivantes, nous noterons la dissimilarit√© avec un $d$ minuscule et la distance avec un $D$ majuscule.

### Association entre objets (mode Q)

#### Objets: Abondance

La **dissimilarit√© de Bray-Curtis** est asym√©trique. Elle est aussi appel√©e l'indice de Steinhaus, de Czekanowski ou de S√∏rensen. Il est important de s'assurer de bien s'entendre la m√©thode √† laquelle on fait r√©f√©rence. L'√©quation enl√®ve toute ambiguit√©. La dissimilarit√© de Bray-Curtis entre les points A et B est calcul√©e comme suit.

$$d_{AB} =  \frac {\sum \left| A_{i} - B_{i} \right| }{\sum \left(A_{i}+B_{i}\right)}$$

Utilisons `vegdist` pour g√©n√©rer les matrices d'association. Le format "liste" de R est pratique pour enregistrer la collection d'objets, dont les matrice d'association que nous allons cr√©er dans cette section.

```{r}
associations_abund <- list()
associations_abund[['BrayCurtis']] <- vegdist(abundance, method = "bray")
associations_abund[['BrayCurtis']]
```

La dissimilarit√© de Bray-Curtis est souvent utilis√©e dans la litt√©rature. Toutefois, la version originale de Bray-Curtis n'est pas tout √† fait m√©trique (semim√©trique). Cons√©quemment, la **dissimilarit√© de Ruzicka** (une variante de la dissimilarit√© de Jaccard pour les donn√©es d'abondance) est m√©trique, et devrait probablement √™tre pr√©f√©r√© √† Bary-Curtis ([Oksanen, 2006](http://ocw.um.es/ciencias/geobotanica/otros-recursos-1/documentos/vegantutorial.pdf)).

$$d_{AB, Ruzicka} =  \frac { 2 \times d_{AB, Bray-Curtis} }{1 + d_{AB, Bray-Curtis}}$$

```{r}
associations_abund[['Ruzicka']] <- associations_abund[['BrayCurtis']] * 2 / (1 + associations_abund[['BrayCurtis']])
```

La **dissimilarit√© de Kulczynski** (aussi √©crit Kulsinski) est asym√©trique et semim√©trique, tout comme celle de Bray-Curtis. Elle est calcul√©e comme suit.

$$d_{AB} = 1-\frac{1}{2} \times \left[ \frac{\sum min(A_i, B_i)}{\sum A_i} + \frac{\sum min(A_i, B_i)}{\sum B_i} \right]$$

```{r}
associations_abund[['Kulczynski']] <- vegdist(abundance, method = "kulczynski")
```

Une approche commune pour mesurer l'association entre sites d√©crits par des donn√©es d'abondance est la **distance de Hellinger**. Notez qu'il s'agit ici d'une distance, non pas d'une dissimilarit√©. Pour l'obtenir, on doit d'abord diviser chaque donn√©e d'abondance par l'abondance totale pour chaque site pour obtenir les esp√®ces en tant que proportions, puis on extrait la racine carr√©e de chaque √©l√©ment. Enfin, on calcule la distance euclidienne entre les proportions de chaque site. Pour rappel, une distance euclidienne est la g√©n√©ralisation en plusieurs dimensions du th√©or√®me de Pythagore, $c = \sqrt{a^2 + b^2}$.

$$D_{AB} = \sqrt {\sum \left( \frac{A_i}{\sum A_i} - \frac{B_i}{\sum B_i} \right)^2}$$

------------------ -----------------------------------------------
üò±\ **Attention**   La distance d'Hellinger h√©rite des biais li√©es aux donn√©es compositionnelles. Elle peut √™tre substiti√©e par une matrice de distances d'Aitchison.

------------------------------------------------------------------

```{r}
associations_abund[['Hellinger']] <- dist(decostand(abundance, method="hellinger"))
```

Toute comme la distance d'Hellinger, la **distance de chord** est calcul√©e par une distance euclidienne sur des donn√©es d'abondance transform√©es de sorte que chaque ligne ait une longueur (norme) de 1.

```{r}
associations_abund[['Chord']] <- dist(decostand(abundance, method="normalize"))
```

La **m√©trique du chi-carr√©**, ou $\chi$-carr√©, ou chi-square, donne davantage de poids aux esp√®ces rares qu'aux esp√®ces communes. Son utilisation est recommand√©e lorsque les esp√®ces rares sont de bons indicateurs de conditions √©cologiques particuli√®res ([Legendre et Legendre, 2012](https://www.elsevier.com/books/numerical-ecology/legendre/978-0-444-53868-0), p. 308).

$$  d_{AB} = \sqrt{\sum _j \frac{1}{\sum y_j} \left( \frac{A_j}{\sum A} - \frac{B_j}{\sum B} \right)^2 }  $$

La m√©trique peut √™tre transform√©e en distance en la multipliant par la racine carr√©e de la somme totale des esp√®ces dans la matric d'abondance ($X$).

$$ D_{AB} = \sqrt{\sum X} \times d_{AB} $$


```{r}
associations_abund[['ChiSquare']] <- dist(decostand(abundance, method="chi.square"))
```

Une manni√®re visuellement plus int√©ressante de pr√©senter une matrice d'association est un graphique de type *heatmap*.

```{r}
associations_abund_df <- list()

for (i in 1:length(associations_abund)) {
  associations_abund_df[[i]] <- data.frame(as.matrix(associations_abund[[i]]))
  colnames(associations_abund_df[[i]]) <- rownames(associations_abund_df[[i]])
  associations_abund_df[[i]]$row <- rownames(associations_abund_df[[i]])
  associations_abund_df[[i]] <- associations_abund_df[[i]] %>% gather(key=row)
  associations_abund_df[[i]]$column = rep(1:4, 4)
  associations_abund_df[[i]]$dist <- names(associations_abund)[i]
}
associations_abund_df <- do.call(rbind, associations_abund_df)

ggplot(associations_abund_df, aes(x=row, y=column)) +
  facet_wrap(. ~ dist, nrow = 2) +
  geom_tile(aes(fill = value)) +
  geom_text(aes(label = round(value, 2))) +
  scale_fill_gradient2(low = "#00ccff", mid = "#aad400", high = "#ff0066", midpoint = 2) +
  labs(x="Site", y="Site")
```

Peu importe le type d'association utilis√©e, les *heatmaps* montrent les m√™mes tendances. Les assocaitions de dissimilarit√© (Bray-Curtis, Kulczynski et Ruzicka) s'√©talent de 0 √† 1, tandis que les distances (Chi-Square, Chord et Hellinger) partent de z√©ro, mais n'ont pas de limite sup√©rieure. On note les plus grandes diff√©rences entre les sites 2 et 4, tandis que les sites 2 et 3 sont les plus semblables pour toutes les mesures d'association √† l'exception de la dissimilarit√© de Kulczynski.

#### Objets: Occurence (pr√©sence-absence)

Des indices d'association diff√©rents devraient √™tre utilis√©s lorsque des donn√©es sont compil√©es sous forme bool√©enne. En g√©n√©ral, les tableaux de donn√©es d'occurence seront compil√©s avec des 1 (pr√©sence) et des 0 (absence).

La **similarit√© de Jaccard** entre le site A et le site B est la proportion de double 1 (pr√©sences de 1 dans A et B) parmi les esp√®ces. La dissimilari√© est la proportion compl√©mentaire (comprenant [1, 0], [0, 1] et [0, 0]). La distance de Jaccard est la racine carr√©e de la dissimilarit√©.

```{r}
associations_occ <- list()
associations_occ[['Jaccard']] <- vegdist(occurence, method = "jaccard")
```

Les **distances d'Hellinger, de chord et de chi-carr√©** sont aussi appropri√©es pour les calculs de distances sur des tableaux d'occurence.


```{r}
associations_occ[['Hellinger']] <- dist(decostand(occurence, method="hellinger"))
associations_occ[['Chord']] <- dist(decostand(occurence, method="normalize"))
associations_occ[['ChiSquare']] <- dist(decostand(occurence, method="chi.square"))
```

Graphiquement,

```{r}
associations_occ_df <- list()

for (i in 1:length(associations_occ)) {
  associations_occ_df[[i]] <- data.frame(as.matrix(associations_occ[[i]]))
  colnames(associations_occ_df[[i]]) <- rownames(associations_occ_df[[i]])
  associations_occ_df[[i]]$row <- rownames(associations_occ_df[[i]])
  associations_occ_df[[i]] <- associations_occ_df[[i]] %>% gather(key=row)
  associations_occ_df[[i]]$column = rep(1:4, 4)
  associations_occ_df[[i]]$dist <- names(associations_occ)[i]
}
associations_occ_df <- do.call(rbind, associations_occ_df)

ggplot(associations_occ_df, aes(x=row, y=column)) +
  facet_wrap(. ~ dist) +
  geom_tile(aes(fill = value)) +
  geom_text(aes(label = round(value, 2))) +
  scale_fill_gradient2(low = "#00ccff", mid = "#aad400", high = "#ff0066", midpoint = 1) +
  labs(x="Site", y="Site")

```

Il est attendu que les matrices d'association sur l'occurence sont semblables √† celles sur l'abondance. Dans ce cas-ci, la distance d'Hellinger donne des r√©sultats semblables √† la dissimilarit√© de Jaccard.

#### Objets: Donn√©es quantitatives

Les donn√©es quantitative en √©cologie peuvent d√©crire l'√©tat de l'environnement: le climat, l'hydrologie, l'hydrog√©ochimie, la p√©dologie, etc. En r√®gle g√©n√©rale, les coordonn√©es des sites ne sot pas des variables environnementales, √† que l'on soup√ßonne la coordonn√©e elle-m√™me d'√™tre responsable d'effets sur notre syst√®me: mais il s'agira la plupart du temps d'effets confondants (par exemple, on peut mesurer un effet de lattitude sur le rendement des agrumes, mais il s'agira probablement avant tout d'effets dus aux conditions climatiques, qui elles changent en fonction de la lattitude). D'autre types de donn√©es quantitative pouvant √™tre appr√©hend√©es par des distances sont les traits ph√©nologiques, les ionomes, les g√©nomes, etc.

La **distance euclidienne** est la racine carr√©e de la somme des carr√©s des distances sur tous les axes. Il s'agit d'une application multidimensionnelle du th√©or√®me de Pythagore. La **distance d'Aitchison**, couverte dans le chapitre 6, est une distance euclidienne calcul√©e sur des donn√©es compositionnelles pr√©alablement transform√©es. La distance euclidienne est sensible aux unit√©s utilis√©s: utiliser des milim√®tres plut√¥t que des m√®tres enflera la distance euclidienne. Il est recommand√© de porter une attention particuli√®re aux unit√©s, et de standardiser les donn√©es au besoin (par exemple, en centrant la moyenne √† z√©ro et en fixant l'√©cart-type √† 1).

On pourrait, par exemple, mesurer la distance entre des observations des dimensions de diff√©rentes esp√®ces d'iris. Ce tableau est inclu dans R par d√©faut.

```{r}
data(iris)
iris %>% sample_n(5)
```

Les mesures du tableau sont en centim√®tres. Pour √©viter de donner davantage de poids aux longueur des s√©pales et en m√™me temps de n√©gliger la largeur des p√©tales, nous allons standardiser le tableau.

```{r}
iris_sc <- iris %>%
  select(-Species) %>% 
  scale(.)%>% 
  as_tibble(.) %>% 
  mutate(Species = iris$Species) 
iris_sc
```

Pour les comparaisons des dimensions, prenons la moyenne des dimensions (mises √† l'√©chelle) par esp√®ce.

```{r}
iris_means <- iris_sc %>%
  group_by(Species) %>%
  summarise_all(mean) %>%
  select(-Species)
iris_means
```

Nous pouvons utiliser la distance euclidienne, commune en g√©om√©trie, pour comparer les esp√®ces. La distance euclidienne est calcul√©e comme suit.


$$ \mathcal{E} = \sqrt{\Sigma_i \left( A_i - B_i \right) ^2 } $$

```{r}
associations_cont = list()
associations_cont[['Euclidean']] <- dist(iris_sc %>% select(-Species), method="euclidean")
```

La **distance de Mahalanobis** est semblable √† la distance euclidienne, mais qui tient compte de la covariance de la matrice des objets. Cette covariance peut √™tre utilis√©e pour d√©crire la structure d'un nuage de points. La figure suivante montre deux points verts qui se trouvent aux extr√™mes d'un nuage de point. Ces points ont des distances euclidiennes par rapport au centre diff√©rentes: les lignes d'√©quidistance eucl√©dienne sont trac√©es en rose. Toutefois, les deux points ont un distance de Mahalanobis √©gale √† partir du centre.

<img src="images/07_eucl-maha.png" width=400>
<p style="text-align: center">Source: [Parent et al. (2012)](https://www.intechopen.com/books/soil-fertility/nutrient-balance-as-paradigm-of-plant-and-soil-chemometricsnutrient-balance-as-paradigm-of-soil-and-).</p>

La diastance de Mahalanobis se calcule comme suit.

$$\mathcal{M} = \sqrt{(A - B)^T S^{-1} (A-B)}$$

Notez qu'il s'agit d'une g√©n√©ralisation de la distance euclidienne, qui √©quivaut √† une distance de Mahalanobis dont la matrice de covariance est une matrice identit√©.

La distance de Mahalanobis permet de repr√©senter des distances dans un espace fortement corr√©l√©. Elle est courramment utilis√©e pour d√©tecter les valeurs aberrantes selon des crit√®res de distance √† partir du centre d'un jeu de donn√©es multivari√©es.

```{r}
associations_cont[['Mahalanobis']] <- vegdist(iris_sc %>% select(-Species), 'mahalanobis')
```

La **distance de Manhattan** porte aussi le nom de distance de cityblock ou de taxi. C'est la distance que vous devrez parcourir pour vous rendre du point A au point B √† Manhattan, c'est-√†-dire selon une s√©quence de tron√ßons perpendiculaires.

$$ D_{AB} = \sum _i \left| A_i - B_i \right| $$

La distance de Manhattan est appropri√©e lorsque les gradients (changements d'un √©tat √† l'autre ou d'une r√©gion √† l'autre) ne permettent pas des changements simultan√©s. Mieux vaut standardiser les variables pour √©viter qu'une dimension soit pr√©pond√©rante.

```{r}
associations_cont[['Manhattan']] <- vegdist(iris_sc %>% select(-Species), 'manhattan')
```

Graphiquement

```{r}
associations_cont_df <- list()

for (i in 1:length(associations_cont)) {
  associations_cont_df[[i]] <- data.frame(as.matrix(associations_cont[[i]]))
  colnames(associations_cont_df[[i]]) <- rownames(associations_cont_df[[i]])
  associations_cont_df[[i]]$row <- rownames(associations_cont_df[[i]])
  associations_cont_df[[i]] <- associations_cont_df[[i]] %>% gather(key=row)
  associations_cont_df[[i]]$column = rep(1:nrow(iris), nrow(iris))
  associations_cont_df[[i]]$dist <- names(associations_cont)[i]
}
associations_cont_df <- do.call(rbind, associations_cont_df)

ggplot(associations_cont_df, aes(x=row, y=column)) +
  facet_wrap(. ~ dist) +
  geom_tile(aes(fill = value), colour = NA) +
  #geom_text(aes(label = round(value, 2))) +
  scale_fill_gradient2(low = "#00ccff", mid = "#aad400", high = "#ff0066", midpoint = 5) +
  labs(x="Site", y="Site")
```


Le tableau `iris` est ordonn√© par esp√®ce. Les distances euclidienne et de Manhattan permettent ais√©ment de distinguer les esp√®ces selon les dimensions des p√©tales et des s√©pales. Toutefois, l'utilsation de la covariance avec la distance de Mahalanobis cr√©e des distinction moins tranch√©es.

#### Objets: Donn√©es mixtes

Les donn√©es cat√©gorielles ordinales peuvent √™tre transform√©es en donn√©es continues par gradations lin√©aires ou quadratiques. Les donn√©es cat√©gorielles nominales, quant √† elles, peuvent √™tre *dummyfi√©es* en donn√©es similaires √† des occurences. Attention toutefois: contrairement √† la r√©gression lin√©aire qui demande d'exclure une cat√©gorie, la *dummyfication* doit inclure toutes les cat√©gories. Le comportement par d√©faut de la fonction `pandas.get_dummies` est de garder toutes les cat√©gories. La **similarit√© de Gower** a √©t√© d√©velopp√©e pour mesurer des associations entre des objets dont les donn√©es sont mixtes: bool√©ennes, cat√©gorielles et continues. La similarit√© de Gower est calcul√©e en additionnant les distances calcul√©es par colonne, individuellement. Si la colonne est bool√©enne, on utilise les distances de Jaccard (qui exclue les double-z√©ro) de mani√®re univari√©e: une variable √† la fois. Pour les variables continues, on utilise la distance de Manhattan divis√©e par la plage de valeurs de la variable (pour fin de standardisation). Puisqu'elle h√©rite de la particularit√© de la distance de Manhattan et de la similarit√© de Jaccard univari√©e, la **similarit√© de Gower** reste une combinaison lin√©aire de distances univari√©es.


```{r}
X <- tibble(ID = 1:8,
            age = c(21, 21, 19, 30, 21, 21, 19, 30),
            gender = c('M','M','N','M','F','F','F','F'),
            civil_status = c('MARRIED','SINGLE','SINGLE','SINGLE','MARRIED','SINGLE','WIDOW','DIVORCED'),
            salary = c(3000.0,1200.0 ,32000.0,1800.0 ,2900.0 ,1100.0 ,10000.0,1500.0),
            children = c(TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE),
            available_credit = c(2200,100,22000,1100,2000,100,6000,2200))
X
```

Il faut pr√©alablement *dummifier* les variables cat√©gorielles nominales.

```{r}
X_dum <- model.matrix(~ 0 + ., X[, -1])
X_dum
```

Calculons la dissimilarit√© de Gower (cette fois le graphique est fait avec `pheatmap`).

```{r}
library("pheatmap")
d_gow <- as.matrix(vegdist(X_dum, 'gower'))
colnames(d_gow) <- rownames(d_gow) <- X$ID
pheatmap(d_gow)
```

Les dendrogrammes apparaissants sur les axes du graphique sont issus d'un processus de partitionnement bas√© sur la distance, que nous verrons plus loin dans ce chapiter. Les profils des clients 4 et 7, ainsi que ceux des clients 3 et 7 diff√®rent le plus. Les profils 3 et 4 sont n√©anmoins plut√¥t diff√©rents.

### Associations entre variables (mode R)

Il existe de nombreuses approches pour mesurer les associations entre variables. La plus connue est la corr√©lation. Mais les donn√©es d'abondance et d'occurence demandent des approches diff√©rentes.

#### Variables: Abondance

La distance du chi-carr√© est sugg√©r√©e par [Borcard et al. (2011)](http://www.springer.com/us/book/9781441979759).

```{r}
abundance_r <- t(abundance)
D_chisq_R <- as.matrix(dist(decostand(abundance_r, method="chi.square")))
pheatmap(D_chisq_R, display_numbers = round(D_chisq_R, 2))
```

Des coabondances sont notables pour la m√©sange √† t√™te noire, le jaseur bor√©al, la citelle √† poitrine rousse et le bruant √† gorge blanche (tache bleu au centre).

#### Variables: Occurence

La dissimilarit√© de Jaccard peut √™tre utilis√©e.

```{r}
occurence_r <- t(occurence)
D_jacc_R <- as.matrix(vegdist(occurence_r, method = "jaccard"))
pheatmap(D_jacc_R, display_numbers = round(D_jacc_R, 2))
```

Des cooccurences sont notables pour le jaseur bor√©al, la citelle √† poitrine rousse et le bruant √† gorge blanche (tache bleu au centre).

#### Variables: Quantit√©s

La matrice des corr√©lations de Pearson peut √™tre utilis√©e pour les donn√©es continues. Quant aux variables ordinales, elles devraient id√©alement √™tre li√©es lin√©airement ou quadratiquement. Si ce n'est pas le cas, c'est-√†-dire que les cat√©gories sont ordonn√©es par rang seulement, vous pourrez avoir recours aux coefficients de corr√©lation de Spearman ou de Kendall.

```{r}
iris_cor <- iris %>%
  select(-Species) %>%
  cor(.)
pheatmap(iris_cor, cluster_rows = FALSE, cluster_cols = FALSE,
         display_numbers = round(iris_cor, 2))
```

### Conclusion sur les associations

Il n'existe pas de r√®gle claire pour d√©terminer quelle technique d'association utiliser. Cela d√©pend en premier lieu de vos donn√©es. Vous s√©lectionnerez votre m√©thode d'association selon le type de donn√©es que vous abordez, la question √† laquelle vous d√©sirez r√©pondre ainsi l'exp√©rience dans la litt√©rature comme celle de vos coll√®gues scientifiques. S'il n'existe pas de r√®gle clair, c'est qu'il existe des dizaines de m√©thodes diff√©rentes, et la plupart d'entre elles vous donneront une perspective juste et valide. Il faut n√©anmoins faire attention pour √©viter de s√©lectionner les m√©thodes qui ne sont pas appropri√©es. 

## Partitionnement

Les donn√©es suivantes ont √©t√© g√©n√©r√©es par [Leland McInnes](https://github.com/scikit-learn-contrib/hdbscan/blob/master/notebooks/clusterable_data.npy) (Tutte institute of mathematics, Ottawa). √ätes-vous en mesure d'identifier des groupes? Combien en trouvez-vous?

```{r}
df_mcinnes <- read_csv("data/clusterable_data.csv", col_names = c("x", "y"), skip = 1)
ggplot(df_mcinnes, aes(x=x, y=y)) + geom_point() + coord_fixed()
```

En 2D, l'oeil humain peut facilement d√©tecter les groupes. En 3D, c'est toujours possible, mais au-del√† de 3D, le partitionnement cognitive devient rapidement maladroite. Les algorithmes sont alors d'une aide pr√©cieuse. Mais ils transportent en pratique tout un baggage de limitations. Quel est le crit√®re d'association entre les groupes? Combien de groupe devrions-nous cr√©er? Comment distinguer une donn√©e trop bruit√©e pour √™tre classifi√©e?

Le partitionnement de donn√©es (*clustering* en anglais), et inversement leur regroupement, permet de cr√©er des ensembles selon des crit√®res d'association. On suppose donc que Le partitionnement permet de cr√©er des groupes selon l'information que l'on fait √©merger des donn√©es. Il est cons√©quemment entendu que les donn√©es ne sont pas cat√©goris√©es √† priori: **il ne s'agit pas de pr√©dire la cat√©gorie d'un objet, mais bien de cr√©er des cat√©gories √† partir des objets** par exemple selon leurs dimensions, leurs couleurs, leurs signature chimique, leurs comportements, leurs g√®nes, etc. 

Plusieurs m√©thodes sont aujourd'hui offertes aux analystes pour partitionner leurs donn√©es. Dans le cadre de ce manuel, nous couvrirons ici deux grandes tendances dans les algorithmes.

1. *M√©thodes hi√©rarchique et non hi√©rarchiques*. Dans un partitionnement hi√©rarchique, l'ensemble des objets forme un groupe, comprenant des sous-regroupements, des sous-sous-regroupements, etc., dont les objets forment l'ultime partitionnement. On pourra alors identifier comment se d√©cline un partitionnement. √Ä l'inverse, un partitionnement non-hi√©rarchique des algorhitmes permettent de cr√©er les groupes non hi√©rarchis√©s les plus diff√©rents que possible.

2. *Membership exclusif ou flou*. Certaines techniques attribuent √† chaque une classe unique: l'appartenance sera indiqu√©e par un 1 et la non appartenance par un 0. D'autres techniques vont attribuer un membership flou o√π le degr√© d'appartenance est une variable continue de 0 √† 1. Parmi les m√©thodes floues, on retrouve les m√©thodes probabilistes.

### √âvaluation d'un partitionnement

Le choix d'une technique de partitionnement parmi de nombreuses disponibles, ainsi que le choix des param√®tres gouvernant chacune d'entre elles, est avant tout bas√© sur ce que l'on d√©sire d√©finir comme √©tant un groupe, ainsi que la mani√®re d'interpr√©ter les groupes. En outre, **le nombre de groupe √† d√©partager est *toujours* une d√©cision de l'analyste**. N√©anmoins, on peut se fier [des indicateurs de performance de partitionnement](http://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation). Parmis ceux-ci, retenons le score [silouhette](https://rdrr.io/cran/cluster/man/silhouette.html) ainsi que l'[indice de Calinski-Harabaz](https://www.tandfonline.com/doi/abs/10.1080/03610927408827101).

#### Score silouhette

En anglais, le *h* dans silouhette se trouve apr√®s le *l*: on parle donc de *silhouette coefficient* pour d√©signer le score de chacun des objets dans le partitionnement. Pour chaque objet, on calcule la distance moyenne qui le s√©pare des autres points de son groupe ($a$) ainsi que la distance moyenne  qui le s√©pare des points du groupe le plus rapproch√©.

$$s = \frac{b-a}{max \left(a, b \right)}$$

Un coefficient de -1 indique le pire classement, tandis qu'un coefficient de 1 indique le meilleur classement. La moyenne des coefficients silouhette est le score silouhette.

#### Indice de Calinski-Harabaz

L'indice de Calinski-Harabaz est proportionnel au ratio des dispersions intra-groupe et la moyenne des dispersions inter-groupes. Plus l'indice est √©lev√©, mieux les groupes sont d√©finis. La math√©matique est d√©crite [dans la documentation de scikit-learn](http://scikit-learn.org/stable/modules/clustering.html#calinski-harabaz-index), un module d'analyse et autoapprentissage sur Python.

**Note**. Les coefficients silouhette et l'indice de Calinski-Harabaz sont plus appropri√©s pour les formes de groupes convexes (cercles, sph√®res, hypersph√®res) que pour les formes irr√©guli√®res (notamment celles obtenues par la DBSCAN, discut√©e ci-desssous).

### Partitionnement non hi√©rarchique

Il peut arriver que vous n'ayez pas besoin de comprendre la structure d'agglom√©ration des objets (ou variables).  Plusieurs techniques de partitionnement non hi√©rarchique [sont disponibles dans le module scikit-learn](http://scikit-learn.org/stable/modules/clustering.html). On s'int√©ressera en particulier √† celles-ci.

**Kmeans**. L'objectif des kmeans est de minimiser la distance eucl√©dienne entre un nombre pr√©d√©fini de *k* groupes exclusifs.

1. L'algorhitme commence par placer une nombre *k* de centroides au hasard dans l'espace d'un nombre *p* de variables (vous devez fixer *k*, et *p* est le nombre de colonnes de vos donn√©es).
2. Ensuite, chaque objet est √©tiquett√© comme appartenant au groupe du centroid le plus pr√®s.
3. La position du centroide est d√©plac√©e √† la moyenne de chaque groupe.
4. Recommencer √† partir de l'√©tape 2 jusqu'√† ce que l'assignation des objets aux groupes ne change plus.

![](https://media.giphy.com/media/12vVAGkaqHUqCQ/giphy.gif)
<center>Source: [David Sheehan](https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/)</center>

La technique des kmeans suppose que les groupes ont des distributions multinormales - repr√©sent√©es par des cercles en 2D, des sph√®res en 3D, des hypersph√®res en plus de 3D. Cette limitation est probl√©matique lorsque les groupes se pr√©sentent sous des formes irr√©guli√®res, comme celles du nuage de points de Leland McInnes, pr√©sent√© plus haut. De plus, la technique classique des kmeans est bas√©e sur des distances euclidiennes: l'utilisation des kmeans n'est appropri√©e pour les donn√©es comprenant beaucoup de z√©ros, comme les donn√©es d'abondance, qui devraient pr√©alablement √™tre transform√©es en variables centr√©es et r√©duites ([Legendre et Legendre, 2012](https://www.elsevier.com/books/numerical-ecology/legendre/978-0-444-53868-0)). La technique des **mixtures gaussiennes** ([*gaussian mixtures*](https://www.stat.washington.edu/mclust/)) est une g√©n√©ralisation des kmeans permettant d'int√©grer la covariance des groupes. Les groupes ne sont plus des hyper-sph√®res, mais des hyper-ellipso√Ødes.

**DBSCAN**. La technique DBSCAN (* **D**ensity-**B**ased **S**patial **C**lustering of **A**pplications with **N**oise*) sousentend que les groupes sont compos√©s de zones o√π l'on retrouve plus de points (zones denses) s√©par√©es par des zones de faible densit√©. Pour lancer l'algorithme, nous devons sp√©cifier une mesure d'association critique (distance ou dissimilarit√©) *d* ainsi qu'un nombre de point critique *k* dans le voisinage de cette distance.

1. L'algorithme comme √©tiqueter chaque point selon l'une de ces cat√©gories:

- *Noyau*: le point a au moins *k* points dans son voisinage, c'est-√†-dire √† une distance inf√©rieure ou √©gale √† *d*.
- *Bordure*: le point a moins de *k* points dans son voisinage, mais l'un de des points voisins est un *noyau*.
- *Bruit*: le cas √©ch√©ant. Ces points sont consid√©r√©s comme des outliers.

<img src="images/07_dbscan_1.svg" width=600>

2. Les noyaux distanc√©s de *d* ou moins sont connect√©s entre eux en englobant les bordures.

<img src="images/07_dbscan_2.svg" width=600>

Le nombre de groupes est prescrit par l'algorithme DBSCAN, qui permet du coup de d√©tecter des donn√©es trop bruit√©es pour √™tre class√©es.

[Damiani et al. (2014)](https://doi.org/10.1145/2666310.2666417) a d√©velopp√© une approche utilisant la technique DBSCAN pour partitionner des zones d'escale pour les flux de populations migratoires.

#### Application

Nous pouvons utilis√© la fonction `kmeans` de R. Toutefois, puisque l'on d√©sire ici effectuer des tests de partitionnement pour plusieurs nombres de groupes, nous utiliserons `cascadeKM`, du module vegan. Notez que de nombreux param√®tres par d√©faut sont utilis√©s dans les ex√©cutions ci-dessous. Ces notes de cours ne forment pas un travail de recherche scientifique. Lors de travaux de recherche, l'utilsation d'un argument ou d'un autre dans une fonction doit √™tre justifi√©: qu'un param√®tre soit utilis√© par d√©faut dans une fonction n'est a priori pas une justification convainquante.

Pour les kmeans, on doit fixer le nombre de groupes. Le graphique des donn√©es de Leland McInnes montrent 6 groupes. Toutefois, il est rare que l'on puisse visualiser des d√©marquations aussi tranch√©es que celles de l'exemple, qui plus est dans des cas o√π l'on doit traiter de plus de deux dimensions. Je vais donc lancer le partitionnement en boucle pour plusieurs nombres de groupes, de 3 √† 10 et pour chaque groupe, √©valuer le score silouhette et de Calinski-Habaraz. J'utilise un argument random_state pour m'assurer que les groupes seront les m√™mes √† chaque fois que la cellule sera lanc√©e.

```{r}
library("vegan")
mcinnes_kmeans <- cascadeKM(df_mcinnes, inf.gr = 3, sup.gr = 10, criterion = "calinski")
str(mcinnes_kmeans)
```

L'objet `mcinnes_kmeans`, de type `cascadeKM`, peut √™tre visualis√© directement avec la fonction `plot`.

```{r}
plot(mcinnes_kmeans)
```

On obtient un maximum de Calinski √† 4 groupes, qui correspons √† la deuxi√®me simulation effectu√©e de 3 √† 10.

Examinons les scoers silouhette (module: cluster).

```{r}
library("cluster")
asw <- c()
for (i in 1:ncol(mcinnes_kmeans$partition)) {
  mcinnes_kmeans_silhouette <- silhouette(mcinnes_kmeans$partition[, i], dist = vegdist(df_mcinnes, method = "euclidean"))
  asw[i] <- summary(mcinnes_kmeans_silhouette)$avg.width
}
plot(3:10, asw, type = 'b')
````

Le score silouhette maximum est √† 3 groupes. La forme des groupes n'√©tant pas convexe, il fallait s'attendre √† ce que indicateurs maximaux pour les deux indicateurs soient diff√©rents. C'est d'ailleurs souvent le cas. Cet exemple supporte que le choix du nombre de groupe √† d√©partager repose sur l'analyste, non pas uniquement sur les indicateurs de performance. Choisissons 6 groupes, puisque que c'est visuellement ce que l'on devrait chercher pour ce cas d'√©tude.

```{r}
kmeans_group <- mcinnes_kmeans$partition[, 4]
mcinnes_kmeans$partition %>% head(3)
df_mcinnes %>% 
  mutate(kmeans_group = kmeans_group) %>% # ajouter une colonne de regoupement
  ggplot(aes(x=x, y=y)) +
  geom_point(aes(colour = factor(kmeans_group))) +
  coord_fixed()

```

L'algorithme kmeans est loin d'√™tre statisfaisant. Cela est attendu, puisque les kmeans recherchent des distribution gaussiennes sur des groupes vraisemblablement non-gaussiens.

Nous pouvons cr√©er un graphique silouhette pour nos 6 groupes. Notez qu'√† cause d'un bogue, il n'est pas possible de pr√©senter les donn√©es clairement lorsqu'elles sont nombreuses.

```{r, fig.height=30, fig.width=8}
sil <- silhouette(mcinnes_kmeans$partition[, 6],
                  dist = vegdist(df_mcinnes[, ], method = "euclidean"))
sil <- sortSilhouette(sil)
plot(sil, col = 'black')
```

La technique **DBSCAN** n'est pas bas√©e sur le nombre de groupe, mais sur la densit√© des points. L'argument `x` ne constitue pas les donn√©es, mais une matrice d'association. L'argument minPts sp√©cifie le nombre minimal de points qui l'on doit retrouver √† une distance critique d* pour la formation des *noyaux et la propagation des groupes, sp√©cifi√©e dans l'argument eps. La distance d peut √™tre estim√©e en prenant une fraction de la moyenne, mais on aura volontiers recours √† sont bon jugement.

```{r}
library("dbscan")
mcinnes_dbscan <- dbscan(x = vegdist(df_mcinnes[, ], method = "euclidean"),
                         eps = 0.03, minPts = 10)
dbscan_group <- mcinnes_dbscan$cluster
unique(dbscan_group)
```

Les param√®tres sp√©cifi√©s donnent 5 groupes (`1, 2, ..., 5`) et des points trop bruit√©s pour √™tre classifi√©s (√©tiquet√©s `0`). Voyons comment les groupes ont √©t√© form√©s.

```{r}
df_mcinnes %>% 
  mutate(dbscan_group = dbscan_group) %>% # ajouter une colonne de regoupement
  ggplot(aes(x=x, y=y)) +
  geom_point(aes(colour = factor(dbscan_group))) +
  coord_fixed()
```

Le partitionnement semble plus conforme √† ce que l'on recherche. N√©anmoins, DBSCAN cr√© quelques petits groupes ind√©sirables (groupe 6,  en rose) ainsi qu'un grand groupe (violet) qui auraient lieu d'√™tre partitionn√©. Ces d√©faut pourraient √™tre r√©gl√©s en jouant sur les param√®tres `eps` et `minPts`.

### Partitionnement hi√©rarchique

Les techniques de partitionnement hi√©rarchique sont bas√©es sur les matrices d'association. La technique pour mesurer l'association (entre objets ou variables) d√©terminera en grande partie le paritionnement des donn√©es. Les partitionnements hi√©rarchiques ont l'avantage de pouvoir √™tre repr√©sent√©s sous forme de dendrogramme (ou arbre) de partition. Un tel dendrogramme pr√©sente des sous-groupes qui se joignent en groupes jusqu'√† former un seul ensemble.

Le partitionnement hi√©rarchique est abondamment utilis√© en phylog√©nie, pour √©tudier les relations de parent√© entre organismes vivants, populations d'organismes et esp√®ces. La ph√©n√©tique, branche empirique de la phylog√©n√®se intersp√©cifique, fait usage du partitionnement hi√©rarchique √† partir d'associations g√©n√©tiques entre unit√©s taxonomiques. On retrouve de nombreuses ressources acad√©miques en phylog√©n√©tique ainsi que des outils pour [R](https://www.springer.com/us/book/9781461495413) et [Python](https://academic.oup.com/bioinformatics/article/26/12/1569/287181/DendroPy-a-Python-library-for-phylogenetic). Toutefois, la phylog√©n√©tique en particulier ne fait pas partie de la pr√©sente itt√©ration de ce manuel.

#### Techniques de partitionnement hi√©rarchique

Le partitionnement hi√©rarchique est typiquement effectu√© avec une des quatres m√©thodes suivantes, dont chacune poss√®de ses particularit√©s, mais sont toutes agglom√©ratives: √† chaque √©tape d'agglom√©ration, on fusionne les deux groupes ayant le plus d'affinit√© sur la base des deux sous-groupes les plus rapproch√©s.

**Single link** (`single`). Les groupes sont agglom√©r√©s sur la base des deux points parmi les groupes, qui sont les plus proches.

**Complete link** (`complete`). √Ä la diff√©rence de la m√©thode *single*, on consid√®re comme crit√®re d'agglom√©ration les √©l√©ments les plus √©loign√©s de chaque groupe.

**Agglom√©ration centrale**. Il s'agit d'une fammille de m√©thode bas√©es sur les diff√©rences entre les tendances centrales des objets ou des groupes.

- **Average** (`average`). Appel√©e UPGMA (Unweighted Pair-Group Method unsing Average), les groupes sont agglom√©r√©s selon un centre calcul√©s par la moyenne et le nombre d'objet pond√®re l'agglom√©ration (le poids des groupes est retir√©). Cette technique est historiquement utilis√©e en bioinformatique pour partitionner des groupes phylog√©n√©tiques ([Sneath et Sokal, 1973](https://www.cabdirect.org/cabdirect/abstract/19730310919)).
- **Weighted** (`weighted`). La version de average, mais non pond√©r√©e (WPGMA).
- **Centroid** (`centroid`). Tout comme average, mais le centro√Øde (centre g√©om√©trique) est utilis√© au lieu de la moyenne. Accronyme: UPGMC.
- **Median** (`median`). Appel√©e WPGMC. Devinez! ;)

**Ward** (`ward`). L'optimisation vise √† minimiser les sommes des carr√©s par regroupement.

#### Quel outil de partitionnement hi√©rarchique utiliser?

Alors que le choix de la matrice d'association d√©pend des donn√©es et de leur contexte, la technique de partitionnement hi√©rarchique peut, quant √† elle, √™tre bas√©e sur un crit√®re num√©rique. Il en existe plusieurs, mais le crit√®re recommand√© pour le choix d'une technique de partitionnement hi√©rarchique est la **corr√©lation coph√©n√©tique**. La distance coph√©n√©tique est la distance √† laquelle deux objets ou deux sous-groupes deviennent membres d'un m√™me groupe. La corr√©lation coph√©n√©tique est la corr√©lation de Pearson entre le vecteur d'association des objets et le vecteur de distances coph√©n√©tiques.

#### Application

Les techniques de partitionnement hi√©rarchique pr√©sent√©es ci-dessus sont disponibles dans le module `stats` de R, qui est charg√© automatiquement lors de l'ouversture de R. Nous allons classifier les dimensions des iris gr√¢ce √† la distance de Manhattan.

```{r}
mcinnes_hclust_distmat <- vegdist(df_mcinnes, method = "manhattan")

clustering_methods <- c('single', 'complete', 'average', 'centroid', 'ward')

clust_l <- list()
coph_corr_l <- c()

for (i in seq_along(clustering_methods)) {
  clust_l[[i]] <- hclust(mcinnes_hclust_distmat, method = clustering_methods[i])
  coph_corr_l[i] <- cor(mcinnes_hclust_distmat, cophenetic(clust_l[[i]]))
}

tibble(clustering_methods, coph_corr = coph_corr_l) %>% 
  ggplot(aes(x = clustering_methods, y = coph_corr)) +
  geom_col()

```

La m√©thode `average` retourne la corr√©lation la plus √©lev√©e. Pour plus de flexibilit√©, ench√¢ssons le nom de la m√©thode dans une variable. Ainsi, en chageant le nom de cette variable, le reste du code sera cons√©quent.

```{r}
names(clust_l) <- clustering_methods
best_method <- "average"
```

Le partitionnement hi√©rarchique peut √™tre visualis√© par un dendrogramme.

```{r, fig.width=15, fig.height=5}
plot(clust_l[[best_method]])
```

#### Combien de groupes utiliser?

La longueur des lignes verticales est la distance s√©parant les groupes enfants. Bien que la s√©lection du nombre de groupe soit avant tout bas√©e sur les besoins du probl√®me, nous pouvons nous appuyer sur certains outils. La hauteur totale peut servir de crit√®re pour d√©finir un nombre de groupes ad√©quat. On pourra s√©lectionner le nombre de groupe o√π la hauteur se stabilise en fonction du nombre de groupe. On pourra aussi utiliser le *graphique silhouette*, comprenant une collection de *largeurs de silouhette*, repr√©sentant le degr√© d'appartenance √† son groupe. La fonction `sklearn.metrics.silhouette_score`, du module scikit-learn, s'en occupe.



```{r}
asw <- c()
num_groups <- 3:10
for(i in seq_along(num_groups)) {
  sil <- silhouette(cutree(clust_l[[best_method]], k = num_groups[i]), mcinnes_hclust_distmat)
  asw[i] <- summary(sil)$avg.width
}

plot(num_groups, asw, type = "b")

```

Le nombre optimal de groupes serait de 5. Coupons le dendrorgamme √† la hauteur correspondant √† 5 groupes avec la fonction `cutree`.

```{r, fig.width=15, fig.height=5}
k_opt <- num_groups[which.max(asw)]
hclust_group <- cutree(clust_l[[best_method]], k = k_opt)
plot(clust_l[[best_method]])
rect.hclust(clust_l[[best_method]], k = k_opt)
```

La classification hi√©rarchique, uniquement bas√©e sur la distance, peut √™tre inappropri√©e pour d√©finir des formes complexes.

```{r}
df_mcinnes %>% 
  mutate(hclust_group = hclust_group) %>% # ajouter une colonne de regoupement
  ggplot(aes(x=x, y=y)) +
  geom_point(aes(colour = factor(hclust_group))) +
  coord_fixed()
```

### Partitionnement hi√©rarchique bas√©e sur la densit√© des points

La tecchinque HDBSCAN, dont l'algorithme est relativement r√©cent ([Campello et al., 2013](https://link.springer.com/chapter/10.1007%2F978-3-642-37456-2_14)), permet une partitionnement hi√©rarchique sur le m√™me principe des zones de densit√© de la technique DBSCAN. Le HDBSCAN a √©t√© utilis√©e pour partitionner les lieux d'escale d'oiseaux migrateurs en Chine ([Xu et al., 2013](https://www.jstage.jst.go.jp/article/dsj/12/0/12_WDS-027/_article)).

Avec DBSCAN, un rayon est fix√© dans une m√©trique appropri√©e. Pour chaque point, on compte le nombre de point voisins, c'est √† dire le nombre de point se situant √† une distance (ou une dissimilarit√©) √©gale ou inf√©rieure au rayon fix√©. Avec HDBSCAN, on sp√©cifie le nombre de points devant √™tre recouverts et on calcule le rayon n√©cessaire pour les recouvrir. Ainsi, chaque point est associ√© √† un rayon critique que l'on nommera $d_{noyau}$. La m√©trique initiale est ensuite alt√©r√©e: on remplace les associations entre deux objets A et B par la valeur maximale entre cette association, le rayon critique de A et le rayon critique de B. Cette nouvelle distance est appel√©e la *distance d'atteinte mutuelle*: elle accentue les distances pour les points se trouvant dans des zones peu denses. On applique par la suite un algorithme semblable √† la partition hi√©rarchique *single link*: En s'√©largissant, les rayons se superposent, chaque superposition de rayon forment graduellement des groupes qui s'agglom√®rent ainsi de mani√®re hi√©rarchique. Au lieu d'effectuer une tranche √† une hauteur donn√©e dans un dendrogramme de partitionnement, la technique HDBSCAN se base sur un dendrogramme condens√© qui discarte les sous-groupes comprenant moins de *n* objets ($n_{gr min}$). Dans nouveau dendrogramme, on recherche des groupes qui occupent bien l'espace d'analyse. Pour ce faitre, on utilise l'inverse de la distance pour cr√©er un indicateur de *persistance* (semblable √† la similarit√©), $\lambda$. Pour chaque groupe hi√©rarchique dans le dendrogramme condens√©, on peut calculer la persistance o√π le groupe prend naissance. De plus, pour chaque objet d'un groupe, on peut aussi calculer une distance √† laquelle il quitte le groupe. La *stabilit√©* d'un groupe est la domme des diff√©rences de persistance entre la persistance √† la naissance et les persistances des objets. On descend dans le dendrogramme. Si la somme des stabilit√© des groupes enfants est plus grande que la stabilit√© du groupe parent, on accepte la division. Sinon, le parent forme le groupe. La [documentation du module `hdbscan`](http://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html) pour Python offre une description intuitive et plus exhaustive des principes et algorithme de HDBSCAN.

#### Param√®tres

Outre la m√©trique d'association dont nous avons discut√©, HDBSCAN demande d'√™tre nourri avec [quelques param√®tres importants](https://www.rdocumentation.org/packages/dbscan/versions/1.1-3/topics/hdbscan). En particulier, le **nombre minimum d'objets par groupe**, $n_{gr min}$ d√©pend de la quantit√© de donn√©es que vous avez √† votre disposition, ainsi que de la quantit√© d'objets que vous jugez suffisante pour cr√©er des groupes. Nous utiliserons l'impl√©mentation de HDBSCAN du module dbscan. Si vous d√©sirez davantage d'options, vous pr√©f√©rerez probablement l'[impl√©mentation du module largeVis](https://www.rdocumentation.org/packages/largeVis/versions/0.2.1.1/topics/hdbscan).

```{r, fig.width=15, fig.height=7}
mcinnes_hdbscan <- hdbscan(x = vegdist(df_mcinnes, method = "euclidean"),
                           minPts = 20,
                           gen_hdbscan_tree = TRUE,
                           gen_simplified_tree = FALSE)
hdbscan_group <- mcinnes_hdbscan$cluster
unique(hdbscan_group)
```

Nous avons 6 groupes, num√©rot√©s de 1 √† 6, ainsi que des √©tiquettes identifiant des objets d√©sign√©s comme √©tant du bruit de fond, num√©rot√© 0. Le dendrogramme non condens√© peu √™tre produit.

```{r}
plot(mcinnes_hdbscan$hdbscan_tree)
```

Difficile d'y voir clair avec autant d'objets. L'objet `mcinnes_hdbscan` a un nombre minimum d'objets par groupe de 20. Ce qui permet de pr√©senter le dendrogramme de mani√®re condens√©e.

```{r, fig.width=15, fig.height=7}
plot(mcinnes_hdbscan)
```

Enfin, un aper√ßu des strat√©gies de partitionnement utilis√©s jusqu'ici.

```{r, fig.width=8, fig.height=8}
clustering_group <- df_mcinnes %>% 
  mutate(kmeans_group,
         hclust_group,
         dbscan_group,
         hdbscan_group) %>% 
  gather(-x, -y, key = "method", value = "cluster")
clustering_group$cluster <- factor(clustering_group$cluster)
clustering_group %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point(aes(colour = cluster)) +
  facet_wrap(~method, ncol = 2) +
  coord_equal() +
  theme_bw()
```

Clairement, le partitionnement avec HDBSCAN donne les meilleurs r√©sultats.

### Conclusion sur le partitionnement

Au chapitre 4, nous avons vu avec le jeu de donn√©es "datasaurus" que la visualisation peut permettre de d√©tecter des structures en segmentant les donn√©es selon des groupes.

<img src="images/07_datasaurus_mix.png" width=400>

<img src="images/07_datasaurus_facet.png">

Or, si les donn√©es n'√©taient pas √©tiquet√©es, leur structure serait ind√©tectable avec les algorithmes disponibles actuellement. Le partitionnement permet d'explorer des donn√©es, de d√©tecter des tendances et de d√©gager des groupes permettant la prise de d√©cision.

Plusieurs techniques de partitionnement ont √©t√© pr√©sent√©es. Le choix de la technique sera d√©terminante sur la mani√®re dont les groupes seront partitionn√©s. La d√©finition d'un groupe variant d'un cas √† l'autre, il n'existe pas de r√®gle pour prescrire une m√©thode ou une autre. La partitionnement hi√©rarchique a l'avantage de permetre de visualiser comment les groupes s'agglom√®rent. Parmi les m√©thodes de partitionnement hi√©rarchique disponibles, les m√©thodes bas√©es sur la densit√© permettent une grande flexibilit√©, ainsi qu'une d√©tection d'observations ne faisant partie d'aucun goupe.
