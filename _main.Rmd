--- 
title: "Analyse et mod√©lisation d'agro√©cosyst√®mes"
author: "Serge-√âtienne Parent"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook 
description: "Ce cours a pour objectif de former les √©tudiants gradu√©s en g√©nie agroenvironnemental, g√©nie civil, g√©nie √©cologique, agronomie, biologie, foresterie et √©cologie en analyse et mod√©lisation de syst√®mes vivants. Les sujets trait√©s sont l‚Äôintroduction au langage de programmation R, l‚Äôanalyse statistique descriptive, la visualisation, la mod√©lisation inf√©rentielle, pr√©dictive et d√©terministe."
#bibliography: [book.bib, packages.bib]
#biblio-style: apalike
#link-citations: yes # bookdown::render_book("index.Rmd")
---

# Introduction {#chapitre-intro-cours}

En d√©veloppant son jeu de la vie (*game of life*) en 1970, John Horton Connway a pr√©sent√© un exemple percutant que des r√®gles simples peuvent mener √† des r√©sultats inattendus. Le jeu consiste √† placer des jetons sur les cases d'un plateau de jeu consistant en une simple grille orthogonale. Le jeu √©volue en fonction du nombre de jetons pr√©sents parmi les huit cases du voisinage des jetons ou des cases vides.

1. Les jetons ayant 0 ou 1 voisin sont retir√©s.
2. Les jetons ayant 2 ou 3 voisins restent intacts
3. Les jetons ayant plus de 3 voisins sont retir√©s
4. Un jeton est pos√© sur les cases ayant exactement 3 voisins 

C'est tout. Selon la mani√®re dont les jetons sont plac√©s au d√©part, il se peut que la grille se vide de ses jetons, ou que les jetons y prennent beaucoup de place. Il arrive aussi que des cycles r√©guliers se d√©gagent ou que l'on se retrouve avec des formes r√©guli√®res. Vous aurez peut-√™tre compris √† ce stade pourquoi le jeu est appel√© "jeu de la vie". La premi√®re r√®gle est une situation localis√©e de sous-population, condition dans laquelle la reproduction est difficile. La deuxi√®me r√®gle est une situation localis√©e stable. La troisi√®me est une situation de surpopulation, o√π des individus meurent dans un environnement rendu inad√©quat par une insuffisance de ressource ou une toxicit√© excessive. Enfin, la quatri√®me r√®gle indique une situation favorable √† la reproduction.

Une grille vid√©e correspond √† une extinction et une grille remplie correspond √† une explosion de population. Une oscillation est un "climax", un √©tat stable en √©cologie. Un l√©ger changement dans la disposition initiale des jetons peut mener √† des solutions diff√©rentes.

Le jeu est une application de la technique des *automates cellulaires*. Il se complexifie √† mesure que le nombre de jetons grandit. Un humain passera des heures √† calculer une seule ronde √† 50 jetons, commettra probablement quelques erreurs et prendra quelques caf√©s. Un processeur pourra g√©rer des centaines de rondes sur des grilles de centaines de jetons en quelques secondes.

En √©tablissant des r√®gles correspondant aux m√©canismes de l'objet √©tudi√©, il devient possible de mod√©liser l'√©volution des syst√®mes vivants, comme l'√©mergence ou le d√©clin d'esp√®ces. La figure \@ref(fig:index-cellular-automata) pr√©sente un cas simple d'automates cellularies g√©n√©r√© dans le langage de programmation R.

```{r index-cellular-automata, out.width="50%", fig.align="center", fig.cap="Simulation avec automates cellulaires [g√©n√©r√©s en R](https://www.r-bloggers.com/fast-conways-game-of-life-in-r/).", echo = FALSE}
knitr::include_graphics("images/01_conway.gif")
```

## D√©finitions

Les math√©matiques conf√®rent aux humains une capacit√© d'abstraction suffisamment complexe pour leur permettre de toucher les √©toiles et les atomes, de comprendre le pass√© et de pr√©dire le futur, de toucher l'infini et de go√ªter √† l'√©ternit√©. √Ä partir des maths, on a pu cr√©er des outils de calcul qui permettent de projeter des images de l'univers, bien au-del√† de la Voie lact√©e. Mais appr√©hender le vivant, tout pr√®s de nous, demeure une t√¢che complexe.

```{r index-ecologie-mathematique, out.width="40%", fig.align="center", fig.cap="Domaines scientifiques de l'√©cologie math√©matique.", echo = FALSE}
knitr::include_graphics("images/01_disciplines.png")
```

L'√©cologie math√©matique couvre un large spectre de domaines (figure \@ref(fig:index-ecologie-mathematique)), mais peut √™tre divis√©e en deux branches: l'**√©cologie th√©orique** et l'**√©cologie quantitative** ([Legendre et Legendre, 2012](https://www.elsevier.com/books/numerical-ecology/legendre/978-0-444-53868-0)). Alors que l'√©cologie th√©orique s'int√©resse √† l'expression math√©matique des m√©canismes √©cologiques, l'√©cologie quantitative, plus empirique, en √©tudie principalement les ph√©nom√®nes. La **mod√©lisation √©cologique** vise √† pr√©voir une situation selon des conditions donn√©es. Faisant partie √† la fois de l'√©cologie th√©orique et de l'√©cologie quantitative, elle superpose souvent des m√©canismes de l'√©cologie th√©orique et des ph√©nom√®nes empiriques de l'√©cologie quantitative. L'**√©cologie num√©rique** comprend la branche descriptive de l'√©cologie quantitative, c'est-√†-dire qu'elle s'int√©resse √† √©valuer des effets √† partir de donn√©es empiriques. L'exploration des donn√©es dans le but d'y d√©couvrir des structures passe souvent par des techniques multivari√©es comme la classification hi√©rarchique ou la r√©duction d'axe (par exemple, l'analyse en composantes principales), qui sont davantage heuristiques (dans notre cas, **bioheuristique**) que statistiques. Les tests d'hypoth√®ses et l'analyse des probabilit√©s, quant √† eux, rel√®vent de la **biostatistique**.

Le **g√©nie √©cologique**, une discipline intimement li√©e √† l'√©cologie math√©matique, est vou√© √† l'analyse, la mod√©lisation, la conception et la construction de syst√®mes vivants dans le but de r√©soudre de mani√®re efficace des probl√®mes li√©s √† l'√©cologie et √† une panoplie de domaines qui lui sont raccord√©s. L'agriculture est l'un de ces domaines. C'est d'embl√©e la discipline qui sera pris√©e dans ce manuel. N√©anmoins, les principes qui seront discut√©s sont transf√©rables √† l'√©cologie g√©n√©rale.

## √Ä qui s'adresse ce manuel?

Le cours vise √† introduire des √©tudiant.e.s gradu√©.e.s en agronomie, biologie, √©cologie, sols, g√©nie agroenvironnemental, g√©nie civil et g√©nie √©cologique √† l'analyse et la mod√©lisation dans leur domaine, tant pour les appuyer pour leurs travaux de recherche que pour leur fournir une trousse d'outil √©mancipatrice pour leur cheminement professionnel. Plus sp√©cifiquement, vous serez accompagn√© √† d√©couvrir diff√©rents outils num√©riques qui vous permettront d'appr√©hender vos donn√©es, d'en faire √©merger l'information et de construire des mod√®les. L'objectif de ce cours n'est pas de vous former en math√©matique, mais de vous aider √† les utiliser. En ce sens, **c'est un cours de pilotage, pas un cours de m√©canique**. Vous ferez tout de m√™me un peu de m√©canique pour mieux comprendre les r√©actions de notre machine.

Bien que des connaissances en programmation et en statistiques aideront grandement les √©tudiant.e.s √† appr√©hender ce document, une litt√©ratie informatique n'est pas requise. Dans tous les cas, quiconque voudra tirer profit de ce manuel devra faire preuve d'autonomie. Vous serez guid√©s vers des ressources et des r√©f√©rences, mais je vous sugg√®re vivement de d√©velopper votre propre biblioth√®que adapt√©e √† vos besoins et √† votre mani√®re de comprendre.

## Les logiciels libres

Tous les outils num√©riques qui sont propos√©s dans ce cours sont des logiciels libres:

> ¬´ Logiciel libre ¬ª [free software] d√©signe des logiciels qui respectent la libert√© des utilisateurs. En gros, cela veut dire que les utilisateurs ont la libert√© d'ex√©cuter, copier, distribuer, √©tudier, modifier et am√©liorer ces logiciels. Ainsi, ¬´ logiciel libre ¬ª fait r√©f√©rence √† la libert√©, pas au prix1 (pour comprendre ce concept, vous devez penser √† ¬´ libert√© d'expression ¬ª, pas √† ¬´ entr√©e libre ¬ª). - [Projet GNU](https://www.gnu.org/philosophy/free-sw.fr.html)

Donc: codes sources ouverts, d√©veloppement souvent communautaire, gratuit√©. Plusieurs [raisons √©thiques](https://www.youtube.com/watch?v=Ag1AKIl_2GM), principalement li√©es au contr√¥le de l'environnement virtuel par les utilisateurs et les communaut√©s, peuvent justifier l'utilisation de logiciels libres. Plusieurs raisons pratiques justifient aussi cette orientation. Les logiciels libres vous permettent de transporter vos outils avec vous, d'une entreprise √† l'autre, au bureau, ou √† la maison, et ce, sans vous soucier d'acheter de co√ªteuses licences.

Il existe tout de m√™me des risques li√©s aux possibles erreurs dans les codes des logiciels communautaires. Ces risques sont d'ailleurs les m√™mes que ceux li√©s aux logiciels propri√©taires. Pour les scientifiques, une erreur peut mener √† une √©tude retir√©e de la litt√©rature et m√™me, potentiellement, des politiques publiques mal avis√©es. Pour les ing√©nieurs, les cons√©quences pourraient √™tre dramatiques. Mais retenez qu'en toute circonstance, **comme professionnel.le, vous √™tes responsable des outils que vous utilisez: vous devez vous assurer de la bonne qualit√© d'un logiciel, qu'il soit propri√©taire ou communautaire**.

Alors que la qualit√© des logiciels propri√©taires est g√©n√©ralement suivie par audits, celle des logiciels libres est plut√¥t soumise √† la vigilance communautaire. Chaque approche a ses avantages et inconv√©nients, mais elles ne sont pas exclusives. Ainsi les logiciels libres peuvent √™tre audit√©s √† l'externe par quiconque d√©cide de le faire. Diff√©rentes entreprises, souvent concurrentes, participent tant √† cette vigilance qu'au d√©veloppement des logiciels libres: elles en sont m√™me souvent les instigatrices (comme [RStudio](https://www.rstudio.com/), [Anaconda](https://www.anaconda.com/) et [Enthought](https://www.enthought.com/)).

Par ailleurs, ce manuel est distribu√© librement sous licence Creative commons, selon les termes suivants.

<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Licence Creative Commons" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br /><span xmlns:dct="http://purl.org/dc/terms/" href="http://purl.org/dc/dcmitype/Text" property="dct:title" rel="dct:type">Analyse et mod√©lisation d‚Äôagro√©cosyst√®mes</span> de <a xmlns:cc="http://creativecommons.org/ns#" href="https://essicolo.github.io/ecologie-mathematique-R/" property="cc:attributionName" rel="cc:attributionURL">Serge-√âtienne Parent</a> est mis √† disposition selon les termes de la <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">licence Creative Commons Attribution - Pas d‚ÄôUtilisation Commerciale - Partage dans les M√™mes Conditions 4.0 International</a>.<br />Fond√©(e) sur une ≈ìuvre √† <a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/essicolo/ecologie-mathematique-R" rel="dct:source">https://github.com/essicolo/ecologie-mathematique-R</a>.

## Langage de programmation

### R

Ce cours est bas√© sur le langage [R](https://www.r-project.org/). En plus d'√™tre libre, R est un langage de programmation dynamique largement utilis√© dans le monde universitaire, et dont l'utilisation s'√©tend de mani√®re soutenue hors des tours d'ivoire.

> R is also the name of a popular programming language used by a growing number of data analysts inside corporations and academia.  It is becoming their lingua franca partly because data mining has entered a golden age, whether being used to set ad prices, find new drugs more quickly or fine-tune financial models. [New York Times, janvier 2019](https://www.nytimes.com/2009/01/07/technology/business-computing/07program.html)

Son d√©veloppement est support√© par la [R Foundation for Statistical Computing](https://www.r-project.org/foundation/), bas√©e √† l'Universit√© de Vienne. √âgalement, l'√©quipe de [RStudio](https://www.rstudio.com/) contribue largement au [d√©veloppement de modules g√©n√©riques](https://www.rstudio.com/products/rpackages/). R est principalement utilis√© pour le calcul statistique, mais les r√©cents d√©veloppements le rendent un outil de choix pour tout ce qui entoure la science des donn√©es, de l'interaction avec les bases de donn√©es au d√©ploiement d'outils d'intelligence artificielle en passant par la visualisation. Une fois impl√©ment√© avec des modules de calcul scientifique sp√©cialis√©s en biologie, en √©cologie et en agronomie (que nous couvrirons au long du cours), R devient un outil de calcul convivial, rapide et fiable.

### Pourquoi pas Python?

La [premi√®re mouture de ce cours](https://github.com/essicolo/ecologie-mathematique-Py) se fondait sur le langage Python. Tout comme R, Python est un langage de programmation dynamique pris√© pour le calcul scientifique. Python est un langage g√©n√©rique appr√©ci√© pour sa polyvalence et sa simplicit√©. Python est utilis√© autant pour cr√©er des logiciels ou des sites web que pour le calcul scientifique. Ainsi, Python peut √™tre utilis√© en interop√©rabilit√© avec une panoplie de logiciels libres, comme [QGIS](http://www.qgis.org) pour la cartographie et [FreeCAD](https://github.com/FreeCAD/FreeCAD) pour le dessin technique. Il est particuli√®rement appr√©ci√© en ing√©nierie pour ses modules de calcul par √©l√©ments finis (e.g. [FeNICS](https://fenicsproject.org/)) et en bioinformatique pour ses outils li√©s au s√©quen√ßage ([scikit-bio](http://scikit-bio.org/)), mais ses lacunes en analyse statistique, en particulier en statistiques multivari√©es m'ont amen√© √† favoriser R.

Bien que leurs possibilit√©s se superposent largement, ce serait une erreur d'aborder R et Python comme des langages rivaux. Les deux langages s'expriment de mani√®re similaire et s'inspirent mutuellement: apprendre √† travailler avec l'un revient √† apprendre l'autre. Les sp√©cialistes en calcul scientifique tendent √† apprendre √† travailler avec plus d'un langage de programmation. Par ailleurs, il existe de plus en plus des moyens de travailler en R et en Python dans un m√™me flux de travail. L'interface de calcul RStudio, que nous utiliserons pendant le cours, permet d'inclure des blocs de code en Python.

### Pourquoi pas Matlab?

Parce qu'on est en `r format(Sys.Date(), "%Y")`.

### Et... SAS?

Parce qu'on est √† l'universit√©.

### Mais pourquoi pas ______ ?

D'autres langages, comme [Julia](http://julialang.org), [Scala](http://www.scala-lang.org), [Javascript](https://dtabio.gitbooks.io/data-science-with-javascript/content/) et m√™me [Ruby](http://sciruby.com) sont utilis√©s en calcul scientifique. Ils sont n√©anmoins moins garnis et moins document√©s que R. Des langages de plus bas niveau, comme Fortran et C++, viennent souvent appuyer les fonctions des autres langages: ces langages sont plus ardus √† utiliser au jour le jour, mais leur rapidit√© de calcul est imbattable.

## Contenu du manuel

Le pire angle avec lequel je pourrais aborder le sujet, c'est avec du code et des formules math√©matiques. √Ä travers chacun des chapitres, je tenterai de vous amener √† r√©soudre des probl√®mes de la mani√®re la plus intuitive possible. Nous aborderons l‚Äôanalyse et la mod√©lisation inf√©rentielle, pr√©dictive et d√©terministe appliqu√©e aux agro√©cosyst√®mes.

**Chapitre \@ref(chapitre-intro-a-R) - Introduction au langage de programmation R**. Qu'est-ce que R? Comment l'aborder? Quelles sont les fonctionnalit√©s de base et comment tirer profit de tout l'√©cosyst√®me de programmation?

**Chapitre \@ref(chapitre-tableaux) - Organisation des donn√©es et op√©rations sur des tableaux**. Les tableaux permettent d'ench√¢sser l'information dans un format pr√™t-√†-porter pour R. Comment les importer, les exporter, les filtrer, et en faire des sommaires?

**Chapitre \@ref(chapitre-visualisation) - Visualisation**. Comment pr√©senter l'information contenue dans un long tableau en un seul coup d'oeil?

**Chapitre \@ref(chapitre-git) - Le travail collaboratif, le suivi de version et la science ouverte**. Ce chapitre offre une introduction √† l'utilisation des outils de calcul collaboratif, ainsi qu'un aper√ßu du syst√®me de suivi de version *git* et de son utilisation sur [GitHub](https://github.com/).

**Chapitre \@ref(chapitre-biostats) - Biostatistiques**. Il est audacieux de ne consacrer qu'un seul chapitre sur ce vaste sujet. Nous irons √† l'essentiel... pour vous donner les outils qui permettront d'approfondir le sujet.

**Chapitre \@ref(chapitre-biostats-bayes) - Biostatistiques bay√©siennes**. Une tr√®s br√®ve introduction pour qui est int√©ress√© √† l'analyse bay√©sienne.

**Chapitre \@ref(chapitre-explorer) - Explorer R**. La science des donn√©es √©volue rapidement. Vous gagnerez √† vous tenir au courrant de son √©volution, et immanquablement vous vous buterez sur des op√©rations qui vous sembleront insolubles. Ce chapitre vous accompagnera √† rester √† jour sur le d√©veloppement de R, √† poser de bonnes questions et proposera des modules int√©ressants en √©cologie math√©matique.

**Chapitre \@ref(chapitre-ordination) - Association, partitionnement et ordination**. Les √©cosyst√®mes diff√®rent, mais en quoi sont-ils semblables, et en quoi dff√®rent-ils? Ces questions importantes peuvent √™tre abord√©s par l'√©cologie num√©rique, domaine d'√©tude au sein duquel l'association, le partitionnement et l'ordination sont des outils pr√©dominants.

**Chapitre \@ref(chapitre-outliers) - D√©tection de valeurs aberrantes et imputation**. Une donn√©e aberrante sortira du lot, pour une raison ou pour une autre. Comment les d√©tecter de mani√®re syst√©matique? D'autre part, que faire lorsqu'une donn√©e est manquante? Peut-on l'imputer? Comment?

**Chapitre \@ref(chapitre-temps) - Les s√©ries temporelles**. Les capteurs modernes permettent de g√©n√©rer des donn√©es en fonction du temps. Que ce soit des donn√©es m√©t√©orologiques enregistr√©es quotidiennement ou des donn√©es de teneur en eau enregistr√©es au 5 secondes, les donn√©es en fonction du temps forment un signal. Comment analyser ces signaux?

**Chapitre \@ref(chapitre-ml) - L'autoapprentissage**. Les applications de l'intelligence artificielle ne sont limit√©es que par votre imagination. Encore faut-il l'utiliser intelligemment.

**Chapitre \@ref(chapitre-geo) - Les donn√©es spatiales**. Non, nous n'aborderons pas les g√©ostatistiques. Ce chapitre porte plut√¥t sur l'utilisation de R comme syst√®me d'information g√©ographique de base. Nous utiliserons aussi l'autoapprentissage comme outil d'interpolation spatial.

**Chapitre \@ref(chapitre-ode) - La mod√©lisation d√©terministe**. Les mod√®les sont des maquettes simplifi√©es. Comment utiliser les √©quations diff√©rentielles ordinaires pour cr√©er ces maquettes?

Si les chapitres 3 √† 5 peuvent √™tre consid√©r√©s comme fondamentaux pour bien ma√Ætriser R, les autres peuvent √™tre feuillet√©s √† la pi√®ce, bien qu'ils forment une suite logique.

Chaque chapitre de ce manuel est r√©dig√© en format  *R notebook*, dans un environnement RStudio. Pour ex√©cuter les commandes, les vous pourrez soit copier-coller les commandes dans R (ou RStudio), soit [t√©l√©charger les fichiers-sources](https://github.com/essicolo/ecologie-mathematique-R) et ex√©cuter les blocs de code.

## Objectifs g√©n√©raux

√Ä la fin du cours, l'√©tudiant.e sera en mesure:

- de programmer en langage R
- d'importer, de manipuler (s√©lection des colonnes, filtres, sommaires statistiques) et d'exporter des tableaux
- de g√©n√©rer des graphiques d'utilisation commune
- d'appr√©hender des donn√©es √©cologiques et agronomiques √† l'aide de tests statistiques fr√©quentiels
- d'explorer par lui.elle-m√™me les possibilit√©s offertes par la communaut√© de d√©veloppement de modules R
- d'explorer les donn√©es √† l'aide des outils de l'√©cologie num√©rique (association, partitionnement et ordination)
- d'imputer des donn√©es manquantes dans un tableau et de d√©tecter des valeurs aberrantes
- d'effectuer une analyse de s√©rie temporelle
- de s'assurer que ses calculs soit auditables et reproductibles dans une perspective de science ouverte
- de cr√©er un mod√®le d'autoapprentissage
- d'intrapoler des donn√©es spatiales
- de mod√©liser des √©quations diff√©rentielles ordinaires

## Lectures compl√©mentaires

### √âcologie math√©matique

- [How to be a quantitative ecologist](). Jason Mathipoulos vous prend par la main pour d√©couvrir les notions de math√©matiques fondamentales en √©cologie, appliqu√©es avec le langage R.  
- [Numerical ecology](https://www.elsevier.com/books/numerical-ecology/legendre/978-0-444-53868-0). L'ouvrage hautement d√©taill√© des fr√®res Legendre est non seulement fondamental, mais aussi fondateur d'une science qui √©volue encore aujourd'hui: l'analyse des donn√©es √©cologiques.
- [A practical guide to ecological modelling](http://www.springer.com/us/book/9781402086236). Soetaert et Herman portent une attention particuli√®re √† la pr√©sentation des principes de mod√©lisation dans un langage accessible - ce qui est rarement le cas dans le domaine de la mod√©lisation. Les mod√®les pr√©sent√©s concernent principalement les bilans de masse, en termes de syst√®mes de r√©actions chimiques et de relations biologiques.
- [Mod√©lisation math√©matique en √©cologie](http://www.documentation.ird.fr/hor/fdi:010050350). Rare livre en mod√©lisation √©cologique publi√© en fran√ßais, la premi√®re partie s'attarde aux concepts math√©matiques, alors que la deuxi√®me planche √† les appliquer. Si le haut niveau d'abstraction de la premi√®re partie vous rebute, n'h√©sitez pas d√©buter par la seconde partie et de vous r√©f√©rer √† la premi√®re au besoin.
- [A new ecology: systems perspective](https://www.elsevier.com/books/a-new-ecology/jorgensen/978-0-444-53160-5). Principalement gr√¢ce au soleil, la Terre forme un ensemble de gradients d'√©nergie qui se d√©clinent en des syst√®mes d'une √©tonnante complexit√©. C'est ainsi que le regrett√© Sven Erik J√∏rgensen (1934-2016, figure \@ref(fig:se-jorgensen)) et ses collaborateurs d√©crivent les √©cosyst√®mes dans cet ouvrage qui fait suite aux travaux fondateurs de Howard Thomas Odum.
- Ecological engineering. Principle and Practice.
- Ecological processes handbook.
- Modeling complex ecological dynamics

```{r se-jorgensen, out.width="25%", fig.align="center", fig.cap="Sven Erik J√∏rgensen, Source: [Elsevier](http://scitechconnect.elsevier.com/in-memoriam-of-dr-sven-erik-jorgensen/).", echo = FALSE}
knitr::include_graphics("images/01_sven-jorgensen.png")
```

### Programmation

- [R for data science](http://r4ds.had.co.nz/). L'analyse de donn√©es est une branche importante de l'√©cologie math√©matique. Ce manuel traite des matrices et la manipulation de donn√©es chapitre 3), de la visualisation (chapitre 4) ainsi que de l'apprentissage automatique (chapitre 11). *R for data science* repasse ces sujets plus en profondeur. En particulier, l'ouvrage de [Garrett Grolemund](https://twitter.com/StatGarrett) et [Hadley Wickham](https://twitter.com/hadleywickham) offre une introduction au module graphique `ggplot2`.
- [Numerical ecology with R](http://www.springer.com/la/book/9781441979759). Daniel Borcard enseigne l'√©cologie num√©rique √† l'Universit√© de Montr√©al. Son cours est condens√© dans ce livre recettes vou√© √† l'application des principes lourdement d√©crits dans [Numerical ecology](https://www.elsevier.com/books/numerical-ecology/legendre/978-0-444-53868-0).

### Divers

- [The truthful art](http://www.thefunctionalart.com/p/the-truthful-art-book.html). Dans cet ouvrage, Alberto Cairo s'int√©resse √† l'utilisation des donn√©es et de leurs pr√©sentations pour fournir une information ad√©quate √† diff√©rents publics.

## Besoin d'aide?

Les ouvrages de r√©f√©rence reconnus vous offrent des bases solides sur lesquelles vous pouvez vous appuyer dans vos travaux. Mais au-del√† des principes, au jour le jour, vous vous buterez immanquablement √† toutes sortes de petits probl√®mes. Quel module utiliser pour cette t√¢che pr√©cise? Que veut dire ce message d'erreur? Comment interpr√©ter ce r√©sultat? Pour tous les petits accrocs du quotidien en calcul scientifique, internet offre de nombreuses ressources qui sont tr√®s h√©t√©rog√®nes en qualit√©. Vous apprendrez √† reconna√Ætre les ressources fiables √† celles qui sont douteuses. Les plateformes bas√©es sur Stack Exchange, comme [Stack Overflow](https://stackoverflow.com) et [Cross Validated](https://stats.stackexchange.com), m'ont souvent √©t√© d'une aide pr√©cieuse. Vous aurez avantage √† vous construire une petite banque d'information ([Turtl](https://turtlapp.com/), [Notion](https://www.notion.so/), [Evernote](https://evernote.com/), Google Keep, One Note, etc.) en collectant des liens, en prenant en notes certaines recettes et en suivant des sites d'int√©r√™t avec des flux RSS.

## √Ä propos de l'auteur

Je m'appelle Serge-√âtienne Parent. Je suis ing√©nieur √©cologue et professeur adjoint au D√©partement des sols et de g√©nie agroalimentaire de l'Universit√© Laval, Qu√©bec, Canada. Je crois que la science est le meilleur moyen d'appr√©hender le monde pour prendre des d√©cisions avis√©es.

## Un cours compl√©mentaire √† d'autres cours

Ce cours a √©t√© d√©velopp√© pour ouvrir des perspectives math√©matiques en √©cologie et en agronomie √† la FSAA de l'Universit√© Laval. Il est compl√©mentaire √† certains cours offerts dans d'autres institutions acad√©miques au Qu√©bec, dont ceux-ci.

- [BIO2041. Biostatistiques 1](https://admission.umontreal.ca/cours-et-horaires/cours/bio-2041/), Universit√© de Montr√©al
- [BIO2042. Biostatistiques 2](https://admission.umontreal.ca/cours-et-horaires/cours/BIO-2042/), Universit√© de Montr√©al
- [BIO109. Introduction √† la programmation scientifique](https://github.com/EcoNumUdS/BIO109), Universit√© de Sherbrooke
- [BIO500. M√©thodes en √©cologie computationnelle](https://github.com/EcoNumUdS/BIO500), Universit√© de Sherbrooke.

## Contribuer au manuel

Je suis ouvert aux commentaires et suggestions. Pour contribuer directement, dirigez-vous sur le d√©p√¥t du manuel sur [GitHub](https://github.com), puis ouvrez une *Issue* pour en discuter. Cr√©ez une nouvelle branche (*fork*), effectuez les modifications, puis lancer une requ√™te de fusion (*pull resquest*).

<!--chapter:end:index.Rmd-->

--- 
site: bookdown::bookdown_site
output: bookdown::gitbook 
---

# La science des donn√©es avec R {#chapitre-intro-a-R}

 ***
Ô∏è\ **Objectifs sp√©cifiques**:

√Ä la fin de ce chapitre, vous

- saurez contextualiser la science des donn√©es par rapport aux statistiques,
- serez en mesure de vous lancer dans un environnement de programmation R,
- serez en mesure d'effectuer des op√©rations de base en R,
- saurez diff√©rencier les grands types d'objets de R et
- saurez installer et charger des modules compl√©mentaire.

 ***

Un projet en science des donn√©es comprend trois grandes √©tapes. D'abord, vous devez **collecter des donn√©es** et vous les compilez ad√©quatement. Cela peut consister √† t√©l√©charger des donn√©es existantes, ex√©cuter un dispositif exp√©rimental ou effectuer une recensement (√©tude observationnelle). Compiler les donn√©es dans un format qui puisse √™tre import√© est une t√¢che souvent longue et fastidieuse. Puis, vous **investiguez les donn√©es** collect√©es, c'est-√†-dire vous les visualisez, vous appliquez des mod√®les et testez des hypoth√®ses. Enfin, la **communication des r√©sultats** consiste √† pr√©senter les connaissances qui √©mergent de votre analyse sous forme visuelle et narrative, *avec un langage adapt√© √† la personne qui vous √©coute*, qu'elle soit experte ou novice, r√©viseure de revue savante ou gestionnaire [Grolemund et Wickham (2018)](http://r4ds.had.co.nz/introduction.html) propose la structure d'analyse de la figure \@ref(fig:R-data-flow-chart), avec de l√©g√®res modifications de ma part.

```{r R-data-flow-chart, out.width="100%", fig.align="center", fig.cap="Flux des donn√©es en sciences des donn√©es.", echo = FALSE}
knitr::include_graphics("images/02_science-des-donnees-flow_.png")
```

Le grand cadre sp√©cifie **Programmer**. Oui, vous aurez besoin d'√©crire du code. Mais comme je l'ai indiqu√© dans le premier chapitre, ceci n'est pas un cours de programmation et je pr√©f√©rerai les approches intuitives.

## Statistiques ou science des donn√©es?

Selon [Whitlock et Schluter (2015)](http://whitlockschluter.zoology.ubc.ca/), la statistique est l'*√©tude des m√©thodes pour d√©crire et mesurer des aspects de la nature √† partir d'√©chantillon*. Pour [Grolemund et Wickham (2018)](http://r4ds.had.co.nz/introduction.html), la science des donn√©es est *une discipline excitante permettant de transformer des donn√©es brutes en compr√©hension, perspectives et connaissances*. Oui, *excitante*! La diff√©rence entre les deux champs d'expertise est subtile, et certaines personnes n'y voient qu'une diff√©rence de ton.

<blockquote class="twitter-tweet" data-lang="fr"><p lang="en" dir="ltr">Data Science is statistics on a Mac.</p>&mdash; Big Data Borat (@BigDataBorat) <a href="https://twitter.com/BigDataBorat/status/372350993255518208?ref_src=twsrc%5Etfw">27 ao√ªt 2013</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

Confin√©es √† ses applications traditionnelles, les statistiques sont davantage vou√©es √† la d√©finition de dispositifs exp√©rimentaux et √† l'ex√©cution de tests d'hypoth√®ses, alors que la science des donn√©es est moins lin√©aire, en particulier dans sa phase d'analyse, o√π de nouvelles questions (donc de nouvelles hypoth√®ses) peuvent √™tre pos√©es au fur et √† mesure de l'analyse. Cela arrive g√©n√©ralement davantage lorsque l'on fait face √† de nombreuses observations sur lesquelles de nombreux param√®tres sont mesur√©s.

La quantit√© de donn√©es et de mesures auxquelles nous avons aujourd'hui acc√®s gr√¢ce aux technologies de mesure et de stockage relativement peu dispendieux rend la science des donn√©es une discipline particuli√®rement attrayante, pour ne pas dire [sexy](https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century).

## D√©buter en R

[R](https://www.r-project.org) est un langage de programmation d√©riv√© du langage S, qui fut initialement lanc√© en 1976.

```{r R-logo, out.width="25%", fig.align="center", fig.cap="Logo officiel du language R.", echo = FALSE}
knitr::include_graphics("images/02_R_logo.svg.png")
```

R figure parmi [les langages de programmation les plus utilis√©s au monde](https://www.tiobe.com/tiobe-index/). Bien qu'il soit bas√© sur les langages statiques C et Fortran, R est un langage dynamique, c'est-√†-dire que le code peut √™tre ex√©cut√© ligne par ligne ou bloc par bloc: un avantage majeur pour des activit√©s qui n√©cessitent des interactions fr√©quentes. Bien que R soit surtout utilis√© pour le calcul statistique, il s'impose de plus en plus comme outil privil√©gi√© en sciences des donn√©es en raison des r√©cents d√©veloppements de modules d'analyse, de mod√©lisation et de visualisation, dont plusieurs seront utilis√©s dans ce manuel.

Un langage de programmation s'apprend un peu comme une langue. Au d√©but, un code R peut sembler incompr√©hensible. Et face √† son clavier, on ne sait pas trop comment exprimer ce que l'on d√©sire. Au fur et √† mesure de l'apprentissage, les symboles, les fonctions et le style deviennent de plus en plus familiers et on apprend tranquillement √† traduire en code ce que l'on d√©sire effectuer. Comme une langue s'apprend en la parlant dans la vie de tous les jours, un language de programmation s'apprend avantageusement en solutionnant vos propres probl√®mes.

```{r R-first-and-then, out.width="100%", fig.align="center", fig.cap="R avant et maintenant, Illustration de [Allison Horst](https://github.com/allisonhorst/stats-illustrations)", echo = FALSE}
knitr::include_graphics("images/02_r_first_then.png")
```

## Pr√©parer son flux de travail

Il existe de nombreuses mani√®res d'utiliser R. Parmi celles-ci, j'en couvrirai 3:

- Installation classique (installation sugg√©r√©e)
- Installation avec Anaconda
- Utilisation infonuagique

### Installation classique

**Installation sugg√©r√©e**. Sur Windows ou Mac, dirigez-vous [ici](https://cloud.r-project.org/), t√©l√©chargez et installez. Sur Linux, ouvrez votre gestionnaire d'application, chercher `r-base` (Ubuntu, Debian), `R-base` (openSuse) ou `R-core` (Fedora) et installez-le (assurez-vous que les librairies suivantes sont aussi install√©es: `gcc`, `gcc-fortran`, `gcc-c++` et `make`), vous aurez peut-√™tre besoin d'installer des librairies suppl√©mentaires pour faire fonctionner certains modules.

> **Note**. Les modules pr√©sent√©s dans ce cours devraient √™tre disponibles sur Linux, Windows et Mac. Ce n'est pas le cas pour tous les modules R. La plupart fonctionnent n√©anmoins sur Linux, dont les syst√®mes d'op√©ration (je recommande [Ubuntu](https://www.ubuntu.com/download/desktop) ou l'une de ses d√©riv√©es comme [elementary OS](https://elementary.io/)) sont de bonnes options pour le calcul scientifique.

√Ä cette √©tape, R devrait fonctionner dans un interpr√©teur de commande . Si vous lancez R dans un terminal (chercher `cmd` dans le menu si vous √™tes sur Windows), vous obtiendrez quelque chose comme ceci.

```{r R-terminal, out.width="100%", fig.align="center", fig.cap="R dans le terminal.", echo = FALSE}
knitr::include_graphics("images/02_terminal-prompt.png")
```

Le symbole `>` indique que R attend que vos instructions. Vous voil√† dans un √©tat m√©ditatif devant l'ind√©chiffrable vide du terminal üòµ. Ne vous en faites pas: nous commencerons bient√¥t √† jaser avec R.

Avant cela, installons-nous au salon. Afin de travailler dans un environnement de travail plus confortable, je recommande l'installation de l'interface [RStudio](https://www.rstudio.com/products/rstudio/download/), gratuite et open source: t√©l√©chargez l'installateur et suivez les instructions. RStudio ressemble √† ceci.

```{r R-rstudio, out.width="100%", fig.align="center", fig.cap="Fen√™tre de RStudio.", echo = FALSE}
knitr::include_graphics("images/02_rstudio.png")
```

En haut √† droite se trouve un menu *Project (None)*. Il s'agit d'un menu de vos projets. Je recommande d'utiliser ces projets avec RStudio, qui vous permettront de mieux g√©rer vos sessions de travail, en particulier en lien avec les chemins vers de vos donn√©es, graphiques, etc., que vous pouvez g√©rer relativement √† l'emplacement de votre dossier de projet plut√¥t qu'√† l'emplacement des fichiers sur votre machine: nous verrons plus en d√©tails au chapitre \@ref(chapitre-git).

- En haut √† gauche, vous avez vos feuilles de calcul, qui appara√Ætront en tant qu'onglets. Une feuille de calcul est une s√©rie de commandes que vous lancez en s√©quence. Il peut aussi s'agir d'un livre de calcul (*notebook*) si vous choisissez de travailler en format *R markdown*. Ce format vous permettra de d'√©crire du texte en format [*Markdown*](https://github.com/adam-p/markdown-here/wiki/Markdown-Here-Cheatsheet) entre des blocs de code. Il est question du format *R markdown* au chapitre \ref(chapitre-git).
- En bas √† gauche appara√Æt la Console, o√π vous voyez les commandes envoy√©es √† R ainsi que ses sorties.
- En haut √† droite, les diff√©rents onglets indiquent o√π vous en √™tes dans vos calculs. En particulier, la liste sous *Environment* indique les objets qui ont √©t√© g√©n√©r√©s ou charg√©s jusqu'alors.
- En bas √† droite, on retrouve des onglets de nature vari√©s. *Files* contient les sous-dossiers et fichiers du dossier de projets. *Plots* est l'endroit o√π appara√Ætront vos graphiques. *Packages* contient la liste des modules d√©j√† install√©s, ainsi qu'un outil de gestion des modules pour leur installation, leur d√©sinstallation et leur mise √† jour. *Help* affiche les fiches d'aide des fonctions (pour obtenir de l'aide sur une fonction dans RStudio, surlignez la fonction dans votre feuille de calcul, puis appuyez sur `F1`). Enfin, l'onglet *Viewer* affichera les sorties HTML, en particulier les graphiques interactifs que vous g√©n√©rerez par exemple avec le module `plotly`. Si votre environnement de travail √©tait un avion, R serait le moteur et RStudio serait le cockpit!

```{r R-fifi, out.width="100%", fig.align="center", fig.cap="Sc√®ne de Fifi Brindacier ([Astrid Lindgren, 1945](https://fr.wikipedia.org/wiki/Fifi_Brindacier)).", echo = FALSE}
knitr::include_graphics("https://media.giphy.com/media/GmaV9oet9MAmI/giphy.gif")
```

### Installation avec Anaconda

Si vous cherchez une trousse compl√®te d'analyse de donn√©es, comprenant R et Python, vous pourrez pr√©f√©rer [Anaconda](https://www.anaconda.com/download/#linux). Une fois install√©e, vous pourrez isoler un environnement de travail sur R, ou m√™me isoler des environnements de travail particuliers pour vos projets. Une mani√®re conviviale de cr√©er des environnements de travail est de passer par l'interface *Anaconda navigator*, que vous lancerez soit dans le menu Windows, soit en ligne de commande `anaconda-navigator` sous Mac et Linux, puis d'installer `r-essentials`, `rstudio` et `jupyterlab` dans l'onglet *Environment*. Vous pourrez aussi installer `RStudio` et `Jupyter lab` via l'onglet *Home* de `Anaconda navigator`. Dans l'environnement de base, installez le package `nb_conda_kernels` pour vous assurer que tous les noyaux (R, Python, etc.) install√©s dans les environnements de travail soient automatiquement accessibles dans Jupyter. Si vous d√©sirez utiliser dans Jupyter la version de R install√©e avec l'installation classique, r√©f√©rez-vous au guide pr√©sent√© [en extra au bas de la page](#extra-jupyter).

```{r R-anaconda, out.width="100%", fig.align="center", fig.cap="Anaconda navigator.", echo = FALSE}
knitr::include_graphics("images/02_anaconda-navigator.png")
```

*Jupyter lab* est une interface notebook semblable √† *R markdown*  - les format *Jupyter* (`*.ipynb`) et *R markdown* (`*.Rmd`) sont par ailleurs convertibles gr√¢ce au module [jupytext](https://towardsdatascience.com/introducing-jupytext-9234fdff6c57). L'utilisation de R en Anaconda n'est pas tout √† fait au point, et pourrait poser probl√®me pour l'installation de certains modules. Si vous optez pour cette option, pr√©parez-vous √† avoir √† bidouiller un peu. Plusieurs pr√©f√®rent Jupyter √† RStudio (ce n'est pas mon cas).

### Utilisation infonuagique

Pas besoin d'avoir une machine super puissante pour travailler en R. Il existe [une multitude de services infonuagiques](https://github.com/markusschanta/awesome-jupyter#hosted-notebook-solutions) (dans le *cloud*) vous permettant de lancer vos calculs sur des serveurs plut√¥t que sur votre Chromebook ou votre vieux laptop d√©glingu√©. Certains services sont gratuits, et d'autres souvent plus √©labor√©s sont payants.

Un service gratuit qui fonctionne bien en R est [Azure Notebooks](https://notebooks.azure.com), offert par Microsoft. Vous y aurez acc√®s avec un compte Microsoft ou un compte Exchange (par exemeple avec un IDUL de l'Universit√© Laval). Azure notebooks offre des dossiers comportant des notebooks de type Jupyter ainsi que des fichiers de donn√©es. Les dossiers peuvent √™tre rendus publics et cl√¥n√©s≈ù par des coll√®gues. Le travail collaboratif n'est pas disponible.

Pour collaborer en temps r√©el dans un document, vous pourrez utiliser Google Colaboratory, offert gratuitement par Google. Bien que les noyaux de calcul soient seulement offerts de mani√®re explicite pour Python, vous pouvez [cl√¥ner ou importer un notebook con√ßu pour fonctionner en R](https://stackoverflow.com/a/54595286), et un noyau R sera charg√©. Il vous faudra [bidouiller un peu pour charger vos donn√©es depuis Google drive ou votre ordinateur](https://colab.research.google.com/notebooks/io.ipynb). Un avantage de Google Colaboratory est que vous pouvez sp√©cifier que vous d√©sirez que vos calculs tournent sur un processeur graphique, utile pour les calculs lourds et parall√©lisables comme les r√©seaux neuronnaux. Si je viens de vous perdre, pas de probl√®me, c'est de l'extra.

Si vous d√©sirer autant que possible rester ind√©pendant des g√©ants du web, [CoCalc](https://cocalc.com/) est une option avec un volet gratuit et un autre payant.

## Premiers pas avec R

R ne fonctionne pas avec des menus, en faisant danser une souris sous une musique de clics. Vous devrez donc entrer des commandes avec votre clavier, que vous apprendrez par c≈ìur au fur et √† mesure, ou que vous retrouverez en lan√ßant des recherches sur internet. Par exp√©rience personnelle, lorsque je travaille avec R, j'ai toujours un navigateur ouvert pr√™t √† recevoir une question.

Les √©tapes qui suivent sont des premiers pas. Elles ne feront pas de vous des *ceintures noires* del√† programmation. La plupart des utilisateurs de R ont appris R en se pratiquant sur leurs donn√©es, en frappant des murs, en apprenant comment les escalader ou les contourner...

Pour l'instant, ouvrez seulement un interpr√©teur de commande, et lancez R. Voyons si R est aussi libre qu'on le pr√©tend.

> "La libert√©, c‚Äôest la libert√© de dire que deux et deux font quatre. Si cela est accord√©, tout le reste suit." - George Orwell, 1984

```{r intro-sum}
2 + 2
```

Et voil√†.

<img width="200" src="images/02_braveheart224.png">

Les op√©rations math√©matiques sont effectu√©es telles que l'on devrait s'attendre.

```{r intro-operations}
67.1 - 43.3

2 * 4

1 / 2
```

L'exposant peut √™tre not√© `^`, comme c'est le cas dans *Excel*, ou `**` comme c'est le cas en Python.

```{r intro-power}
2^4
```

```{r intro-power-alt}
2**4
```

```{r intro-division}
1 / 2 # utilisez des espaces de part et d'autre des op√©rateurs (sauf pour l'exposant) pour √©claircir le code
```

R ne lit pas ce qui suit le caract√®re `#`. Cela vous laisse l'opportunit√© de commenter un code comprenant une s√©quence de plusieurs lignes. Remarquez √©galement que la derni√®re op√©ration comporte des espaces entre les nombres et l'op√©rateur `/`. Dans ce cas (ce n'est pas toujours le cas), les espaces ne signifient rien: ils aident seulement √† √©claircir le code. Il existe des guides pour l'√©criture de code en R. Je recommande le guide de style de [Hadley Wickahm](http://adv-r.had.co.nz/Style.html).

Assigner des objets √† des variables est fondamental en programmation. En R, on assigne traditionnellement avec la fl√®che `<-`, mais vous verrez parfois le `=`, qui est davantage utilis√© comme standard dans d'autres langages de programmation. Par exemple.

```{r intro-assignation}
a <- 3
```

Techniquement, `a` pointe vers le nombre entier 3. Cons√©quemment, on peut effectuer des op√©rations sur `a`.

```{r intro-assignation-mult}
a * 6
```

```{r intro-sensible-a-la-case}
# A + 2
```

Le message d'erreur nous dit que `A` n'est pas d√©fini. Sa version minuscule, `a`, l'est pourtant. La raison est que R consid√®re la *case* dans la d√©finition des objets. Utiliser la mauvaise case m√®ne donc √† des erreurs.

**Note**. Les messages d'erreur ne sont pas toujours clairs, mais vous apprendrez √† les comprendre. Dans tous les cas, ils sont fait pour vous aider. Lisez-les attentivement!

En g√©n√©ral, le nom d'une variable doit toujours commencer par une lettre, et ne doit pas contenir de caract√®res r√©serv√©s (espaces, `+`, `*`). Dans la d√©finition des variables, plusieurs utilisent des symboles `.` pour d√©limiter les mots, mais la barre de soulignement `_` est √† pr√©f√©rer. En effet, dans d'autres langages de programmation comme Python, le `.` a une autre signification: son utilisation est √† √©viter autant que possible.

**Note**. √Ä ce stade, vous serez probablement plus √† l'aise de copier-coller ces commandes dans votre terminal.

```{r intro-bloc-calcul}
rendement_arbre <- 50 # pomme/arbre
nombre_arbre <- 300 # arbre
nombre_pomme <- rendement_arbre * nombre_arbre
nombre_pomme
```

Comme chez la plupart des langages de programmation, R respecte les conventions des [priorit√©s des op√©rations math√©atiques](https://fr.wikipedia.org/wiki/Ordre_des_op%C3%A9rations).

```{r intro-priorite-des-operations}
10 - 9^0.5 * 2
```

### Types de donn√©es

Jusqu'√† maintenant, nous n'avons utilis√© que des **nombres entiers** (*integer* ou `int`) et des **nombres r√©els** (*numeric* ou `float64`). R inclut d'autres types. La **cha√Æne de caract√®re** (*string* ou *character*) contient un ou plusieurs symboles. Elle est d√©finie entre des doubles guillemets `" "` ou des apostrophes `' '`. Il n'existe pas de standard sur l'utilisation de l'un ou de l'autre, mais en r√®gle g√©n√©rale, on utilise les apostrophes pour les expressions courtes, contenant un simple mot ou s√©quence de lettres, et les guillemets pour les phrases. Une raison pour cela: les guillemets sont utiles pour ins√©rer des apostrophes dans une cha√Æne de caract√®re.

```{r intro-paste}
a <- "L'ours"
b <- "polaire"
paste(a, b)
```

On *colle* `a` et `b` avec la fonction `paste`. Notez que l'objet `a` a √©t√© d√©fini pr√©c√©demment. Il est possible en R de r√©assigner une variable, mais cela peut porter √† confusion, jusqu'√† g√©n√©rer des erreurs de calcul si une variable n'est pas assign√©e √† l'objet auquel on voulait r√©f√©rer.

Combien de caract√®res contient la cha√Æne `"L'ours polaire"`? R sait compter. Demandons-lui.

```{r introchar-count}
c <- paste(a, b)
nchar(c)
```

Quatorze, c'est bien cela (comptez "L'ours polaire", en incluant l'espace). Comme `paste`, `nchar` est une fonction incluse par d√©faut dans l'environnement de travail de R: plus pr√©cis√©ment, ces fonctions sont incluses dans le module `base`, inclut par d√©faut lorsque R est lanc√©. La fonction est appel√©e en √©crivant `nchar()`. Mais une fonction de quoi? Des *arguments*, qui se trouvent entre les parenth√®ses. Dans ce cas, il y a un seul argument: `c`.

En calcul scientifique, il est courant de lancer des requ√™tes sur si un r√©sultat est vrai ou faux.

```{r intro-bool}
a <- 17
a < 10
a > 10
a == 10
a != 10
a == 17
!(a == 17)
```

Je viens d'introduire un nouveau type de donn√©e: les donn√©es bool√©ennes (*boolean*, ou `logical`), qui ne peuvent prendre que deux √©tats - `TRUE` ou `FALSE`. En m√™me temps, j'ai utilis√© la fonction `print` parce que dans mon carnet, seule la derni√®re op√©ration permet d'afficher le r√©sultat. Si l'on veut forcer une sortie, on utilise `print`. Puis, on a vu plus haut que le symbole `=` est r√©serv√© pour assigner des objets: pour les tests d'√©galit√©, on utilise le double √©gal, `==`, ou `!=` pour la non-√©galit√©. Enfin, pour inverser une donn√©e de type bool√©enne, on utilise le point d'exclamation `!`.

### Les collections de donn√©es

Les exercices pr√©c√©dents ont permis de pr√©senter les types de donn√©es offerts par d√©faut sur R qui sont les plus importants pour le calcul scientifique: `int` (*integer*, ou nombre entier), `numeric` (nombre r√©el), `character` (*string*, ou cha√Æne de caract√®re) et `logical` (bool√©en). D'autres s'ajouteront tout au long du cours, comme les cat√©gories (`factor`) et les unit√©s de temps (date-heure).

Lorsque l'on proc√®de √† des op√©rations de calcul en science, nous utilisons rarement des valeurs uniques. Nous pr√©f√©rons les organiser et les traiter en collections. Par d√©faut, R offre quatre types importants de collections: les **vecteurs**, les **matrices**, les **listes** et les **tableaux**.

#### Vecteurs

D'abord, les **vecteurs** sont une s√©rie de variables de m√™me type. Un vecteur est d√©limit√© par la fonction `c( )` (`c` pour **c**oncat√©nation). Les √©l√©ments de la liste sont s√©par√©s par des virgules.

```{r intro-vector}
espece <- c("Petromyzon marinus", "Lepisosteus osseus", "Amia calva", "Hiodon tergisus")
espece
```

Pour acc√©der aux √©l√©ments d'une liste, appelle la liste suivie de la position de l'objet d√©sir√© entre crochets.

```{r intro-vect-index}
espece[1]
espece[2]
espece[1:3]
espece[c(1, 3)]
```

On peut noter que le premier √©l√©ment de la liste est not√© `1`, et non `0` comme c'est le cas de la plupart de langages. Le raccourcis `1:3` cr√©e une liste de nombres entiers de `1` √† `3` inclusivement, c'est-√†-dire l'√©quivalent de `c(1, 2, 3)`. En effet, on cr√©e une liste d'indices pour soutirer des √©l√©ments d'une liste. On peut utiliser le symbole de soustraction pour retirer un ou plusieurs √©l√©ments d'un vecteur.

```{r intro-vect-index-vect}
espece[-c(1, 3)]
```

Pour ajouter un √©l√©ment √† notre liste, on peut utiliser la fonction `c( )`.

```{r intro-vector-add}
espece <- c(espece, "Cyprinus carpio")
espece
```

Notez que l'on efface l'objet `espece` par une concat√©nation de l'objet `espece`, pr√©c√©demment d√©finie, et d'un autre √©l√©ment.

En lan√ßant `espece[3] <- "Lepomis gibbosus"`, il est possible de changer un √©l√©ment de la liste.

```{r intro-vet-change}
espece[3] <- "Lepomis gibbosus"
espece
```

#### Matrices

Une **matrice** est un vecteur de dimension plus √©lev√©e que 1. En √©cologie, on d√©passe rarement la deuxi√®me dimension, quoi que les matrices en `N` dimensions soient courantes en mod√©lisation math√©matique. Je ne consid√©rerai pour le moment que des matrices `2D`. Comme c'est la cas des vecteurs, les matrices contiennent des valeurs de m√™me type. En R, on peut attribuer aux matrices `2D` des noms de ligne et de colonne.

```{r intro-mat}
mat <- matrix(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12), ncol = 3)
mat
```

```{r intro-mat-def}
colnames(mat) <- c("A", "B", "C")
rownames(mat) <- c("site_1", "site_2", "site_3", "site_4")
mat
```

On peut soutirer les noms de colonne et les noms de ligne. Le r√©sultat est un vecteur.

```{r intro-mat-def-getnames}
colnames(mat)
rownames(mat)
```

#### Listes

Les **listes** sont des collections h√©t√©rog√®nes dans lesquelles on peut placer les objets d√©sir√©s, sans distinction: elles peuvent m√™me inclure d'autres listes. Chacun des √©l√©ments de la liste peut √™tre identifi√© par une cl√©.

```{r intro-list}
ma_liste <- list(
  especes = c(
    "Petromyzon marinus", "Lepisosteus osseus",
    "Amia calva", "Hiodon tergisus"
  ),
  site = "A101",
  stations_meteos = c("746583", "783786", "856363")
)
ma_liste
```

Les √©l√©ments de la liste peuvent √™tre soutir√©s par le nom de la cl√© ou par l'indice, de cette mani√®re.

```{r intro-list-index}
ma_liste$especes
ma_liste[[1]]
```

**Exercice**. Acc√©der au deuxi√®me √©l√©ment du vecteur d'esp√®ces dans la liste ma_liste.

#### Tableaux

Enfin, le type de collection de donn√©es le plus important est le tableau, ou `data.frame`. Techniquement, il s'agit d'une liste compos√©e de vecteurs de m√™me longueur. Chaque colonne peut ainsi prendre un type de donn√©e ind√©pendamment des autres colonnes.

```{r intro-df}
tableau <- data.frame(
  espece = c(
    "Petromyzon marinus", "Lepisosteus osseus",
    "Amia calva", "Hiodon tergisus"
  ),
  poids = c(10, 13, 21, 4),
  longueur = c(35, 44, 50, 8)
)
tableau
```

En programmation classique en R (nous verrons plus loin la m√©thode `tidyverse`), les √©l√©ments d'un tableau se manipulent comme ceux d'une matrice et les colonnes peuvent √™tre appel√©s comme les √©l√©ments d'une liste.

```{r intro-df-col}
tableau[, 2:3]
tableau$poids
```

Vous verrez aussi, quoi que rarement, ce format, qui √† la diff√©rence du format `$` g√©n√®re un tableau.

```{r intro-df-col2}
tableau["poids"]
```

Le tableau est le format de collection √† privil√©gier pour manipuler des donn√©es. R√©cemment, le format de tableau `tibble` a √©t√© cr√©√© par l'√©quipe de RStudio pour offrir un format plus moderne.

### Les fonctions

Lorsque vous √©crivez une commande suivit de parenth√®ses, comme `data.frame(especes = ...)`, vous demandez √† R de passer √† l'action en appelant une fonction. De mani√®re tr√®s g√©n√©rale, une fonction transforme quelque chose en quelque chose d'autre (figure \@ref(fig:intro-fonction)).

```{r intro-fonction, out.width="25%", fig.align="center", fig.cap="Sch√©ma simplifi√© d'une fonction.", echo = FALSE}
knitr::include_graphics("images/images/02_fonctions_io_.png")
```

Par exemple, la fonction `mean()` prend une collection de nombre comme entr√©e, puis en sort vous devinez quoi.

```{r intro-mean}
mean(tableau$poids)
```

Les entr√©es sont appel√©s les **arguments** de la fonction. Leur d√©finition est toujours disponible dans la documentation.

**Exercice**. Familiarisez-vous avec la documentation de R en lan√ßant `?mean`. Truc: si vous avez pris de l'avance et que vous travaillez d√©j√† en RStudio, mettez le terme en surbrillance, puis appuyez sur F1.

Vous verrez dans la documentation que la fonction `mean()` demande trois arguments, `x`, `trim` et `na.rm`. Or nous avons seulement plac√© un vecteur, sans sp√©cifier d'argument!

En effet. En l'absence d'une d√©finition des arguments, R supposera que les arguments dans la parenth√®se, s√©par√©s par une virgule, sont pr√©sent√©s dans le m√™me ordre que celui sp√©cifi√© dans la d√©finition de la fonction (celle qui est pr√©sent√©e dans le fichier d'aide). Dans le cas qui nous int√©resse, `mean(tableau$poids)` est √©quivalent √† `mean(x = tableau$poids)`.

Maintenant, selon la fiche d'aire l'argument `na.rm` est un valeur logique sp√©cifiant si oui (`TRUE`) ou non (`FALSE`) les valeurs manquantes doivent √™tre consid√©r√©es (une moyenne d'un vecteur comprenant au moins un `NA` sera de `NA`). En ne sp√©cifiant rien, R prend la valeur par d√©faut, telle que sp√©cifi√©e dans la documentation. Il en va de m√™me que l'argument `trim`, qui permet d'√©laguer des valeurs extr√™mes. Dans la fiche d'aide,  `mean(x, trim = 0, na.rm = FALSE, ...)` signifie que par d√©faut, l'argument `x` est vide (il doit donc √™tre sp√©cifi√©), l'argument `trim` est de 0 et l'argument `na.rm` est `FALSE`.

```{r intro-mean-2}
mean(c(6, 1, 7, 4, 9, NA, 1))
mean(c(6, 1, 7, 4, 9, NA, 1), na.rm = TRUE)
```

Vous n'√™tes pas emprisonn√© par les fonctions offertes par R. Vous pouvez installer des modules qui compl√®tent les fonctions de base de R: on le verra un peu plus loin dans ce chapitre. Mais pour l'instant, voyons comment vous pouvez cr√©er vos propres fonctions. Disons que vous voulez cr√©er une fonction qui calcule la sortie de $x^3-2y+a$. Pour obtenir la r√©ponse on a besoin des arguments `x`, `y` et `a`. La sortie de la fonction est ici triviale: la r√©ponse de l'√©quation. L'op√©ration `function` permet de prendre √ßa en charge.

```{r intro-define-function}
operation_f <- function(x, y, a = 10) {
  return(x^3 - 2 * y + a)
}
```

Notez que `a` a une valeur par d√©faut. La sortie de la fonction est ce qui se trouve entre les parenth√®ses de `return`. Vous pouvez maintenant utiliser la fonction operation_nl au besoin.

```{r intro-exec-function}
operation_f(x = 2, y = 3, a = 1)
```

Une telle fonction est peu utile. Mais l'utilisation de fonctions personnalis√©es vous permettra d'√©viter de r√©p√©ter la m√™me op√©ration plusieurs fois dans un flux de travail, en √©vitant de g√©n√©rer trop de code, donc aussi de potentielles erreurs. Personnellement, j'utilise les fonctions surtout pour g√©n√©rer des graphiques personnalis√©s.

**Exercice**. Afin d'acqu√©rir de l'autonomie, vous devrez √™tre en mesure de trouver le nom des commandes dont vous avez besoin pour effectuer la t√¢che que vous d√©sirer effectuer. Cela peut causer des frustrations, mais vous vous sentirez toujours plus √† l'aise avec R jour apr√®s jour. L'exercice ici est de trouver par vous-m√™me la commande qui vous permettra mesurer la longueur d'un vecteur.

### Les boucles

Les boucles permettent d'effectuer une m√™me suite d'op√©rations sur plusieurs objets. Pour faire suite √† notre exemple, nous d√©sirons obtenir le r√©sultat de l'op√©ration *f* pour des param√®tres que nous enregistrons dans ce tableau.

```{r intro-df-loop}
params <- data.frame(
  x = c(2, 4, 1, 5, 6),
  y = c(3, 4, 8, 1, 0),
  a = c(6, 1, 8, 2, 5)
)
params
```

Nous cr√©ons un vecteur vide, puis nous it√©rons ligne par ligne en remplissant le vecteur.

```{r intro-loop}
operation_res <- c()
for (i in 1:nrow(params)) {
  operation_res[i] <- operation_f(x = params[i, 1], y = params[i, 2], a = params[i, 3])
}
operation_res
```

En faisant varier `i` sur des valeurs du vecteur donn√© par la s√©quence de nombre entiers de 1 au nombre de ligne du tableau de param√®tres, nous demandons √† R d'effectuer la suite d'op√©ration entre les accolades `{}`. √Ä chaque boucle, `i` prend une valeur de la s√©quence. `i` est utilis√© ici comme indice de la ligne √† soutirer du tableau `params`, qui correspond √† l'indice dans le vecteur operation_res.

Ainsi, chaque r√©sultat est calcul√© dans l'ordre des lignes du tableau de param√®tres et l'on pourra tr√®s bien y coller nos r√©sultats:

```{r intro-store-loop}
params$resultats <- operation_res
params
```

Notez que puisque la colonne `resultat` n'existe pas dans le tableau `params`, R cr√©e automatiquement une nouvelle colonne.

Les boucles `for` vous permettront par exemple de g√©n√©rer en peu de temps 10, 100, 1000 graphiques (autant que vous voulez), chacun issu de simulations obtenues √† partir de conditions initiales diff√©rentes, et de les enregistrer dans un r√©pertoire sur votre ordinateur. Un travail qui pourrait prendre des semaines sur Excel peut √™tre effectu√© en R en quelques secondes.

Un second outil est disponible pour les it√©rations: les boucles **`while`**. Elles effectuent une op√©ration tant qu'un crit√®re n'est pas atteint. Elles sont utiles pour les op√©rations o√π l'on cherche une convergence. Je les couvre rapidement puisqu‚Äôelles sont rarement utilis√©es dans les flux de travail courants. En voici un petit exemple.

```{r intro-while}
x <- 100
while (x > 1.1) {
  x <- sqrt(x)
  print(x)
}
```

Nous avons initi√© x √† une valeur de 100. Puis, tant que (`while`) le test `x > 1.1` est vrai, attribuer √† `x` la nouvelle valeur calcul√©e en extrayant la racine de la valeur pr√©c√©dente de `x`. Enfin, indiquer la valeur avec `print`.

### Conditions: `if`, `else if`, `else`

> Si la condition 1 est remplie, effectuer une suite d'instruction 1. Si la condition 1 n'est pas remplie, et si la condition 2 est remplie, effectuer la suite d'instruction 2. Sinon, effectuer la suite d'instruction 3.

Voil√† comment on exprime une suite de conditions. Prenons l'exemple simple d'une discr√©tisation d'une valeur continue. Si $x<10$, il est class√© comme faible. Si $10 \leq x <20$, il est class√© comme moyen. Si $x \geq 20$, il est class√© comme √©lev√©. Pla√ßons cette classification dans une fonction.

```{r intro-if}
classification <- function(x, lim1 = 10, lim2 = 20) {
  if (x < lim1) {
    categorie <- "faible"
  } else if (x < lim2) {
    categorie <- "moyen"
  } else {
    categorie <- "√©lev√©"
  }
  return(categorie)
}
classification(-10)
classification(15.4)
classification(1000)
```

Une condition est d√©finie avec le `if`, suivi du test √† vrai ou faux entre parenth√®ses. Si le test retourne un *vrai* (`TRUE`), l'instruction entre accolades est ex√©cut√©e. Si elle est fausse, on passe au suivant.

**Exercice**. Explorer les commandes `ifelse` et `cut` et r√©fl√©chissez √† la mani√®re qu'elles pourraient √™tre utilis√©es pour effectuer une discr√©tisation plus efficacement qu'avec les `if` et les `else`.

### Installer et charger un module

La plupart des op√©rations d'ordre g√©n√©ral (comme les racines carr√©es, les tests statistiques, la gestion de matrices et de tableau, les graphiques, etc.) sont accessibles gr√¢ce aux modules de base de R, qui sont install√©s et charg√©s par d√©faut lors du d√©marrage de R. Des √©quipes de travail ont n√©anmoins d√©velopp√© plusieurs modules pour r√©pondre √† leurs besoins sp√©cialis√©s, et les ont laiss√©es disponibles au grand public dans des modules que vous pouvez installer d'un d√©p√¥t CRAN (le AppStore de R), d'un d√©p√¥t Anaconda (le AppStore de Anaconda, si vous utilisez cette plate-forme), d'un d√©p√¥t Github (d√©p√¥ts d√©centralis√©s), etc.

RStudio poss√®de un pratique bouton *Install* qui vous permet d'y inscrire une liste de modules. Le navigateur anaconda offre aussi une interface d'installation. La commande R pour installer un module est `install.packages("ggplot2")`, si par exemple vous d√©sirez installer `ggplot2`, le module graphique par excellence en R. C'est la commande que RStudio lancera tout seul si vous lui demandez d'installer `ggplot2`.

Les modules sont l'√©quivalent des applications sp√©cialis√©es que vous installez sur un t√©l√©phone mobile. Pour les utiliser, il faut les ouvrir.

G√©n√©ralement, j'ouvre toutes les applications n√©cessaires √† mon flux de travail au tout d√©but de ma feuille de calcul (la prochaine cellule retournera un message d'erreur si les packages ne sont pas install√©s).

```{r intro-package}
library("tidyverse") # m√©ta-package qui charge entre autres dplyr et ggplot2
library("vegan")
library("nlme")
```

Les modules sont install√©s sur votre ordinateur √† un endroit que vous pourrez retrouver avec la commande `.libPaths()`

**Exercice**. √Ä partir d'ici jusqu'√† la fin du cours, nous utiliserons RStudio. Ouvrez-le et familiarisez-vous avec l'interface! Quelques petits trucs:

- pour lancer une ligne, placez votre curseur sur la ligne, puis appuyez sur Ctrl+Enter
- pour lancer une partie de code pr√©cise, mettez le en surbrillance, puis Ctrl+Enter
- utilisez toujours le gestionnaire de projets, en haut √† droite!
- installez le module **`tidyverse`**
- lancez `data("iris")` pour obtenir un tableau d'exercice, puis cliquez sur l'objet dans la fen√™tre environnement

## Enfin...

Comme une langue, on n'apprend √† s'exprimer en un langage informatique qu'en se mettant √† l'√©preuve, ce que vous ferez tout au long de ce cours. Pour vous encourager, voici quelques trucs pour apprendre √† coder en R.

- **R n'aime pas l‚Äôambigu√Øt√©**. Une simple virgule mal plac√©e et il ne sait plus quoi faire. Cela peut √™tre frustrant au d√©but, mais cette rigidit√© est n√©cessaire pour effectuer du calcul scientifique.
- **Le copier-coller est votre ami**. En gardant √† l'esprit que vous √™tre responsable de votre code et que vous respectez les droits d'auteur, n'ayez pas peur de copier-coller des lignes de code et de personnaliser par la suite.
- **L'erreur que vous obtenez: d'autres l'ont obtenue avant vous**. Le site de question-r√©ponse [stackoverflow](https://stackoverflow.com/questions/tagged/r) est une ressource inestimable o√π des gens ayant post√© des questions ont re√ßu des r√©ponses d'experts (les meilleures r√©ponses et les meilleures questions apparaissent en premier). Apprenez √† chercher intelligemment des r√©ponses en formulant pr√©cis√©ment vos questions!
- **√âtudiez et pratiquez**. Les messages d'erreur en R sont courants, m√™me chez les personnes exp√©riment√©es. La meilleure mani√®re d'apprendre une langue est de la parler, d'√©tudier ses susceptibilit√©s, de les tester dans une conversation, etc.

## Petit truc!

RStudio peut √™tre impl√©ment√© avec des extensions. L'une d'elle permet d'ajuster votre style de code. Par exemple, vous voulez vous assurer que toutes les allocations sont bien effectu√©es avec des `<-` et non pas des `=`, 'il y a bien des espaces de part et d'autre de `<-`, que les retours de lignes sont bien plac√©s, etc. Installez le module **`stryler`**, et des options appara√Ætront dans le menu `Addins` comme √† la figure \@ref(fig:vis-conference-2018).

```{r intro-fig-styler, out.width="40%", fig.align="center", fig.cap="L'extension styler permet de formater votre code dans un style particulier", echo = FALSE}
knitr::include_graphics("images/02_styler.png")
```

## Extra: Utiliser R avec Jupyter {#extra-jupyter}

Pour utiliser R dans Jupyter notebook ou Jupyter lab, vous devez installer le module **`IRkernel`** dans la version de R que vous d√©sirez utiliser avec Jupyter, puis de lancer la commande `IRkernel::installspec()`. La prochaine fois que vous ouvrirez Jupyter, le noyau de R devrait appara√Ætre.

Je n'ai aucune exp√©rience sur Mac, mais semble-t-il cela fonctionne comme en Linux. Ouvrez R √† partir d'un terminal (R + Enter), puis lancez `IRkernel::installspec()` apr√®s avoir install√© **`IRkernel`**. Si vous travaillez en Windows, il vous faudra lancer R par son chemin complet dans l'invite de commande de Anaconda (*Anaconda Powershell Prompt*). Par exemple, ouvrir *Anaconda Powershell Prompt*, puis, si votre installation de R se trouve dans `C:\Program Files\R\R-3.6.2`, 

```
(base) PS C:\Users\fifi> cd "C:\Program Files\R\R-3.6.2\bin"
(base) PS C:\Program Files\R\R-3.6.2\bin> .\R.exe

R version 3.6.2 (2019-12-12) -- "Dark and Stormy Night"
Copyright (C) 2019 The R Foundation for Statistical Computing
Platform: x86_64-w64-mingw32/x64 (64-bit)

R est un logiciel libre livr√© sans AUCUNE GARANTIE.
Vous pouvez le redistribuer sous certaines conditions.
Tapez 'license()' ou 'licence()' pour plus de d√©tails.

R est un projet collaboratif avec de nombreux contributeurs.
Tapez 'contributors()' pour plus d'information et
'citation()' pour la fa√ßon de le citer dans les publications.

Tapez 'demo()' pour des d√©monstrations, 'help()' pour l'aide
en ligne ou 'help.start()' pour obtenir l'aide au format HTML.
Tapez 'q()' pour quitter R.

> install.packages("IRkernel")
Installation du package dans 'C:/Users/fifi/Documents/R/win-library/3.6'
(car 'lib' n'est pas sp√©cifi√©)
--- SVP s√©lectionner un miroir CRAN pour cette session ---
essai de l'URL 'https://cloud.r-project.org/bin/windows/contrib/3.6/IRkernel_1.1.zip'
Content type 'application/zip' length 138696 bytes (135 KB)
downloaded 135 KB

le package 'IRkernel' a √©t√© d√©compress√© et les sommes MD5 ont √©t√© v√©rifi√©es avec succ√©s

Les packages binaires t√©l√©charg√©s sont dans
       C:\Users\fifi\AppData\Local\Temp\Rtmp6xJtB3\downloaded_packages

> IRkernel::installspec()
[InstallKernelSpec] Installed kernelspec ir in C:\Users\fifi\AppData\Roaming\jupyter\kernels\ir
> qui()
```

```{r intro-rmlist, include=FALSE}
rm(list = ls())
```

<!--chapter:end:02_R.Rmd-->

--- 
site: bookdown::bookdown_site
output: bookdown::gitbook 
---

```{r, include=FALSE}
library("tidyverse")
```


# Organisation des donn√©es et op√©rations sur des tableaux {#chapitre-tableaux}

***

\ **Objectifs sp√©cifiques**:

√Ä la fin de ce chapitre, vous

- comprendrez les r√®gles guidant la cr√©ation et la gestion des tableaux,
- saurez importer et exporter des donn√©es et
- saurez effectuer des op√©rations en cascade avec le module tidyverse, dont
- des filtres sur les lignes,
- des s√©lections de colonnes,
- des sommaires statistiques et
- des jointures entre tableaux.

***

Les donn√©es sont utilis√©es √† chaque √©tape dans les flux de travail en sciences. Elles alimentent l'analyse et la mod√©lisation. Les r√©sultats qui en d√©coulent sont aussi des donn√©es qui peuvent alimenter les travaux subs√©quents. Une bonne organisation des donn√©es facilitera le flux de travail.

> **Dicton**. Proportions de temps vou√© au calcul scientifique¬†: 80 % de nettoyage de donn√©es mal organis√©es, 20 % de calcul.

Qu'est-ce qu'une donn√©e ? De mani√®re abstraite, il s'agit d'une valeur associ√©e √† une variable. Une variable peut √™tre une dimension, une date, une couleur, le r√©sultat d'un test statistique, √† laquelle on attribue la valeur quantitative ou qualitative d'un chiffre, d'une cha√Æne de caract√®re, d'un symbole conventionn√©, etc. Par exemple, lorsque vous commandez un caf√© *latte* v√©gane, *au latte* est la valeur que vous attribuez √† la variable *type de caf√©*, et *v√©gane* est la valeur de la variable *type de lait*.

L'exemple est peut-√™tre horrible. J'ai besoin d'un caf√©...

![](https://media.giphy.com/media/3nbxypT20Ulmo/giphy.gif)

Ce chapitre traite de l'importation, l'utilisation et l'exportation de donn√©es structur√©es, en R, sous forme de vecteurs, matrices, tableaux et ensemble de tableaux (bases de donn√©es).

Bien qu'il soit toujours pr√©f√©rable d'organiser les structures qui accueilleront les donn√©es d'une exp√©rience avant-m√™me de proc√©der √† la collecte de donn√©es, l'analyste doit s'attendre √† r√©organiser ses donn√©es en cours de route. Or, des donn√©es bien organis√©es au d√©part faciliteront aussi leur r√©organisation.

Ce chapitre d√©bute avec quelques d√©finitions : les donn√©es, les matrices, les tableaux et les bases de donn√©es, ainsi que leur signification en R. Puis nous verrons comment organiser un tableau selon quelques r√®gles simples, mais importantes pour √©viter les erreurs et les op√©rations fastidieuses pour reconstruire un tableau mal con√ßu. Ensuite, nous traiterons des formats de tableau courant, pour enfin passer √† l'utilisation de [**`dplyr`**](https://dplyr.tidyverse.org/), le module du *tidyverse* pour effectuer des op√©rations sur les tableaux.

## Les collections de donn√©es

Dans le chapitre \@ref(chapitre-intro-a-R), nous avons survol√© diff√©rents types d'objets : r√©els, entiers, cha√Ænes de caract√®res et bool√©ens. Les donn√©es peuvent appartenir √† d'autres types : dates, cat√©gories ordinales (ordonn√©es : faible, moyen, √©lev√©) et nominales (non ordonn√©es : esp√®ces, cultivars, couleurs, unit√© p√©dologique, etc.). Comme mentionn√© en d√©but de chapitre, une donn√©e est une valeur associ√©e √† une variable. Les donn√©es peuvent √™tre organis√©es en collections.

Nous avons aussi vu au chapitre \@ref(chapitre-intro-a-R) que la mani√®re privil√©gi√©e d'organiser des donn√©es √©tait sous forme de **tableaux**. De mani√®re g√©n√©rale, un tableau de donn√©es est une organisation de donn√©es en deux dimensions, comportant des *lignes* et des *colonnes*. Il est pr√©f√©rable de respecter la convention selon laquelle **les lignes sont des observations et les colonnes sont des variables**. Ainsi, un tableau est une liste de vecteurs de m√™me longueur, chaque vecteur repr√©sentant une variable. Chaque variable est libre de prendre le type de donn√©es appropri√©. La position d'une donn√©e dans le vecteur correspond √† une observation. Lorsque les vecteurs sont pos√©s les uns √† c√¥t√© des autres, la position dans le vecteur devient une ligne qui d√©finit les valeurs des variables d‚Äôune observation.

Imaginez que vous consignez des donn√©es m√©t√©orologiques comme les pr√©cipitations totales ou la temp√©rature moyenne pour chaque jour, pendant une semaine sur les sites A, B et C. Chaque site poss√®de ses propres caract√©ristiques, comme la position en longitude-latitude. Il est redondant de r√©p√©ter la position du site pour chaque jour de la semaine. Vous pr√©f√©rerez cr√©er deux tableaux : un pour d√©crire vos observations, et un autre pour d√©crire les sites. De cette mani√®re, vous cr√©ez une collection de tableaux interreli√©s : une **base de donn√©es**. Nous couvrirons cette notion un peu plus loin. R peut soutirer des donn√©es des bases de donn√©es gr√¢ce au module DBI, qui n'est pas couvert √† ce stade de d√©veloppement du cours.

Dans R, les donn√©es structur√©es en tableaux, ainsi que les op√©rations sur les tableaux, peuvent √™tre g√©r√©s gr√¢ce aux modules **`readr`**, **`dplyr`** et **`tidyr`**, tous des modules faisant partie du m√©ta-module **`tidyverse`**. Mais avant de se lancer dans l'utilisation de ces modules, voyons quelques r√®gles √† suivre pour bien structurer ses donn√©es en format *tidy*, un jargon du *tidyverse* qui signifie *proprement organis√©*.

## Organiser un tableau de donn√©es

Afin de rep√©rer chaque cellule d'un tableau, on attribue √† chaque ligne et √† chaque colonne un identifiant *unique*, que l'on nomme *indice* pour les lignes et *ent√™te* pour les colonnes.

> **R√®gle no 1.** Une variable par colonne, une observation par ligne, une valeur par cellule.

Les unit√©s exp√©rimentales sont d√©crits par une ou plusieurs variables par des chiffres ou des lettres. Chaque variable devrait √™tre pr√©sente en une seule colonne, et chaque ligne devrait correspondre √† une unit√© exp√©rimentale o√π ces variables ont √©t√© mesur√©es. La r√®gle parait simple, mais elle est rarement respect√©e. Prenez par exemple le tableau suivant.

```{r tab-exwide, echo = FALSE}
extab_wide <- tibble(
  Site = c("Sainte-Souris", "Sainte-Fourmi", "Saint-Ours"),
  `Traitement A` = c(4.1, 5.8, 2.9),
  `Traitement B` = c(8.2, 5.9, 3.4),
  `Traitement C` = c(6.8, NA, 4.6)
)
knitr::kable(
  extab_wide,
  caption = "Rendements obtenus sur les sites exp√©rimentaux selon les traitements.", booktabs = TRUE
)
```

Qu'est-ce qui cloche avec ce tableau? Chaque ligne est une observation, mais contient plusieurs observations d'une m√™me variable, le rendement, qui devient √©tal√© sur plusieurs colonnes. *√Ä bien y penser*, le type de traitement est une variable et le rendement en est une autre:

```{r tab-exlong, echo = FALSE}
extab_long <- extab_wide %>%
  pivot_longer(cols = -Site, names_to = "Traitement", values_to = "Rendement")
knitr::kable(
  extab_long,
  caption = "Rendements obtenus sur les sites exp√©rimentaux selon les traitements.", booktabs = TRUE
)
```

Plus pr√©cis√©ment, l'expression *√† bien y penser* sugg√®re une r√©flexion sur la signification des donn√©es. Certaines variables peuvent parfois √™tre int√©gr√©es dans une m√™me colonne, parfois pas. Par exemple, les concentrations en cuivre, zinc et plomb dans un sol contamin√© peuvent √™tre plac√©s dans la m√™me colonne "Concentration" ou d√©clin√©es en plusieurs colonnes Cu, Zn et Pb. La premi√®re version trouvera son utilit√© pour des cr√©er des graphiques (chapitre 3), alors que la deuxi√®me favorise le traitement statistique (chapitre 5). Il est possible de passer d'un format √† l'autre gr√¢ce √† la fonction `pivot_longer()` et `pivot_wider()` du module tidyr.

> **R√®gle no 2.** Un tableau par unit√© observationnelle: ne pas r√©p√©ter les informations.

Reprenons la m√™me exp√©rience. Supposons que vous mesurez la pr√©cipitation √† l'√©chelle du site.

```{r tab-exlong-prec, echo = FALSE}
extab_long_prec <- extab_long %>%
  mutate(`Pr√©cipitations` = c(813, 813, 813, 642, 642, 642, 1028, 1028, 1028))
knitr::kable(
  extab_long_prec,
  caption = "Rendements et pr√©cipitations obtenus sur les sites exp√©rimentaux selon les traitements.", booktabs = TRUE
)
```

Segmenter l'information en deux tableaux serait pr√©f√©rable.

```{r tab-prec, echo = FALSE}
extab_wide <- tibble(
  Site = c("Sainte-Souris", "Sainte-Fourmi", "Saint-Ours"),
  `Pr√©cipitations` = c(813, 642, 1028)
)
knitr::kable(
  extab_wide,
  caption = "Pr√©cipitations sur les sites exp√©rimentaux.", booktabs = TRUE
)
```

Les tableaux \@ref(tab:tab-exlong) et \@ref(tab:tab-prec), ensemble, forment une base de donn√©es (collection organis√©e de tableaux). Les op√©rations de fusion entre les tableaux peuvent √™tre effectu√©es gr√¢ce aux fonctions de jointure (`left_join()`, par exemple) du module **`tidyr`**. Une jointure de \@ref(tab:tab-prec) vers \@ref(tab:tab-exlong) donnera le tableau \@ref(tab:tab-exlong-prec).

> **R√®gle no 3.** Ne pas bousiller les donn√©es.

Par exemple.

- *Ajouter des commentaires dans des cellules*. Si une cellule m√©rite d'√™tre comment√©e, il est pr√©f√©rable de placer les commentaires soit dans un fichier d√©crivant le tableau de donn√©es, soit dans une colonne de commentaire juxtapos√©e √† la colonne de la variable √† commenter. Par exemple, si vous n'avez pas mesure le pH pour une observation, n'√©crivez pas "√©chantillon contamin√©" dans la cellule, mais annoter dans un fichier d'explication que l'√©chantillon no X a √©t√© contamin√©. Si les commentaires sont syst√©matiques, il peut √™tre pratique de les inscrire dans une colonne `commentaire_pH`.
- *Inscription non syst√©matique*. Il arrive souvent que des cat√©gories d'une variable ou que des valeurs manquantes soient annot√©es diff√©remment. Il arrive m√™me que le s√©parateur d√©cimal soit non syst√©matique, parfois not√© par un point, parfois par une virgule. Par exemple, une fois import√©s dans votre session, les cat√©gories `St-Ours` et `Saint-Ours` seront trait√©es comme deux cat√©gories distinctes. De m√™me, les cellules correspondant √† des valeurs manquantes ne devraient pas √™tre inscrite parfois avec une cellule vide, parfois avec un point, parfois avec un tiret ou avec la mention `NA`. Le plus simple est de laisser syst√©matiquement ces cellules vides.
- *Inclure des notes dans un tableau*. La r√®gle "une colonne, une variable" n'est pas respect√©e si on ajoute des notes un peu n'importe o√π sous ou √† c√¥t√© du tableau.
- *Ajouter des sommaires*. Si vous ajoutez une ligne sous un tableau comprenant la moyenne de chaque colonne, qu'est-ce qui arrivera lorsque vous importerez votre tableau dans votre session de travail? La ligne sera consid√©r√©e comme une observation suppl√©mentaire.
- *Inclure une hi√©rarchie dans les ent√™tes*. Afin de consigner des donn√©es de texture du sol, comprenant la proportion de sable, de limon et d'argile, vous organisez votre ent√™te en plusieurs lignes. Une ligne pour la cat√©gorie de donn√©e, *Texture*, fusionn√©e sur trois colonnes, puis trois colonnes intitul√©es *Sable*, *Limon* et *Argile*. Votre tableau est joli, mais il ne pourra pas √™tre import√© conform√©ment dans un votre session de calcul : on recherche *une ent√™te unique par colonne*. Votre tableau de donn√©es devrait plut√¥t porter les ent√™tes *Texture sable*, *Texture limon* et *Texture argile*. Un conseil : r√©server le travail esth√©tique √† la toute fin d'un flux de travail.

## Formats de tableau

Plusieurs outils sont √† votre disposition pour cr√©er des tableaux. Je vous pr√©sente ici les plus communs.

### *xls* ou *xlsx*

Microsoft Excel est un logiciel de type *tableur*, ou chiffrier √©lectronique. L'ancien format *xls* a √©t√© remplac√© par le format *xlsx* avec l'arriv√©e de Microsoft Office 2010. Il s'agit d'un format propri√©taire, dont l'alternative libre la plus connue est le format *ods*, popularis√© par la suite bureautique LibreOffice. Les formats *xls*, *xlsx* ou *ods* sont davantage utilis√©s comme outils de calcul que d'entreposage de donn√©es. Ils contiennent des formules, des graphiques, du formatage de cellule, etc. *Je ne les recommande pas pour stocker des donn√©es*.

### *csv*

Le format *csv*, pour *comma separated values*, est un fichier texte, que vous pouvez ouvrir avec n'importe quel √©diteur de texte brut (Bloc note, [Atom](https://atom.io), [Notepad++](https://notepad-plus-plus.org), etc.). Chaque colonne doit √™tre d√©limit√©e par un caract√®re coh√©rent (conventionnellement une virgule, mais en fran√ßais un point-virgule ou une tabulation pour √©viter la confusion avec le s√©parateur d√©cimal) et chaque ligne du tableau est un retour de ligne. Il est possible d'ouvrir et d'√©diter les fichiers csv dans un √©diteur texte, mais il est plus pratique de les ouvrir avec des tableurs (LibreOffice Calc, Microsoft Excel, Google Sheets, etc.).

**Encodage des fichiers texte**. Puisque le format *csv* est un fichier texte, un souci particulier doit √™tre port√© sur la mani√®re dont le texte est encod√©. Les caract√®res accentu√©s pourrait √™tre import√© incorrectement si vous importez votre tableau en sp√©cifiant le mauvais encodage. Pour les fichiers en langues occidentales, l'encodage UTF-8 devrait √™tre utilis√©. Toutefois, par d√©faut, Excel utilise un encodage de Microsoft. Si le *csv* a √©t√© g√©n√©r√© par Excel, il est pr√©f√©rable de l'ouvrir avec votre √©diteur texte et de l'enregistrer dans l'encodage UTF-8.

### *json*

Comme le format *csv*, le format *json* indique un fichier en texte clair. En permettant des structures de tableaux embo√Æt√©s et en ne demandant pas que chaque colonne ait la m√™me longueur, le format *json* permet plus de souplesse que le format *csv*, mais il est plus compliqu√© √† consult√© et prend davantage d‚Äôespace sur le disque que le *csv*. Il est utilis√© davantage pour le partage de donn√©es des applications web, mais en ce qui concerne la mati√®re du cours, ce format est surtout utilis√© pour les donn√©es g√©or√©f√©renc√©es. L'encodage est g√©r√© de la m√™me mani√®re qu'un fichier *csv*.

### SQLite

SQLite est une application pour les bases de donn√©es relationnelles de type SQL qui n'a pas besoin de serveur pour fonctionner. Les bases de donn√©es SQLite sont encod√©s dans des fichiers portant l'extension *db*, qui peuvent √™tre facilement partag√©s.

### Suggestion

En *csv* pour les petits tableaux, en *sqlite* pour les bases de donn√©es plus complexes. Ce cours se concentre toutefois sur les donn√©es de type *csv*.

## Entreposer ses donn√©es

La mani√®re la plus s√©curis√©e pour entreposer ses donn√©es est de les confiner dans une base de donn√©es s√©curis√©e sur un serveur s√©curis√© dans un environnement s√©curis√© et d'encrypter les communications. C'est aussi... la mani√®re la moins accessible. Des espaces de stockage nuagiques, comme Dropbox ou d'autres [options similaires](https://alternativeto.net/software/dropbox/), peuvent √™tre pratiques pour les backups et le partage des donn√©es avec une √©quipe de travail (qui risque en retour de bousiller vos donn√©es). Le suivi de version est possible chez certains fournisseurs d'espace de stockage. Mais pour un suivi de version plus rigoureux, les espaces de d√©veloppement (comme GitHub et GitLab) sont plus appropri√©s (couverts au chapitre \@ref(chapitre-git)). Dans tous les cas, il est important de garder (1) des copies anciennes pour y revenir en cas d'erreurs et (2) un petit fichier d√©crivant les changements effectu√©s sur les donn√©es.

## Manipuler des donn√©es en mode tidyverse

Le m√©ta-module **`tidyverse`** regroupe une collection de pr√©cieux modules pour l'analyse de donn√©es en R. Il permet d'importer des donn√©es dans votre session de travail avec **`readr`**, de les explorer avec le module de visualisation **`ggplot2`**, de les transformer avec **`tidyr`** et **`dplyr`** et de les exporter avec **`readr`**. Les tableaux de classe `data.frame`, comme ceux de la plus moderne classe `tibble`, peuvent √™tre manipul√©s √† travers le flux de travail pour l'analyse et la mod√©lisation. Comme c'√©tait le cas pour le chapitre sur la visualisation, ce chapitre est loin de couvrir les nombreuses fonctionnalit√©s qui sont offertes dans le *tidyverse*.

### Importer vos donn√©es dans votre session de travail

Supposons que vous avec bien organis√© vos donn√©es en mode *tidy*. Pour les importer dans votre session et commencer √† les inspecter, vous lancerez une des commandes du module **`readr`**, d√©crites dans la documentation d√©di√©e.

- ` read_csv()` si le s√©parateur de colonne est une virgule
- ` read_csv2()` si le s√©parateur de colonne est un point-virgule et que le s√©parateur d√©cimal est une virgule
- ` read_tsv()` si le s√©parateur de colonne est une tabulation
- ` read_table()` si le s√©parateur de colonne est un espace blanc
- ` read_delim()` si le s√©parateur de colonne est un autre caract√®re (comme le point-virgule) que vous sp√©cifierez dans l'argument `delim = ";"`

Les principaux arguments sont les suivants.

- `file`: le chemin vers le fichier. Ce chemin peut aussi bien √™tre une adresse locale (data/...) qu'une adresse internet (https://...).
- `delim`: le symbole d√©limitant les colonnes dans le cas de `read_delim`.
- `col_names`: si TRUE, la premi√®re ligne est l'ent√™te du tableau, sinon FALSE. Si vous sp√©cifiez un vecteur num√©rique, ce sont les num√©ros des lignes utilis√©es pour le nom de l'ent√™te. Si vous utilisez un vecteur de caract√®res, ce sont les noms des colonnes que vous d√©sirez donner √† votre tableau.
- `na`: le symbole sp√©cifiant une valeur manquante. L'argument `na=''` signifie que les cellules vides sont des donn√©es manquantes. Si les valeurs manquantes ne sont pas uniformes, vous pouvez les indiquer dans un vecteur, par exemple `na = c("", "NA", "NaN", ".", "-")`.
- `local`: cet argument prend une fonction `local()` qui peut inclure des arguments de format de temps, mais aussi d'encodage ([voir documentation](https://readr.tidyverse.org/reference/locale.html))

D'autres arguments peuvent √™tre sp√©cifi√©s au besoin, et les r√©p√©ter ici dupliquerait l'information de la documentation de [la fonction `read_csv` de readr](https://readr.tidyverse.org/reference/read_delim.html).

Je d√©conseille d'importer des donn√©es en format xls ou xlsx. Si toutefois cela vous convient, je vous r√©f√®re au module [readxl](https://readxl.tidyverse.org/).

L'[aide-m√©moire de readr](https://github.com/rstudio/cheatsheets/raw/master/data-import.pdf) (figure \@ref(fig:tab-readr-cheatsheet)) est √† afficher pr√®s de soi.

```{r tab-readr-cheatsheet, out.width="100%", fig.align="center", fig.cap="Aide-m√©moire de readr, Source: https://github.com/rstudio/cheatsheets/raw/master/data-import.pdf", echo = FALSE}
knitr::include_graphics("images/04_data-import-cs.png")
```

Nous allons charger des donn√©es de culture de la chicout√© (*Rubus chamaemorus*), un petit fruit nordique, tir√© de Parent et al. (2013). Ouvrons d'abord le fichier pour v√©rifier les s√©parateurs de colonne et de d√©cimale (figure \@ref(fig:tab-csv)).

```{r tab-csv, out.width="100%", fig.align="center", fig.cap="Aper√ßu brut d'un fichier csv.", echo = FALSE}
knitr::include_graphics("images/04_chicoute-csv-atom.png")
```

Le s√©parateur de colonne est un point-virgule et le d√©cimal est une virgule.

Avec [Atom](https://atom.io/), mon √©diteur texte pr√©f√©r√© (il y en a d'[autres](https://alternativeto.net/software/atom/)), je vais dans Edit > Select Encoding et j'obtiens bien le UTF-8 (figure \@ref(fig:tab-csv-enc)).

```{r tab-csv-enc, out.width="100%", fig.align="center", fig.cap="Changer l'encodage d'un fichier csv.", echo = FALSE}
knitr::include_graphics("images/04_chicoute-csv-encoding.png")
```

Nous allons donc utiliser `read_csv2()` avec ses arguments par d√©faut.

```{r tab-read-csv}
library("tidyverse")
chicoute <- read_csv2("data/chicoute.csv")
```

Quelques commandes utiles inspecter le tableau:

- `head()` pr√©sente l'ent√™te du tableau, soit ses 6 premi√®res lignes
- `str()` et `glimpse()` pr√©sentent les variables du tableau et leur type - `glimpse()`est la fonction tidyverse et `str()` est la fonction classique (je pr√©f√®re `str()`)
- `summary()` pr√©sente des statistiques de base du tableau
- `names()` ou `colnames()` sort les noms des colonnes sous forme d'un vecteur
- `dim()` donne les dimensions du tableau, `ncol()` son nombre de colonnes et `nrow()` son nombre de lignes
- `skim` est une fonction du module skimr montrant un portrait graphique et num√©rique du tableau

**Extra 1**. Plusieurs modules ne se trouvent pas dans les d√©p√¥ts CRAN, mais sont disponibles sur GitHub. Pour les installer, installez d'abord le module devtools disponible sur CRAN. Vous pourrez alors installer les packages de GitHub comme on le fait avec le package skimr.

**Extra 2**. Lorsque je d√©sire utiliser une fonction, mais sans charger le module dans la session, j'utilise la notation `module::fonction`. Comme dans ce cas, pour skimr.

```{r tab-skimr}
# devtools::install_github("ropenscilabs/skimr")
skimr::skim(chicoute)
```

**Exercice**. Inspectez le tableau.

### Comment s√©lectionner et filtrer des donn√©es ?

On utilise le terme *s√©lectionner* lorsque l'on d√©sire choisir une ou plusieurs lignes et colonnes d'un tableau (la plupart du temps des colonnes). L'action de *filtrer* signifie de s√©lectionner des lignes selon certains crit√®res.

#### S√©lectionner

Voici 4 mani√®res de s√©lectionner une colonne en R.

- Une m√©thode rapide mais peu expressive consiste √† indiquer les valeurs num√©riques de l'indice de la colonne entre des crochets. Il s'agit d'appeler le tableau suivit de crochets. L'int√©rieur des crochets comprend deux √©l√©ments s√©par√©s par une virgule. Le premier √©l√©ment sert √† filtrer selon l'indice, le deuxi√®me sert √† s√©lectionner selon l'indice. Ainsi:
  - `chicoute[, 1]`: s√©lectionner la premi√®re colonne
  - `chicoute[, 1:10]`: s√©lectionner les 10 premi√®res colonnes
  - `chicoute[, c(2, 4, 5)]`: s√©lectionner les colonnes 2, 4 et 5
  - `chicoute[c(10, 13, 20), c(2, 4, 5)]`: s√©lectionner les colonnes 2, 4 et 5 et les lignes 10, 13 et 20.
- Une autre m√©thode rapide, mais plus expressive, consiste √† appeler le tableau, suivi du symbole `$`, puis le nom de la colonne, e.g. `chicoute$Site`.
- Ou bien d'inscrire le nom de la colonne, ou du vecteur des colonnes, entre des crochets suivant le nom du tableau, c'est-√†-dire `chicoute[c("Site", "Latitude_m", "Longitude_m")]`.
- Enfin, dans une s√©quence d'op√©rations en mode pipeline (chaque op√©ration est mise √† la suite de la pr√©c√©dente en pla√ßant le *pipe* `%>%` entre chacune), il peut √™tre pr√©f√©rable de s√©lectionner des colonnes avec la fonction `select()`, i.e.

```
chicoute %>%
  select(Site, Latitude_m, Longitude_m)
```

> **Truc**. La plupart des IDE, comme RStudio, peuvent vous proposer des colonnes dans une liste. Apr√®s avoir entrer le `$`, taper sur la touche de tabulation: vous pourrez s√©lectionner la colonne dans une liste d√©filante (figure \@ref(fig:tab-auto-complete)).

```{r tab-auto-complete, fig.align="center", fig.cap="Autocompl√©tion dans RStudio.", echo = FALSE}
knitr::include_graphics("images/04_auto-complete-cols.png")
```

La fonction `select()` permet aussi de travailler en exclusion. Ainsi pour enlever des colonnes, on placera un `-` (signe de soustraction) devant le nom de la colonne.

‚ö†Ô∏è **Attention**. Plusieurs modules utilisent la fonction `select` (et `filter`, plus bas). Lorsque vous lancez `select` et que vous obtenez un message d'erreur comme 

```
Error in select(., ends_with("pourc")) : 
  argument inutilis√© (ends_with("pourc"))
```

il se pourrait bien que R utilise la fonction `select` d'un autre module. Pour sp√©cifier que vous d√©sirez la fonction `select` du module **`dplyr`**, sp√©cifiez `dplyr::select`.

D'autre arguments de `select()` permettent une s√©lection rapide. Par exemple, pour obtenir les colonnes contenant des pourcentages:

```{r tab-pipe-select}
chicoute %>%
  select(ends_with("pourc")) %>%
  head(3)
```

#### Filtrer

Comme c'est le cas de la s√©lection, on pourra filtrer un tableau de plusieurs mani√®res. J'ai d√©j√† pr√©sent√© comment filtrer selon les indices des lignes. Les autres mani√®res reposent n√©anmoins sur une op√©ration logique `==`, `<`, `>` ou `%in%` (le %in% signifie *se trouve parmi* et peut √™tre suivi d'un vecteur de valeur que l'on d√©sire accepter).

Les conditions bool√©ennes peuvent √™tre combin√©es avec les op√©rateurs *et*,  `&`, et *ou*, `|`. Pour rappel,

| Op√©ration | R√©sultat |
| --------- | -------- |
| Vrai **et** Vrai | Vrai |
| Vrai **et** Faux | Faux |
| Faux **et** Faux | Faux |
| Vrai **ou** Vrai | Vrai |
| Vrai **ou** Faux | Vrai |
| Faux **ou** Faux | Faux |

- La m√©thode classique consiste √† appliquer une op√©ration logique entre les crochets, par exemple `chicoute[chicoute$CodeTourbiere == "BEAU", ]`
- La m√©thode *tidyverse*, plus pratique en mode pipeline, passe par la fonction `filter()`, i.e.

```
chicoute %>%
  filter(CodeTourbiere == "BEAU")
```

Combiner le tout.

```{r tab-pipe-filter}
chicoute %>%
  filter(Ca_pourc < 0.4 & CodeTourbiere %in% c("BEAU", "MB", "WTP")) %>%
  select(contains("pourc"))
```

### Le format long et le format large

Dans le tableau `chicoute`, chaque √©l√©ment poss√®de sa propre colonne. Si l'on voulait mettre en graphique les boxplot des facettes de concentrations d'azote, de phosphore et de potassium dans les diff√©rentes tourbi√®res, il faudrait obtenir une seule colonne de concentrations.

Pour ce faire, nous utiliserons la fonction `pivot_longer()`. L'argument obligatoire (excluant le tableau, qui est implicite dans la cha√Æne d'op√©rations), est `cols`, le nom des colonnes √† allonger. Pour obtenir des noms de colonnes allong√©es personnalis√©es, on sp√©cifie le nom des variables consistant aux anciens noms de colonnes avec `names_to` et celui de la nouvelle colonne contenant les valeurs dans `values_to`. La suite consiste √† d√©crire les colonnes √† inclure ou √† exclure. Dans le cas qui suit, j'exclue CodeTourbiere de la refonte j'utilise `sample_n()` pour pr√©senter un √©chantillon du r√©sultat. Notez la ligne comprenant la fonction `mutate`, que l'on verra plus loin. Cette fonction ajoute une colonne au tableau. Dans ce cas-ci, j'ajoute une colonne constitu√©e d'une s√©quence de nombres allant de 1 au nombre de lignes du tableau (il y en a 90). Cet identifiant unique pour chaque ligne permettra de reconstituer par la suite le tableau initial.

```{r tab-longer}
chicoute_long <- chicoute %>%
  select(CodeTourbiere, N_pourc, P_pourc, K_pourc) %>%
  mutate(ID = 1:nrow(.)) %>% # mutate ajoute une colonne au tableau
  pivot_longer(cols = contains("pourc"), names_to = "nutrient", values_to = "concentration")
chicoute_long %>% sample_n(10)
```

L'op√©ration inverse est `pivot_wider()`, avec laquelle nous s√©lectionnons une colonne sp√©cifiant les nouvelles colonnes √† construire (`names_from`) ainsi que les valeurs √† placer dans ces colonnes (`values_from`).

```{r tab-wider}
chicoute_large <- chicoute_long %>%
  pivot_wider(names_from = nutrient, values_from = concentration)
chicoute_large %>% sample_n(10)
```

### Combiner des tableaux

Nous avons introduit plus haut la notion de base de donn√©es. Nous voudrions peut-√™tre utiliser le code des tourbi√®res pour inclure leur nom, le type d'essai men√© √† ces tourbi√®res, etc. Importons d'abord le tableau des noms li√©s aux codes.

```{r tab-read-tourbieres}
tourbieres <- read_csv2("data/chicoute_tourbieres.csv")
tourbieres
```

Notre information est organis√©e en deux tableaux, li√©s par la colonne `CodeTourbiere`. Comment fusionner l'information pour qu'elle puisse √™tre utilis√©e dans son ensemble?  La fonction `left_join` effectue cette op√©ration typique avec les bases de donn√©es.

```{r tab-left-join}
chicoute_merge <- left_join(x = chicoute, y = tourbieres, by = "CodeTourbiere")
# ou bien chicoute %>% left_join(y = tourbieres, by = "CodeTourbiere")
chicoute_merge %>% sample_n(4)
```

D'autres types de jointures sont possibles, et d√©crites en d√©tails dans la [documentation](https://dplyr.tidyverse.org/reference/join.html).

[Garrick Aden-Buie](https://www.garrickadenbuie.com/) a pr√©par√© de [jolies animations](https://gist.github.com/gadenbuie/077bcd2700ac1241c65c324581a9f619) pour d√©crire les diff√©rents types de jointures.

`left_join(x, y)` colle y √† x seulement ce qui dans y correspond √† ce que l'on trouve dans x.

![](images/04_animated-left-join.gif)

`right_join(x, y)` colle y √† x seulement ce qui dans x correspond √† ce que l'on trouve dans y.

![](images/04_animated-right-join.gif) 

`inner_join(x, y)` colle x et y en excluant les lignes o√π au moins une variable de joint est absente dans x et y.

![](images/04_animated-inner-join.gif) 

`full_join(x, y)`garde toutes les lignes et les colonnes de x et y.

![](images/04_animated-full-join.gif) 

### Op√©rations sur les tableaux

Les tableaux peuvent √™tre segment√©s en √©l√©ments sur lesquels on calculera ce qui nous chante.

On pourrait vouloir obtenir :

- la somme avec la function `sum()`
- la moyenne avec la function `mean()` ou la m√©diane avec la fonction `median()`
- l'√©cart-type avec la function `sd()`
- les maximum et minimum avec les fonctions `min()` et `max()`
- un d√©compte d‚Äôoccurrence avec la fonction `n()` ou `count()`

Par exemple,

```{r tab-mean-col}
mean(chicoute$Rendement_g_5m2, na.rm = TRUE)
```

**En mode classique**, pour effectuer des op√©rations sur des tableaux, on utilisera la fonction `apply()`. Cette fonction prend, comme arguments, le tableau, l'axe (op√©ration par ligne = 1, op√©ration par colonne = 2), puis la fonction √† appliquer.

```{r tab-op-apply-col}
apply(chicoute %>% select(contains("pourc")), 2, mean)
```

Les op√©rations peuvent aussi √™tre effectu√©es par ligne, par exemple une somme (je garde seulement les 10 premiers r√©sultats).

```{r tab-op-apply-row}
apply(chicoute %>% select(contains("pourc")), 1, sum)[1:10]
```

La fonction √† appliquer peut √™tre personnalis√©e, par exemple:

```{r tab-aop-apply-custom}
apply(
  chicoute %>% select(contains("pourc")), 2,
  function(x) (prod(x))^(1 / length(x))
)
```

Vous reconnaissez cette fonction? C'√©tait la moyenne g√©om√©trique (la fonction `prod()` √©tant le produit d'un vecteur).

**En mode tidyverse**, on aura besoin principalement des fonction suivantes:

* `group_by()` pour effectuer des op√©rations par groupe, l‚Äôop√©ration `group_by()` s√©pare le tableau en plusieurs petits tableaux, en attendant de les recombiner. C'est un peu l'√©quivalent des facettes avec le module de visualisation ggplot2, que nous explorons au chapitre \@ref(chapitre-visualisation).
* `summarise()` pour r√©duire plusieurs valeurs en une seule, il applique un calcul sur le tableau ou s'il y a lieu sur chaque petit tableau segment√©. Il en existe quelques variantes.
+ `summarise_all()` applique la fonction √† toutes les colonnes
+ `summarise_at()` applique la fonction aux colonnes sp√©cifi√©es
+ `summarise_if()` applique la fonction aux colonnes qui ressortent comme `TRUE` selon une op√©ration bool√©enne
* `mutate()` pour ajouter une nouvelle colonne
+ Si l'on d√©sire ajouter une colonne √† un tableau, par exemple le sommaire calcul√© avec `summarise()`. √Ä l'inverse, la fonction `transmute()` retournera seulement le r√©sultat, sans le tableau √† partir duquel il a √©t√© calcul√©. De m√™me que `summarise()`, `mutate()` et `transmute()` poss√®dent leurs √©quivalents `_all()`, `_at()` et `_if()`.
* `arrange()` pour r√©ordonner le tableau
+ On a d√©j√† couvert `arrange()` dans le chapitre 3. Rappelons que cette fonction n'est pas une op√©ration sur un tableau, mais plut√¥t un changement d'affichage en changeant l'ordre d'apparition des donn√©es.

Ces op√©rations sont d√©crites dans l'aide-m√©moire [*Data Transformation Cheat Sheet*](https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf) (figure \@ref(fig:tab-trans-cs))).

```{r tab-trans-cs, fig.align="center", fig.cap="Aide-m√©moire pour la transformation des donn√©es, https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf", echo = FALSE}
knitr::include_graphics("images/04_data-transformation-cs.png")
```

Pour effectuer des statistiques par colonne, on utilisera `summarise` pour des statistiques effectu√©es sur une seule colonne. `summarise` peut prendre le nombre d√©sir√© de statistiques dont la sortie est un scalaire.

```{r tab-summarise}
chicoute %>%
  summarise(
    moyenne = mean(TotalFloral_nombre_m2, na.rm = TRUE),
    ecart_type = sd(TotalFloral_nombre_m2, na.rm = TRUE)
  )
```

Si l'on d√©sire un sommaire sur toutes les variables s√©lectionn√©es, on utilisera `summarise_all()`. Pour sp√©cifier que l'on d√©sire la moyenne et l'√©cart-type on inscrit les noms des fonctions dans `list()`.

```{r tab-summ-list}
chicoute %>%
  select(contains("pourc")) %>%
  summarise_all(list(mean, sd))
```

On utilisera `group_by()` pour segmenter le tableau, et ainsi obtenir des statistiques pour chaque groupe.

```{r tab-summ-groupby}
chicoute %>%
  group_by(CodeTourbiere) %>%
  summarise(
    moyenne = mean(TotalFloral_nombre_m2, na.rm = TRUE),
    ecart_type = sd(TotalFloral_nombre_m2, na.rm = TRUE)
  )
```

Dans le cas de `summarise_all`, les r√©sultats s'affichent de la m√™me mani√®re.

```{r tab-summall-groupby}
chicoute %>%
  group_by(CodeTourbiere) %>%
  select(N_pourc, P_pourc, K_pourc) %>%
  summarise_all(list(mean, sd))
```

Pour obtenir des statistiques √† chaque ligne, mieux vaut utiliser `apply()`, tel que vu pr√©c√©demment. Le point, `.`, repr√©sente le tableau dans une fonction qui n'a pas √©t√© con√ßu pour fonctionner de facto avec **`dplyr`**.

```{r tab-pipe-apply}
chicoute %>%
  select(contains("pourc")) %>%
  apply(., 1, sum)
```

Prenons ce tableau des esp√®ces menac√©es issu de l'Union internationale pour la conservation de la nature [distribu√©es par l'OCDE](https://stats.oecd.org/Index.aspx?DataSetCode=WILD_LIFE).

```{r tab-read-species, warning=FALSE}
library("tidyverse")
especes_menacees <- read_csv("data/WILD_LIFE_14012020030114795.csv")
```

Nous ex√©cutons le pipeline suivant.

```{r tab-spec-top10}
especes_menacees %>%
  dplyr::filter(IUCN == "CRITICAL", SPEC == "VASCULAR_PLANT") %>%
  dplyr::select(Country, Value) %>%
  dplyr::group_by(Country) %>%
  dplyr::summarise(n_critical_plants = sum(Value)) %>%
  dplyr::arrange(desc(n_critical_plants)) %>%
  dplyr::top_n(10)
```

Ce pipeline consiste √†:

```
prendre le tableau especes_menacees, puis
  filtrer pour n'obtenir que les esp√®ces critiques dans la cat√©gorie des plantes vascularies, puis
  s√©lectionner les colonnes des pays et des valeurs (nombre d'esp√®ces), puis
  segmenter le tableaux en plusieurs tableaux selon le pays, puis
  appliquer la fonction sum pour chacun de ces petits tableaux (puis de recombiner ces sommaires), puis
  trier les pays en nombre d√©croissant de d√©compte d'esp√®ces, puis
  afficher le top 10
```

### Exemple (difficile)

Pour revenir √† notre tableau `chicoute`, imaginez que vous aviez une station m√©t√©o (station_A) situ√©e aux coordonn√©es (490640, 5702453) et que vous d√©siriez calculer la distance entre l'observation et la station. Prenez du temps pour r√©fl√©chir √† la mani√®re dont vous proc√©derez... 

On pourra cr√©er une fonction qui mesure la distance entre un point x, y et les coordonn√©es de la station A...

```{r tab-stations-f}
dist_station_A <- function(x, y) {
  return(sqrt((x - 490640)^2 + (y - 5702453)^2))
}
```

... puis ajouter une colonne avec mutate gr√¢ce √† une fonction prenant les arguments x et y sp√©cifi√©s.

```{r tab-stations-mutate}
chicoute %>%
  mutate(dist = dist_station_A(x = Longitude_m, y = Latitude_m)) %>%
  select(ID, CodeTourbiere, Longitude_m, Latitude_m, dist) %>%
  top_n(10)
```

Nous pourrions proc√©der de la m√™me mani√®re pour fusionner des donn√©es climatiques. Le tableau `chicoute` ne poss√®de pas d'indicateurs climatiques, mais il est possible de les soutirer de stations m√©t√©o plac√©es pr√®s des sites. Ces donn√©es ne sont pas disponibles pour le tableau de la chicout√©, alors j'utiliserai des donn√©es fictives pour l'exemple.

Voici ce qui pourrait √™tre fait.

1. Cr√©er un tableau des stations m√©t√©o ainsi que des indices m√©t√©orologiques associ√©s √† ces stations.
2. Lier chaque site √† une station (√† la main o√π selon la plus petite distance entre le site et la station).
3. Fusionner les indices climatiques aux sites, puis les sites aux mesures de rendement.

Ces op√©rations demandent habituellement du t√¢tonnement. Il serait surprenant que m√™me une personne exp√©riment√©e soit en mesure de compiler ces op√©rations sans obtenir de message d'erreur, et retravailler jusqu'√† obtenir le r√©sultat souhait√©. L'objectif de cette section est de vous pr√©sent√© un flux de travail que vous pourriez √™tre amen√©s √† effectuer et de fournir quelques √©l√©ments nouveaux pour mener √† bien une op√©ration. Il peut √™tre frustrant de ne pas saisir toutes les op√©rations: passez √† travers cette section sans jugement. Si vous devez vous frotter √† probl√®me semblable, vous saurez que vous trouverez dans ce manuel une recette int√©ressante.

```{r tab-mes-stations}
mes_stations <- data.frame(
  Station = c("A", "B", "C"),
  Longitude_m = c(490640, 484870, 485929),
  Latitude_m = c(5702453, 5701870, 5696421),
  t_moy_C = c(13.8, 18.2, 16.30),
  prec_tot_mm = c(687, 714, 732)
)
mes_stations
```

La fonction suivante calcule la distance entre des coordonn√©es x et y et chaque station d'un tableau de stations, puis retourne le nom de la station dont la distance est la moindre.

```{r tab-stations-dist-f}
dist_station <- function(x, y, stations_df) {
  # stations est le tableau des stations √† trois colonnes
  # 1iere: nom de la station
  # 2ieme: longitude
  # 3ieme: latitude
  distance <- c()
  for (i in 1:nrow(stations_df)) {
    distance[i] <- sqrt((x - stations_df[i, 2])^2 + (y - stations_df[i, 3])^2)
  }
  nom_station <- as.character(stations_df$Station[which.min(distance)])
  return(nom_station)
}
```

Testons la fonction avec des coordonn√©es.

```{r tab-stations-calc-dist}
dist_station(x = 459875, y = 5701988, stations_df = mes_stations)
```

Nous appliquons cette fonction √† toutes les lignes du tableau, puis en retournons un √©chantillon.

```{r tab-stations-rowwise}
chicoute %>%
  rowwise() %>%
  mutate(Station = dist_station(x = Longitude_m, y = Latitude_m, stations_df = mes_stations)) %>%
  select(ID, CodeTourbiere, Longitude_m, Latitude_m, Station) %>%
  sample_n(10)
```

Cela semble fonctionner. On peut y ajouter un `left_join()` pour joindre les donn√©es m√©t√©o au tableau principal.

```{r tab-stations-leftjoin}
chicoute_weather <- chicoute %>%
  rowwise() %>%
  mutate(Station = dist_station(x = Longitude_m, y = Latitude_m, stations_df = mes_stations)) %>%
  left_join(y = mes_stations, by = "Station")
chicoute_weather %>% sample_n(10)
```

### Exporter un tableau

Simplement avec `write_csv()`.

```{r tab-export}
write_csv(chicoute_weather, "data/chicoute_weather.csv")
```

### Aller plus loin dans le tidyverse

Le livre [R for Data Science](http://r4ds.had.co.nz), de Hadley Wickham et Garrett Grolemund (couverture √† la figure \@ref(fig:tab-wickham-book)), est un incontournable.

```{r tab-wickham-book, out.width="30%", fig.align="center", fig.cap="Couverture du libre de Hadley Wickham et Garrett Grolemund, Source: https://r4ds.had.co.nz", echo = FALSE}
knitr::include_graphics("images/04_wickhamcover.png")
```

## R√©f√©rences

Parent L.E., Parent, S.√â., Herbert-Gentile, V., Naess, K. et  Lapointe, L. 2013. Mineral Balance Plasticity of Cloudberry (Rubus chamaemorus) in Quebec-Labrador Bogs. American Journal of Plant Sciences, 4, 1508-1520. DOI: 10.4236/ajps.2013.47183


```{r, include=FALSE}
rm(list = ls())
```

<!--chapter:end:03_tableaux.Rmd-->

--- 
site: bookdown::bookdown_site
output: bookdown::gitbook 
---

# Visualisation {#chapitre-visualisation}

 ***
Ô∏è\ **Objectifs sp√©cifiques**:

√Ä la fin de ce chapitre, vous

- comprendrez l'importance de l'exploration des donn√©es
- comprendrez les guides g√©n√©raux pour cr√©er un graphique appropri√©
- comprendrez la diff√©rence entre les modes imp√©ratifs et d√©claratifs pour la cr√©ation de graphique
- serez en mesure de cr√©er des nuages de points, lignes, histogrammes, diagrammes en barres et boxplots en R
- saurez exporter un graphique en vue d'une publication

 ***

Reconnaissez-vous cette image (figure \@ref(fig:vis-hockey-stick))?

```{r vis-hockey-stick, out.width="100%", fig.align="center", fig.cap="Le b√¢ton de hocket de Mann et al. (2001) Source: [GIEC, Bilan 2001 des changements climatiques : Les √©l√©ments scientifiques](https://www.ipcc.ch/pdf/climate-changes-2001/scientific-basis/scientific-spm-ts-fr.pdf))", echo = FALSE}
knitr::include_graphics("images/03_mann-hockey-stick-2001.png")
```

Elle a √©t√© con√ßue par Michael E. Mann, Raymond S. Bradley et Malcolm K. Hughes. Le graphique montre l'√©volution des temp√©ratures en ¬∞C normalis√©es selon la temp√©rature moyenne entre 1961 et 1990 sur l'axe des Y en fonction du temps, sur l'axe des X. On le connait aujourd'hui comme le *b√¢ton de hockey*, et on reconnait son r√¥le cl√© pour sensibiliser la civilisation enti√®re face au r√©chauffement global.

Cr√©er des graphiques est une t√¢che courante dans un flux de travail en science. Un graphique bien con√ßu est dense en information. La visualisation des donn√©es permet d'explorer des tableaux jusqu'√† cr√©er des √©l√©ments visuels vou√©s √† la publication, dont l'information serait autrement difficile, voire impossible √† transmettre ad√©quatement.

## Pourquoi explorer graphiquement?

La plupart des graphiques que vous g√©n√©rerez ne seront pas destin√©s √† √™tre publi√©s. Ils viseront probablement d'abord √† explorer des donn√©es. Cela vous permettra de mettre en √©vidence de nouvelles perspectives.

Prenons par exemple deux variables, $X$ et $Y$. Vous calculez leur moyenne, √©cart-type et la corr√©lation entre les deux variables (nous verrons les statistiques en plus de d√©tails dans un prochain chapitre).

```{r vis-import-datasaurus, message=FALSE}
library("tidyverse")
datasaurus <- read_tsv("data/DatasaurusDozen.tsv")

cor_datasaurus <- datasaurus %>%
  group_by(dataset) %>%
  summarise(cor = cor(x = x, y = y, method = "pearson"))

datasaurus %>%
  group_by(dataset) %>%
  summarise_all(list(mean = mean, sd = sd)) %>%
  left_join(cor_datasaurus, by = "dataset")
```

Les moyennes, √©carts-types et corr√©lations sont √† peu pr√®s les m√™mes pour tous les groupes. Peut-on conclure que tous les groupes sont semblables? Pas encore.

Pour d√©montrer que ces statistiques ne vous apprendront pas grand chose sur la structure des donn√©es, [Matejka et Fitzmaurice (2017)](https://www.autodeskresearch.com/publications/samestats) ont g√©n√©r√© 12 jeux de donn√©es $X$ et $Y$, ayant chacun pratiquement les m√™mes statistiques. Mais avec des structures bien diff√©rentes (figure \@ref(fig:vis-datasaurus-gif))!

```{r vis-datasaurus-gif, out.width="100%", fig.align="center", fig.cap="Animation montrant la progression du jeu de donn√©es *Datasaurus* pour toutes les formes vis√©es. Source: [Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing](https://www.autodeskresearch.com/publications/samestats)", echo = FALSE}
knitr::include_graphics("images/04_DinoSequentialSmaller.gif")
```

## Publier un graphique

Vous voil√† sensibilis√© √† l'importance d'explorer les donn√©es graphiquement. Mais ce qui ultimement √©manera d'un projet sera le rapport que vous d√©poserez, l'article scientifique que vous ferez publier ou le billet de blogue que vous partagerez su les r√©seaux sociaux. Les graphiques inclus dans vos publications m√©ritent une attention particuli√®re pour que votre audience puisse comprendre les d√©couvertes et perspectives offertes par vos travaux. Pour ce faire, un graphique doit r√©pondre honn√™tement √† la question pos√©e tout en √©tant attrayant.

### Cinq qualit√©s d'un bon graphique

Alberto Cairo, chercheur sp√©cialis√© en visualisation de donn√©es, a fait para√Ætre en 2016 le livre [*The Truthful art*](http://www.thefunctionalart.com/p/the-truthful-art-book.html), note cinq qualit√©s d'une visualisation bien con√ßue (les citations de cette section proviennent de ma traduction de Alberto Cairo, *The Truthful Art* (2016), p. 45.).

> 1- **Elle est v√©ritable**, puisqu'elle est bas√©e sur une recherche exhaustive et honn√™te.

Cela vaut autant pour les graphiques que pour l'analyse de donn√©es. Il s'agit froidement de **pr√©senter les donn√©es selon l'interpr√©tation la plus exacte**. Les pi√®ges √† √©viter sont le *picorage de cerises* et la *surinterpr√©tation des donn√©es*. Le *picorage*, c'est lorsqu'on r√©duit les perspectives afin de soutenir un argumentaire. Par exemple, retirer des donn√©es d'une r√©gion ou d'une d√©cennie qui rendraient factice une conclusion fix√©e *a priori*. Ceci vaut autant pour les graphiques que pour les statistiques (nous parlerons du p-hacking au prochain chapitre). La *surinterpr√©tation*, c'est lorsque l'on saute rapidement aux conclusions: par exemple, que l'on g√©n√®re des corr√©lations, voire m√™me des relations de causalit√©s √† partir de ce qui n'est que du bruit de fond. √Ä ce titre, lors [d'une conf√©rence](https://youtu.be/uw1Tag08dK4), Heather Krause insiste sur l'importance de faire en sorte que les repr√©sentations graphiques r√©pondent correctement aux questions pos√©es dans une √©tude (figure \@ref(fig:vis-conference-2018)).

```{r vis-conference-2018, out.width="100%", fig.align="center", fig.cap="The F word: Protect your work from four hidden fallacies when working with data, une [conf√©rence de Heather Krause, 2018](https://youtu.be/uw1Tag08dK4)", echo = FALSE}
knitr::include_graphics("images/03_Heather-Krause_youtube_.png")
```

> 2- **Elle est fonctionnelle**, puisqu'elle constitue une repr√©sentation pr√©cise des donn√©es, et qu'elle est construite de mani√®re √† laisser les observateurs.trices prendre des initiatives cons√©quentes.

"La seule chose qui est pire qu'un diagramme en pointe de tarte, c'est d'en pr√©senter plusieurs" (Edward Tufte, designer, cit√© par Alberto Cairo, 2016, p. 50). Choisir le bon graphique pour repr√©senter vos donn√©es est beaucoup moins une question de bon go√ªt qu'**une question de d√©marche rationnelle sur l'objectif vis√© par la pr√©sentation d'un graphique**. Je pr√©senterai des lignes guides pour s√©lectionner le type de graphique qui pr√©sentera vos donn√©es de mani√®re fonctionnelle en fonction de l'objectif d'un graphique (d'ailleurs, avez-vous vraiment besoin d'un graphique?).

> 3- **Elle est attrayante** et intrigante, et m√™me esth√©tiquement plaisante pour l'audience vis√©e - les scientifiques d'abord, mais aussi le public en g√©n√©ral.

En sciences naturelles, la pens√©e rationnelle, la capacit√© √† organiser la connaissance et cr√©er de nouvelles avenues sont des qualit√©s qui sont privil√©gi√©es au talent artistique. **Que vous ayez o√π non des aptitudes en art visuel, pr√©sentez de l'information, pas des d√©corations**. Excel vous permet d'ajouter une perspective 3D √† un diagramme en barres. La profondeur contient-elle de l'information? Non. Cette d√©coration ne fait qu'ajouter de la confusion. Minimalisez, fournissez le plus d'information possible avec le moins d'√©l√©ments graphiques possibles. C'est ce que vous proposent les guides graphiques que j'introduirai plus loin.

> 4- **Elle est pertinente**, puisqu'elle r√©v√®le des √©vidences scientifiques autrement difficilement accessibles.

Il s'agit de susciter un *eur√™ka*, dans le sens qu'**elle g√©n√®re une id√©e, et parfois une initiative, en un coup d‚Äô≈ìil**. Le graphique en b√¢ton de hockey est un exemple o√π l'on a spontan√©ment une id√©e de la situation. Cette situation peut √™tre la pr√©sence d'un ph√©nom√®ne comme l'augmentation de la temp√©rature globale, mais aussi l'absence de ph√©nom√®nes pourtant attendus.

> 5- **Elle est instructive**, parce que si l'on saisit et accepte les √©vidences scientifiques qu'elle d√©crit, cela changera notre perception pour le mieux.

En pr√©sentant cette qualit√©, Alberto Cairo voulait insister ses lecteurs.trices √† choisir des sujets de discussion visuelle de mani√®re √† participer √† un monde meilleur. En ce qui nous concerne, il s'agit de bien **s√©lectionner l'information que l'on d√©sire transmettre**. Imaginez que vous avez travaill√© quelques jours pour cr√©er un graphique, sont vous √™tes fier, mais vous (ou un coll√®gue hi√©rarchiquement favoris√©) vous rendez compte que le graphique soutient peu ou pas le propos ou l'objectif de votre th√®se/m√©moire/rapport/article. Si c'est bien le cas, vous feriez mieux de laisser tomber votre oeuvre et consid√©rer votre d√©marche comme une occasion d'apprentissage.

Alberto Cairo r√©sume son livre *The Truthful Art* dans [une entrevue avec le National Geographic](http://news.nationalgeographic.com/2015/10/151016-data-points-alberto-cairo-interview/).

## Choisir le type de graphique le plus appropri√©

De nombreuses mani√®res de pr√©senter les donn√©es sont courrament utilis√©es, comme les nuages de point, les lignes, les histogrammes, les diagrammes en barre et en pointe de tarte. Les principaux types de graphique seront couverts dans ce chapitre. D'autres types sp√©cialis√©s seront couverts dans les chapitres appropri√©s (graphiques davantage orient√©s vers les statistiques, les biplots, les dendrogrammes, les diagrammes ternaires, les cartes, etc.).

La visualisation de donn√©es est aujourd‚Äôhui devenue un m√©tier pour plusieurs personnes ayant des affinit√©s pour la science, les arts et la communication, dont certaines partagent leur expertise sur le web. √Ä ce titre, le site [*from data to viz*](https://www.data-to-viz.com/) est √† conserver dans vos marques-page. Il comprend des arbres d√©cisionnels qui vous guident vers les options appropri√©es pour pr√©senter vos donn√©es, puis fournissent des exemples pour produire ces visualisations en R. √âgalement, je sugg√®re le [site internet de Ann K. Emery](https://annkemery.com/essentials/), qui pr√©sente des lignes guide pour pr√©senter le graphique ad√©quat selon les donn√©es en main. De nombreuses recettes sont √©galement propos√©es sur [r-graph-gallery.com](https://www.r-graph-gallery.com/). En ce qui a trait aux couleurs, le choix n'est pas anodin. Si vous avez le souci des d√©tails sur les √©l√©ments esth√©tiques de vos graphiques, je recommande la lecture de ce billet de blog de [Lisa Charlotte Rost](https://blog.datawrapper.de/colors/).

Retenez n√©anmois que *La couleur est une information*. Les couleurs devraient √™tre s√©lectionn√©es d'abord pour √™tre lisibles par les personnes ne percevant pas les couleurs (figure \@ref(fig:vis-colorbrewer)), selon le support (apte √† √™tre photocopi√©, lisible √† l'√©cran, lisible sur des documents imprim√©s en noir et blanc) et selon le type de donn√©es.

- Donn√©es continues ou cat√©gorielles ordinales: gradient (transition graduelle d'une couleur √† l'autre), s√©quence (transition saccad√©e selon des groupes de donn√©es continues) ou divergentes (transition saccad√©e d'une couleur √† l'autre vers des couleurs divergentes, par exemple orange vers blanc vers bleu).
- Donn√©es cat√©gorielles nominales: couleurs √©loign√©es d'une cat√©gorie √† une autre (plus il y a de cat√©gories, plus les couleurs sont susceptibles de se ressembler).

```{r vis-colorbrewer, out.width="30%", fig.align="center", fig.cap="Capture d'√©cran de [colorbrewer2.org](http://colorbrewer2.org), qui propose des palettes de couleurs pour cr√©er des cartes, mais l'information est pertinente pour tout type de graphique.", echo = FALSE}
knitr::include_graphics("images/03_colorbrewer2.png")
```

Le [Financial Times](https://ft.com/vocabulary) offre √©galement ce guide visuel (figure \@ref(fig:vis-guide-ft)).

```{r vis-guide-ft, out.width="100%", fig.align="center", fig.cap="Guide de s√©lection de grapjhique du [Financial Times](https://ft.com/vocabulary)", echo = FALSE}
knitr::include_graphics("images/03_poster.png")
```

Cairo (2016) propose de proc√©der avec ces √©tapes:

1. R√©fl√©chissez au message que vous d√©sirez transmettre: comparer les cat√©gories $A$ et $B$, visualiser une transition ou un changement de $A$ vers $B$, pr√©senter une relation entre $A$ et $B$ ou la distribution de $A$ et $B$ sur une carte.
1. Essayez diff√©rentes repr√©sentations: si le message que vous d√©sirez transmettre a plusieurs volets, il se pourrait que vous ayez besoin de plus d'un graphique.
1. Mettez de l'ordre dans vos donn√©es. C'√©tait le sujet du chapitre \@ref(chapitre-tableaux).
1. Testez le r√©sultat. "H√©, qu'est-ce que tu comprends de cela?" Si la personne hausse les √©paules, il va falloir r√©√©valuer votre strat√©gie.

## Choisir son outil de visualisation

Les modules et logiciels de visualisation sont bas√©s sur des approches que l'on pourrait placer sur un spectre allant de l'imp√©ratif au d√©claratif.

### Approche imp√©rative

Selon cette approche, vous indiquez comment placer l'information dans un espace graphique. Vous indiquer les symboles, les couleurs, les types de ligne, etc. Peu de choses sont automatis√©es, ce qui laisse une grande flexibilit√©, mais demande de vouer beaucoup d'√©nergie √† la mani√®re de coder pour obtenir le graphique d√©sir√©. Le module graphique de Excel, ainsi que le module graphique de base de `R`, utilisent des approches imp√©ratives.

### Approche d√©clarative

Les strat√©gies d'automatisation graphique se sont grandement am√©lior√©es au cours des derni√®res ann√©es. Plut√¥t que de vouer vos √©nergies √† cr√©er un graphique, il est maintenant possible de sp√©cifier ce que l'on veut pr√©senter.

> La visualisation d√©clarative vous permet de penser aux donn√©es et √† leurs relations, plut√¥t que des d√©tails accessoires.
>
> [*Jake Vanderplas, Declarative Statistical Visualization in Python with Altair*](https://www.youtube.com/watch?v=FytuB8nFHPQ) (ma traduction)

L'approche d√©clarative passe souvent par une *grammaire graphique*, c'est-√†-dire un langage qui explique ce que l'on veut pr√©senter - en mode imp√©ratif, on sp√©cifie plut√¥t comment on veut pr√©senter les donn√©es. Le module **`ggplot2`** est le module d√©claratif par excellence en R.

## Visualisation en R

En R, votre trousse d'outils de visualisation m√©riterait de comprendre les modules suivants.

- **`base`**. Le module de base de R contient des fonctions graphique tr√®s polyvalentes. Les axes sont g√©n√©r√©es automatiquement, on peut y ajouter des titres et des l√©gendes, on peut cr√©er plusieurs graphiques sur une m√™me figure, on peut y ajouter diff√©rentes g√©om√©tries (points, lignes et polygones), avec diff√©rents types de points ou de trait, et diff√©rentes couleurs, etc. Les modules sp√©cialis√©s viennent souvent avec leurs graphiques sp√©cialis√©s, construit √† partir du module de base. En tant que module graphique imp√©ratif, on peut tout faire ou presque (pas d‚Äôinteractivit√©), mais l'√©criture du code est peut expressive.
- **`ggplot2`**. C'est le module graphique par excellence en R (et j'ose dire: en calcul scientifique). **`ggplot2`** se base sur une grammaire graphique. √Ä partir d'un tableau de donn√©es, une colonne peut d√©finir l'axe des x, une autre l'axe des y, une autre la couleur couleur des points ou leur dimension. Une autre colonne d√©finissant des cat√©gories peut segmenter la visualisation en plusieurs graphiques align√©s horizontalement ou verticalement. Des extensions de **`ggplot2`** permettent de g√©n√©rer des cartes (ggmap), des diagrammes ternaires (ggtern), des animations (gganimate), etc.
- **`plotly`**. plotly est un module graphique particuli√®rement utile pour g√©n√©rer des graphiques interactifs. plotly offre une fonction toute simple pour rendre interactif un graphique **`ggplot2`**.

Nous survolerons rapidement le module de base, irons plus en profondeur avec **`ggplot2`**, puis je pr√©senterai bri√®vement les graphiques interactifs avec plotly.

## Module de base pour les graphiques

Nous allons d'abord survoler le module de base, en mode imp√©ratif. La fonction de base pour les graphiques en R est `plot()`. Pour nous exercer avec cette fonction, chargeons d'abord le tableau de donn√©es d'exercice [`iris`](https://en.wikipedia.org/wiki/Iris_flower_data_set), publi√© en 1936 par le c√©l√®bre biostatisticien Ronald Fisher.

```{r}
data("iris")
head(iris)
```

Le tableau `iris` contient 5 colonnes, les 4 premi√®res d√©crivant les longueurs et largeurs des p√©tales et s√©pales de diff√©rentes esp√®ces d'iris dont le nom appara√Æt √† la 5i√®me colonne. La mani√®re la plus rapide d‚Äôextraire une colonne d'un tableau est d'appeler le tableau, suivit du `$`, puis du nom de la colonne, par exemple `iris$Species`. Pour g√©n√©rer un graphique avec la fonction `plot()`:

```{r}
plot(iris$Sepal.Length, iris$Petal.Length)
```

Par d√©faut, le premier argument est le vecteur d√©finissant l'axe des x et le deuxi√®me est celui d√©finissant l'axe des y. Vous rencontrerez souvent de telles utilisations d'arguments implicites, mais je pr√©f√®re √™tre explicite en d√©finissant bien les arguments: `plot(x = iris$Sepal.Length, y = iris$Petal.Length)`. Le graphique pr√©c√©dent peut √™tre amplement personnalis√© en utilisant diff√©rents arguments (figure \@ref(fig:vis-plot-iris-elements)).

```{r vis-plot-iris-elements, fig.align="center", fig.cap="√âl√©ments personnalisables d'un graphique de base,", echo = FALSE}
knitr::include_graphics("images/03_plot-iris.png")
```

**Exercice**. Utilisez ces arguments dans la cellule de code de la figure `plot(iris$Sepal.Length, iris$Petal.Length)`.

Remarquez que la fonction a d√©cid√© toute seule de cr√©er un nuage de point. La fonction plot() est con√ßue pour cr√©er le graphique appropri√© selon le type des donn√©es sp√©cifi√©es: lignes, boxplot, etc. Si l'on sp√©cifiait les esp√®ces comme argument `x`:

```{r}
plot(x = iris$Species, y = iris$Petal.Length)

# ou bien
# iris %>%
#   select(Species, Petal.Length) %>%
#   plot()
```

De m√™me, la fonction `plot()` appliqu√©e √† un tableau de donn√©es g√©n√©rera une repr√©sentation bivari√©e.

```{r}
plot(iris)
```

Il est possible d'encoder des attributs gr√¢ce √† des vecteurs de facteurs (cat√©gories).

```{r}
plot(iris, col = iris$Species)
```

L'argument `type = ""` permet de personnaliser l'apparence:

- `type = "p"`: points
- `type = "l"`: ligne
- `type = "o"` et `type = "b"`: ligne et points
- `type = "n"`: ne rien afficher

Cr√©ons un jeu de donn√©es.

```{r}
time <- seq(from = 0, to = 100, by = 10)
height <- abs(time * 0.1 + rnorm(length(time), 0, 2)) # abs pourvaleur absolue (changement de signe si n√©gatif)
plot(x = time, y = height, type = "b", lty = 2, lwd = 1)
```

Le type de ligne est sp√©cifi√© par l'argument `lty` (qui peut prendre un chiffre ou une ch√¢ine de caract√®res, i.e. `1` est √©quivalent de `"solid"`, `2` de `"dashed"`, `3` de `"dotted"`, etc.) et la largeur du trait (valeur num√©rique), par l'argument `lwd`.

La fonction `hist()` permet quant √† elle de cr√©er des histogrammes. Parmi ses arguments, `breaks` est particuli√®rement utile, car il permet d'ajuster la segmentation des incr√©ments.

```{r}
hist(iris$Petal.Length, breaks = 60)
```

**Exercice**. Ajustez le titre de l'axe des x, ainsi que les limites de l'axe des x. √ätes-vous en mesure de colorer l'int√©rieur des barres en bleu?

La fonction `plot()` peut √™tre suivie de plusieurs autres couches comme des lignes (`lines()` ou `abline()`), des points (`points()`), du texte (`text()`), des polygones (`polygon()`, des l√©gendes (`legend()`)), etc. On peut aussi personnaliser les couleurs, les types de points, les types de lignes, etc. L'exemple suivant ajoute une ligne au graphique. Ne pr√™tez pas trop attention aux fonctions `predict()` et `lm()` pour l'instant: nous les verrons au chapitre \@ref(chapitre-biostats).

```{r}
plot(x = time, y = height)
lines(x = time, y = predict(lm(height ~ time)))
```

Pour exporter un graphique, vous pouvez passer par le menu Export de RStudio. Mais pour des graphiques destin√©s √† √™tre publi√©s, je vous sugg√®re d'exporter vos graphiques avec une haute r√©solution √† la suite de la commande `png()` (ou `jpg()` ou `svg()`).

```{r}
svg(filename = "images/mon-graphique.svg", width = 3000, height = 2000)
# png(filename = 'images/mon-graphique.png', width = 3000, height=2000, res=300)
plot(
  x = iris$Petal.Length,
  y = iris$Sepal.Length,
  col = iris$Species,
  cex = 3, # dimension des points
  pch = 16
) # type de points
dev.off()
```

Le format svg cr√©e une version vectorielle du graphique, c'est-√†-dire que l'image export√©e est un fichier contenant les formes, non pas les pixels. Cela vous permet d'√©diter votre graphique dans un logiciel de dessin vectoriel (comme [Inkscape](https://inkscape.org/)).

Dans le bloc de code pr√©c√©dent, j'ai mis en commentaire (`# ...`) le format d'image *png*, utile pour les images de type graphique, avec des changements de couleurs drastiques. J'y ai sp√©cifi√© une haute r√©solution, √† 300 pixels par pouce. Pour les photos, vous pr√©f√©rerez le format *jpg*. Des √©diteurs demanderont peut-√™tre des formats vectoriels comme *pdf* ou *eps*. Si vous ne trouvez pas de moyen de modifier un aspect du graphique dans le code (bouger des √©tiquettes ou des l√©gendes, ajouter des √©l√©ments graphiques), vous pouvez exporter votre graphique en format svg et √©diter votre graphique dans [Inkscape](https://inkscape.org/).

Le module de base de R comprend une panoplie d'autres particularit√©s que je ne couvrirai pas ici, en faveur du module **`ggplot2`**.

## La grammaire graphique **`ggplot2`**

Le module **`esquisse`** est une extension de RStudio permettant de g√©n√©rer du code pour le module graphique **`ggplot2`**. La vid√©o suivant, o√π j'utilise **`esquisse`**, montre ce en quoi consiste une grammaire graphique.

<video width="480" height="320" controls="controls">
<source src="images/03_esquisse.mp4" type="video/mp4">
</video>

Chaque colonne est un √©l√©ment graphique et peut √™tre encod√© pour former la position en x, en y, la taille des points, leur couleur, ou m√™me le panneau (facet). Mais quelle forme prendra le bidule positionner? Des points, lignes, boxplots, barres? C'est ce que d√©fini une grammaire graphique. Bri√®vement, une grammaire graphique permet de sch√©matiser des donn√©es avec des marqueurs (points, lignes, etc.) sur des attributs visuels (couleurs, dimension, forme). Cette approche permet de d√©gager 5 composantes.

1. **Les donn√©es**. Votre tableau est bien s√ªr un argument n√©cessaire pour g√©n√©rer le graphique.
1. **Les marqueurs**. Un terme abstrait pour d√©signer les points, les lignes, les polygones, les barres, les fl√®ches, etc. En **`ggplot2`**, ce sont des *g√©om√©tries*, par exemple `geom_point()` pour d√©finir une g√©om√©trie de points.
1. **Les attributs encod√©s**. La position, la dimension, la couleur ou la forme que prendront les g√©om√©tries. En **`ggplot2`**, on les nomme les *aesthetics*.
1. **Les attributs globaux**. Les attributs sont globaux lorsqu'ils sont constant (ils ne d√©pendent pas d'une variable). Les valeurs par d√©faut conviennent g√©n√©ralement, mais certains attributs peuvent √™tre sp√©cifi√©s: par exemple la forme ou la couleur des points, le type de ligne, etc.
1. **Les th√®mes**. Le th√®me du graphique permet de personnalis√© la mani√®re dont le graphique est rendu. Il existe des th√®mes pr√©d√©finis, que vous pouvez ajuster, mais il est possible de cr√©er vos propres th√®mes (nous ne couvrirons pas cela dans ce cours).

```{r vis-allisonhorst-ggplot-art, out.width="100%", fig.align="center", fig.cap="Cr√©er une oeuvre d'art avec **`ggplot2`**, dessin de [@allison_horst](https://twitter.com/allison_horst).", echo = FALSE}
knitr::include_graphics("images/03_ggplot2_masterpiece.png")
```

Le flux de travail pour cr√©er un graphique √† partir d'une grammaire ressemble donc √† ceci:

```
Avec mon tableau,
Cr√©er un marqueur (
encoder(position X = colonne A,
position Y = colonne B,
couleur = colonne C),
forme globale = 1)
Avec un th√®me noir et blanc
```

Le module **`tidyverse`** installera des modules utilis√©s de mani√®re r√©currente dans ce cours, comme **`ggplot2`**, **`dplyr`**, **`tidyr`** et **`readr`**. Je recommande de le charger au d√©but de vos sessions de travail.

```{r vs-load-tidyverse, message=FALSE}
library("tidyverse")
```

L'approche *tidyverse* est une grammaire des donn√©es. Le module **`ggplot2`**, qui en fait partie, est une *grammaire graphique* (d'o√π le *gg* de *ggplot*).

## Mon premier ggplot

Pour notre premier exercice, je vais charger un tableau depuis le fichier de donn√©es [`abalone.data`](https://github.com/ajschumacher/gadsdc1/blob/master/dataset_research/clara_abalone.md). Pour plus de d√©tails sur les tableaux de donn√©es, consultez le chapitre \@ref(chapitre-tableaux). Le fichier de donn√©es porte sur un escargot de mer et comprend le sexe (M: m√¢le, F: femelle et I: enfant), des poids et dimensions des individus observ√©s, et le nombre d'anneaux compt√©s dans la coquille.

```{r vis-load-data}
abalone <- read_csv("data/abalone.csv")
```

Inspectons l'ent√™te du tableau avec la fonction `head()`.

```{r vis-head-data}
head(abalone)
```

Suivant la grammaire graphique **`ggplot2`**, on pourra cr√©er ce graphique de points comprenant les attributs suivants suivants.

1. `data = abalone`, le fichier de donn√©es.
1. `mapping = aes(...)`, sp√©cifi√© comme attribut de la fonction `ggplot()`, cet encodage (ou `aes`thetic) reste l'encodage par d√©faut pour tous les marqueurs du graphique. Toutefois, l'encodage `mapping = aes()` peut aussi √™tre sp√©cifi√© dans la fonction du marqueur (par exemple `geom_point()`). Dans l'encodage global du graphique, on place en x la longueur de la coquille (`x = LongestShell`) et on place en y le poids de la coquille (`y = ShellWeight`).
1. Pour ajouter une fonction √† `ggplot`, comme une nouvelle couche de marqueur ou des √©l√©ments de th√®me, on utilise le `+`. G√©n√©ralement, on change aussi de ligne.
1. Le marqueur ajout√© est un point, `geom_point()`, dans lequel on sp√©cifie un encodage de couleur sur la variable Type (`colour = Type`) et un encodage de dimension du point sur la variable rings (`size = Rings`). L'attribut `alpha = 0.5` se situe hors du mapping et de la fonction `aes()`: c'est un attribut identique pour tous les points.

```{r}
ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) +
  geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5)
```

Il existe plusieurs types de marqueurs:

- `geom_point()` pour les points
- `geom_line()` pour les lignes
- `geom_bar()` pour les diagrammes en barre en d√©compte, `geom_col` en terme de grandeur  et `geom_histogram` pour les histogrammes
- `geom_boxplot()` pour les boxplots
- `geom_errorbar()`, `geom_pointrange()` ou `geom_crossbar()` pour les marges d'erreur
- `geom_map()` pour les cartes
- etc.

Il existe plusieurs attributs d'encodage:

- la position `x`, `y` et `z` (`z` pertinent notamment pour le marqueur `geom_tile()`)
- la taille `size`
- la forme des points `shape`
- la couleur, qui peut √™tre discr√®te ou continue :
  - `colour`, pour la couleur des contours
  - `fill`, pour la couleur de remplissage
- le type de ligne `linetype`
- la transparence `alpha`
- et d'autres types sp√©cialis√©s que vous retrouverez dans la documentation des marqueurs

Les types de marqueurs et leurs encodages sont d√©crits dans la [documentation de **`ggplot2`**](https://ggplot2.tidyverse.org/), qui fournit des  feuilles aide-m√©moire qu'il est commode d'imprimer et d'afficher pr√®s de soi (figure \@ref(fig:vis-ggplot-cs)).

```{r vis-ggplot-cs, out.width="100%", fig.align="center", fig.cap="Aide-m√©moire de **`ggplot2`**, source: https://www.rstudio.com/resources/cheatsheets/", echo = FALSE}
knitr::include_graphics("images/03_data-visualization-2.1.png")
```

#### Les facettes

Dans **`ggplot2`**, les `facet`ttes sont un type sp√©cial d'encodage utilis√©s pour d√©finir des grilles de graphique. Elles prennent deux formes:

- Le collage, `facet_wrap()`. Une variable cat√©gorielle est utilis√©e pour segmenter les graphiques en plusieurs graphiques, qui sont plac√©s l'un √† la suite de l'autre dans un arrangement sp√©cifi√© par un nombre de colonne ou un nombre de ligne.
- La grille, `facet_grid()`. Une ou deux variables segmentent les graphiques selon les colonnes et les lignes.

Les facettes peuvent √™tre sp√©cifi√©es n'importe o√π dans la cha√Æne de commande de **`ggplot2`**, mais conventionnellement, on les place tout de suite apr√®s la fonction `ggplot()`.

```{r}
ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) +
  facet_wrap(~Type, ncol = 2) +
  geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5)
```

La fonction `cut()` permet de discr√©tiser des variables continues en cat√©gories ordonn√©es - les fonctions peuvent √™tre utilis√©es √† l'int√©rieur de la fonction ggplot.

```{r}
ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) +
  facet_grid(Type ~ cut(Rings, breaks = seq(0, 30, 5))) +
  geom_point(mapping = aes(colour = Type), alpha = 0.5)
```

Par d√©faut, les axes des facettes, ainsi que leurs dimensions, sont les m√™mes. Une telle repr√©sentation permet de comparer les facets sur une m√™me √©chelle. Les axes peuvent √™tre d√©finis selon les donn√©es avec l'argument `scales`, tandis que l'espace des facettes peut √™tre conditionn√© selon l'argument `space` - pour plus de d√©tails, [voir la fiche de documentation](https://ggplot2.tidyverse.org/reference/facet_grid.html).

**Exercice**. Personnalisez le graphique avec les donn√©es `abalone` en rempla√ßant les variables et en r√©organisant les facettes.

### Plusieurs sources de donn√©es

Il peut arriver que les donn√©es pour g√©n√©rer un graphique proviennent de plusieurs tableaux. Lorsqu'on ne sp√©cifie pas la source du tableau dans un marqueur, la valeur par d√©faut est le tableau sp√©cifier dans l'amorce `ggplot()`. Il est n√©anmoins possible de d√©finir une source personnalis√©e pour chaque marqueur en sp√©cifiant `data = ...` comme argument du marqueur.

```{r}
abalone_siteA <- data.frame(
  LongestShell = c(0.3, 0.8, 0.7),
  ShellWeight = c(0.05, 0.81, 0.77)
)

ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) +
  geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5) +
  geom_point(data = abalone_siteA, size = 8, shape = 4)
```

### Exporter avec style

Le fond gris est une marque distinctive de **`ggplot2`**. Il n'est toutefois pas appr√©ci√© de tout le monde. D'autres th√®mes dits *complets* peuvent √™tre utilis√©s ([liste des th√®mes complets](https://ggplot2.tidyverse.org/reference/ggtheme.html)). Les th√®mes complets sont appel√©s avant la fonction `theme()`, qui permet d'effectuer des ajustements pr√©cis dont la liste exhaustive se trouve [dans la documentation de **`ggplot2`**](https://ggplot2.tidyverse.org/reference/theme.html).

Vous pouvez aussi personnaliser le titre des axes (`xlab()` et `ylab()`) ou du graphique (`ggtitle()`), ou bien tout sp√©cifier dans une m√™me fonction ou bien tout en m√™me temps dans `labs(x = "...", y = "...", title = "...")`. Il est possible d'utiliser des exposants dans le titre des axes avec la fonction `expression()`, par exemple `labs(x = expression("Dose (kg ha"^"-1"~")"))` pour intituler l'axe des x avec $Dose~(kg~ha^{-1})$. Aussi convient parfois de sp√©cifier les limites (`xlim()` et `ylim()`, ou `expand_limits(x = c(0, 1), y = c(0, 1))`).

Pour exporter un ggplot, on pourra utiliser les commandes de R `png()`, `svg()` ou `pdf()`, ou les outils de RStudio. Toutefois, **`ggplot2`** offre la fonction `ggsave()`, que l'on place en remorque du graphique, en sp√©cifiant les dimensions (`width` et `height`) ainsi que la r√©solution (`dpi`). La r√©solution d'un graphique destin√© √† la publication est typiquement de plus de 300 dpi.

```{r}
ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) +
  geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5) +
  #xlab("Length (mm)") +
  #ylab("Shell weight (g)") +
  #ggtitle("Abalone") + # pr√©f√©rablement dans une m√™me ligne
  labs(x = "Length (mm)", y = "Shell weight (g)", title = "Abalone") +
  xlim(c(0, 1)) +
  theme_classic() +
  theme(
    axis.title = element_text(size = 20),
    axis.text = element_text(size = 20),
    axis.text.y = element_text(size = 20, angle = 90, hjust = 0.5),
    legend.box = "horizontal"
  )
ggsave("images/abalone.png", width = 8, height = 8, dpi = 300)
```

Nous allons maintenant couvrir diff√©rents types de graphiques, accessibles selon diff√©rents marqueurs:

- les nuages de points
- les diagrammes en ligne
- les boxplots
- les histogrammes
- les diagrammes en barres

### Nuages de points

L'exemple pr√©c√©dent est un nuage de points, que nous avons g√©n√©r√© avec le marqueur `geom_point()`, qui a d√©j√† √©t√© passablement introduit. L'exploration de ces donn√©es a permis de d√©tecter une croissance exponentielle du poids de la coquille en fonction de sa longueur. Il est clair que les abalones juv√©niles (Type I) sont plus petits et moins lourds, mais nous devrons probablement proc√©der √† des tests statistiques pour v√©rifier s'il y a des diff√©rences entre m√¢les et femelles.

Le graphique √©tant tr√®s charg√©, nous avons utilis√© des strat√©gies pour l'all√©ger en utilisant de la transparence et des facettes. Le marqueur `geom_jitter()` peut permettre de mieux appr√©cier la dispersion des points en ajoutant une dispersion randomis√©e en x ou en y.

```{r}
ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) +
  geom_jitter(mapping = aes(colour = Type, size = Rings), alpha = 0.5, width = 0.05, height = 0.1)
```

Dans ce cas-ci, √ßa ne change pas beaucoup, mais retenons-le pour la suite.

### Diagrammes en lignes

Les lignes sont utilis√©es pour exprimer des liens entre une suite d'information. Dans la plupart des cas, il s'agit d'une suite d'information dans le temps que l'on appelle les s√©ries temporelles. En l‚Äôoccurrence, les lignes devraient √™tre √©vit√©es si la s√©quence entre les variables n'est pas √©vidente. Nous allons utiliser un tableau de donn√©es de R portant sur la croissance des orangers.

```{r}
data("Orange")
head(Orange)
```

La premi√®re colonne sp√©cifie le num√©ro de l'arbre mesur√©, la deuxi√®me son √¢ge et la troisi√®me sa circonf√©rence. Le marqueur `geom_line()` permet de tracer la tendance de la circonf√©rence selon l'√¢ge. En encodant la couleur de la ligne √† l'arbre, nous pourrons tracer une ligne pour chacun d'entre eux.

```{r}
ggplot(data = Orange, mapping = aes(x = age, y = circumference)) +
  geom_line(aes(colour = Tree))
```

La l√©gende ne montre pas les num√©ros d'arbre en ordre croissance. En effet, la l√©gende (tout comme les facettes) classe les cat√©gories prioritairement selon l'ordre des cat√©gories si elles sont ordinales, ou par ordre alphab√©tique si les cat√©gories sont nominales. Inspectons la colonne `Tree` en inspectant le tableau avec la commande `str()` - la commande `glimpse()` du tidyverse donne un sommaire moins complet que `str()`.

```{r}
str(Orange)
```

En effet, la colonne `Tree` est un facteur ordinal dont les niveaux sont dans le m√™me ordre que celui la l√©gende.

### Les histogrammes

Nous avons vu les histogrammes dans la br√®ve section sur les fonctions graphiques de base dans R: il s'agit de segmenter l'axe des x en incr√©ments, puis de pr√©senter sur l'axe de y le nombre de donn√©es que l'on retrouve dans cet incr√©ment. Le marqueur √† utiliser est `geom_histogram()`.

Revenons √† nos escargots. Comment pr√©senteriez-vous la longueur de la coquille selon la variable `Type`? Selon des couleurs ou des facettes? La couleur, dans le cas des histogrammes, est celle du pourtour des barres. Pour colorer l'int√©rieur des barres, l'argument √† utiliser est `fill`.

```{r}
ggplot(data = abalone, mapping = aes(x = LongestShell)) +
  geom_histogram(mapping = aes(fill = Type), colour = "black")
```

On n'y voit pas grand chose. Essayons plut√¥t les facettes.

```{r}
ggplot(data = abalone, mapping = aes(x = LongestShell)) +
  facet_grid(Type ~ .) +
  geom_histogram()
```

Les facettes permettent maintenant de bien distinguer la distribution des longueur des juv√©niles. L'argument `bins`, tout comme l'argument `breaks` du module graphique de base, permet de sp√©cifier le nombre d'incr√©ments, ce qui peut √™tre tr√®s utile en exploration de donn√©es.

```{r}
ggplot(data = abalone, mapping = aes(x = LongestShell)) +
  facet_grid(Type ~ .) +
  geom_histogram(bins = 60, colour = "white")
```

Le nombre d'incr√©ments est un param√®tre qu'il ne faut pas sous-estimer. √Ä preuve, ce tweet de [@NicholasStrayer](https://twitter.com/NicholasStrayer):

<blockquote class="twitter-tweet" data-lang="fr"><p lang="en" dir="ltr">Histograms are fantastic, but make sure your bin-width/number is chosen well. This is the _exact_ same data, plotted with different bin-widths. Notice that the pattern doesn&#39;t necessarily get clearer as bin num increases. <a href="https://twitter.com/hashtag/dataviz?src=hash&amp;ref_src=twsrc%5Etfw">#dataviz</a> <a href="https://t.co/3MhSFwTVPH">pic.twitter.com/3MhSFwTVPH</a></p>&mdash; Nick Strayer (@NicholasStrayer) <a href="https://twitter.com/NicholasStrayer/status/1026893778404225024?ref_src=twsrc%5Etfw">7 ao√ªt 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

### Boxplots

Les boxplots sont une autre mani√®re de visualiser des distributions. L'astuce est de cr√©er une bo√Æte qui s'√©tant du premier quartile (valeur o√π l'on retrouve 25% de donn√©es dont la valeur est inf√©rieure) au troisi√®me quartile  (valeur o√π l'on retrouve 75% de donn√©es dont la valeur est inf√©rieure). Une barre √† l'int√©rieur de cette bo√Æte est plac√©e √† la m√©diane (qui est en fait le second quartile). De part et d'autre de la bo√Æte, on retrouve des lignes sp√©cifiant l'√©tendue hors quartile. Cette √©tendue peut √™tre d√©termin√©e de plusieurs mani√®res, mais dans le cas de **`ggplot2`**, il s'agit de 1.5 fois l'√©tendue de la bo√Æte (l'*√©cart interquartile*). Au-del√† de ces lignes, on retrouve les points repr√©sentant les valeurs extr√™mes. Le marqueur √† utiliser est `geom_boxplot()`. L'encodage x est la variable cat√©gorielle et l'encodage y est la variable continue.

```{r}
ggplot(data = abalone, mapping = aes(x = Type, y = LongestShell)) +
  geom_boxplot()
```

**Exercice**. On sugg√®re parfois de pr√©senter les mesures sur les boxplots. Utiliser `geom_jitter()` avec un bruit horizontal.

### Les diagrammes en barre

Les diagrammes en barre repr√©sente une variable continue associ√©e √† une cat√©gorie. Les barres sont g√©n√©ralement horizontales et ordonn√©es. Nous y reviendrons √† la fin de ce chapitre, mais retenez pour l'instant que dans tous les cas, les diagrammes en barre doivent inclure le z√©ro pour √©viter les mauvaises interpr√©tations.

Pour les diagrammes en barre, nous allons utiliser les donn√©es de l'union internationale pour la conservation de la nature [distribu√©es par l'OCDE](https://stats.oecd.org/Index.aspx?DataSetCode=WILD_LIFE).

```{r}
# Certaines  colonnes de caract√®re sont consid√©r√©es comme bool√©ennes
# mieux vaut d√©finir leur type pour s'assurer que le bon type
# soit attribu√©
especes_menacees <- read_csv("data/WILD_LIFE_14012020030114795.csv",
  col_types = list(
    "c", "c", "c", "c",
    "c", "c", "c", "c",
    "d", "c", "c", "c",
    "d", "c", "c"
  )
)
head(especes_menacees)
```

L'exercice consiste √† cr√©er un diagramme en barres horizontales du nombre de plantes vasculaires menac√©es de mani√®re critique pour les 10 pays qui en contiennent le plus. Je vais effectuer quelques op√©rations sur ce tableau afin d'en arriver avec un tableau que nous pourrons convenablement mettre en graphique: n'y portez pas trop attention pour l'instant: ces op√©rations sont un avant-go√ªt du prochain chapitre.

Nous allons filtrer le tableau pour obtenir le nombre de plantes vascularies critiquement menac√©es, s√©lectionner seulement le pays et le nombre d'esp√®ces, les grouper par pays, additionner toutes les esp√®ces pour chaque pays, les placer en ordre descendant et enfin s√©lectionner les 10 premiers. Comme vous le voyez, la cr√©ation de graphique est li√©e de pr√®s avec la manipulation des tableaux!

```{r}
especes_crit <- especes_menacees %>%
  filter(IUCN == "CRITICAL", SPEC == "VASCULAR_PLANT") %>%
  dplyr::select(Country, Value) %>%
  group_by(Country) %>%
  summarise(n_critical_species = sum(Value)) %>%
  arrange(desc(n_critical_species)) %>%
  head(10)
especes_crit
```

Le premier type de diagramme en barre que nous allons couvrir est obtenu par le marqueur `geom_col()`.

```{r}
ggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) +
  geom_col()
```

Ce graphique est perfectible. Les barres sont verticales et non ordonn√©es. Souvenons-nous que **`ggplot2`** ordonne par ordre alphab√©tique si aucun autre ordre est sp√©cifi√©. Nous pouvons changer l'ordre en changeant l'ordre des niveaux de la variable `Country` selon le nombre d'esp√®ces gr√¢ce √† la fonction `fct_reorder`.

```{r}
especes_crit <- especes_crit %>%
  mutate(Country = fct_reorder(Country, n_critical_species))
```

Pour faire pivoter le graphique, nous ajoutons `coord_flip()` √† la s√©quence.

```{r}
ggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) +
  geom_col() +
  coord_flip()
```

Une autre m√©thode, `geom_bar()`, est un raccourcis permettant de compter le nombre d‚Äôoccurrence d'une variable unique. Par exemple, dans le tableau abalone, le nombre de fois que chaque niveau de la variable Type

```{r}
ggplot(data = abalone, mapping = aes(x = Type)) +
  geom_bar() +
  coord_flip()
```

Personnellement, j'aime bien passer par un diagramme en lignes avec le marqueur `geom_segment()`. Cela me donne la flexibilit√© pour d√©finir un largeur de trait et √©ventuellement d'ajouter un point au bout pour en faire un [diagramme en su√ßon](https://www.r-graph-gallery.com/lollipop-plot/). Tenez, j'en profite aussi pour y ajouter du texte (d√©cal√© horizontalement) et √©tendre les limtes pour m'assurer que les chiffres apparaissent bien.

```{r}
ggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) +
  geom_segment(mapping = aes(xend = Country, yend = 0), lwd = 2) +
  geom_point(size = 5, colour = "black") +
  geom_text(aes(label = n_critical_species), hjust = -0.5) + # si ce ne sont pas des valeurs enti√®res, arrondir avec signif()
  expand_limits(y = c(0, 1300)) +
  coord_flip() +
  theme_bw()
```

Les diagrammes en barre peuvent √™tre plac√©s en relation avec d'autres. Reprenons notre manipulation de donn√©es pr√©c√©dente, mais en incluant tous les pays, pour les trois niveaux d'alerte, pour les poissons.

```{r}
especes_pays_iucn <- especes_menacees %>%
  filter(IUCN %in% c("ENDANGERED", "VULNERABLE", "CRITICAL"), SPEC == "FISH_TOT") %>%
  dplyr::select(IUCN, Country, Value) %>%
  group_by(Country, IUCN) %>%
  summarise(n_species = sum(Value)) %>%
  group_by(Country) %>%
  mutate(n_tot = sum(n_species)) %>%
  ungroup() %>% # pour pouvoir modifier Country, non modifiable tant qu'elle est une variable de regroupement (voir group_by)
  mutate(Country = fct_reorder(Country, n_tot))
head(especes_pays_iucn)
```

Pour placer les barres les unes √† c√¥t√© des autres, nous sp√©cifions `position = "dodge"`.

```{r}
ggplot(data = especes_pays_iucn, mapping = aes(x = Country, y = n_species)) +
  geom_col(aes(fill = IUCN), position = "dodge") +
  coord_flip()
```

Il est parfois plus pratique d'utiliser les facettes.

```{r}
ggplot(data = especes_pays_iucn, mapping = aes(x = Country, y = n_species)) +
  facet_grid(IUCN ~ .) +
  geom_col() +
  coord_flip()
```

Pour perfectionner encore ce graphique, on pourrait [r√©ordonner les facettes individuellement](https://trinkerrstuff.wordpress.com/2016/12/23/ordering-categories-within-ggplot2-facets/), mais ne nous √©garons par trop.

### Exporter un graphique

Plus besoin d'utiliser la fonction `png()` en mode **`ggplot2`**. Utilisons plut√¥t `ggsave()`.

```{r}
ggplot(data = especes_pays_iucn, mapping = aes(x = Country, y = n_species)) +
  facet_grid(IUCN ~ .) +
  geom_col(aes(fill = IUCN)) +
  coord_flip()
ggsave("images/especes_pays_iucn.png", width = 6, height = 8, dpi = 300)
```

## Les graphiques comme outil d'exploration des donn√©es

```{r vis-allisonhorst-ggplot-expl, out.width="100%", fig.align="center", fig.cap="Explorer les donn√©es avec **`ggplot2`**, dessin de [@allison_horst](https://twitter.com/allison_horst).", echo = FALSE}
knitr::include_graphics("images/03_ggplot2_exploratory.png")
```

La plupart des graphiques que vous cr√©erez ne seront pas destin√©s √† √™tre publi√©s, mais serviront d'outil d'exploration des donn√©es. Le jeu de donn√©es datasaurus, pr√©sent√© en [d√©but de chapitre](#Pourquoi-explorer-grapiquement%3F), permet de saisir l'importance des outils graphiques pour bien comprendre les donn√©es.

```{r}
datasaurus <- read_tsv("data/DatasaurusDozen.tsv")
head(datasaurus)
```

Projetons d'abord les coordonn√©es x et y sur un graphique. J'utilise FacetGrid ici, sachant que ce sera utile pour l'exploration.

```{r}
ggplot(data = datasaurus, mapping = aes(x = x, y = y)) +
  geom_point()
```

Ce graphique pourrait ressembler √† une distribution binormale, ou *un coup de 12 dans une porte de grange*. Mais on aper√ßoit des donn√©es align√©es, parfois de mani√®re rectiligne, parfois en forme d'ellipse. Le tableau `datasaurus` a une colonne d'information suppl√©mentaire. Utilisons-la comme cat√©gorie pour g√©n√©rer des couleurs diff√©rente.

```{r}
ggplot(data = datasaurus, mapping = aes(x = x, y = y)) +
  geom_point(mapping = aes(colour = dataset))
```

Ce n'est pas vraiment plus clair. Il y a toutefois des formes qui se d√©gage, comme des ellipse et des lignes. Et si je regarde bien, j'y vois une √©toile. La cat√©gorisation pourrait-elle √™tre mieux utilis√©e si on segmentait par facettes au lieu de des couleurs?

```{r}
ggplot(data = datasaurus, mapping = aes(x = x, y = y)) +
  facet_wrap(~dataset, nrow = 2) +
  geom_point(size = 0.5) +
  coord_equal()
```

Voil√†! Fait int√©ressant, ni les statistiques, ni les algorithmes de regroupement ne nous auraient √©t√© utiles pour diff√©rencier les groupes!

### Des graphiques interactifs!

Les graphiques sont traditionnellement des images statiques. Toutefois, les graphiques n'√©tant pas d√©pendants de supports papiers peuvent √™tre utilis√©s de mani√®re diff√©rente, en ajoutant une couche d‚Äôinteraction. Con√ßue √† Montr√©al, plotly est un module graphique interactif en soi. Il peut √™tre utilis√© gr√¢ce √† son outil web, tout comme il peut √™tre interfac√© avec R, Python, javascript, etc. Mais ce qui retient notre attention ici est son interface avec **`ggplot2`**.

Les graphiques **`ggplot2`** peuvent √™tre enregistr√©s en tant qu'objets. Il peuvent cons√©quemment √™tre manipul√©s par des fonctions. La fonction ggplotly permet de rendre votre ggplot interactif.

```{r vis}
library("plotly")
especes_crit_bar <- ggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) +
  geom_segment(mapping = aes(xend = Country, yend = 0), lwd = 2) +
  geom_point(size = 6) +
  coord_flip()
ggplotly(especes_crit_bar)
```

Vous pouvez publier votre graphique plotly en ligne pour le partager ou l'inclure dans une publication web. Il vous faudra cr√©er un compte plotly, puis g√©n√©rer une cl√© d'utilisation dans Settings > API Keys > Generate key. Pour des raisons de s√©curit√©, la cl√© du bloc ci-dessous ne fonctionnera pas. J'ai d√©sactiv√© le bloc de code, mais le r√©sultat se trouve en suivant le lien g√©n√©r√© par plotly: https://plot.ly/~essicolo/152/.

```
Sys.setenv("plotly_username"="essicolo")
Sys.setenv("plotly_api_key"="iavd1ycE2iiqOp9YD45I")

chart_link <- api_create(x = ggplotly(especes_crit_bar), 
                         filename = "public-graph",
                         sharing = "public",
                         fileopt = "overwrite")
chart_link
```

### Des extensions de **`ggplot2`**

**`ggplot2`** est un module graphique √©l√©gant et polyvalent. Il a pourtant bien des limitations. Justement, le module est con√ßu pour √™tre impl√©ment√© avec des extensions. Vous en trouverez plusieurs sur [ggplot2-exts.org](http://www.ggplot2-exts.org/gallery/), mais en trouverez de nombreuses autres en cherchant avec le terme **`ggplot2`** sur [github.com](https://github.com/search?q=ggplot2), probablement la plate-forme (voire un r√©seau social) de d√©veloppement de logiciels la plus utilis√©e dans le monde. En voici quelques unes.

- [**`ggthemr`**](https://github.com/cttobin/ggthemr): sp√©cifier un th√®me graphique une seule fois dans votre session, et tout le reste suit.
- [**`cowplot`**](https://github.com/wilkelab/cowplot) et [**`patchwork`**](https://patchwork.data-imaginist.com/index.html) permettent de cr√©er des graphiques pr√™ts pour la publication, par exemple en cr√©ant des grilles de plusieurs ggplots, en les num√©rotant, etc.
- Si les th√®mes de base ne vous conviennent pas, vous en trouverez d'autres en installant [**`ggthemes`**](https://github.com/jrnold/ggthemes).
- [**`ggmap`**](https://github.com/dkahle/ggmap) et [**`ggspatial`**](https://github.com/paleolimbot/ggspatial) sont deux extensions pour cr√©er des cartes. Un chapitre sur les donn√©es spatiales est en d√©veloppement.
- [**`ggtern`**](http://www.ggtern.com/) permet de cr√©er des diagrammes ternaires, qui sont utiles pour la visualisation de proportions incluant trois composantes. Ce sujet est couvert au chapitre 6, en d√©veloppement.

### Aller plus loin avec **`ggplot2`**

- [Claus O. Wilke](@ClausWilke) est professeur en biologie int√©grative √† l'Universit√© du Texas √† Austin. Son livre [Fundamentals of Data Visualization](https://serialmentor.com/dataviz/) est un guide th√©orique et pratique pour la visualisation de donn√©es avec **`ggplot2`**.
- Le site [data-to-viz.com](https://www.data-to-viz.com/) vous accompagne dans le choix du graphique √† cr√©er selon vos donn√©es.
- Le site [r-graph-gallery.com](https://www.r-graph-gallery.com/) offre des recettes pour cr√©er des graphiques avec **`ggplot2`**.

## Extra: R√®gles particuli√®res

> Les mauvais graphiques peuvent survenir √† cause de l'ignorance, bien s√ªr, mais souvent ils existent pour la m√™me raison que la boeuferie [*bullhist*] verbale ou √©crite. Parfois, les gens ne se soucient pas de la fa√ßon dont ils pr√©sentent les donn√©es aussi longtemps que √ßa appuie leurs arguments et, parfois, ils ne se soucient pas que √ßa porte √† confusion tant qu'ils ont l'air impressionnant. $-$ Carl Bergstorm et Jevin West, [Calling Bullshit Read-Along Week 6: Data Visualization](https://graphpaperdiaries.com/2017/04/09/calling-bs-read-along-week-6-data-visualization/)

Une repr√©sentation visuelle est un outil tranchant qui peut autant pr√©senter un √©tat v√©ritable des donn√©es qu'une perspective trompeuse. Bien souvent, une ou plusieurs des 5 qualit√©s ne sont pas respect√©es. Les occasions d'erreur ne manquent pas - j'en ferai mention dans la section *Choisir le bon type de graphique*. Pour l'instant, notons quelques r√®gles particuli√®res.

### Ne tronquez pas inutilement l'axe des $y$

Tronquer l'axe vertical peut amener √† porter de fausses conclusions. 

<center>
<img src="https://i2.wp.com/flowingdata.com/wp-content/uploads/2015/08/bar-plots1.png?w=1800">
Effets sur la perception d'utiliser diff√©rentes r√©f√©rences. Source: Yau (2015), [Real Chart Rules to Follow](https://flowingdata.com/2015/08/11/real-chart-rules-to-follow/).
</center>

La r√®gle semble simple: les diagrammes en barre (utilis√©s pour repr√©senter une grandeur) devraient toujours pr√©senter le 0 et les diagrammes en ligne (utilis√©s pour pr√©senter des tendances) ne requiert pas n√©cessairement le z√©ro ((Bergstrom et West, Calling bullshit: Misleading axes on graphs)[http://callingbullshit.org/tools/tools_misleading_axes.html]). Mais le z√©ro n'est pas toujours li√© √† une quantit√© particuli√®re, par exemple, la temp√©rature ou un log-ratio. De plus, avec un diagramme en ligne on pourra toujours magnifier des tendances en zoomant sur une variation somme toute mineure. On arrive donc moins √† une r√®gle qu'une qualit√© d'un bon graphique, en particulier la qualit√© no 1 de Cairo: offrir une repr√©sentation honn√™te des donn√©es. Par exemple, Nathan Yau, auteur du blogue Flowing Data, [propose](https://flowingdata.com/2015/08/31/bar-chart-baselines-start-at-zero/) de pr√©senter des r√©sultats de mani√®re relative √† la mesure initiale. C'est d'ailleurs ce qui a √©t√© fait pour g√©n√©rer le graphique de Michael Mann et al., ci-dessus, o√π le z√©ro correspond √† la moyenne des temp√©ratures enregistr√©es entre 1961 et 1990.

Il peut √™tre tentant de tronquer l'axe des $y$ lorsque l'on d√©sire superposer deux axes verticaux. Souvent, l'utilisation de plusieurs axes verticaux am√®ne une perception de causalit√© dans des situations de [fausses corr√©lations](http://www.tylervigen.com/spurious-correlations). On ne devrait pas utiliser plusieurs axes verticaux.

### Utilisez un encrage proportionnel

Cette r√®gle a √©t√© propos√©e par Edward Tufte dans [Visual Display of Quantitative Information](https://ariane25.bibl.ulaval.ca/ariane/wicket/detail?c=ariane&m=S&rq.ct=PE&rq.fa=false&rq.r.esc=false&rq.r.l[0].c=TI&rq.r.l[0].ex=false&rq.r.l[0].op=AND&rq.r.l[0].v=Visual+Display+of+Quantitative+Information&rq.r.la=*&rq.r.loc=*&rq.r.pft=false&rq.r.ta=*&rq.r.td=*&rq.rows=2&rq.st=1). Une des raisons pour lesquelles on √©vite de tronquer l'axe des $y$ en particulier pour les diagrammes en barre est que l'aire repr√©sentant une mesure (la quantit√© d'"encre" n√©cessaire pour la dessiner) devrait √™tre proportionnelle √† sa magnitude. Les diagrammes en barre sont particuli√®rement sensibles √† cette r√®gle, √©tant donn√©e que la largeur des barres peuvent amplifier l'aire occup√©e. Deux solutions dans ce cas: (1) utiliser des barres minces ou (2) pr√©f√©rer des "diagrammes de points" (*dot charts*, √† ne pas confondre aux nuages de points).

L'encrage a beau √™tre proportionnel, la difficult√© que les humains √©prouvent √† comparer la dimension des cercles, et *a fortiori* la dimension de parties de cercle, donne peu d'avantage √† utiliser des diagrammes en pointe de tarte, souvent utilis√©s pour illustrer des proportions. Nathan Yau [sugg√®re](https://flowingdata.com/2015/08/11/real-chart-rules-to-follow/) de les utiliser avec suspicions et d'explorer d'[autres options](https://flowingdata.com/2009/11/25/9-ways-to-visualize-proportions-a-guide/).

![](https://i0.wp.com/flowingdata.com/wp-content/uploads/2015/08/pies.png?w=1800&ssl=1)

Pour comparer deux proportions, une avenue int√©ressante est le diagramme en pente, sugg√©r√© notamment par [Ann K. Emery](http://annkemery.com/avoiding-diagonal-text/#).

![](images/03_ann-emery-slope-chart.png)

Par extension, le diagramme en pente devient un diagramme en ligne lorsque plusieurs types de proportions sont compar√©es, ou lorsque des proportions √©voluent selon des donn√©es continuent.

De la m√™me mani√®re, les [diagrammes en bulles](https://datavizcatalogue.com/methods/bubble_chart.html) ne devraient pas √™tre repr√©sentatifs de la quantit√©, mais plut√¥t de contextualiser des donn√©es. Justement, le graphique tir√© des donn√©es de *Gap minder* pr√©sent√© plus haut est une contextualisation: l'aire d'un cercle ne permet pas de saisir la population d'un pays, mais de comparer grossi√®rement la population d'un pays par rapport aux autres.

### Publiez vos donn√©es

Vous avez peut-√™tre d√©j√† feuillet√© un article et voulu avoir acc√®s aux donn√©es incluses dans un graphique. Il existe des outils pour digitaliser des graphiques pour en extraire les donn√©es. Mais le processus est fastidieux, long, souvent peu pr√©cis. De plus en plus, les chercheurs sont encourag√©s √† publier leurs donn√©es et leurs calculs. Matplotlib et Seaborn sont des outils graphiques classiques qui devraient √™tre accompagn√©s des donn√©es et calculs ayant servi √† les g√©n√©rer. Mais ce n'est pas id√©al non plus. En revanche, les outils graphiques modernes comme Plotly et Altair peuvent √™tre export√©s en code javascipt, qui contient toutes les informations sur les donn√©es et la mani√®re de les repr√©senter graphiquement. Ce chapitre a pour objectif de vous familiariser avec les outils de base les plus commun√©ment utilis√©s en calcul scientifique avec Python, mais je vous encourage √† explorer la nouvelle g√©n√©ration d'outils graphiques. Nous verrons √ßa au chapitre \@ref(chapitre-git).

### Visitez www.junkcharts.typepad.com de temps √† autre 

Le statisticien et blogueur Kaiser Fung s'affaire quotidiennement √† proposer des am√©liorations √† de mauvais graphiques sur son blogue [Junk Charts](www.junkcharts.typepad.com).


```{r, include=FALSE}
rm(list = ls())
```

<!--chapter:end:04_visualisation.Rmd-->

--- 
site: bookdown::bookdown_site
output: bookdown::gitbook 
---

# Science ouverte et reproductibilit√© {#chapitre-git}

***
Ô∏è\ **Objectifs sp√©cifiques**:

√Ä la fin de ce chapitre, vous

- saurez exprimer l'importance et les enjeux de la science ouverte
- saurez arranger vos donn√©es (format csv) et votre code (format notebook) afin de rendre vos recherches reproductibles
- saurez comment cr√©er un d√©p√¥t sur GitHub, puis administrer son d√©veloppement

***

```{r git-set-seed, echo=FALSE}
set.seed(645654)
```

La science ouverte favorise la diffusion des connaissances √† travers plusieurs aspects.

- **M√©thodologie ouverte**. Ce n'est pas pour rien que les revues scientifiques demandent de la minutie dans la description de la m√©thodologie: c'est pour s'assurer de bien comprendre la signification des donn√©es collect√©es et faire en sorte que vos donn√©es puissent √™tre √©chantillonn√©es de la m√™me mani√®re dans une potentielle exp√©rience subs√©quente. √Ä ce titre, la revue *Nature* a cr√©√© le site de publication de protocoles exp√©rimentaux [Protocol exchange](https://www.nature.com/protocolexchange/), "o√π la communaut√© scientifique met en commun son savoir-faire exp√©rimental pour acc√©l√©rer la recherche" (ma traduction).
- **Donn√©es ouvertes**. En rendant nos donn√©es publiques, on permet √† la post√©rit√© de les utiliser pour am√©liorer les connaissances, d√©couvrir des structures qui nous avaient √©chapp√©es, etc. Dans certains cas, l'ouverture des donn√©es peut √™tre contrainte par des enjeux l√©gaux (donn√©es priv√©es) ou √©thiques (donn√©es pouvant √™tre utilis√©es √† mauvais escient). Dans la plupart des cas, les avantages surpassent largement les risques encourus par la publication des donn√©es, et les informations personnelles peuvent √™tre retir√©es. Des journaux comme Plos [exigent](https://blogs.plos.org/everyone/2014/02/24/plos-new-data-policy-public-access-data-2/) que les donn√©es minimales √† la reproduction de l'exp√©rience soient fournies en tant que mat√©riel suppl√©mentaire.
- **Code source ouvert**. Les logiciels *open source*, comme R, sont gratuits pour la plupart. Cela permet √† quiconque de les utiliser, pourvu que l'on poss√®de le support mat√©riel (un ordinateur) et une connection internet. De la m√™me mani√®re, le code R qui vous a permis de g√©n√©rer des r√©sultats √† partir de vos donn√©es peut √™tre rendu public sous toutes sortes de licenses *open source* peu restrictive (GPL, BSD, MIT, etc.). Avec les donn√©es et le code, vos travaux pourront √™tre reproduits.
- **R√©vision ouverte**. La r√©vision est un travail essentiel en science. Traditionnellement, les publications scientifiques sont r√©vis√©s de mani√®re anonyme, le but √©tant d'√©viter les conflits. R√©cemment, des revues comme [Frontiers](https://www.frontiersin.org/about/review-system) ont d√©ploy√© des modes de r√©vision ouverts, permettant (1) des √©changes plus constructifs entre auteurs et r√©viseurs et (2) de remercier ouvertement la contribution des r√©viseurs √† l'article final.
- **Acc√®s ouvert**. Les √©diteurs scientifiques sont [largement critiqu√©s](https://www.nature.com/articles/d41586-019-00492-4) pour demander des frais usuraires aux biblioth√®ques et pour la consultation √† la pi√®ce, ainsi que des frais de publication d√©mesur√©s. En r√©action √† cela, le site [Sci-Hub](https://fr.wikipedia.org/wiki/Sci-Hub) d√©bloque gratuitement des millions d'articles scientifiques. Aussi, des journaux s√©rieux comme Plos et Frontiers publient *de facto* les articles sur leur site internet, de sorte qu'ils peuvent √™tre librement t√©l√©charg√©s.

Le manque d'ouverture dans la science a men√© plusieurs scientifiques √† parler d'une crise de la reproductibilit√© ([Baker, 2016](https://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970)). Dans ce chapitre, nous verrons quelques astuces pour que R devienne un outil favorisant la science ouverte. √Ä la fin de ce chapitre, vous devriez √™tre en mesure de d√©ployer votre code sur une archive en ligne, comme ceci.

```{r github-exemple, fig.align="center", fig.cap="Exemple d'un dossier de code et de donn√©es ouvertes, ([Jeanne et al. 2019]())", echo = FALSE}
knitr::include_graphics("images/10_github_exemple.png")
```

## Un code reproductible

```{r repre-code-bes, out.width='50%', fig.align="center", fig.cap="A Guide to Reproducible Code in Ecology and Evolution, [BES 2017](https://www.britishecologicalsociety.org/wp-content/uploads/2017/12/guide-to-reproducible-code.pdf)", echo = FALSE}
knitr::include_graphics("images/10_bes_frontpage.png")
```

La *British ecological society* offre des lignes guide pour cr√©er un flux de travail reproductible ([BES, 2017](https://www.britishecologicalsociety.org/wp-content/uploads/2017/12/guide-to-reproducible-code.pdf)). En outre, les principes suivants doivent √™tre respect√©s (ma traduction, avec ajouts).

- Commencez votre analyse √† partir d'une copie des donn√©es brutes. Les donn√©es doivent √™tre fournies dans un format ouvert (csv, json, sqlite, etc.). √âvitez de d√©marrer une analyse par un chiffrier √©lectronique ou un logiciel propri√©taire (qui n'est pas *open source*). En ce sens, d√©marrer avec Excel (`xls` ou `xlsx`) est √† √©viter, tout comme les sont les donn√©es encod√©es pour SPSS ou SAS.
- Toute op√©ration sur les donn√©es, que ce soit du nettoyage, des fusions, des transformations, etc. devrait √™tre effectu√©e avec du code, non pas manuellement. S'il s'agit d'une erreur de frappe dans un tableau, on peut d√©roger √† la r√®gle. Mais s'il s'agit par exemple d'√©limier des outliers, ne supprimez pas des entr√©es de vos donn√©es brutes. De m√™me, n'effectuez pas de transformation de vos donn√©es brutes √† l'ext√©rieur du code. En somme, vos calculs devraient √™tre en mesure d'√™tre lanc√©s d'un seul coup, sans op√©rations manuelles interm√©diaires.
- S√©parez vos op√©rations en unit√©s logiques th√©matiques. Par exemple, vous pourriez s√©parer votre code en parties: (i) charger, fusionner et nettoyer les donn√©es, (ii) analyser les donn√©es, (iii) cr√©er des fichiers comme des tableaux et des figures.
- √âliminez la duplication du code en cr√©ant des fonctions personnalis√©es. Assurez-vous de commenter vos fonctions en d√©tails, expliquez ce qui est attendu comme entr√©es et comme sorties, ce qu'elles font et pourquoi.
- Documentez votre code et vos donn√©es √† m√™me les feuilles de calcul ou dans un fichier de documentation s√©par√©.
- Tout fichier interm√©diaire devrait √™tre s√©par√© de vos donn√©es brutes.

### Structure d'un projet

Un projet de calcul devrait √™tre contenu en un seul dossier. Si vous n'avez que quelques projets, il est assez facile de garder l'info en m√©moire. Toutefois, en particulier en milieu d'entreprise, il se pourrait fort bien que vous ayez √† mener plusieurs projets de front. Certaines entreprises cr√©ent des num√©ros de projet: vous aurez avantage √† nommer vos dossiers avec ces num√©ros, incluant une br√®ve description. Pour ma part, j'ordonne mes projets chronologiquement par ann√©e, avec un descriptif.

```
üìÅ 2019_abeille-canneberge
```

Notez que je n'utilise ni espace, ni caract√®re sp√©cial dans le nom du fichier, pour √©viter les erreurs potentielles avec des logiciels capricieux.

√Ä l'int√©rieur du dossier racine du projet, j'inclus l'information g√©n√©rale: donn√©es source (souvent des fichiers Excel), manuscrit (m√©moire, th√®se, article, etc.) documentation particuli√®re (pour les articles, j'utilise Zotero, un gestionnaire de r√©f√©rence), photos et, √©videmment, mon dossier de code (par exemple `rstats`).

```

üìÅ 2019_abeille-canneberge
|-üìÅ documentation
|-üìÅ manuscrit
|-üìÅ photos
|-üìÅ rstats
|-üìÅ source

```

Si vous r√©digez votre manuscrit √† m√™me votre code (en Latex, [Lyx](https://www.lyx.org/), markdown ou R markdown que nous verrons cela plus loin), vous pouvez tr√®s bien l'inclure dans votre fichier de calcul.

√Ä l'int√©rieur du fichier de calcul, vous aurez votre projet RStudio et vos feuilles de calcul s√©quenc√©es. J'utilise `01-`, et non pas `1-` pour √©viter que le `10-` suive le `1-` dans le classement en ordre alpha-num√©rique au cas o√π j'aurais plus de 10 feuilles de calcul. J'inclus un fichier `README.md` (extension md pour markdown), qui contient les informations g√©n√©rales de mes calculs. Les donn√©es brutes (`csv`) sont plac√©es dans un dossier `data`, mes graphiques sont export√©s dans un dossier `image`, mes tableaux sont export√©s dans un dossier `tables` et mes fonctions externes sont export√©es dans un dossier `lib`.

```

üìÅ rstats
|-üìÅ data
|-üìÅ images
|-üìÅ lib
|-üìÅ tables
üìÑ bees.Rproj
üìÑ 01_clean-data.R
üìÑ 02_data-mining.R
üìÑ 03_data-analysis.R
üìÑ 04_data-modeling.R
üìÑ README.md

```

Je d√©cris les noms de fichiers dans la langue de communication utile pour le rendu final du projet, souvent en anglais lors de publications acad√©miques. J'√©vite les noms de fichier qui ne sont pas informatifs, par exemple `01.R` ou `Rplot1.png`, ainsi que les majuscules, les caract√®res sp√©ciaux et les espaces comme dans `Deuxi√®me essai.R` (le `README.md` est une exception).

Pour partager un dossier de projet sur R, on n'a qu'√† le compresser (*zip*), puis l'envoyer. Pour que le code fonctionne sur un autre ordinateur, les liens vers les fichiers de donn√©es √† importer ou les graphiques export√©s doivent √™tre relatifs au fichier R ouvert dans votre projet, non pas le chemin complet sur votre ordinateur.

```{r rel-link-allison-horst, out.width="100%", fig.align="center", fig.cap="Retrouvez votre chemin, dessin de [Allison Horst](https://github.com/allisonhorst/stats-illustrations)", echo = FALSE}
knitr::include_graphics("https://raw.githubusercontent.com/allisonhorst/stats-illustrations/master/rstats-artwork/here.png")
```

Tout comme la BSE, l'organisme sans but lucratif [rOpenSci](https://ropensci.org) offre [un guide sur la reproductibilit√©](http://ropensci.github.io/reproducibility-guide/).

### Le format [R markdown](https://rmarkdown.rstudio.com/)

Un code reproductible est un code bien d√©crit. La structure de projet pr√©sent√©e pr√©c√©demment propose de segmenter le code en plusieurs fichiers R. Cette mani√®re de proc√©der est optionnelle. Si le fichier de calcul n'est pas trop encombrant, on pourra n'en utiliser qu'un seul, par exemple `stats.R`. √Ä l'int√©rieur m√™me des feuilles de calcul R, vous devrez commenter votre code pour en expliquer les √©tapes, par exemple:

```
#############
## Titre 1 ##
#############

# Titre 2
## Titre 3
data <- read_csv("data/abeilles.csv") # commentaire particulier

```

RStudio a d√©velopp√© une approche plus conviviale avec son format **R markdown**. Le langage [*markdown*](https://daringfireball.net/projects/markdown/basics) permet de formater un texte avec un minimum de d√©corations, et R markdown permet d'int√©grer du texte et des codes. Ces notes de cours sont par ailleurs enti√®rement √©crites en R markdown.

```{r git-allison-horst-rmarkdown, out.width="100%", fig.align="center", fig.cap="La magie de *R markdown*, dessin de [Allison Horst](https://github.com/allisonhorst/stats-illustrations)", echo = FALSE}
knitr::include_graphics("images/10_rmarkdown_wizards.png")
```

#### Le langage markdown

Un fichier portant l'extension `.md` ou `.markdown` est un fichier texte clair (que vous pouvez ouvrir et √©diter dans n'importe votre √©diteur texte pr√©f√©r√©), tout comme un fichier `.R`. Il existe n√©anmoins de nombreux √©diteurs de texte sp√©cialis√©s en √©dition markdown - mon pr√©f√©r√© est [Typora](https://typora.io). Les d√©corations principales en markdown sont les suivantes (les citations utilis√©es ci-apr√®s sont tir√©es du roman Dune, de Frank Herbert).

**Italique**. Pour emphaser en italique, balisez le texte avec des ast√©risques. Par exemple, "`Pourrais-je porter parmi vous le nom de *Paul-Muad'dib*?`" devient "Pourrais-je porter parmi vous le nom de *Paul-Muad'dib*?"

**Gras**. Pour emphaser en gras, balisez le texte avec des doubles ast√©risques. Par exemple, "`L'esp√©rance **ternit** l'observation.`" devient "L'esp√©rance **ternit** l'observation".

**Largeur fixe**. Pour un texte √† largeur fixe (signifiant du code), balisez le texte avec des accents graves. Par exemple, "``Quel nom donnez-vous √† la petite `souris`, celle qui saute ?``" devient "Quel nom donnez-vous √† la petite `souris`, celle qui saute?"

**Listes**. Pour effectuer une liste num√©rot√©e, utilisez le chiffre `1.` Par exemple,

```

1. Paul
1. Leto
1. Alia

```

devient

1. Paul
1. Jessica
1. Alia

De m√™me, pour une liste √† puces, changez le `1.` par le `- ` ou le `* `.

**Ent√™tes**. Les titres sont pr√©c√©d√©s par des `#`. Un `# ` pour un titre 1, deux `## ` pour un titre 2, etc. Par exemple,

```

# Imperium
## Landsraad
### Maison des Atr√©ides
### Maison des Harkonnen
## CHOAM
# Guilde des navigateurs

```

Ins√©rera les titres appropri√©s (que je n'ins√®re pas pour ne pas bousiller la structure de ce texte).

**Liens**. Pour ins√©rer des liens, le texte est entre crochet directement suivi du lien entre parenth√®ses. Par exemple, "`Longue vie aux [combattants](https://youtu.be/Cv87NJ2xX0k?t=59)`" devient "Longue vie aux [combattants](https://youtu.be/Cv87NJ2xX0k?t=59)".

**√âquations**. Les √©quations suivent la syntaxe Latex entre deux `$$` pour les √©quations sur une ligne et entre des doubles `$$$$` pour les √©quations sur un paragraphe. Par exemple, `$c = \sqrt{a^2 + b^2}$` devient $c = \sqrt{a^2 + b^2}$.

**Images**. Pour ins√©rer une image, `![nom de l'image](images/spice-must-flow.png)`.

Une liste exhaustive des balises markdown est disponible sous forme d'[aide-m√©moire](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). L'extension de RStudio **`remedy`**, installable tout comme un module, fera appara√Ætre une section REMEDY dans le menu Addins, o√π vous trouverez toutes sortes d'options de formatage automatique (figure \@ref(fig:git-remedy)).

```{r git-remedy, out.width="40%", fig.align="center", fig.cap="Menu des extensions de RStudio, avec l'extension **`remedy`**", echo = FALSE}
knitr::include_graphics("images/10_menu-remedy.png")
```

#### R markdown

Dans RStudio, ouvrez un R markdown par `File > New file > R Markdown`. Si le module **`rmarkdown`** n'est pas install√©, RStudio vous demandera de l'installer. Une fen√™tre appara√Ætra.

```{r r-md-new-file, out.width="60%", fig.align="center", fig.cap="Nouveau fichier R markdown", echo = FALSE}
knitr::include_graphics("images/10_file-new-rmarkdown.png")
```

Les options d'exportation pourront √™tre modifi√©es par la suite.

Un fichier d'exemple sera cr√©√©, et vous pourrez le modifier. Les parties de texte sont √©crits en markdown, et le code R est ench√¢ss√© entre les balises ```` ```{r} ```` et ```` ``` ````. Je nommerai ces parties de code des *cellules* de code.

Des options de code l'int√©rieur peuvent √™tre utilis√©es √† l'int√©rieur des accolades `{r}`. Par exemple

- `{r, filtre-outliers}` donne le nom `filtre-outliers` au bloc de code, qui permet nomm√©ment de nommer les images cr√©er dans le bloc de code.
- `{r, eval = FALSE}` permet d'activer (`TRUE`, valeur par d√©faut) ou de d√©sactiver (`FALSE`) le calcul de la cellule.
- `{r, echo = FALSE}` permet de n'afficher que la sortie de la cellule de code en n'affichant pas le code, par exemple un graphique ou le sommaire d'une r√©gression.
- `{r, results = FALSE}` permet de n'afficher que le code, mais pas la sortie.
- `{r, warning = FALSE, message = FALSE, error = FALSE}` n'affichera pas les avertissements, les messages automatiques et les messages d'erreur.
- `{r, fig.width = 10, fig.height = 5, fig.align = "center"}` affichera les graphiques dans les dimensions voulues, align√©e au centre (`"center"`), √† gauche (`"left"`) ou √† droite (`"right"`).

Notez que vous pouvez ex√©cuter rapidement du code sur une ligne avec la formulation ``` `r ` ```, par exemple ```la moyenne des nombres `\r a<-round(runif(4, 0, 10)); a` est de `\r mean(a)` ```, en enlevant les `\` devant les `r` (ajout√©es artificiellement pour √©viter que le code soit calcul√©) sera la moyenne des nombres `r a<-round(runif(4, 0, 10)); a` est de `r mean(a)`

Une fois que vous serez satisfait de votre document, cliquer sur `Knit` ![](images/10_knit-button.png) et le fichier de sortie sera g√©n√©r√©. Le guide qui permet de g√©n√©rer le fichier de sortie est tout en haut du fichier. Nous l'appelons le YAML (acronyme r√©cursif de *YAML Ain't Markup Language*). Prenez le YAML suivant.

```
---
title: "Dune"
author: "Frank Herbert"
date: "1965-08-01"
output: github_document
---
```

Le titre, l'auteur et la date sont sp√©cifi√©es. Pour indiquer la date courante, on peut simplement la g√©n√©rer avec R en rempla√ßant `"1965-08-01"` par `r format(Sys.Date())`. La sp√©cification `output` indique le type de document √† g√©n√©rer, par exemple `html_document` pour une page web, `pdf_document` pour un pdf, ou `word_document` pour un docx. Dans ce cas-ci, j'indique `github_document` pour cr√©er un fichier markdown comprenant nomm√©ment des liens relatifs vers les images des graphiques g√©n√©r√©s. Pourquoi un `github_document`? C'est le sujet de la prochaine sous-section. Mais avant cela, je vous r√©f√®re √† un autre aide-m√©moire.

```{r r-md-cs, out.width="100%", fig.align="center", fig.cap="[Aide-m√©moire pour R Markdown, Source: RStudio](https://github.com/rstudio/cheatsheets/raw/master/rmarkdown-2.0.pdf)", echo = FALSE}
knitr::include_graphics("https://www.rstudio.com/wp-content/uploads/2018/08/rmarkdown-2.0-600x464.png")
```

## Introduction √† GitHub

Le system de suivi de version **`git`** (*open source*) a √©t√© cr√©√© par Linus Torvalds, aussi connu pour avoir cr√©√© Linux. **`git`** prend une photo de votre r√©pertoire de projet √† chaque fois que vous *commettez* un changement. Vous pourrez revenir sans probl√®me sur d'anciennes versions si quelque chose tourne mal, et vous pourrez publier le r√©sultat final sur un service d'h√©bergement utilisant **`git`**.

Il existe plusieurs services pour rendre **`git`** utilisable en ligne, mais GitHub est d√©finitivement le plus utilis√© d'entre tous. La plateforme [GitHub](https://github.com/) est presque devenue un r√©seau social de d√©veloppement. GitHub, maintenant la propri√©t√© de Microsoft, n'est en soi pas *open source*. Si comme moi vous avez un penchant pour l'*open source*, je vous redirige vers la plateforme [GitLab](https://about.gitlab.com/), qui fonctionne √† peu pr√®s de la m√™me mani√®re que GitHub, mais dans sa version gratuite GitLab vous octroie autant de r√©pertoires priv√©s que vous d√©sirez. Seul hic, alors que la plateforme GitHub sera fort probablement toujours vivante dans plusieurs ann√©es, on en est moins s√ªr pour GitLab. C'est pourquoi, en r√®gle g√©n√©rale, j'utilise [GitHub](https://github.com/essicolo) √† des fins professionnelles mais [GitLab](gitlab.com/essicolo) √† des fins personnelles.

Pour suivre cette partie du cours, je vous invite √† [cr√©er un compte sur GitHub](https://github.com/join?source=header-home) ou [GitLab](https://gitlab.com/users/sign_in?redirect_to_referer=yes&nav_source=navbar), √† votre choix. Cr√©ez un nouveau d√©p√¥t (*New repository*).

```{r github-new-repo, out.width="100%", fig.align="center", fig.cap="Nouveau d√©p√¥t avec GitHub", echo = FALSE}
knitr::include_graphics("images/10_github-new-repo.gif")
```

```{r gitlab-new-repo, out.width="100%", fig.align="center", fig.cap="Nouveau d√©p√¥t avec GitLab", echo = FALSE}
knitr::include_graphics("images/10_gitlab-new-repo.gif")
```

Pour utiliser **`git`**, vous pourrez toujours travailler en ligne de commande, mais je vous sugg√®re d'utiliser [GitHub Desktop](https://desktop.github.com/) (qui fonctionne aussi sur GitLab) - √©videmment, d'[autres logiciels similaires existent](https://git-scm.com/downloads/guis). Github Desktop vous permettra d'abord de *cloner* un r√©pertoire en ligne. Le clonage vous permet de cr√©er une copie locale (sur votre ordinateur) du r√©pertoire.

```{r github-clone-repo, out.width="100%", fig.align="center", fig.cap="Cloner d√©p√¥t avec GitHub", echo = FALSE}
knitr::include_graphics("images/10_github-clone-repo.gif")
```

```{r gitlab-clone-repo, out.width="100%", fig.align="center", fig.cap="Cloner d√©p√¥t avec GitLab", echo = FALSE}
knitr::include_graphics("images/10_gitlab-clone-repo.gif")
```

Une fois que le d√©p√¥t est clon√©, il est sur votre ordinateur. Lorsque vous effectuez un changement, vous devez commettre (*commit*), puis envoyer (*push*) vos changements vers le d√©p√¥t en ligne. Pour que votre document markdown soit lisible par GitHub et GitLab, il doit √™tre export√© sous forme de `github_document`. Un fichier `.md` sera cr√©√©, et inclura les d√©tails de votre feuille de calculs, images y compris!

```{r github-push-repo, out.width="100%", fig.align="center", fig.cap="Commettre et d√©ployer un d√©p√¥t avec GitHub", echo = FALSE}
knitr::include_graphics("images/10_github-push.gif")
```

L'interface de GitHub Desktop vous permet de revenir en arri√®re en √©liminant des *commits* pr√©c√©dents.

```{r github-revert, out.width="100%", fig.align="center", fig.cap="Revenir en arri√®re avec GitHub desktop", echo = FALSE}
knitr::include_graphics("images/10_github-revert.png")
```

Vous pourrez ajouter des collaborateurs √† votre d√©p√¥t, pour que plusieurs personnes travaillent de front sur un m√™me d√©p√¥t. Il est aussi possible de cr√©er une branche d'un d√©p√¥t, fusionner la branche de d√©veloppement avec la branche principale, commenter les codes, sugg√©rer des changements, etc., mais cela sort du cadre d'un cours sur la reproductibilit√©.

Enfin, pour renvoyer un article vers votre mat√©riel suppl√©mentaire, ins√©rez le lien dans la section m√©thodologie. Il peut s'agit du lien complet, ou bien d'un lien raccourci avec [git.io](https://git.io/). Par exemple,

> The data and the R code used to compute the results are both available as supplementary material at https://git.io/fhHEj.

Notez que RStudio offre une interface pour utiliser **`git`** via un onglet afich√© en haut √† droite dans l'affichage par d√©faut. Ne l'ayant jamais utilis√©, et je ne me sens pas √† l'aise d'en sugg√©rer l'utilisation, mais libre √† vous d'explorer cet outil et de vous l'approprier!

```{r rstudio-git, out.width="100%", fig.align="center", fig.cap="L'outil *Git* de RStudio", echo = FALSE}
knitr::include_graphics("10_git-rstudio.png")
```

## Introduction √† Pakrat üì¶üêÄ

Alors que les modules sont continuellement mis √† jour, on doit s'assurer que l'on sache exactement quelle version a √©t√© utilis√©e si l'on d√©sire √™tre stricte sur la reproductibilit√©. Lorsque je r√©vise un article, je demande √† ce que le nom des modules utilis√©s et leur num√©ro de version soient explicitement cit√©s et r√©f√©renc√©s. Par exemple, dans un article sur l'analyse de compositions foliaires de laitues inocul√©es par une bact√©rie, j'√©crivais:

> Computations were performed in the R statistical language version 3.4.1 (R Development Core Team, 2017). The main packages used in the data analysis workflow were the vegan package version 2.4-3 (Oksanen et al., 2017) for ordination, the compositions package version 1.40-1 (van den Boogaart and Tolosana-Delgado, 2013) for ilr transformations, the nlme version 3.1-131 (Pinheiro et al., 2017) package to compute the random experimental effect, the mvoutlier package version 2.0.8 (Filzmoser and Gschwandtner, 2017) for multivariate outlier detection, and the ggplot2 package version 2.2.1 (Wickham and Chang, 2017) for data visualization. The data and computations are publicly available at https://github.com/essicolo/Nicolas-et-al_Infected-lettuce-ionomics. [Nicolas et al., 2019](https://www.frontiersin.org/articles/10.3389/fpls.2019.00351)

De cette mani√®re, une personne (que ce soit vos coll√®gues, quiconque voudra auditer ou √©valuer votre code ou vous-m√™me dans le futur) pourra reproduire le code publi√© sur GitHub en installant les versions de R et des modules cit√©s. Mais cela est fastidieux. C'est pourquoi l'√©quipe de RStudio (oui, encore ceux-l√†) ont d√©velopp√© le module [**`packrat`**](https://rstudio.github.io/packrat/), qui permet d'installer les modules √† m√™me voter dossier de projet (le dossier contenant le fichier `.Rproj`).

Pour l'utiliser √† tout moment en cours de projet,

```{r rstudio-packrat, out.width="100%", fig.align="center", fig.cap="L'outil *Packrat* de RStudio", echo = FALSE}
knitr::include_graphics("images/10_packrat-rstudio.png")
```

Le `.gitignore` contient tous les documents et les types de documents qui sont ignor√©s par **`git`**. L'option par d√©faut est d'ignorer le dossier `lib`, qui contient les modules install√©s, mais de garder le dossier `src`, qui contient la source des modules non install√©s (qui devront √™tre install√©s par les autres personnes utilisant votre projet). Mieux vaut garder les options par d√©faut. Initialiser *Packrat* revient √† scanner vos documents de projet pour trouver les modules utilis√©s et cr√©er un paquet contenant tout cela √† m√™me votre projet, dans un dossier `packrat`.

```

üìÅ rstats
|-üìÅ data
|-üìÅ images
|-üìÅ lib
|-üìÅ packrat
|-üìÅ tables
üìÑ sentier-d-or.Rproj
üìÑ stats.Rmd
üìÑ README.md

```

Ce dossier contiendra tout ce qu'il faut pour utiliser les modules du projet d'une personne que l'on nommera Leto. Lorsqu'une autre personne, appellons-la Ghanima, utilisera le projet de Leto, RStudio v√©rifiera si le module **`packrat`** est bien install√©, et l'installera s'il ne l'est pas (Leto et Ghanima sont deux personnage de la s√©rie de science-fiction Dune). Pour utiliser les modules du projet et non pas les modules de son ordinateur, Ghanima lancera la fonction `packrat::restore()`. Si Leto d√©cide de mettre √† jour ses modules en cours de projet, il lancera la fonction `packrat::snapshot()` pour que ces nouveaux modules soit int√©gr√©s √† son projet. Lorsque Leto commettra (*commit*) ses changements dans git et les publiera (*push*) sur GitHub, puis lorsque Ghanima mettra √† jour (*fetch*) son d√©p√¥t local git li√© au d√©p√¥t GitHub, elle devra √† nouveau lancer `packrat::restore()` pour que les modules soient bel et bien ceux utilis√©s par Leto.

## Pour terminer, le **reprex**

Lorsque j'ai d√©couvert un bogue dans le module **`weathercan`**, [j'ai ouvert une *issue* sur GitHub](https://github.com/ropensci/weathercan/issues/70) en indiquant le message d'erreur obtenu, en esp√©rant que l'origine du bogue puisse √™tre facilement d√©duit. Un d√©veloppeur de **`weathercan`** m'a demand√© un **reprex**. J'ai √©t√© d√©√ßu lorsque j'ai compris que le **reprex** n'√©tait pas une esp√®ce de dinosaure, mais plut√¥t un exemple reproductible (***re**producible **ex**ample*).

> üìó **Reprex**: Un exemple reproductible.

J'ai essay√© d'isoler le probl√®me pour reproduire l'erreur avec le minimum de code possible. √Ä partir d'un code de plus de 7000 lignes (les pr√©sentes notes de cours), j'en suis arriv√© √† ceci:

```{r git-reprex, eval = FALSE}
stations <- data.frame(A = 1)

library("weathercan")
mont_bellevue <- weather_dl(
  station_ids = c(5397, 48371),
  start = "2019-02-01",
  end = "2019-02-07",
  interval = "hour",
  verbose = TRUE
)
```

, qui me retournait l'erreur

```
Getting station: 5397
Formatting station data: 5397
Error in strptime(xx, f, tz = tz) : valeur 'tz' incorrecte
```

Le bogue: la fonction `weather_dl()` utilisait √† l'interne un objet nomm√© `stations`, qui entrait en conflit avec un objet `stations` s'il √©tait d√©fini hors de la fonction.

Synth√©tiser une question n'est pas facile (cr√©er cet exemple reprductible m'a pris pr√®s de 2 heures). Mais r√©pondre √† une question non synth√©tis√©e, c'est encore plus difficile. C'est pourquoi on (moi y compris) vous demandera syst√©matiquement un *reprex* lorsque vous poserez une question li√©e √† une erreur syst√©matique, le plus souvent en programmation.

> Un exemple reproductible permet √† quelqu'un de recr√©er l'erreur que vous avez obtenue simplement en copiant-collant votre code. - [Hadley Wickham](https://gist.github.com/hadley/270442)

Selon [Hadley Wickham](https://gist.github.com/hadley/270442) (gourou de R), un *reprex* devrait comprendre quatre √©l√©ments (je joue √† l'h√©r√©tique en me permettant d'adapter le document du gourou):

1. Les modules devraient √™tre charg√©s en d√©but de code.
1. Puis vous chargez des donn√©es, qui peuvent √™tre des donn√©es d'exemple ou des donn√©es incluses √† m√™me le code R (comme des donn√©es g√©n√©r√©es au hasard).
1. Assurez-vous que voter code est un exemple minimal (retirer le superflu) et qu'il soit facilement lisible.
1. Incluez la sortie de la fonction `sessionInfo()`, qui indique la plateforme mat√©rielle et logicielle sur laquelle vous avez g√©n√©r√© l'erreur. Ceci est important en particulier s'il s'agit d'un bogue.

Lorsque vous pensez avoir g√©n√©r√© votre *reprex*, red√©marrez R (`Session > Restart R` dans RStudio), puis lancez votre code pour vous assurer que l'erreur puisse √™tre g√©n√©r√©e dans un nouvel environnement tout propre.

```{r git-rm, include=FALSE}
rm(list = ls())
```

<!--chapter:end:05_github.Rmd-->

# Biostatistiques {#chapitre-biostats}

 ***
Ô∏è\ **Objectifs sp√©cifiques**:

√Ä la fin de ce chapitre, vous

- serez en mesure de d√©finir les concepts de base en statistique: population, √©chantillon, variable, probabilit√© et distribution
- serez en mesure de calculer des statistiques descriptives de base: moyenne et √©cart-type, quartiles, maximum et minimum
- comprendrez les notions de test d'hypoth√®se, d'effet et de p-value, ainsi qu'√©viter les erreurs communes dans leur interpr√©tation
- saurez effectuer une mod√©lisation statistique lin√©aire simple, multiple et mixte, entre autre sur des cat√©gories
- saurez effectuer une mod√©lisation statistique non lin√©aire simple, multiple et mixte

 ***


Aux chapitres pr√©c√©dents, nous avons vu comment visualiser, organiser et manipuler des tableaux de donn√©es. La statistique est une collection de disciplines li√©es √† la collecte, l‚Äôorganisation, l'analyse, l'interpr√©tation et la pr√©sentation de donn√©es. Les biostatistiques est l'application de ces disciplines √† la biosph√®re.

Dans [*Principles and procedures of statistics: A biometrical approach*](https://www.amazon.com/Principles-Procedures-Statistics-Biometrical-Approach/dp/0070610282), Steel, Torie et Dickey (1997) d√©finissent les statistiques ainsi:

> Les statistiques forment la science, pure et appliqu√©e, de la cr√©ation, du d√©veloppement, et de l'application de techniques par lesquelles l'incertitude de l'induction inf√©rentielle peut √™tre √©valu√©e. (ma traduction)

Alors que l'inf√©rence consiste √† g√©n√©raliser des observations sur des √©chantillons √† l'ensemble d'une population, l'induction est un type de raisonnement qui permet de g√©n√©raliser des observations en th√©ories. Les statistiques permettent d'√©valuer l'incertitude d√©coulant du processus qui permet d'abord de passer de l'√©chantillon √† la population repr√©sent√©e par cet √©chantillon, puis de passer de cette repr√©sentation d'une population en lois g√©n√©rales la concernant.

La d√©finition de Whitlock et Schuluter (2015), dans [The Analysis of Biological Data](http://whitlockschluter.zoology.ubc.ca/), est plus simple, insistant sur l'inf√©rence:

> La statistique est l‚Äô√©tude des m√©thodes pour mesurer des aspects de populations √† partir d‚Äô√©chantillons et pour quantifier l'incertitude des mesures. (ma traduction)

Les statistiques consistent √† *faire du sens* (anglicisme assum√©) avec des observations dans l'objectif de r√©pondre √† une question que vous aurez formul√©e clairement, pr√©alablement √† votre exp√©rience.

<blockquote class="twitter-tweet" data-lang="fr"><p lang="en" dir="ltr">The more time I spend as The Statistician in the room, the more I think the best skill you can cultivate is the ability to remain calm and repeatedly ask &quot;What question are you trying to answer?&quot;</p>&mdash; Bryan Howie (@bryan_howie) <a href="https://twitter.com/bryan_howie/status/1073054519808876544?ref_src=twsrc%5Etfw">13 d√©cembre 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

Le flux de travail conventionnel consiste √† collecter des √©chantillons, transformer les donn√©es, effectuer des tests, analyser les r√©sultats, les interpr√©ter et les visualiser. Bien que ces t√¢ches soient complexes, en particulier en ce qui a trait aux tests statistiques, la plupart des op√©rations statistiques peuvent √™tre effectu√©es sans l'assistance de statisticien.ne.s... √† condition de comprendre suffisamment les concepts utilis√©s. Ce chapitre √† lui seul est trop court pour permettre d'int√©grer toutes les connaissances n√©cessaires √† une utilisation raisonn√©e des statistiques, mais fourni les bases pour aller plus loin. Notez que les erreurs d'interpr√©tation statistiques sont courantes et la consultation de sp√©cialistes n'est souvent pas un luxe.

Dans ce chapitre, nous verrons comment r√©pondre correctement √† une question valide et ad√©quate avec l'aide d'outils de calcul scientifique. Nous couvrirons les notions de bases des distributions et des variables al√©atoires qui nous permettront d'effectuer des tests statistiques communs avec R. Nous couvrirons aussi les erreurs commun√©ment commises en recherche acad√©mique et les moyens simples de les √©viter.

Ce chapitre est une introduction aux statistiques avec R, et ne remplacera pas un bon cours de stats.

En plus des modules de base de R nous utiliserons

* les modules de la **`tidyverse`**,
* le module de donn√©es agricoles **`agridat`**, ainsi que 
* le module **`nlme`** sp√©cialis√© pour la mod√©lisation mixte. 

Avant de survoler les applications statistiques avec R, je vais d'abord et rapidement pr√©senter quelques notions importantes en statistiques : populations et √©chantillons, variables, probabilit√©s et distributions. Nous allons effectuer des tests d'hypoth√®se univari√©s (notamment les tests de *t* et les analyses de variance) et d√©tailler la notion de p-value. Mais avant tout, je vais m'attarder plus longuement aux mod√®les lin√©aires g√©n√©ralis√©s, incluant en particulier des effets fixes et al√©atoires (mod√®les mixtes), qui fournissent une trousse d'analyse polyvalente en analyse multivari√©e. Je terminerai avec les perspectives multivari√©es que sont les matrices de covariance et de corr√©lation.

## Populations et √©chantillons

Le principe d'inf√©rence consiste √† g√©n√©raliser des conclusions √† l'√©chelle d'une population √† partir d'√©chantillons issus de cette population. Alors qu'une **population** contient tous les √©l√©ments √©tudi√©s, un **√©chantillon** d'une population est une observation unique. Une exp√©rience bien con√ßue fera en sorte que les √©chantillons sont repr√©sentatifs de la population qui, la plupart du temps, ne peut √™tre observ√©e enti√®rement pour des raisons pratiques.

Les principes d'exp√©rimentation servant de base √† la conception d'une bonne m√©thodologie sont pr√©sent√©s dans le cours [*Dispositifs exp√©rimentaux (BVG-7002)*](https://www.ulaval.ca/les-etudes/cours/repertoire/detailsCours/bvg-7002-dispositifs-experimentaux.html). √âgalement, je recommande le livre *Principes d'exp√©rimentation: planification des exp√©riences et analyse de leurs r√©sultats* de Pierre Dagnelie (2012), [disponible en ligne en format PDF](http://www.dagnelie.be/docpdf/ex2012.pdf). Un bon aper√ßu des dispositifs exp√©rimentaux est aussi pr√©sent√© dans [*Introductory Statistics with R*](https://www.springer.com/us/book/9780387790534), de Peter Dalgaard (2008).

Une population est √©chantillonn√©e pour induire des **param√®tres**: un rendement typique dans des conditions m√©t√©orologiques, √©daphiques et manag√©riales donn√©es, la masse typique des faucons p√®lerins, m√¢les et femelles, le microbiome typique d'un sol agricole ou forestier, etc. Une **statistique** est une estimation d'un param√®tre calcul√©e √† partir des donn√©es, par exemple une moyenne et un √©cart-type.

Par exemple, la moyenne ($\mu$) et l'√©cart-type ($\sigma$) d'une population sont estim√©s par les moyennes ($\bar{x}$) et √©carts-types ($s$) calcul√©s sur les donn√©es issues de l'√©chantillonnage.

Chaque param√®tre est li√©e √† une perspective que l'on d√©sire conna√Ætre chez une population. Ces angles d'observations sont les **variables**.

## Les variables

Nous avons abord√© au chapitre 4 la notion de *variable* par l'interm√©diaire d'une donn√©e. Une variable est l'observation d'une caract√©ristique d√©crivant un √©chantillon et qui est susceptible de varier d'un √©chantillon √† un autre. Si les observations varient en effet d'un √©chantillon √† un autre, on parlera de variable al√©atoire. M√™me le hasard est r√©gi par certaines lois: ce qui est al√©atoire dans une variable peut √™tre d√©crit par des **lois de probabilit√©**, que nous verrons plus bas.

Mais restons aux variables pour l'instant. Par convention, on peut attribuer aux variables un symbole math√©matique. Par exemple, on peut donner √† la masse volumique d'un sol (qui est le r√©sultat d'une m√©thodologie pr√©cise) le symbole $\rho$. Lorsque l'on attribue une valeur √† $\rho$, on parle d'une donn√©e. Chaque donn√©e d'une observation a un indice qui lui est propre, que l'on d√©signe souvent par $i$, que l'on place en indice $\rho_i$. Pour la premi√®re donn√©e, on a $i=1$, donc $\rho_1$. Pour un nombre $n$ d'√©chantillons, on aura $\rho_1$, $\rho_2$, $\rho_3$, ..., $\rho_n$, formant le vecteur $\rho = \left[\rho_1, \rho_2, \rho_3, ..., \rho_n \right]$.

En R, une variable est associ√©e √† un vecteur ou une colonne d'un tableau.

```{r}
rho <- c(1.34, 1.52, 1.26, 1.43, 1.39) # matrice 1D
data <- data.frame(rho = rho) # tableau
data
```

Il existe plusieurs types de variables, qui se regroupe en deux grandes cat√©gories: les **variables quantitatives** et les **variables qualitatives**.

### Variables quantitatives

Ces variables peuvent √™tre continues dans un espace √©chantillonnal r√©el ou discr√®tes dans un espace √©chantillonnal ne consid√©rant que des valeurs fixes. Notons que la notion de nombre r√©el est toujours une approximation en sciences exp√©rimentales comme en calcul num√©rique, √©tant donn√©e que l'on est limit√© par la pr√©cision des appareils comme par le nombre d'octets √† utiliser. Bien que les valeurs fixes des distributions discr√®tes ne soient pas toujours des valeurs enti√®res, c'est bien souvent le cas en biostatistiques comme en d√©mographie, o√π les d√©comptes d'individus sont souvent pr√©sents (et o√π la notion de fraction d'individus n'est pas accept√©e).

### Variables qualitatives

On exprime parfois qu'une variable qualitative est une variable impossible √† mesurer num√©riquement: une couleur, l'appartenance √† esp√®ce ou √† une s√©rie de sol. Pourtant, dans bien des cas, les variables qualitatives peut √™tre encod√©es en variables quantitatives. Par exemple, on peut accoler des pourcentages de sable, limon et argile √† un loam sableux, qui autrement est d√©crit par la classe texturale d'un sol. Pour une couleur, on peut lui associer des pourcentages de rouge, vert et bleu, ainsi qu'un ton. En ce qui a trait aux variables ordonn√©es, il est possible de supposer un √©talement. Par exemple, une variable d'intensit√© faible-moyenne-forte peut √™tre transform√©e lin√©airement en valeurs quantitatives -1, 0 et 1. Attention toutefois, l'√©talement peut parfois √™tre quadratique ou logarithmique. Les s√©ries de sol peuvent √™tre encod√©es par la proportion de gleyfication ([Parent et al., 2017](https://www.frontiersin.org/articles/10.3389/fenvs.2017.00081/full#B4)). Quant aux cat√©gories difficilement transformables en quantit√©s, on pourra passer par l'**encodage cat√©goriel**, souvent appel√© *dummyfication*, qui nous verrons plus loin.

## Les probabilit√©s

> ¬´ Nous sommes si √©loign√©s de conna√Ætre tous les agens de la nature, et leurs divers modes d'action ; qu'il ne serait pas philosophique de nier les ph√©nom√®nes, uniquement parce qu'ils sont inexplicables dans l'√©tat actuel de nos connaissances. Seulement, nous devons les examiner avec une attention d'autant plus scrupuleuse, qu'il para√Æt plus difficile de les admettre ; et c'est ici que le calcul des probabilit√©s devient indispensable, pour d√©terminer jusqu'√† quel point il faut multiplier les observations ou les exp√©riences, afin d'obtenir en faveur des agens qu'elles indiquent, une probabilit√© sup√©rieure aux raisons que l'on peut avoir d'ailleurs, de ne pas les admettre. ¬ª ‚Äî Pierre-Simon de Laplace

Une probabilit√© est la vraisemblance qu'un √©v√®nement se r√©alise chez un √©chantillon. Les probabilit√©s forment le cadre des syst√®mes stochastiques, c'est-√†-dire des syst√®mes trop complexes pour en conna√Ætre exactement les aboutissants, auxquels on attribue une part de hasard. Ces syst√®mes sont pr√©dominants dans les processus vivants.

On peut d√©gager deux perspectives sur les probabilit√©s: l'une passe par une interpr√©tation fr√©quentielle, l'autre bay√©sienne. L'interpr√©tation **fr√©quentielle** repr√©sente la fr√©quence des occurrences apr√®s un nombre infini d‚Äô√©v√®nements. Par exemple, si vous jouez √† pile ou face un grand nombre de fois, le nombre de pile sera √©gal √† la moiti√© du nombre de lanc√©s. Il s'agit de l'interpr√©tation commun√©ment utilis√©e.

L'interpr√©tation **bay√©sienne** vise √† quantifier l'incertitude des ph√©nom√®nes. Dans cette perspective, plus l'information s'accumule, plus l'incertitude diminue. Cette approche gagne en notori√©t√© notamment parce qu'elle permet de d√©crire des ph√©nom√®nes qui, intrins√®quement, ne peuvent √™tre r√©p√©t√©s infiniment (absence d'asymptote), comme celles qui sont bien d√©finis dans le temps ou sur des populations limit√©s.

L'approche fr√©quentielle teste si les donn√©es concordent avec un mod√®le du r√©el, tandis que l'approche bay√©sienne √©value la probabilit√© que le mod√®le soit r√©el. Une erreur courante consiste √† aborder des statistiques fr√©quentielles comme des statistiques bay√©siennes. Par exemple, si l'on d√©sire √©valuer la probabilit√© de l‚Äôexistence de vie sur Mars, on devra passer par le bay√©sien, car avec les stats fr√©quentielles, l'on devra plut√¥t conclure si les donn√©es sont conformes ou non avec l'hypoth√®se de la vie sur Mars (exemple tir√©e du blogue [Dynamic Ecology](https://dynamicecology.wordpress.com/2011/10/11/frequentist-vs-bayesian-statistics-resources-to-help-you-choose/)).

Des rivalit√©s factices s'installent enter les tenants des diff√©rentes approches, dont chacune, en r√©alit√©, r√©pond √† des questions diff√©rentes dont il convient r√©fl√©chir sur les limitations. Bien que les statistiques bay√©siennes soient de plus en plus utilis√©es, nous ne couvrirons dans ce chapitre que l'approche fr√©quentielle. L'approche bay√©sienne est n√©anmoins trait√©e dans le chapitre \@ref(chapitre-biostats-bayes).

## Les distributions

Une variable al√©atoire peut prendre des valeurs selon des mod√®les de distribution des probabilit√©s. Une distribution est une fonction math√©matique d√©crivant la probabilit√© d'observer une s√©rie d'√©v√®nements. Ces √©v√®nements peuvent √™tre des valeurs continues, des nombres entiers, des cat√©gories, des valeurs bool√©ennes (Vrai/Faux), etc. D√©pendemment du type de valeur et des observations obtenues, on peut associer des variables √† diff√©rentes lois de probabilit√©. Toujours, l'aire sous la courbe d'une distribution de probabilit√© est √©gale √† 1.

En statistiques inf√©rentielles, les distributions sont les mod√®les, comprenant certains param√®tres comme la moyenne et la variance pour les distributions normales, √† partir desquelles les donn√©es sont g√©n√©r√©es.

Il existe deux grandes familles de distribution: **discr√®tes** et **continues**. Les distributions discr√®tes sont contraintes √† des valeurs pr√©d√©finies (finies ou infinies), alors que les distributions continues prennent n√©cessairement un nombre infini de valeur, dont la probabilit√© ne peut pas √™tre √©valu√©e ponctuellement, mais sur un intervalle.

L'**esp√©rance** math√©matique est une fonction de tendance centrale, souvent d√©crite par un param√®tre. Il s'agit de la moyenne d'une population pour une distribution normale. La **variance**, quant √† elle, d√©crit la variabilit√© d'une population, i.e. son √©talement autour de l'esp√©rance. Pour une distribution normale, la variance d'une population est aussi appel√©e variance, souvent pr√©sent√©e par l'√©cart-type.

### Distribution binomiale

En tant que sc√©nario √† deux issues possibles, des tirages √† pile ou face suivent une loi binomiale, comme toute variable bool√©enne prenant une valeur vraie ou fausse. En biostatistiques, les cas communs sont la pr√©sence/absence d'une esp√®ce, d'une maladie, d'un trait phylog√©n√©tique, ainsi que les cat√©gories encod√©es. Lorsque l'op√©ration ne comprend qu'un seul √©chantillon (i.e. un seul tirage √† pile ou face), il s'agit d'un cas particulier d'une loi binomiale que l'on nomme une loi de *Bernouilli*.

Pour 25 tirages √† pile ou face ind√©pendants (i.e. dont l'ordre des tirages ne compte pas), on peut dessiner une courbe de distribution dont la somme des probabilit√©s est de 1. La fonction `dbinom` est une fonction de distribution de probabilit√©s. Les fonctions de distribution de probabilit√©s discr√®tes sont appel√©es des fonctions de masse.

```{r}
library("tidyverse")
x <- 0:25
y <- dbinom(x = x, size = 25, prob = 0.5)
print(paste('La somme des probabilit√©s est de', sum(y)))
ggplot(data = tibble(x, y), mapping = aes(x, y)) +
  geom_segment(aes(x = x, xend = x, y = 0, yend = y), color = "grey50") +
  geom_point()
```

### Distribution de Poisson

La loi de Poisson (avec un P majuscule, introduite par le math√©maticien fran√ßais Sim√©on Denis Poisson et non pas l'animal) d√©crit des distributions discr√®tes de probabilit√© d'un nombre d‚Äô√©v√®nements se produisant dans l'espace ou dans le temps. Les distributions de Poisson d√©crive ce qui tient du d√©compte. Il peut s'agir du nombre de grenouilles traversant une rue quotidiennement, du nombre de plants d'ascl√©piades se trouvant sur une terre cultiv√©e, ou du nombre d‚Äô√©v√®nements de pr√©cipitation au mois de juin, etc. La distribution de Poisson n'a qu'un seul param√®tre, $\lambda$, qui d√©crit tant la moyenne des d√©comptes.

Par exemple, en un mois de 30 jours, et une moyenne de 8 √©v√®nements de pr√©cipitation pour ce mois, on obtient la distribution suivante.

```{r}
x <- 1:30
y <- dpois(x, lambda = 8)
print(paste('La somme des probabilit√©s est de', sum(y)))
ggplot(data = data.frame(x, y), mapping = aes(x, y)) +
  geom_segment(aes(x = x, xend = x, y = 0, yend = y), color = "grey50") +
  geom_point()
```

### Distribution uniforme

La distribution la plus simple est probablement la distribution uniforme. Si la variable est discr√®te, chaque cat√©gorie est associ√©e √† une probabilit√© √©gale. Si la variable est continue, la probabilit√© est directement proportionnelle √† la largeur de l'intervalle. On utilise rarement la distribution uniforme en biostatistiques, sinon pour d√©crire des *a priori* vagues pour l'analyse bay√©sienne (ce sujet est trait√© dans le chapitre \@ref(chapitre-biostats-bayes)). Nous utilisons la fonction `dunif`. √Ä la diff√©rence des distributions discr√®tes, les fonctions de distribution de probabilit√©s continues sont appel√©es des fonctions de densit√© d'une loi de probabilit√© (*probability density function*).

```{r}
increment <- 0.01
x <- seq(-4, 4, by = increment)
y1 <- dunif(x, min = -3, max = 3)
y2 <- dunif(x, min = -2, max = 2)
y3 <- dunif(x, min = -1, max = 1)

print(paste('La somme des probabilit√©s est de', sum(y3 * increment)))

gg_unif <- data.frame(x, y1, y2, y3) %>% gather(variable, value, -x)

ggplot(data = gg_unif, mapping = aes(x = x, y = value)) +
  geom_line(aes(colour = variable))
```

### Distribution normale

La plus r√©pandue de ces lois est probablement la loi normale, parfois nomm√©e loi gaussienne et plus rarement loi laplacienne. Il s'agit de la distribution classique en forme de cloche.

La loi normale est d√©crite par une moyenne, qui d√©signe la tendance centrale, et une variance, qui d√©signe l'√©talement des probabilit√©s autour de la moyenne. La racine carr√©e de la variance est l'√©cart-type.

Les distributions de mesures exclusivement positives (comme le poids ou la taille) sont parfois avantageusement approxim√©es par une loi **log-normale**, qui est une loi normale sur le logarithme des valeurs: la moyenne d'une loi log-normale est la moyenne g√©om√©trique.

```{r}
increment <- 0.01
x <- seq(-10, 10, by = increment)
y1 <- dnorm(x, mean = 0, sd = 1)
y2 <- dnorm(x, mean = 0, sd = 2)
y3 <- dnorm(x, mean = 0, sd = 3)

print(paste('La somme des probabilit√©s est de', sum(y3 * increment)))

gg_norm <- data.frame(x, y1, y2, y3) %>% gather(variable, value, -x)

ggplot(data = gg_norm, mapping = aes(x = x, y = value)) +
  geom_line(aes(colour = variable))
```

Quelle est la probabilit√© d'obtenir le nombre 0 chez une observation continue distribu√©e normalement dont la moyenne est 0 et l'√©cart-type est de 1? R√©ponse: 0. La loi normale √©tant une distribution continue, les probabilit√©s non-nulles ne peuvent √™tre calcul√©s que sur des intervalles. Par exemple, la probabilit√© de retrouver une valeur dans l'intervalle entre -1 et 2 est calcul√©e en soustrayant la probabilit√© cumul√©e √† -1 de la probabilit√© cumul√©e √† 2.

```{r}
increment <- 0.01
x <- seq(-5, 5, by = increment)
y <- dnorm(x, mean = 0, sd = 1)

prob_between <- c(-1, 2)

gg_norm <- data.frame(x, y)
gg_auc <- gg_norm %>%
  filter(x > prob_between[1], x < prob_between[2]) %>%
  rbind(c(prob_between[2], 0)) %>%
  rbind(c(prob_between[1], 0))

ggplot(data.frame(x, y), aes(x, y)) +
  geom_polygon(data = gg_auc, fill = '#71ad50') + # #71ad50 est un code de couleur format hexad√©cimal
  geom_line()

prob_norm_between <- pnorm(q = prob_between[2], mean = 0, sd = 1) - pnorm(q = prob_between[1], mean = 0, sd = 1)
print(paste("La probabilit√© d'obtenir un nombre entre", 
            prob_between[1], "et", 
            prob_between[2], "est d'environ", 
            round(prob_norm_between, 2) * 100, "%"))
```

La courbe normale peut √™tre utile pour √©valuer la distribution d'une population. Par exemple, on peut calculer les limites de r√©gion sur la courbe normale qui contient 95% des valeurs possibles en tranchant 2.5% de part et d'autre de la moyenne. Il s'agit ainsi de l'intervalle de confiance sur la d√©viation de la distribution.

```{r}
increment <- 0.01
x <- seq(-5, 5, by = increment)
y <- dnorm(x, mean = 0, sd = 1)

alpha <- 0.05
prob_between <- c(qnorm(p = alpha/2, mean = 0, sd = 1),
                  qnorm(p = 1 - alpha/2, mean = 0, sd = 1))

gg_norm <- data.frame(x, y)
gg_auc <- gg_norm %>%
  filter(x > prob_between[1], x < prob_between[2]) %>%
  rbind(c(prob_between[2], 0)) %>%
  rbind(c(prob_between[1], 0))

ggplot(data = data.frame(x, y), mapping = aes(x, y)) +
  geom_polygon(data = gg_auc, fill = '#71ad50') + # #71ad50 est un code de couleur format hexad√©cimal
  geom_line() +
  geom_text(data = data.frame(x = prob_between,
                              y = c(0, 0),
                              labels = round(prob_between, 2)),
            mapping = aes(label = labels))
```

On pourrait aussi √™tre int√©ress√© √† l'intervalle de confiance sur la moyenne. En effet, la moyenne suit aussi une distribution normale, dont la tendance centrale est la moyenne de la distribution, et dont l'√©cart-type est not√© *erreur standard*. On calcule cette erreur en divisant la variance par le nombre d'observation, ou en divisant l'√©cart-type par la racine carr√©e du nombre d'observations. Ainsi, pour 10 √©chantillons:

```{r}
increment <- 0.01
x <- seq(-5, 5, by = increment)
y <- dnorm(x, mean = 0, sd = 1)

alpha <- 0.05
prob_between <- c(qnorm(p = alpha/2, mean = 0, sd = 1) / sqrt(10),
                  qnorm(p = 1 - alpha/2, mean = 0, sd = 1) / sqrt(10))

gg_norm <- data.frame(x, y)
gg_auc <- gg_norm %>%
  filter(x > prob_between[1], x < prob_between[2]) %>%
  rbind(c(prob_between[2], 0)) %>%
  rbind(c(prob_between[1], 0))

ggplot(data = data.frame(x, y), mapping = aes(x, y)) +
  geom_polygon(data = gg_auc, fill = '#71ad50') + # #71ad50 est un code de couleur format hexad√©cimal
  geom_line() +
  geom_text(data = data.frame(x = prob_between,
                              y = c(0, 0),
                              labels = round(prob_between, 2)),
            mapping = aes(label = labels))
```

## Statistiques descriptives

On a vu comment g√©n√©rer des statistiques sommaires en R avec la fonction `summary()`. Reprenons les donn√©es d'iris.

```{r}
data("iris")
summary(iris)
```

Pour pr√©cis√©ment effectuer une moyenne et un √©cart-type sur un vecteur, passons par les fonctions `mean()` et `sd()`.

```{r}
mean(iris$Sepal.Length)
sd(iris$Sepal.Length)
```

Pour effectuer un sommaire de tableau pilot√© par une fonction, nous passons par la gamme de fonctions `summarise()`, de dplyr. Dans ce cas, avec `group_by()`, nous fragmentons le tableau par esp√®ce pour effectuer un sommaire sur toutes les variables.

```{r}
iris %>%
  group_by(Species) %>%
  summarise_all(mean)
```

Vous pourriez √™tre int√©ress√© par les quartiles √† 25, 50 et 75%. Mais la fonction `summarise()` n'autorise que les fonctions dont la sortie est d'un seul objet, alors faisons sorte que l'objet soit une liste - lorsque l'on imbrique une fonction `funs`, le tableau √† ins√©rer dans la fonction est indiqu√© par un `.`.

```{r}
iris %>%
  group_by(Species) %>%
  summarise_all(funs(list(quantile(.))))
```

En mode programmation classique de R, on pourra g√©n√©rer les quartiles √† la pi√®ce.

```{r}
quantile(iris$Sepal.Length[iris$Species == 'setosa'])
quantile(iris$Sepal.Length[iris$Species == 'versicolor'])
quantile(iris$Sepal.Length[iris$Species == 'virginica'])
```

La fonction `table()` permettra d'obtenir des d√©comptes par cat√©gorie, ici par plages de longueurs de s√©pales. Pour obtenir les proportions du nombre total, il s'agit d'encapsuler le tableau crois√© dans la fonction `prop.table()`.

```{r}
tableau_croise <- table(iris$Species, 
                        cut(iris$Sepal.Length, breaks = quantile(iris$Sepal.Length)))
tableau_croise
```

```{r}
prop.table(tableau_croise)
```

## Tests d'hypoth√®ses √† un et deux √©chantillons

Un test d'hypoth√®se permet de d√©cider si une hypoth√®se est confirm√©e ou rejet√©e √† un seuil de probabilit√© pr√©d√©termin√©.

Cette section est inspir√©e du chapitre 5 de [Dalgaard, 2008](https://www.springer.com/us/book/9780387790534).

----

**Information: l'hypoth√®se nulle**. Les tests d'hypoth√®se √©value des *effets* statistiques (qui ne sont pas n√©cessairement des effets de causalit√©). L'effet √† √©valuer peut √™tre celui d'un traitement, d'indicateurs m√©t√©orologiques (e.g. pr√©cipitations totales, degr√©-jour, etc.), de techniques de gestion des paysages, etc. Une recherche est men√©e pour √©valuer l'hypoth√®se que l'on retrouve des diff√©rences entre des unit√©s exp√©rimentales. Par convention, l'**hypoth√®se nulle** (√©crite $H_0$) est l'hypoth√®se qu'il n'y ait pas d'effet (c'est l'hypoth√®se de l'avocat du diable üòà) √† l'√©chelle de la population (et non pas √† l'√©chelle de l'√©chantillon). √Ä l'inverse, l'**hypoth√®se alternative** (√©crite $H_1$) est l'hypoth√®se qu'il y ait un effet √† l'√©chelle de la population.

----

√Ä titre d'exercice en stats, on d√©bute souvent par en testant si deux vecteurs de valeurs continues proviennent de populations √† moyennes diff√©rentes ou si un vecteur de valeurs a √©t√© g√©n√©r√© √† partir d'une population ayant une moyenne donner. Dans cette section, nous utiliserons la fonction `t.test()` pour les tests de t et la fonction `wilcox.test()` pour les tests de Wilcoxon (aussi appel√© de Mann-Whitney).

### Test de t √† un seul √©chantillon

Nous devons assumer, pour ce test, que l'√©chantillon est recueillit d'une population dont la distribution est normale, $\mathcal{N} \sim \left( \mu, \sigma^2 \right)$, et que chaque √©chantillon est ind√©pendant l'un de l'autre. L'hypoth√®se nulle est souvent celle de l'avocat du diable, que la moyenne soit √©gale √† une valeur donn√©e (donc la diff√©rence entre la moyenne de la population et une moyenne donn√©e est de z√©ro): ici, que $\mu = \bar{x}$. L'erreur standard sur la moyenne (ESM) de l'√©chantillon, $\bar{x}$ est calcul√©e comme suit.

$$ESM = \frac{s}{\sqrt{n}}$$

o√π $s$ est l'√©cart-type de l'√©chantillon et $n$ est le nombre d'√©chantillons.

Pour tester l'intervalle de confiance de l'√©chantillon, on multiplie l'ESM par l'aire sous la courbe de densit√© couvrant une certaine proportion de part et d'autre de l'√©chantillon. Pour un niveau de confiance de 95%, on retranche 2.5% de part et d'autre.

```{r}
set.seed(33746)
x <- rnorm(20, 16, 4)

level <-  0.95
alpha <- 1-level

x_bar <- mean(x)
s <- sd(x)
n <- length(x)

error <- qnorm(1 - alpha/2) * s / sqrt(n)
error
```

intervalle de confiance est l'erreur de par et d'autre de la moyenne.

```{r}
c(x_bar - error, x_bar + error)
```

Si la moyenne de la population est de 16, un nombre qui se situe dans l'intervalle de confiance on accepte l'hypoth√®se nulle au seuil 0.05. Si le nombre d'√©chantillon est r√©duit (g√©n√©ralement < 30), on passera plut√¥t par une distribution de t, avec $n-1$ degr√©s de libert√©.

```{r}
error <- qt(1 - alpha/2, n-1) * s / sqrt(n)
c(x_bar - error, x_bar + error)
```

Plus simplement, on pourra utiliser la fonction `t.test()` en sp√©cifiant la moyenne de la population. Nous avons g√©n√©r√© 20 donn√©es avec une moyenne de 16 et un √©cart-type de 4. Nous savons donc que la vraie moyenne de l'√©chantillon est de 16. Mais disons que nous testons l'hypoth√®se que ces donn√©es sont tir√©es d'une population dont la moyenne est 18 (et implicitement que sont √©cart-type est de 4).

```{r}
t.test(x, mu = 18)
```

La fonction retourne la valeur de t (*t-value*), le nombre de degr√©s de libert√© ($n-1 = 19$), une description de l'hypoth√®se alternative (`alternative hypothesis: true mean is not equal to 18`), ainsi que l'intervalle de confiance au niveau de 95%. Le test contient aussi la *p-value*. Bien que la *p-value* soit largement utilis√©e en science

----

#### Information: la *p-value*

La *p-value*, ou valeur-p ou p-valeur, est utilis√©e pour trancher si, oui ou non, un r√©sultat est **significatif** (en langage scientifique, le mot significatif ne devrait √™tre utilis√© *que* lorsque l'on r√©f√®re √† un test d'hypoth√®se statistique). Vous retrouverez des *p-value* partout en stats. Les *p-values* indiquent la confiance que l'hypoth√®se nulle soit vraie, selon les donn√©es et le mod√®le statistique utilis√©es.

> La p-value est la probabilit√© que les donn√©es aient √©t√© g√©n√©r√©es pour obtenir un effet √©quivalent ou plus prononc√© si l'hypoth√®se nulle est vraie.

Une *p-value* √©lev√©e indique que le mod√®le appliqu√© √† vos donn√©es concordent avec la conclusion que l'hypoth√®se nulle est vraie, et inversement si la *p-value* est faible. Le seuil arbitraire utilis√©e en √©cologie et en agriculture, comme dans plusieurs domaines, est 0.05.

Les six principes de l'[American Statistical Association](https://phys.org/news/2016-03-american-statistical-association-statement-significance.html) guident l'interpr√©tation des *p-values*. [ma traduction]

0. Les *p-values* indique l'ampleur de l‚Äôincompatibilit√© des donn√©es avec le mod√®le statistique
0. Les *p-values* ne mesurent pas la probabilit√© que l'hypoth√®se √©tudi√©e soit vraie, ni la probabilit√© que les donn√©es ont √©t√© g√©n√©r√©es uniquement par la chance.
0. Les conclusions scientifiques et d√©cisions d'affaire ou politiques ne devraient pas √™tre bas√©es sur si une *p-value* atteint un seuil sp√©cifique.
0. Une inf√©rence appropri√©e demande un rapport complet et transparent.
0. Une *p-value*, ou une signification statistique, ne mesure pas l'ampleur d'un effet ou l'importance d'un r√©sultat.
0. En tant que tel, une *p-value* n'offre pas une bonne mesure des √©vidences d'un mod√®le ou d'une hypoth√®se.

Cet encadr√© est inspir√© d'un [billet de blogue de Jim Frost](https://blog.minitab.com/blog/adventures-in-statistics-2/how-to-correctly-interpret-p-values) et d'un [rapport de l'American Statistical Association](https://phys.org/news/2016-03-american-statistical-association-statement-significance.html).

----

Dans le cas pr√©c√©dent, la *p-value* √©tait de 0.01014. Pour aider notre interpr√©tation, prenons l'hypoth√®se alternative: `true mean is not equal to 18`. L'hypoth√®se nulle √©tait bien que *la vraie moyenne est √©gale √† 18*. Ins√©rons la *p-value* dans la d√©finition: la probabilit√© que les donn√©es aient √©t√© g√©n√©r√©es pour obtenir un effet √©quivalent ou plus prononc√© si l'hypoth√®se nulle est vraie est de 0.01014. Il est donc tr√®s peu probable que les donn√©es soient tir√©es d'un √©chantillon dont la moyenne est de 18. Au seuil de signification de 0.05, on rejette l'hypoth√®se nulle et l'on conclut qu'√† ce seuil de confiance, l'√©chantillon ne provient pas d'une population ayant une moyenne de 18.

----

### Attention: mauvaises interpr√©tations des *p-values*

> "La p-value n'a jamais √©t√© con√ßue comme substitut au raisonnement scientifique" [Ron Wasserstein, directeur de l'American Statistical Association](https://phys.org/news/2016-03-american-statistical-association-statement-significance.html) [ma traduction]. 

**Un r√©sultat montrant une p-value plus √©lev√©e que 0.05 est-il pertinent?**

Lors d'une conf√©rence, Dr Evil ne pr√©sentent que les r√©sultats significatifs de ses essais au seuil de 0.05. Certains essais ne sont pas significatifs, mais bon, ceux-ci ne sont pas importants... En √©cartant ces r√©sultats, Dr Evil commet 3 erreurs:

1. La *p-value* n'est pas un bon indicateur de l'importance d'un test statistique. L'importance d'une variable dans un mod√®le devrait √™tre √©valu√©e par la valeur de son coefficient. Son incertitude devrait √™tre √©valu√©e par sa variance. Une mani√®re d'√©valuer plus intuitive la variance est l'√©cart-type ou l'intervalle de confiance. √Ä un certain seuil d'intervalle de confiance, la p-value traduira la probabilit√© qu'un coefficient soit r√©ellement nul ait pu g√©n√©rer des donn√©es d√©montrant un coefficient √©gal ou sup√©rieur.
1. Il est tout aussi important de savoir que le traitement fonctionne que de savoir qu'il ne fonctionne pas. Les r√©sultats d√©montrant des effets sont malheureusement davantage soumis aux journaux et davantage publi√©s que ceux ne d√©montrant pas d'effets ([Decullier et al., 2005]( https://doi.org/10.1136/bmj.38488.385995.8F )).
1. Le seuil de 0.05 est arbitraire.

----


----

#### Attention au *p-hacking*

Le *p-hacking* (ou *data dredging*) consiste √† manipuler les donn√©es et les mod√®les pour faire en sorte d'obtenir des *p-values* favorables √† l'hypoth√®se test√©e et, √©ventuellement, aux conclusions recherch√©es. **√Ä √©viter dans tous les cas. Toujours. Toujours. Toujours.**

Vid√©o sugg√©r√©e (en anglais).

[![p-hacking](images/05_p-hacking.png)](https://youtu.be/0Rnq1NpHdmw)

----

### Test de Wilcoxon √† un seul √©chantillon

Le test de t suppose que la distribution des donn√©es est normale... ce qui est rarement le cas, surtout lorsque les √©chantillons sont peu nombreux. Le test de Wilcoxon ne demande aucune supposition sur la distribution: c'est un test non-param√©trique bas√© sur le tri des valeurs.

```{r}
wilcox.test(x, mu = 18)
```

Le `V` est la somme des rangs positifs. Dans ce cas, la *p-value* est semblable √† celle du test de t, et les m√™mes conclusions s'appliquent.

### Tests de t √† deux √©chantillons

Les tests √† un √©chantillon servent plut√¥t √† s'exercer: rarement en aura-t-on besoin en recherche, o√π plus souvent, on voudra comparer les moyennes de deux unit√©s exp√©rimentales. L'exp√©rience comprend donc deux s√©ries de donn√©es continues, $x_1$ et $x_2$, issus de lois de distribution normale $\mathcal{N} \left( \mu_1, \sigma_1^2 \right)$ et $\mathcal{N} \left( \mu_2, \sigma_2^2 \right)$, et nous testons l'hypoth√®se nulle que $\mu_1 = \mu_2$. La statistique t est calcul√©e comme suit.

$$t = \frac{\bar{x_1} - \bar{x_2}}{ESDM}$$

L'ESDM est l'erreur standard de la diff√©rence des moyennes:

$$ESDM = \sqrt{ESM_1^2 + ESM_2^2}$$

Si vous supposez que les variances sont identiques, l'erreur standard (s) est calcul√©e pour les √©chantillons des deux groupes, puis ins√©r√©e dans le calcul des ESM. La statistique t sera alors √©valu√©e √† $n_1 + n_2 - 2$ degr√©s de libert√©. Si vous supposez que la variance est diff√©rente (*proc√©dure de Welch*), vous calculez les ESM avec les erreurs standards respectives, et la statistique t devient une approximation de la distribution de t avec un nombre de degr√©s de libert√© calcul√© √† partir des erreurs standards et du nombre d'√©chantillon dans les groupes: cette proc√©dure est consid√©r√©e comme plus prudente ([Dalgaard, 2008](https://www.springer.com/us/book/9780387790534), page 101).

Prenons les donn√©es d'iris pour l'exemple en excluant l'iris setosa √©tant donn√©e que les tests de t se restreignent √† deux groupes. Nous allons tester la longueur des p√©tales.

```{r}
iris_pl <- iris %>% 
    filter(Species != "setosa") %>%
    select(Species, Petal.Length)
sample_n(iris_pl, 5)
```

Dans la prochaine cellule, nous introduisons l'*interface-formule* de R, o√π l'on retrouve typiquement le `~`, entre les variables de sortie √† gauche et les variables d'entr√©e √† droite. Dans notre cas, la variable de sortie est la variable test√©e, `Petal.Length`, qui varie en fonction du groupe `Species`, qui est la variable d'entr√©e (variable explicative) - nous verrons les types de variables plus en d√©tails dans la section [Les mod√®les statistiques](#Les-mod%C3%A8les-statistiques), plus bas.

```{r}
t.test(formula = Petal.Length ~ Species,
       data = iris_pl, var.equal = FALSE)
```

Nous obtenons une sortie similaire aux pr√©c√©dentes. L'intervalle de confiance √† 95% exclu le z√©ro, ce qui est coh√©rent avec la p-value tr√®s faible, qui nous indique le rejet de l'hypoth√®se nulle au seuil 0.05. Les groupes ont donc des moyennes de longueurs de p√©tale significativement diff√©rentes.

----

#### Enregistrer les r√©sultats d'un test

Il est possible d'enregistrer un test dans un objet.

```{r}
tt_pl <- t.test(formula = Petal.Length ~ Species,
                data = iris_pl, var.equal = FALSE)
summary(tt_pl)
str(tt_pl)
```

----

### Comparaison des variances

Pour comparer les variances, on a recours au test de F (F pour Fisher).

```{r}
var.test(formula = Petal.Length ~ Species,
         data = iris_pl)
```

Il semble que l'on pourrait relancer le test de *t* sans la proc√©dure Welch, avec `var.equal = TRUE`.

### Tests de Wilcoxon √† deux √©chantillons

Cela ressemble au test de t!

```{r}
wilcox.test(formula = Petal.Length ~ Species,
       data = iris_pl, var.equal = TRUE)
```

### Les tests pair√©s

Les tests pair√©s sont utilis√©s lorsque deux √©chantillons proviennent d'une m√™me unit√© exp√©rimentale: il s'agit en fait de tests sur la diff√©rence entre deux observations.

```{r}
set.seed(2555)

n <- 20
avant <- rnorm(n, 16, 4)
apres <- rnorm(n, 18, 3)
```

Il est important de sp√©cifier que le test est pair√©, la valeur par d√©faut de `paired` √©tant `FALSE`.

```{r}
t.test(avant, apres, paired = TRUE)
```

L'hypoth√®se nulle qu'il n'y ait pas de diff√©rence entre l'avant et l'apr√®s traitement est accept√©e au seuil 0.05.

**Exercice**. Effectuer un test de Wilcoxon pair√©.

## L'analyse de variance

L'analyse de variance consiste √† comparer des moyennes de plusieurs groupe distribu√©s normalement et de m√™me variance. Cette section sera √©labor√©e prochainement plus en profondeur. Consid√©rons-la pour le moment comme une r√©gression sur une variable cat√©gorielle.

```{r}
pl_aov <- aov(Petal.Length ~ Species, iris)
summary(pl_aov)
```

La prochaine section, justement, est vou√©e aux mod√®les statistiques explicatifs, qui incluent la r√©gression.

## Les mod√®les statistiques

La mod√©lisation statistique consiste √† lier de mani√®re explicite des variables de sortie $y$ (ou variables-r√©ponse ou variables d√©pendantes) √† des variables explicatives $x$ (ou variables pr√©dictives / ind√©pendantes / covariables). Les variables-r√©ponse sont mod√©lis√©es par une fonction des variables explicatives ou pr√©dictives.

Pourquoi garder les termes *explicatives* et *pr√©dictives*? Parce que les mod√®les statistiques (bas√©s sur des donn√©es et non pas sur des m√©canismes) sont de deux ordres. D'abord, les mod√®les **pr√©dictifs** sont con√ßus pour pr√©dire de mani√®re fiable une ou plusieurs variables-r√©ponse √† partir des informations contenues dans les variables qui sont, dans ce cas, pr√©dictives. Ces mod√®les sont couverts dans le chapitre 11 de ce manuel (en d√©veloppement). Lorsque l'on d√©sire tester des hypoth√®ses pour √©valuer quelles variables expliquent la r√©ponse, on parlera de mod√©lisation (et de variables) **explicatives**. En inf√©rence statistique, on √©valuera les *corr√©lations* entre les variables explicatives et les variables-r√©ponse. Un lien de corr√©lation n'est pas un lien de causalit√©. L'inf√©rence causale peut en revanche √™tre √©valu√©e par des [*mod√®les d'√©quations structurelles*](https://www.amazon.com/Cause-Correlation-Biology-Structural-Equations/dp/1107442591), sujet qui fera √©ventuellement partie de ce cours.

Cette section couvre la mod√©lisation explicative. Les variables qui contribuent √† cr√©er les mod√®les peuvent √™tre de diff√©rentes natures et distribu√©es selon diff√©rentes lois de probabilit√©. Alors que les mod√®les lin√©aires simples (*lm*) impliquent une variable-r√©ponse distribu√©e de mani√®re continue, les mod√®les lin√©aires g√©n√©ralis√©s peuvent aussi expliquer des variables de sorties discr√®tes.

Dans les deux cas, on distinguera les variables fixes et les variables al√©atoires. Les **variables fixes** sont les variables test√©es lors de l'exp√©rience: dose du traitement, esp√®ce/cultivar, m√©t√©o, etc. Les **variables al√©atoires** sont les sources de variation qui g√©n√®rent du bruit dans le mod√®le: les unit√©s exp√©rimentales ou le temps lors de mesures r√©p√©t√©es. Les mod√®les incluant des effets fixes seulement sont des mod√®les √† effets fixes. G√©n√©ralement, les mod√®les incluant des variables al√©atoires incluent aussi des variables fixes: on parlera alors de mod√®les mixtes. Nous couvrirons ces deux types de mod√®le.

### Mod√®les √† effets fixes

Les tests de t et de Wilcoxon, explor√©s pr√©c√©demment, sont des mod√®les statistiques √† une seule variable. Nous avons vu dans l'*interface-formule* qu'une variable-r√©ponse peut √™tre li√©e √† une variable explicative avec le tilde `~`. En particulier, le test de t est r√©gression lin√©aire univari√©e (√† une seule variable explicative) dont la variable explicative comprend deux cat√©gories. De m√™me, l'anova est une r√©gression lin√©aire univari√©e dont la variable explicative comprend plusieurs cat√©gories. Or l'interface-formule peut √™tre utilis√© dans plusieurs circonstances, notamment pour ajouter plusieurs variables de diff√©rents types: on parlera de r√©gression multivari√©e.

La plupart des mod√®les statistiques peuvent √™tre approxim√©s comme une combinaison lin√©aire de variables: ce sont des mod√®les lin√©aires. Les mod√®les non-lin√©aires impliquent des strat√©gies computationnelles complexes qui rendent leur utilisation plus difficile √† man≈ìuvrer.

Un mod√®le lin√©aire univari√© prendra la forme $y = \beta_0 + \beta_1 x + \epsilon$, o√π $\beta_0$ est l'intercept et $\beta_1$ est la pente et $\epsilon$ est l'erreur.

Vous verrez parfois la notation $\hat{y} = \beta_0 + \beta_1 x$. La notation avec le chapeau $\hat{y}$ exprime qu'il s'agit des valeurs g√©n√©r√©es par le mod√®le. En fait, $y = \hat{y} - \epsilon$.

#### Mod√®le lin√©aire univari√© avec variable continue

Prenons les donn√©es [`lasrosas.corn`](https://rdrr.io/cran/agridat/man/lasrosas.corn.html) incluses dans le module `agridat`, o√π l'on retrouve le rendement d'une production de ma√Øs √† dose d'azote variable, en Argentine.

```{r}
library("agridat")
data("lasrosas.corn")
sample_n(lasrosas.corn, 10)
```

Ces donn√©es comprennent plusieurs variables. Prenons le rendement (`yield`) comme variable de sortie et, pour le moment, ne retenons que la dose d'azote (`nitro`) comme variable explicative: il s'agit d'une r√©gression univari√©e. Les deux variables sont continues. Explorons d'abord le nuage de points de l'une et l'autre.

```{r}
ggplot(data = lasrosas.corn, mapping = aes(x = nitro, y = yield)) +
    geom_point()
```

L'hypoth√®se nulle est que la dose d'azote n'affecte pas le rendement, c'est √† dire que le coefficient de pente et nul. Une autre hypoth√®se est que l'intercept est nul: donc qu'√† dose de 0, rendement de 0. Un mod√®le lin√©aire √† variable de sortie continue est cr√©√© avec la fonction `lm()`, pour *linear model*.

```{r}
modlin_1 <- lm(yield ~ nitro, data = lasrosas.corn)
summary(modlin_1)
```

Le diagnostic du mod√®le comprend plusieurs informations. D'abord la formule utilis√©e, affich√©e pour la tra√ßabilit√©. Viens ensuite un aper√ßu de la distribution des r√©sidus. La m√©diane devrait s'approcher de la moyenne des r√©sidus (qui est toujours de 0). Bien que le -3.079 peut sembler important, il faut prendre en consid√©ration de l'√©chelle de y, et ce -3.079 est exprim√© en terme de rendement, ici en quintaux (i.e. 100 kg) par hectare. La distribution des r√©sidus m√©rite d'√™tre davantage investigu√©e. Nous verrons cela un peu plus tard.

Les coefficients apparaissent ensuite. Les estim√©s sont les valeurs des effets. R fournit aussi l'erreur standard associ√©e, la valeur de t ainsi que la p-value (la probabilit√© d'obtenir cet effet ou un effet plus extr√™me si en r√©alit√© il y avait absence d'effet). L'intercept est bien s√ªr plus √©lev√© que 0 (√† dose nulle, on obtient 65.8 quintaux par hectare en moyenne). La pente de la variable `nitro` est de ~0.06: pour chaque augmentation d'un kg/ha de dose, on a obtenu ~0.06 quintaux/ha de plus de ma√Øs. Donc pour 100 kg/ha de N, on a obtenu un rendement moyen de 6 quintaux de plus que l'intercept. Soulignons que l'ampleur du coefficient est tr√®s important pour guider la fertilisation: ne rapporter que la p-value, ou ne rapporter que le fait qu'elle est inf√©rieure √† 0.05 (ce qui arrive souvent dans la litt√©rature), serait tr√®s insuffisant pour l'interpr√©tation des statistiques. La p-value nous indique n√©anmoins qu'il serait tr√®s improbable qu'une telle pente ait √©t√© g√©n√©r√©e alors que celle-ci est nulle en r√©alit√©. Les √©toiles √† c√¥t√© des p-values indiquent l'ampleur selon l'√©chelle `Signif. codes` indiqu√©e en-dessous du tableau des coefficients.

Sous ce tableau, R offre d'autres statistiques. En outre, les R¬≤ et R¬≤ ajust√©s indiquent si la r√©gression passe effectivement par les points. Le R¬≤ prend un maximum de 1 lorsque la droite passe exactement sur les points.

Enfin, le test de F g√©n√®re une p-value indiquant la probabilit√© que les coefficients de pente ait √©t√© g√©n√©r√©s si les vrais coefficients √©taient nuls. Dans le cas d'une r√©gression univari√©e, cela r√©p√®te l'information sur l'unique coefficient.

On pourra √©galement obtenir les intervalles de confiance avec la fonction `confint()`.

```{r}
confint(modlin_1, level = 0.95)
```

Ou soutirer l'information de diff√©rentes mani√®res, comme avec la fonction `coefficients()`.

```{r}
coefficients(modlin_1)
```

√âgalement, on pourra ex√©cuter le mod√®le sur les donn√©es qui ont servi √† le g√©n√©rer:

```{r}
predict(modlin_1)[1:5]
```

Ou sur des donn√©es externes.

```{r}
nouvelles_donnees <- data.frame(nitro = seq(from = 0, to = 100, by = 5))
predict(modlin_1, newdata = nouvelles_donnees)[1:5]
```

#### Analyse des r√©sidus

Les r√©sidus sont les erreurs du mod√®le. C'est le vecteur $\epsilon$, qui est un d√©calage entre les donn√©es et le mod√®le. Le R¬≤ est un indicateur de l'ampleur du d√©calage, mais une r√©gression lin√©aire explicative en bonne et due forme devrait √™tre accompagn√©e d'une analyse des r√©sidus. On peut les calculer par $\epsilon = y - \hat{y}$, mais aussi bien utiliser la fonction `residuals()`.

```{r}
res_df <- data.frame(nitro = lasrosas.corn$nitro,
                     residus_lm = residuals(modlin_1), 
                     residus_calcul = lasrosas.corn$yield - predict(modlin_1))
sample_n(res_df, 10)
```

Dans une bonne r√©gression lin√©aire, on ne retrouvera pas de structure identifiable dans les r√©sidus, c'est-√†-dire que les r√©sidus sont bien distribu√©s de part et d'autre du mod√®le de r√©gression.

```{r}
ggplot(res_df, aes(x = nitro, y = residus_lm)) +
  geom_point() +
  labs(x = "Dose N", y = "R√©sidus") +
  geom_hline(yintercept = 0, col = "red", size = 1)
```

Bien que le jugement soit subjectif, on peut dire avec confiance qu'il n'y a pas structure particuli√®re. En revanche, on pourrait g√©n√©rer un $y$ qui varie de mani√®re quadratique avec $x$, un mod√®le lin√©aire montrera une structure √©vidente.

```{r}
set.seed(36164)
x <- 0:100
y <- 10 + x*1 + x^2 * 0.05 + rnorm(length(x), 0, 50)
modlin_2 <- lm(y ~ x)
ggplot(data.frame(x, residus = residuals(modlin_2)),
       aes(x = x, y = residus)) +
  geom_point() +
  labs(x = "x", y = "R√©sidus") +
  geom_hline(yintercept = 0, col = "red", size = 1)
```

De m√™me, les r√©sidus ne devraient pas cro√Ætre avec $x$.

```{r}
set.seed(3984)
x <- 0:100
y <-  10 + x + x * rnorm(length(x), 0, 2)
modlin_3 <- lm(y ~ x)
ggplot(data.frame(x, residus = residuals(modlin_3)),
       aes(x = x, y = residus)) +
  geom_point() +
  labs(x = "x", y = "R√©sidus") +
  geom_hline(yintercept = 0, col = "red", size = 1)
```

On pourra aussi inspecter les r√©sidus avec un graphique de leur distribution. Reprenons notre mod√®le de rendement du ma√Øs.

```{r}
ggplot(res_df, aes(x = residus_lm)) +
  geom_histogram(binwidth = 2, color = "white") +
  labs(x = "Residual")
```

L'histogramme devrait pr√©senter une distribution normale. Les tests de normalit√© comme le test de Shapiro-Wilk peuvent aider, mais ils sont g√©n√©ralement tr√®s s√©v√®res.

```{r}
shapiro.test(res_df$residus_lm)
```

L'hypoth√®se nulle que la distribution est normale est rejet√©e au seuil 0.05. Dans notre cas, il est √©vident que la s√©v√©rit√© du test n'est pas en cause, car les r√©sidus semble g√©n√©rer trois ensembles. Ceci indique que les variables explicatives sont insuffisantes pour expliquer la variabilit√© de la variable-r√©ponse.

#### R√©gression multiple

Comme c'est le cas pour bien des ph√©nom√®nes en √©cologie, le rendement d'une culture n'est certainement pas expliqu√© seulement par la dose d'azote.

Lorsque l'on combine plusieurs variables explicatives, on cr√©e un mod√®le de r√©gression multivari√©e, ou une r√©gression multiple. Bien que les tendances puissent sembler non-lin√©aires, l'ajout de variables et le calcul des coefficients associ√©s reste un probl√®me d'alg√®bre lin√©aire.

On pourra en effet g√©n√©raliser les mod√®les lin√©aires, univari√©s et multivari√©s, de la mani√®re suivante.

$$ y = X \beta + \epsilon $$

o√π:

$X$ est la matrice du mod√®le √† $n$ observations et $p$ variables.

$$ X = \left( \begin{matrix} 
1 & x_{11} & \cdots & x_{1p}  \\ 
1 & x_{21} & \cdots & x_{2p}  \\ 
\vdots & \vdots & \ddots & \vdots  \\ 
1 & x_{n1} & \cdots & x_{np}
\end{matrix} \right) $$

$\beta$ est la matrice des $p$ coefficients, $\beta_0$ √©tant l'intercept qui multiplie la premi√®re colonne de la matrice $X$.

$$ \beta = \left( \begin{matrix} 
\beta_0  \\ 
\beta_1  \\ 
\vdots \\ 
\beta_p 
\end{matrix} \right) $$

$\epsilon$ est l'erreur de chaque observation.

$$ \epsilon = \left( \begin{matrix} 
\epsilon_0  \\ 
\epsilon_1  \\ 
\vdots \\ 
\epsilon_n
\end{matrix} \right) $$

#### Mod√®les lin√©aires univari√©s avec variable cat√©gorielle **nominale**

Une variable cat√©gorielle nominale (non ordonn√©e) utilis√©e √† elle seule dans un mod√®le comme variable explicative, est un cas particulier de r√©gression multiple. En effet, l'**encodage cat√©goriel** (ou *dummyfication*) transforme une variable cat√©gorielle nominale en une matrice de mod√®le comprenant une colonne d√©signant l'intercept (une s√©rie de 1) d√©signant la cat√©gorie de r√©f√©rence, ainsi que des colonnes pour chacune des autres cat√©gories d√©signant l'appartenance (1) ou la non appartenance (0) de la cat√©gorie d√©sign√©e par la colonne.

##### L'encodage cat√©goriel

Une variable √† $C$ cat√©gories pourra √™tre d√©clin√©e en $C$ variables dont chaque colonne d√©signe par un 1 l'appartenance au groupe de la colonne et par un 0 la non-appartenance. Pour l'exemple, cr√©ons un vecteur d√©signant le cultivar de pomme de terre.

```{r}
data <- data.frame(cultivar = c('Superior', 'Superior', 'Superior', 'Russet', 'Kenebec', 'Russet'))
model.matrix(~cultivar, data)
```

Nous avons trois cat√©gories, encod√©es en trois colonnes. La premi√®re colonne est un intercept et les deux autres d√©crivent l'absence (0) ou la pr√©sence (1) des cultivars Russet et Superior. Le cultivar Kenebec est absent du tableau. En effet, en partant du principe que l'appartenance √† une cat√©gorie est mutuellement exclusive, c'est-√†-dire qu'un √©chantillon ne peut √™tre assign√© qu'√† une seule cat√©gorie, on peut d√©duire une cat√©gorie √† partir de l'information sur toutes les autres. Par exemple, si `cultivar_Russet` et `cultivar_Superior` sont toutes deux √©gales √† $0$, on conclura que `cultivar_Kenebec` est n√©cessairement √©gal √† $1$. Et si l'un d'entre `cultivar_Russet` et `cultivar_Superior` est √©gal √† $1$, `cultivar_Kenebec` est n√©cessairement √©gal √† $0$. L'information contenue dans un nombre $C$ de cat√©gorie peut √™tre encod√©e dans un nombre $C-1$ de colonnes. C'est pourquoi, dans une analyse statistique, on d√©signera une cat√©gorie comme une r√©f√©rence, que l'on d√©tecte lorsque toutes les autres cat√©gories sont encod√©es avec des $0$: cette r√©f√©rence sera incluse dans l'intercept. La cat√©gorie de r√©f√©rence par d√©faut en R est celle la premi√®re cat√©gorie dans l'ordre alphab√©tique. On pourra modifier cette r√©f√©rence avec la fonction `relevel()`.

```{r}
data$cultivar <- relevel(data$cultivar, ref = "Superior")
model.matrix(~cultivar, data)
```

Pour certains mod√®les, vous devrez vous assurer vous-m√™me de l'encodage cat√©goriel. Pour d'autre, en particulier avec l'*interface par formule* de R, ce sera fait automatiquement.

##### Exemple d'application

Prenons la topographie du terrain, qui peut prendre plusieurs niveaux.

```{r}
levels(lasrosas.corn$topo)
```

Explorons le rendement selon la topographie.

```{r}
ggplot(lasrosas.corn, aes(x = topo, y = yield)) +
    geom_boxplot()
```

Les diff√©rences sont √©videntes, et la mod√©lisation devrait montrer des effets significatifs.

L'encodage cat√©goriel peut √™tre visualis√© en g√©n√©rant la matrice de mod√®le avec la fonction `model.matrix()` et l'interface-formule - sans la variable-r√©ponse.

```{r}
model.matrix(~ topo, data = lasrosas.corn) %>% 
    tbl_df() %>% # tbl_df pour transformer la matrice en tableau
    sample_n(10) 
```

Dans le cas d'un mod√®le avec une variable cat√©gorielle nominale seule, l'intercept repr√©sente la cat√©gorie de r√©f√©rence, ici `E`. Les autres colonnes sp√©cifient l'appartenance (1) ou la non-appartenance (0) de la cat√©gorie pour chaque observation.

Cette matrice de mod√®le utilis√©e pour la r√©gression donnera un intercept, qui indiquera l'effet de la cat√©gorie de r√©f√©rence, puis les diff√©rences entre les cat√©gories subs√©quentes et la cat√©gorie de r√©f√©rence.

```{r}
modlin_4 <- lm(yield ~ topo, data = lasrosas.corn)
summary(modlin_4)
```

Le mod√®le lin√©aire est √©quivalent √† l'anova, mais les r√©sultats de `lm` sont plus √©labor√©s.

```{r}
summary(aov(yield ~ topo, data = lasrosas.corn))
```

L'analyse de r√©sidus peut √™tre effectu√©e de la m√™me mani√®re.

#### Mod√®les lin√©aires univari√©s avec variable cat√©gorielle **ordinale**

Bien que j'introduise la r√©gression sur variable cat√©gorielle ordinale √† la suite de la section sur les variables nominales, nous revenons dans ce cas √† une r√©gression simple, univari√©e. Voyons un cas √† 5 niveaux.

```{r}
statut <- c("Totalement en d√©saccord", 
            "En d√©saccord",
            "Ni en accord, ni en d√©saccord",
            "En accord",
            "Totalement en accord")
statut_o <- factor(statut, levels = statut, ordered=TRUE)
model.matrix(~statut_o) # ou bien, sans passer par model.matrix, contr.poly(5) o√π 5 est le nombre de niveaux
```

La matrice de mod√®le a 5 colonnes, soit le nombre de niveaux: un intercept, puis 4 autres d√©signant diff√©rentes valeurs que peuvent prendre les niveaux. Ces niveaux croient-ils lin√©airement? De mani√®re quadratique, cubique ou plus loin dans des distributions polynomiales?

```{r}
modmat_tidy <- data.frame(statut, model.matrix(~statut_o)[, -1]) %>%
    gather(variable, valeur, -statut)
modmat_tidy$statut <- factor(modmat_tidy$statut, 
                             levels = statut, 
                             ordered=TRUE)
ggplot(data = modmat_tidy, mapping = aes(x = statut, y = valeur)) +
    facet_wrap(. ~ variable) +
    geom_point() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

R√®gle g√©n√©rale, pour les variables ordinales, on pr√©f√©rera une distribution lin√©aire, et c'est l'option par d√©faut de la fonction `lm()`. L'utilisation d'une autre distribution peut √™tre effectu√©e √† la mitaine en utilisant dans le mod√®le la colonne d√©sir√©e de la sortie de la fonction `model.matrix()`.

#### R√©gression multiple √† plusieurs variables

Reprenons le tableau de donn√©es du rendement de ma√Øs.

```{r}
head(lasrosas.corn)
```

Pour ajouter des variables au mod√®le dans l'interface-formule, on additionne les noms de colonne. La variable `lat` d√©signe la latitude, la variable `long` d√©signe la latitude et la variable `bv` (*brightness value*) d√©signe la teneur en mati√®re organique du sol (plus `bv` est √©lev√©e, plus faible est la teneur en mati√®re organique).

```{r}
modlin_5 <- lm(yield ~ lat + long + nitro + topo + bv,
               data = lasrosas.corn)
summary(modlin_5)
```

L'ampleur des coefficients est relatif √† l'√©chelle de la variable. En effet, un coefficient de 5541 sur la variable `lat` n'est pas comparable au coefficient de la variable `bv`, de -0.5089, √©tant donn√© que les variables ne sont pas exprim√©es avec la m√™me √©chelle. Pour les comparer sur une m√™me base, on peut centrer (soustraire la moyenne) et r√©duire (diviser par l'√©cart-type).

```{r}
scale_vec <- function(x) as.vector(scale(x)) # la fonction scale g√©n√®re une matrice: nous d√©sirons un vecteur

lasrosas.corn_sc <- lasrosas.corn %>%
    mutate_at(c("lat", "long", "nitro", "bv"), 
                 scale_vec)

modlin_5_sc <- lm(yield ~ lat + long + nitro + topo + bv,
               data = lasrosas.corn_sc)
summary(modlin_5_sc)
```

Typiquement, les variables cat√©gorielles, qui ne sont pas mises √† l'√©chelle, donneront des coefficients plus √©lev√©es, et devrons √™tre √©valu√©es entre elles et non comparativement aux variables mises √† l'√©chelle. Une mani√®re conviviale de repr√©senter des coefficients consiste √† cr√©er un tableau (fonction `tibble()`) incluant les coefficients ainsi que leurs intervalles de confiance, puis √† les porter graphiquement.

```{r}
intervals <- tibble(Estimate = coefficients(modlin_5_sc)[-1], # [-1] enlever l'intercept
                    LL = confint(modlin_5_sc)[-1, 1], # [-1, ] enlever la premi√®re ligne, celle de l'intercept
                    UL = confint(modlin_5_sc)[-1, 2],
                    variable = names(coefficients(modlin_5_sc)[-1])) 
intervals
```

```{r}
ggplot(data = intervals, mapping = aes(x = Estimate, y = variable)) +
    geom_vline(xintercept = 0, lty = 2) +
    geom_segment(mapping = aes(x = LL, xend = UL, 
                               y = variable, yend = variable)) +
    geom_point() +
    labs(x = "Coefficient standardis√©", y = "")
```

On y voit qu'√† l'exception de la variable `long`, tous les coefficients sont diff√©rents de 0. Le coefficient `bv` est n√©gatif, indiquant que plus la valeur de `bv` est √©lev√© (donc plus le sol est pauvre en mati√®re organique), plus le rendement est faible. Plus la latitude est √©lev√©e (plus on se dirige vers le Nord de l'Argentine), plus le rendement est √©lev√©. La dose d'azote a aussi un effet statistique positif sur le rendement.

Quant aux cat√©gories topographiques, elles sont toutes diff√©rentes de la cat√©gorie `E`, ne croisant pas le z√©ro. De plus, les intervalles de confiance ne se chevauchant pas, on peut conclure en une diff√©rence significative d'une √† l'autre. Bien s√ªr, tout cela au seuil de confiance de 0.05.

On pourra retrouver des cas o√π l'effet combin√© de plusieurs variables diff√®re de l'effet des deux variables prises s√©par√©ment. Par exemple, on pourrait √©valuer l'effet de l'azote et celui de la topographie dans un m√™me mod√®le, puis y ajouter une interaction entre l'azote et la topographie, qui d√©finira des effets suppl√©mentaires de l'azote selon chaque cat√©gorie topographique. C'est ce que l'on appelle une interaction.

Dans l'interface-formule, l‚Äôinteraction entre l'azote et la topographie est not√©e `nitro:topo`. Pour ajouter cette interaction, la formule deviendra `yield ~ nitro + topo + nitro:topo`. Une approche √©quivalente est d'utiliser le raccourci `yield ~ nitro*topo`.

```{r}
modlin_5_sc <- lm(yield ~ nitro*topo,
               data = lasrosas.corn_sc)
summary(modlin_5_sc)
```

Les r√©sultats montre des effets de l'azote et des cat√©gories topographiques, mais il y a davantage d'incertitude sur les interactions, indiquant que l'effet statistique de l'azote est sensiblement le m√™me ind√©pendamment des niveaux topographiques.

----

#### Attention √† ne pas surcharger le mod√®le

Il est possible d'ajouter des interactions doubles, triples, quadruples, etc. Mais plus il y a d‚Äôinteractions, plus votre mod√®le comprendra de variables et vos tests d'hypoth√®se perdront en puissance statistique.

----

#### Les mod√®les lin√©aires g√©n√©ralis√©s

Dans un mod√®le lin√©aire ordinaire, un changement constant dans les variables explicatives r√©sulte en un changement constant de la variable-r√©ponse. Cette supposition ne serait pas ad√©quate si la variable-r√©ponse √©tait un d√©compte, si elle est bool√©enne ou si, de mani√®re g√©n√©rale, la variable-r√©ponse ne suivait pas une distribution continue. Ou, de mani√®re plus sp√©cifique, il n'y a pas moyen de retrouver une distribution normale des r√©sidus? On pourra bien s√ªr transformer les variables (sujet du chapitre 6, en d√©veloppement). Mais il pourrait s'av√©rer impossible, ou tout simplement non souhaitable de transformer les variables. Le mod√®le lin√©aire g√©n√©ralis√© (MLG, ou *generalized linear model* - GLM) est une g√©n√©ralisation du mod√®le lin√©aire ordinaire chez qui la variable-r√©ponse peut √™tre caract√©ris√© par une distribution de Poisson, de Bernouilli, etc.

Prenons d'abord cas d'un d√©compte de vers fil-de-fer (`worms`) retrouv√©s dans des parcelles sous diff√©rents traitements (`trt`). Les d√©comptes sont typiquement distribu√© selon une loi de Poisson.

```{r}
cochran.wireworms %>% ggplot(aes(x = worms)) + geom_histogram(bins = 10)
```

Explorons les d√©comptes selon les traitements.

```{r}
cochran.wireworms %>% ggplot(aes(x = trt, y = worms)) + geom_boxplot()
```

Les traitements semble √† premi√®re vue avoir un effet comparativement au contr√¥le. Lan√ßons un MLG avec la fonction `glm()`, et sp√©cifions que la sortie est une distribution de Poisson. Bien que la fonction de lien (`link = "log"`) soit explictement impos√©e, le log est la valeur par d√©faut pour les distributions de Poisson. Ainsi, les coefficients du mod√®les devront √™tre interpr√©t√©s selon un mod√®le $log \left(worms \right) = intercept + pente \times coefficient$.

```{r}
modglm_1 <- glm(worms ~ trt, cochran.wireworms, family = stats::poisson(link="log"))
summary(modglm_1)
```

L'interpr√©tation sp√©cifique des coefficients d'une r√©gression de Poisson doit passer par la fonction de lien $log \left(worms \right) = intercept + pente \times coefficient$. Le traitement de r√©f√©rence (K), qui correspond √† l'intercept, sera accompagn√© d'un nombre de vers de $exp \left(0.1823\right) = 1.20$ vers, et le traitement M, √† $exp \left(1.6422\right) = 5.17$ vers. Cela correspond √† ce que l'on observe sur les boxplots plus haut.

Il est tr√®s probable (p-value de ~0.66) qu'un intercept (traitement K) de 0.18 ayant une erreur standard de 0.4082 ait √©t√© g√©n√©r√© depuis une population dont l'intercept est nul. Quant aux autres traitements, leurs effets sont tous significatifs au seuil 0.05, mais peuvent-ils √™tre consid√©r√©s comme √©quivalents?

```{r}
intervals <- tibble(Estimate = coefficients(modglm_1), # [-1] enlever l'intercept
                    LL = confint(modglm_1)[, 1], # [-1, ] enlever la premi√®re ligne, celle de l'intercept
                    UL = confint(modglm_1)[, 2],
                    variable = names(coefficients(modglm_1))) 
intervals
```

```{r}
ggplot(data = intervals, mapping = aes(x = Estimate, y = variable)) +
    geom_vline(xintercept = 0, lty = 2) +
    geom_segment(mapping = aes(x = LL, xend = UL, 
                               y = variable, yend = variable)) +
    geom_point() +
    labs(x = "Coefficient", y = "")
```

Les intervalles de confiance se superposant, on ne peut pas conclure qu'un traitement est li√© √† une r√©duction plus importante de vers qu'un autre, au seuil 0.05.

Maintenant, √† d√©faut de trouver un tableau de donn√©es plus appropri√©, prenons le tableau `mtcars`, qui rassemble des donn√©es sur des mod√®les de voitures. La colonne `vs`, pour v-shaped, inscrit 0 si les pistons sont droit et 1 s'ils sont plac√©s en V dans le moteur. Peut-on expliquer la forme des pistons selon le poids du v√©hicule (`wt`)?

```{r}
mtcars %>% sample_n(6)
```

```{r}
mtcars %>% 
    ggplot(aes(x = wt, y = vs)) + geom_point()
```

Il semble y avoir une tendance: les v√©hicules plus lourds ont plut√¥t des pistons droits (`vs = 0`). V√©rifions cela.

```{r}
modglm_2 <- glm(vs ~ wt, data = mtcars, family = stats::binomial())
summary(modglm_2)
```

**Exercice**. Analyser les r√©sultats.

#### Les mod√®les non-lin√©aires

La hauteur d'un arbre en fonction du temps n'est typiquement pas lin√©aire. Elle tend √† cro√Ætre de plus en plus lentement jusqu'√† un plateau. De m√™me, le rendement d'une culture trait√© avec des doses croissantes de fertilisants tend √† atteindre un maximum, puis √† se stabiliser.

Ces ph√©nom√®nes ne peuvent pas √™tre approxim√©s par des mod√®les lin√©aires. Examinons les donn√©es du tableau `engelstad.nitro`.

```{r}
engelstad.nitro %>% sample_n(10)
```

```{r}
engelstad.nitro %>%
    ggplot(aes(x = nitro, y = yield)) +
        facet_grid(year ~ loc) +
        geom_line() +
        geom_point()
```

Le mod√®le de Mitscherlich pourrait √™tre utilis√©.

$$ y = A \left( 1 - e^{-R \left( E + x \right)} \right) $$

o√π $y$ est le rendement, $x$ est la dose, $A$ est l'asymptote vers laquelle la courbe converge √† dose croissante, $E$ est l'√©quivalent de dose fourni par l'environnement et $R$ est le taux de r√©ponse.

Explorons la fonction.

```{r}
mitscherlich_f <- function(x, A, E, R) {
    A * (1 - exp(-R*(E + x)))
}

x <- seq(0, 350, by = 5)
y <- mitscherlich_f(x, A = 75, E = 30, R = 0.02)

ggplot(tibble(x, y), aes(x, y)) +
    geom_point(data = engelstad.nitro, aes(x = nitro, y = yield)) +
    geom_line() + ylim(c(0, 100))
```

**Exercice**. Changez les param√®tres pour visualiser comment la courbe r√©agit.

Nous pouvons d√©crire le mod√®le gr√¢ce √† l'interface formule dans la fonction `nls()`. Notez que les mod√®les non-lin√©aires demandent des strat√©gies de calcul diff√©rentes de celles des mod√®les lin√©aires. En tout temps, nous devons identifier des valeurs de d√©part raisonnables pour les param√®tres dans l'argument `start`. Vous r√©ussirez rarement √† obtenir une convergence du premier coup avec vos param√®tres de d√©part. Le d√©fi est d'en trouver qui permettront au mod√®le de converger. Parfois, le mod√®le ne convergera jamais. D'autres fois, il convergera vers des solutions diff√©rentes selon les variables de d√©part choisies.
<
```{r}
#modnl_1 <- nls(yield ~ A * (1 - exp(-R*(E + nitro))),
#                data = engelstad.nitro, 
#                start = list(A = 50, E = 10, R = 0.2))
```

Le mod√®le ne converge pas. Essayons les valeurs prises plus haut, lors de la cr√©ation du graphique, qui semblent bien s'ajuster.

```{r}
modnl_1 <-  nls(yield ~ A * (1 - exp(-R*(E + nitro))),
                data = engelstad.nitro,
                start = list(A = 75, E = 30, R = 0.02))
```

Bingo! Voyons maintenant le sommaire.

```{r}
summary(modnl_1)
```

Les param√®tres sont significativement diff√©rents de z√©ro au seuil 0.05, et donnent la courbe suivante.

```{r}
x <- seq(0, 350, by = 5)
y <- mitscherlich_f(x,
                    A = coefficients(modnl_1)[1],
                    E = coefficients(modnl_1)[2],
                    R = coefficients(modnl_1)[3])

ggplot(tibble(x, y), aes(x, y)) +
    geom_point(data = engelstad.nitro, aes(x = nitro, y = yield)) +
    geom_line() + ylim(c(0, 100))
```

Et les r√©sidus...

```{r}
tibble(res = residuals(modnl_1)) %>%
    ggplot(aes(x = res)) + geom_histogram(bins = 20)
```

```{r}
tibble(nitro = engelstad.nitro$nitro, res = residuals(modnl_1)) %>%
    ggplot(aes(x = nitro, y = res)) + 
        geom_point() +
        geom_hline(yintercept = 0, colour = "red")
```

Les r√©sidus ne sont pas distribu√©s normalement, mais semble bien partag√©s de part et d'autre de la courbe.

### Mod√®les √† effets mixtes

Lorsque l'on combine des variables fixes (test√©es lors de l'exp√©rience) et des variables al√©atoire (variation des unit√©s exp√©rimentales), on obtient un mod√®le mixte. Les mod√®les mixtes peuvent √™tre univari√©s, multivari√©s, lin√©aires ordinaires ou g√©n√©ralis√©s ou non lin√©aires.

√Ä la diff√©rence d'un effet fixe, un effet al√©atoire sera toujours distribu√© normalement avec une moyenne de 0 et une certaine variance. Dans un mod√®le lin√©aire o√π l'effet al√©atoire est un d√©calage d'intercept, cet effet s'additionne aux effets fixes:

$$ y = X \beta + Z b + \epsilon $$

o√π:

$Z$ est la matrice du mod√®le √† $n$ observations et $p$ variables al√©atoires. Les variables al√©atoires sont souvent des variables nominales qui subissent un encodage cat√©goriel.

$$ Z = \left( \begin{matrix} 
z_{11} & \cdots & z_{1p}  \\ 
z_{21} & \cdots & z_{2p}  \\ 
\vdots & \ddots & \vdots  \\ 
z_{n1} & \cdots & z_{np}
\end{matrix} \right) $$

$b$ est la matrice des $p$ coefficients al√©atoires.

$$ b = \left( \begin{matrix} 
b_0  \\ 
b_1  \\ 
\vdots \\ 
b_p 
\end{matrix} \right) $$

Le tableau `lasrosas.corn`, utilis√© pr√©c√©demment, contenait trois r√©p√©titions effectu√©s au cours de deux ann√©es, 1999 et 2001. √âtant donn√© que la r√©p√©tition R1 de 1999 n'a rien √† voir avec la r√©p√©tition R1 de 2001, on dit qu'elle est **embo√Æt√©e** dans l'ann√©e.

Le module `nlme` nous aidera √† monter notre mod√®le mixte.

```{r}
library("nlme")

mmodlin_1 <- lme(fixed = yield ~ lat + long + nitro + topo + bv,
                 random = ~ 1|year/rep,
                 data = lasrosas.corn)

```

√Ä ce stade vous devriez commencer √† √™tre familier avec l'interface formule et vous deviez saisir l'argument `fixed`, qui d√©signe l'effet fixe. L'effet al√©atoire, `random`, suit un tilde `~`. √Ä gauche de la barre verticale `|`, on place les variables d√©signant les effets al√©atoire sur la pente. Nous n'avons pas couvert cet aspect, alors nous le laissons √† `1`. √Ä droite, on retrouve un structure d'embo√Ætement d√©signant l'effet al√©atoire: le premier niveau est l'ann√©e, dans laquelle est embo√Æt√©e la r√©p√©tition.

```{r}
summary(mmodlin_1)
```

La sortie est semblable √† celle de la fonction `lm()`.

#### Mod√®les mixtes non-lin√©aires

Le mod√®le non lin√©aire cr√©√© plus haut liait le rendement √† la dose d'azote. Toutefois, les unit√©s exp√©rimentales (le site `loc` et l'ann√©e `year`) n'√©taient pas pris en consid√©ration. Nous allons maintenant les consid√©rer. 

Nous devons d√©cider la structure de l'effet al√©atoire, et sur quelles variables il doit √™tre appliqu√© - la d√©cision appartient √† l'analyste. Il me semble plus convenable de supposer que le site et l'ann√©e affectera le rendement maximum plut√¥t que l'environnement et le taux: les effets al√©atoires seront donc affect√©s √† la variable `A`. Les effets al√©atoires n'ont pas de structure d'embo√Ætement. L'effet de l'ann√©e sur A sera celui d'une pente et l'effet de site sera celui de l'intercept. La fonction que nous utiliserons est `nlme()`.

```{r}
mm <- nlme(yield ~ A * (1 - exp(-R*(E + nitro))),
           data = engelstad.nitro, 
           start = c(A = 75, E = 30, R = 0.02), 
           fixed = list(A ~ 1, E ~ 1, R ~ 1), 
           random = A ~ year | loc)
summary(mm)
```

Et sur graphique:

```{r}
engelstad.nitro %>%
  ggplot(aes(x = nitro, y = yield)) +
  facet_grid(year ~ loc) +
  geom_line(data = tibble(nitro = engelstad.nitro$nitro,
                          yield = predict(mm, level = 0)),
            colour = "grey35") +
  geom_point() +
  ylim(c(0, 95))
```


Les mod√®les mixtes non lin√©aires peuvent devenir tr√®s complexes lorsque les param√®tres, par exemple A, E et R, sont eux-m√™me affect√©s lin√©airement par des variables (par exemple `A ~ topo`). Pour aller plus loin, consultez [Parent et al. (2017) ](https://doi.org/10.3389/fenvs.2017.00081) ainsi que les [calculs associ√©s √† l'article](https://github.com/essicolo/site-specific-multilevel-modeling-of-potato-response-to-nitrogen-fertilization). Ou √©crivez-moi un courriel pour en discuter!

**Note**. L'interpr√©tation de p-values sur les mod√®les mixtes est controvers√©e. √Ä ce sujet, Douglas Bates a √©crit une longue lettre √† la communaut√© de d√©veloppement du module `lme4`, une alternative √† `nlme`, qui remet en cause l'utilisation des p-values, [ici](https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html). De plus en plus, pour les mod√®les mixtes, on se tourne vers les statistiques bay√©siennes, couvertes dans le chapitre \@ref(chapitre-biostats-bayes) avec le module greta. Mais en ce qui a trait aux mod√®les mixtes, le module [`brms`](https://github.com/paul-buerkner/brms) automatise bien des aspects de l'approche bay√©sienne.

### Aller plus loin

#### Statistiques g√©n√©rales:
- [The analysis of biological data](https://ariane25.bibl.ulaval.ca/ariane/wicket/detail?c=ariane&m=S&rq.ct=PE&rq.fa=false&rq.r.esc=false&rq.r.l%5B0%5D.c=TI&rq.r.l%5B0%5D.ex=false&rq.r.l%5B0%5D.op=AND&rq.r.l%5B0%5D.v=the+analysis+of+biological+data&rq.r.la=*&rq.r.loc=*&rq.r.pft=true&rq.r.ta=*&rq.r.td=*&rq.rows=5&rq.st=1)

#### Statistiques avec R

- Disponibles en version √©lectronique √† la biblioth√®que de l'Universit√© Laval:
    - Introduction aux statistiques avec R: [Introductory statistics with R](https://ariane25.bibl.ulaval.ca/ariane/wicket/detail?c=ariane&m=S&rq.ct=PE&rq.fa=false&rq.r.esc=false&rq.r.l%5B0%5D.c=TI&rq.r.l%5B0%5D.ex=false&rq.r.l%5B0%5D.op=AND&rq.r.l%5B0%5D.v=Introductory+statistics+with+R&rq.r.la=*&rq.r.loc=*&rq.r.pft=true&rq.r.ta=*&rq.r.td=*&rq.rows=1&rq.st=0)
    - Approfondir les statistiques avec R: [The R Book, Second edition](https://ariane25.bibl.ulaval.ca/ariane/wicket/detail?c=ariane&m=S&rq.ct=PE&rq.fa=false&rq.r.esc=false&rq.r.l%5B0%5D.c=TI&rq.r.l%5B0%5D.ex=false&rq.r.l%5B0%5D.op=AND&rq.r.l%5B0%5D.v=the+r+book&rq.r.la=*&rq.r.loc=*&rq.r.pft=true&rq.r.ta=*&rq.r.td=*&rq.rows=15&rq.st=2)
    - Approfondir les mod√®les √† effets mixtes avec R: [Mixed Effects Models and Extensions in Ecology with R](https://ariane25.bibl.ulaval.ca/ariane/wicket/detail?c=ariane&m=S&rq.ct=PE&rq.fa=false&rq.r.esc=false&rq.r.l%5B0%5D.c=TI&rq.r.l%5B0%5D.ex=false&rq.r.l%5B0%5D.op=AND&rq.r.l%5B0%5D.v=Mixed+Effects+Models+and+Extensions+in+Ecology+with+R&rq.r.la=*&rq.r.loc=*&rq.r.pft=false&rq.r.ta=*&rq.r.td=*&rq.rows=2&rq.st=1)
- [ModernDive](https://moderndive.com/index.html), un livre en ligne offrant une approche moderne avec le package `moderndive`.

```{r, include=FALSE}
rm(list = ls())
```

<!--chapter:end:06-0_biostats.Rmd-->

# Introduction √† l'analyse bay√©sienne en √©cologie {#chapitre-biostats-bayes}

 ***
Ô∏è\ **Objectifs sp√©cifiques**:

**Ce chapitre est un extra. Il ne fait pas partie des objectifs du cours. Il ne sera pas √©valu√©.**

√Ä la fin de ce chapitre, vous

- serez en mesure de d√©finir ce que sont les statistiques bay√©siennes
- serez en mesure de calculer des statistiques descriptives de base en mode bay√©sien avec le module [**`greta`**](https://greta-stats.org/).

 ***

Les statistiques bay√©siennes forment une trousse d'outils √† garder dans votre pack sack.

## Qu'est-ce que c'est?

En deux mots: mod√©lisation probabiliste. Un approche de mod√©lisation probabiliste se servant au mieux de l'information disponible. Pour calculer les probabilit√©s d'une variable inconnu en mode bay√©sien, nous avons besoin:

* De donn√©es
* D'un mod√®le
* D'une id√©e plus ou moins pr√©cise du r√©sultat avant d'avoir analys√© les donn√©es

De mani√®re plus formelle, le th√©or√®me de Bayes (qui forme la base de l'analyse bay√©seienne), dit que la distribution de probabilit√© des param√®tres d'un mod√®le (par exemple, la moyenne ou une pente) est proportionnelle √† la mutliplication de la distribution de probabilit√© estim√©e des param√®tres et la distribution de probabilit√© √©mergeant des donn√©es.

Plus formellement,

$$P\left(\theta | y \right) = \frac{P\left(y | \theta \right) \times P\left(\theta\right)}{P\left(y \right)}$$,

o√π $P\left(\theta | y \right)$ $-$ la probabilit√© d'obtenir des param√®tres $\theta$ √† partir des donn√©es $y$ $-$ est la distribution de probabilit√© *a posteriori*, calcul√©e √† partir de votre *a prioti* $P\left(\theta\right)$ $-$ la probabilit√© d'obtenir des param√®tres $\theta$ sans √©gard aux donn√©es, selon votre connaissance du ph√©nom√®ne $-$ et vos donn√©es observ√©es $P\left(y | \theta \right)$ $-$ la probabilit√© d'obtenir les donn√©es $y$ √©tant donn√©s les param√®tres $\theta$ qui r√©gissent le ph√©nom√®ne. $P\left(y\right)$, la probabilit√© d'observer les donn√©es, est appell√©e la *vraissemblance marginale*, et assure que la somme des probabilit√©s est nulle.

## Pourquoi l'utiliser?

Avec la notion fr√©quentielle de probabilit√©, on teste la probabilit√© d'observer les donn√©es recueillies √©tant donn√©e l'absence d'effet r√©el (qui est l'hypoth√®se nulle g√©n√©ralement adopt√©e). La notion bay√©sienne de probabilit√© combine la connaissance que l'on a d'un ph√©nom√®ne et les donn√©es observ√©es pour estimer la probabilit√© qu'il existe un effet r√©el. En d'autre mots, les stats fr√©quentielles testent si les donn√©es concordent avec un mod√®le du r√©el, tandis que les stats bay√©siennes √©valuent, selon les donn√©es, la probabilit√© que le mod√®le soit r√©el.

Le hic, c'est que lorsqu'on utilise les statistiques fr√©quentielles pour r√©pondre √† une question bay√©sienne, on s'expose √† de mauvaises interpr√©tations. Par exemple, lors d'un projet consid√©rant la vie sur Mars, les stats fr√©quentielles √©valueront si les donn√©es recueillies sont conformes ou non avec l'hypoth√®se de la vie sur Mars. Par contre, pour √©valuer la *probabilit√© de l'existance de vie sur Mars*, on devra passer par les stats bay√©siennes (exemple tir√©e du billet [Dynamic Ecology -- Frequentist vs. Bayesian statistics: resources to help you choose](https://dynamicecology.wordpress.com/2011/10/11/frequentist-vs-bayesian-statistics-resources-to-help-you-choose/)).

## Comment l'utiliser?

Bien que la formule du th√©or√®me de Bayes soit plut√¥t simple, calculer une fonction *a posteriori* demandera de passer par des algorithmes de simulation, ce qui pourrait demander une bonne puissance de calcul, et des outils appropri√©s. R comporte une panoplie d'outils pour le calcul bay√©sien g√©n√©rique ([**`rstan`**](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started), [**`rjags`**](https://cran.r-project.org/web/packages/rjags/index.html), [**`MCMCpack`**](https://cran.r-project.org/web/packages/MCMCpack/index.html), etc.), et d'autres outils pour des besoins particuliers ([**`brms`**: R package for Bayesian generalized multivariate non-linear multilevel models using Stan](https://github.com/paul-buerkner/brms)). Nous utiliserons ici le module g√©n√©rique [**`greta`**](https://greta-stats.org/), qui permet de g√©n√©rer de mani√®re conviviale plusieurs types de mod√®les bay√©siens.

Pour installer greta, vous devez pr√©alablement installer Python, gr√©√© des modules tensorflow et tensorflow-probability en suivant [le guide](https://greta-stats.org/articles/get_started.html). En somme, vous devez d'abord installer greta (`install.packages("greta")`). Puis vous devez installer une distribution de Python -- je vous sugg√®re [Anaconda](https://www.anaconda.com/download) (~500 Mo) ou [Miniconda](https://conda.io/miniconda.html) pour une installation minimale (~60 Mo). Enfin, lancez les commandes suivantes (une connection internet est n√©cessaire pour t√©l√©charger les modules). Si vous avez install√© la version compl√®te d'Anaconda, vous avez acc√®s √† Anaconda-navigator, une interface pour la gestion de vos environnements de calcul: assurez-vous qu'il soit fermer pour √©viter que la commande se butte √† des fichiers verouill√©s.

```
 greta::install_tensorflow(
    method = "conda",
    envname = "r-greta",
    version = "1.14.0",
    extra_packages = "tensorflow-probability==0.7.0"
  )
```

Puis, vous devez installer une distribution de Python -- je vous sugg√®re [Anaconda](https://www.anaconda.com/download) (~500 Mo) ou [Miniconda](https://conda.io/miniconda.html) pour une installation minimale (~60 Mo). Enfin, lancez les commandes suivantes pour installer Python, **`tensorflow`** et **`tensorflow-probability`** dans un nouvel environnement de calcul (nomm√© `r-greta`).

```
reticulate::conda_create(envname = "r-greta", packages = c("python", "tensorflow=1.14", "tensorflow-probability=0.7"))
```

## Faucons p√©lerins

Empruntons un exemple du livre [Introduction to WinBUGS for Ecologists: A Bayesian Approach to Regression, ANOVA and Related Analyses](https://www.elsevier.com/books/introduction-to-winbugs-for-ecologists/kery/978-0-12-378605-0), de Marc K√©ry et examinons la masse de faucons p√©lerins. Mais alors que Marc K√©ry utilise WinBUGS, un logiciel de r√©solution de probl√®me en mode bay√©sien, nous utiliserons greta.

![](https://upload.wikimedia.org/wikipedia/commons/thumb/2/21/Falco_peregrinus_-_01.jpg/1024px-Falco_peregrinus_-_01.jpg)
Source: [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Falco_peregrinus_-_01.jpg)

Pour une premi√®re approche, nous allons estimer la masse moyenne d'une population de faucons p√©lerins.

√Ä titre de donn√©es, g√©n√©rons des nombres al√©atoires. Cette strat√©gie permet de valider les statistiques en les comparant aux param√®tre que l'on impose. Ici, nous imposons une moyenne de 600 grammes et un √©cart-type de 30 grammes. G√©n√©rons une s√©ries de donn√©es avec 20 √©chantillons.

```{r}
library("tidyverse")
set.seed(5682)
y20 <- rnorm(n = 20, mean=600, sd = 30)
y200 <- rnorm(n = 200, mean=600, sd = 30)
par(mfrow = c(1, 2))
hist(y20, breaks=5)
hist(y200, breaks=20)
```

Je cr√©e une fonction qui retourne la moyenne et l'erreur sur la moyenne ou sur la distribution. Calculons les statistiques classiques.

```{r}
confidence_interval <- function(x, on="deviation", distribution="t", level=0.95) {
  m <- mean(x)
  se <- sd(x)
  n <- length(x)
  if (distribution == "t") {
    error <- se * qt((1+level)/2, n-1)
  } else if (distribution == "normal") {
    error <- se * qnorm((1+level)/2)
  }
  if (on == "error") {
    error <- error/sqrt(n)
  }
  return(c(ll = m-error, mean = m, ul = m+error))
}
```

```{r}
print("D√©viation, 95%")
print(round(confidence_interval(y20, on='deviation', level=0.95), 2))

print("Erreur, 95%")
print(round(confidence_interval(y20, on='error', level=0.95), 2))

print("√âcart-type")
print(round(sd(y20), 2))
```

En faisant cela, nous prenons pour acquis que les donn√©es sont distribu√©es normalement. En fait, nous savons qu'elles devraient l'√™tre pour de grands √©chantillons, puisque nous avons nous-m√™me g√©n√©r√© les donn√©es. Par contre, comme observateur par exemple de la s√©rie de 20 donn√©es g√©n√©r√©es, la distribution est d√©finitivement asym√©trique. Sous cet angle, la moyenne, ainsi que l'√©cart-type, pourraient √™tre des param√®tres biais√©s. Nous pouvons justifier le choix d'une loi normale par des connaissances a priori des distributions de masse parmi des esp√®ces d'oiseau. Ou bien transformer les donn√©es pour rendre leur distribution normale (chapitre \@ref(chapitre-explorer)).

## Statistiques d'une population

### Calcul analytique

Supposer une distribution normale d'une population implique d'estimer deux param√®tres: sa moyenne et son √©cart-type. Toutefois, pour cet exemple, nous supposons que l'√©cart-type est connu, ce qui n'est √† toute fin pratique jamais le cas, mais vous d√©couvrirez bient√¥t pourquoi nous laisse tomber l'√©cart-type √† cette √©tape. Nous allons donc estimer la moyenne d'une population de faucons dont l'√©cart-type est de 30: $X \sim \mathcal{N}(\mu, 30)$.

On sait qu'une distribution normale est d√©finir par la fonction suivante.

$$f(x) = \frac{1}{\sigma \sqrt{2\pi} } e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}$$

Ou, en R:
  
```{r}
normdist <- function(x, mean, sd) {
  f <- 1 / (sd * sqrt(2*pi)) * exp(-0.5 * ((x-mean)/sd)^2)
  return(f)
}
```

Ce qui n'est utile que pour une petite d√©monstration, √©tant donn√©e cette op√©ration peut √™tre effectu√©e avec la fonction `dnorm()`, qui vient avec le module **`stats`** charg√© en R par d√©faut.

```{r}
x_ <- seq(0, 2000, 100)
plot(x_, dnorm(x = x_, mean = 750, sd = 300), "l", lwd = 4, col = "pink")
points(x_, normdist(x = x_, mean = 750, sd = 300))
```

Reprenons notre √©quation de Bayes.

$$P\left(\theta | y \right) = \frac{P\left(y | \theta \right) \times P\left(\theta\right)}{P\left(y \right)}$$,

En mode bay√©sien, nous devons d√©finir la connaissance *a priori*, P\left(\theta\right), sous forme de variables al√©atoires non-observ√©es selon une distribution. Prenons l'exemple des faucons p√©lerins. Disons que nous ne savons pas √† quoi ressemble la moyenne du groupe a priori. Nous pouvons utiliser un *a priori* peu informatif, o√π la masse moyenne peut prendre n'importe quelle valeur entre 0 et 2000 grammes, sans pr√©f√©rence: nous lui imposons donc un a priori selon une distribution uniforme. Idem pour l'√©cart-type. C'est ce qu'on appelle des *a priori* plats. Mais il est plut√¥t conseiller ([Gelman et al., 2013](http://www.stat.columbia.edu/~gelman/book/)) d'utiliser des *a priori* vagues plut√¥t que plats ou non-informatifs. En effet, des masses de 0 g ou 2000 g ne sont pas aussi probables qu'une masse de 750 g. Si vous √©tudiez les faucons p√©lerins (ce qui n'est pas mon cas), vous aurez une id√©e de sa masse, ne serait-ce qu'en arcourrant la litt√©rature √† son sujet. Mais disons que j'estime tr√®s vaguement qu'une masse moyenne √©chantillonale devrait √™tre autour de 750 g, avec un large √©cart-type de 200 g sur la moyenne. Il s'agit de l'√©cart-type de la moyenne, pas de l'√©cart-type de l'√©chantillon que nous supposons √™tre connu. Notez que l'*a priori* peu avoir la forme que l'on d√©sire: il s'agit seulement de cr√©er un vecteur. Toutefois, g√©n√©rer ce vecteur avec des distributions connues est aussi pratique qu'√©l√©gant.

```{r}
x_mean <- seq(400, 1200, 5)
prob <- tibble(mass = x_mean, 
               Prior = dnorm(x = x_mean, mean = 750, sd = 200))

prob$Prior <- prob$Prior / sum(prob$Prior)

prob %>% 
  ggplot(aes(mass, Prior))+
  geom_line() +
  expand_limits(y=0)
```

Note sur le jargon: √©tant donn√©e que cet *a priori* aura la m√™me distribution que l'*a posteriori*, on dit que cet *a priori* est *conjugu√©*.

Nous allons √©galement utiliser nos donn√©es pour cr√©er une fonction de vraissemblance (likelihood), $P\left(y | \theta \right)$, qui est la distribution de probabilit√© issue des donn√©es: une distribution normale avec une moyenne calcul√©e et une variance connue.

```{r}
prob$Likelihood <- dnorm(x = x_mean, mean = mean(y20), sd = 30)
prob$Likelihood <- prob$Likelihood / sum(prob$Likelihood)

prob %>% 
  pivot_longer(-mass, names_to = "type", values_to = "probability") %>% 
  ggplot(aes(mass, probability, colour = type)) +
  geom_line() +
  expand_limits(y=0)
```

Noter distribution *a posteriori* est proportionnelle √† la multiplication de l'*a priori* et de la vraissemblance. Puis nous allons normaliser l'*a posteriori* pour faire en sorte que la somme des probabilit√©s soit de 1.

```{r}
prob$Posterior <- prob$Likelihood * prob$Prior
prob$Posterior <- prob$Posterior / sum(prob$Posterior)

prob %>% 
  pivot_longer(-mass, names_to = "type", values_to = "probability") %>% 
  ggplot(aes(mass, probability, colour = type)) +
  geom_line() +
  expand_limits(y=0)
```

La distribution *a posteriori* est presque call√©e sur les donn√©es. Pas √©tonnant, √©tant donn√©e que l'*a priori* est tr√®s vague. En revanche, un *a priori* plus affirm√©, avec un √©cart-type plus faible, aurait davantage de poids sur l'*a posteriori*.

> **Exercice**. Changez l'*a priori* et visualisez l'effet sur l'*a posteriori*.

Maintenant, imaginez ajouter l'√©cart-type. Cela reste faisable en calcul analytique, mais √ßa complique le calcul pour normaliser les proabilit√©. Ajoutez encore une variable et le calcul bay√©sien devient un v√©ritable casse-t√™te. En fait, en bay√©sien, la difficult√© de mettre √† l'√©chelle de plus d'un param√®tre rend rare la multiplication distributions de probabilit√©. C'est pourquoi l'on pr√©f√®re les simuler et √©chantillonnant, √† l'aide de diff√©rents algorithmes, la distribution *a posteriori*. En R, le module **`greta`** est con√ßu pour cela.

### greta

Chargeons d'abord les modules n√©cessaires. Avant de charger **`greta`**, il faut s√©lectionner l'environnement coda (Python) auquel se connecter. Lors de l'installation, nous avions sp√©cifi√© que l'installation se fasse dans l'environnement nomm√© `r-greta`.

```{r}
reticulate::use_condaenv("r-greta", required = TRUE) 
library("greta")
library("DiagrammeR")
library ("bayesplot")
library("tidybayes")
```

Reprenons l'*a piori* utilis√© pr√©c√©demment. Dans **`greta`**, nous d√©finissons notre *a priori* ainsi.

```{r}
param_mean <- normal(mean = 750, 200)
```

L'√©cart-type d'un √©chantillon ne peut pas √™tre n√©gatif. Il est commun pour les √©carts-types d'utiliser une distribution en tronqu√©e √† 0. On pourrait utiliser une normale tronqu√©e, mais la cauchy tronqu√©e est souvent recommand√©e (e.g. [Gelman, 2006](http://www.stat.columbia.edu/~gelman/research/published/taumain.pdf)) puisque la queue, plus √©paisse que la distribution normale, permet davantage de flexibilit√©. Disons que nous supposons un √©cart-type d'une moyenne de 50, et d'un √©cart-type de 100, tronqu√© √† 0.

```{r}
x_ <- seq(0, 500, 10)
plot(x_, dcauchy(x = x_, location = 50, scale = 100), "l")
param_sd <- cauchy(location = 50, scale = 100, dim = NULL, truncation = c(0, Inf))
```


La fonction a porteriori inclue la fonction de vraissemblance ainsi que la connaissancew a priori.

```{r}
distribution(y20) <- normal(param_mean, param_sd)
```

Le tout forme un mod√®le pour appr√©cier y, la masse des faucons p√©lerins.

```{r}
m <- model(param_mean, param_sd)
plot(m)
```

**L√©gende**:

![](images/5-1_legende.png)

Le graphique du mod√®le montre que deux param√®tres sont attach√©s √† des distributions pour g√©n√©rer une distribution de sortie.

Nous pouvons enfin lancer le mod√®le.

```{r}
draws <- mcmc(m, n_samples = 1000)
```

L'inspection de l'√©chantillonnage peut √™tre effectu√©e gr√¢ce au module bayesplot.

```{r}
mcmc_combo(draws, combo = c("hist", "trace"))
```

√Ä gauche, nous obtenons la distribution des param√®tres. √Ä droite, nous pouvons observer que l'√©chantillonnage semble stable. Dans le cas o√π il ne le serait pas, il faudrait revoir le mod√®le. Nous pouvons calculer des intervales de cr√©dibilt√©.

```{r}
draws_tidy <- draws %>%
  spread_draws(param_mean, param_sd)

draws_mean <- confidence_interval(x = draws_tidy$param_mean, on = "deviation", distribution = "normal", level = 0.95)
print("Moyenne:")
draws_mean

draws_sd <- confidence_interval(x = draws_tidy$param_sd, on = "deviation", distribution = "normal", level = 0.95)
print("√âcart-type:")
draws_sd
```

L'*a priori* √©tant vague, les r√©sultats de l'analyse bay√©sienne sont comparables aux statistiques fr√©quentielles.

```{r}
print("Erreur, 95%")
print(round(confidence_interval(y20, on='error', level=0.95), 2))
```

Les r√©sultats des deux approches doivent n√©anmoins √™tre interpr√©t√©s de mani√®re diff√©rente. En ce qui a trait √† la moyenne:

- **Fr√©quentiel**. Il y a une probabilit√© de 95% que mes donn√©es aient √©t√© g√©n√©r√©es √† partir d'une moyenne se situant entre 584 et 614 grammes.

- **Bay√©sien**. √âtant donn√©e mes connaissances (vagues) de la moyenne et de l'√©cart-type avant de proc√©der √† l'analyse (*a priori*), il y a une probabilit√© de 95% que la moyenne de la masse de la population se situe entre `r round(draws_mean[1], 1)` et `r round(draws_mean[3], 1)` grammes.

Nous avons une id√©e de la distribution des param√®tres... mais pas de la masse dans la population. Pas de probl√®me: nous avons des √©chantillons de moyennes et d'√©cart-type. Nous pouvons les √©chantilonn√©s avec remplacement pour g√©n√©rer des possibilit√©s de distrbution, puis √©chantillonn√© une masse selon ces distributions √©chantillonn√©es. Disons... 10000?

```{r starwars10000, out.width="100%", fig.align="center", fig.cap="Source: Star Wars, a new hope", echo = FALSE}
knitr::include_graphics("https://media.giphy.com/media/ZEaHzwGBp2qC0LN9Ye/giphy.gif")
```

Yep, 10 000.

```{r}
n_mass <- 10000
sim_mass <- rep(0, n_mass)
for (i in 1:n_mass) {
  sim_mean <- sample(draws_tidy$param_mean)
  sim_sd <- sample(draws_tidy$param_sd)
  sim_mass[i] <- rnorm(1, sim_mean, sim_sd)
}
```

La distribution avec laquelle j'ai cr√©√© les donn√©es `y20` plus haut avait une moyenne de 600 et un √©cart-type de 30. Je la superpose ici avec La distribution mod√©lis√©e avec notre petit mod√®le bay√©sien.

```{r}
x_ <- seq(450, 750, 5)
plot(x_, dnorm(x_, 600, 30), lty = 3, col = "red", type = "l", xlab = "Mass (g)", ylab = "Density")
lines(density(sim_mass), col = "blue")

sim_mass_limits <- confidence_interval(x = sim_mass, on = "deviation", distribution = "normal", level = 0.95)
abline(v = sim_mass_limits[1], lty = 2, col = "blue")
abline(v = sim_mass_limits[3], lty = 2, col = "blue")
text(x = sim_mass_limits[1], y = 0.01, labels = round(sim_mass_limits[1]), pos = 2, col = "blue")
text(x = sim_mass_limits[3], y = 0.01, labels = round(sim_mass_limits[3]), pos = 4, col = "blue")
```

Raisonnement bay√©sien: √âtant donn√©e mes connaissances vagues de la moyenne et de l'√©cart-type avant de proc√©der √† l'analyse, il y a une probabilit√© de 95% que la masse de la population se situe entre `r round(sim_mass_limits[1], 1)` et `r round(sim_mass_limits[3], 1)` grammes.

Nous avons maintenant une id√©e de la distribution de moyenne de la population. Mais, rarement, une analyse s'arr√™tera √† ce stade. Il arrive souvent que l'on doive comparer les param√®tres de deux, voire plusieurs groupes. Par exemple, comparer des populations vivants dans des √©cosyst√®mes diff√©rents, ou comparer un traitement √† un plac√©bo. Ou bien, comparer, dans une m√™me population de faucons p√©lerins, l'envergure des ailes des m√¢les et celle des femelles.

## Test de t: Diff√©rence entre des groupes

Pour comparer des groupes, on exprime g√©n√©ralement une hypoth√®se nulle, qui typiquement pose qu'il n'y a pas de diff√©rence entre les groupes. Puis, on choisit un test statistique **pour d√©terminer si les distributions des donn√©es observ√©es sont plausibles dans si l'hypoth√®se nulle est vraie**.

En d'autres mots, le test statistique exprime la probabilit√© que l'on obtienne les donn√©es obtenues s'il n'y avait pas de diff√©rence entre les groupes. 

Par exemple, si 

1. vous obtenez une *p-value* de moins de 0.05 apr√®s un test de comparaison et
2. l'hypoth√®se nulle pose qu'il n'y a pas de diff√©rence entre les groupes,

cela signifie qu'il y a une probabilit√© de 5% que vous ayiez obtenu ces donn√©es s'il n'y avait en fait pas de diff√©rence entre les groupe. Il serait donc peu probable que vos donn√©es euent √©t√© g√©n√©r√©es comme telles s'il n'y avait en fait pas de diff√©rence.

```{r}
n_f <- 30
moy_f <- 105
n_m <- 20
moy_m <- 77.5
sd_fm <- 2.75

set.seed(21526)
envergure_f <- rnorm(mean=moy_f, sd=sd_fm, n=n_f)
envergure_m <- rnorm(mean=moy_m, sd=sd_fm, n=n_m)

envergure_f_df <- data.frame(Sex = "Female", Wingspan = envergure_f)
envergure_m_df <- data.frame(Sex = "Male", Wingspan = envergure_m)
envergure_df <- rbind(envergure_f_df, envergure_m_df)

envergure_df %>%
  ggplot(aes(x=Wingspan)) +
  geom_histogram(aes(y=..density.., fill=Sex)) +
  geom_density(aes(value=Sex, y=..density..))
```

Et les statistiques des deux groupesL

```{r}
envergure_df %>%
  group_by(Sex) %>%
  summarise(mean = mean(Wingspan),
            sd = sd(Wingspan),
            n = n())
```

√âvaluer s'il y a une diff√©rence significative peut se faire avec un test de t (ou de Student).

```{r}
t.test(envergure_f, envergure_m)
```

La probabilit√© que les donn√©es ait √©t√© g√©n√©r√©es de la sorte si les deux groupes n'√©tait semblables est tr√®s faible (`p-value < 2.2e-16`). On obtiendrait sensiblement les m√™mes r√©sultats avec une r√©gression lin√©aire.

```{r}
linmod <- lm(Wingspan ~ Sex, envergure_df)
summary(linmod)
```

Le mod√®le lin√©aire est plus informatif. Il nous apprend que l'envergure des ailes des m√¢les est en moyenne plus faible de 28.0 cm que celle des femelles...

```{r}
confint(linmod, level = 0.95)
```

... avec un intervalle de confiance entre -29.6 cm √† -26.4 cm.

Utilisons l'information d√©riv√©e de statistiques classiques dans nos a priori. Oui-oui, on peut faire √ßa. Mais attention, un a priori trop pr√©cis ou trop coll√© sur nos donn√©es orientera le mod√®le vers une solution pr√©alablement √©tablie: ce qui constituerait aucune avanc√©e par rapport √† l'*a priori*. Nous allons utiliser a priori pour les deux groupes la moyenne des deux groupes, et comme dispersion la moyenne le double de l'√©cart-type. Rappelons que cet √©cart-type est l'a priori de √©cart-type sur la moyenne, non pas de la population.

Proc√©dons √† la cr√©ation d'un mod√®le greta. Nous utiliserons la r√©gression lin√©aire pr√©f√©rablement au test de t.

```{r}
is_female <- model.matrix(~envergure_df$Sex)[, 2]
```


```{r, eval = FALSE}
int <- normal(600, 30)
coef <- normal(30, 10)
sd <- cauchy(0, 10, truncation = c(0, Inf))

mu <- int + coef * is_female

distribution(envergure_df$Wingspan) <- normal(mu, sd)

m <- model(int, coef, sd, mu)
plot(m)
```

Go!

```{r, eval = FALSE}
draws <- mcmc(m, n_samples = 1000)
```

Et les r√©sultats.

```{r, eval = FALSE}
mcmc_combo(draws, combo = c("dens", "trace"), pars = c("int", "coef", "sd"))
```


```{r, eval = FALSE}
draws_tidy <- draws %>%
  spread_draws(int, coef, sd)
draws_tidy
```

```{r, eval = FALSE}
print("Intercept:")
confidence_interval(x = draws_tidy$int, on = "deviation", distribution = "normal", level = 0.95)

print("Pente:")
confidence_interval(x = draws_tidy$coef, on = "deviation", distribution = "normal", level = 0.95)
```

## Mod√©lisation multiniveau

Vous souvenez-vous en quoi consiste un effet al√©atoire? Pour rappel, il s'agit d'un effet global nul mais variable d'un groupe √† l'autre, alors qu'un effet fixe ne subit pas la contrainte d'effet nul. En mod√©lisation lin√©aire, l'effet al√©atoire peut se trouver sur l'intercept ou sur une pente (ou plusieurs pentes). Ce concept peut √™tre port√© naturellement en mod√©lisation bay√©sienne en ajoutant √† l'intercept ou √† une pente un effet dont l'*a priori* est une distribution √©tal√©e autour de z√©ro (effet global nul, mais variable).

Reprenons le mod√®le consid√©r√© √† la section \@ref(chapitre-biostats). 

```
mmodlin_1 <- lme(fixed = yield ~ lat + long + nitro + topo + bv,
                 random = ~ 1|year/rep,
                 data = lasrosas.corn)
```

```{r}
data(lasrosas.corn, package = "agridat")
lasrosas.corn$year_rep <- paste0(lasrosas.corn$year, "_", lasrosas.corn$rep)

lasrosas.corn_sc <- lasrosas.corn %>%
  select(lat, long, nitro, bv) %>% 
  mutate_all(scale) %>% 
  bind_cols(lasrosas.corn %>% select(-lat, -long, -nitro, -bv)) %>% 
  mutate(year = as.factor(year))

corn_modmat <- model.matrix(~lat + long + nitro + topo + bv + year + year_rep, data = lasrosas.corn_sc)
#head(corn_modmat)
```


D√©finissons d'abord nos *a priori* sur les pentes




## Pour aller plus loin

Le module greta est con√ßu et maintenu par [Nick Golding](https://github.com/goldingn), du Quantitative & Applied Ecology Group de l'University of Melbourne, Australie. La [documentation de greta](https://greta-stats.org/) offre des [recettes](https://greta-stats.org/articles/example_models.html) pour toutes sortes d'analyses en √©cologie.

Les livres de Mark K√©ry, bien que r√©dig√©s pour les calculs en langage R et WinBUGS, offre une approche bien structur√©e et traduisible en greta, qui est plus moderne que WinBUGS.

- [Introduction to WinBUGS for Ecologists (2010)](https://www.amazon.com/Introduction-WinBUGS-Ecologists-Bayesian-regression/dp/0123786053)
- [Bayesian Population Analysis using WinBUGS: A Hierarchical Perspective (2011)](https://www.amazon.com/Bayesian-Population-Analysis-using-WinBUGS/dp/0123870208)
- [Applied Hierarchical Modeling in Ecology: Analysis of distribution, abundance and species richness in R and BUGS (2015)](https://www.amazon.com/Applied-Hierarchical-Modeling-Ecology-distribution/dp/0128013788)

<!--chapter:end:06-1_biostats_bayes.Rmd-->

# Explorer R {#chapitre-explorer}

L'apprentissage de R peut √™tre √©tourdissant. Cette section est une petite pause fourre-tout qui vous introduira aux nombreuses possibilit√©s de R.

***
Ô∏è\ **Objectifs sp√©cifiques**:

√Ä la fin de ce chapitre, vous

- serez en mesure d'identifier les sources d'information principales sur le d√©veloppement de R et de ses modules
- comprendrez l'importance du pr√©traitement des donn√©es, en particulier dans le cadre de l'analyse de donn√©es compositionnelles, et saurez effectuer un pr√©traitement ad√©quat
- saurez comment acqu√©rir des donn√©es m√©t√©o d'Environnement Canada avec le module [weathercan](http://ropensci.github.io/weathercan/)
- saurez identifier les modules d'analyse de sols ([soiltexture](https://github.com/julienmoeys/soiltexture) et [aqp](http://ncss-tech.github.io/aqp/docs/))
- saurez comment d√©buter un projet de m√©ta-analyse et de d√©ploiement d'un logiciel sur R

***

Pour certains, le langage R est un labyrinthe. Pour d'autres, c'est une myriade de portes ouvertes. Si vous lisez ce manuel, vous vous √™tes peut-√™tre engag√© dans un labyrinthe dans l'objectif d'y trouver la cl√© qui d√©v√©rouillera une porte bien pr√©cise qui m√®ne √† un tr√©sor, un objet magique... ou un dipl√¥me. Peut-√™tre aussi prendrez-vous le go√ªt d'errer dans ce labyrinthe, explorant ses d√©bouch√©s, pour y d√©nicher au hasard des petits outils et des d√©bouch√©s.

![](images/06_zelda.gif)

S√©quence du jeu vid√©o *The legend of Zelda*.

Cette section est un amalgame de plusieurs outils de R pertinents en analyse √©cologique.

## R sur le web

Dans un environnement de travail en √©volution rapide et constante, il est difficile de consid√©rer que ses comp√©tences sont abouties. Rester inform√© sur le d√©veloppement de R vous permettra de d√©nicher de r√©soudre des probl√®mes persistants de mani√®re plus efficace ou par de nouvelles avenues, et vous offrira m√™me l'occasion de d√©nicher des probl√®mes dont vous ne soup√ßonniez pas l'existance.  Plusieurs sources d'information vous permettront de vous tenir √† jour sur le d√©veloppement de R, de ses environnement de travail (RStudio, Jupyter, Atom, etc.) et des nouveaux modules qui s'y greffent. Plus largement, vous gagnerez √† vous informer sur les derni√®res tendances en calcul scientifique sur d'autres plate-forme que R (Python, Javascript, Julia, etc.). √âvidemment, nos t√¢ches quotidiennes ne nous permettent pas de tout suivre. M√™me si vous pouviez n'attrapper qu'1% du d√©filement, ce sera d√©j√† 1% de plus que rien du tout.

> √âvidemment, rester au courant aide parce que vous en apprenez davantage sur les outils et leurs applications. Mais √ßa aide aussi parce que √ßa vous permet de conna√Ætre des gens et des organisations! Il est tr√®s utile de savoir qui travaille sur quoi et o√π se d√©roulent les d√©veloppements sur un sujet donn√©, car si vous cherchez consciemment quelque chose plus tard, √ßa vous aidera √† trouver votre chemin plus facilement. - Ma√´lle Salmon, [Keeping up to date with R news](https://masalmon.eu/2019/01/25/uptodate/) (ma traduction)

Je vous propose une liste de ressources. Ne vous y tenez surtout pas: discartez ce qui ne vous convient pas, et partez √† l'aventure!

![](images/06_hobbit.gif)

The Hobbit: An Unexpected Journey, Peter Jackson (2012)

### GitHub

Nous verrons au chapitre \@ref(chapitre-git) l'importance d'utilser des outils d'archivage et de suivi de version, comme *git*, dans le d√©ploiement de la *science ouverte*. Pour l'instant, retenons que [GitHub](https://www.github.com) est une plate-forme *git* en ligne acquise par Microsoft qui est devenue un r√©seau social de d√©veloppement informatique. [De nombreux modules de R y sont d√©velopp√©s](https://github.com/topics/r). Au chapitre \@ref(chapitre-git), vous serez invit√©s √† y ouvrir un compte et √† y archiver du contenu. Vous pourrez alors **suivre** (dans le m√™me sens que sur Facebook ou Twitter) le d√©veloppement de projets et suivre les travaux des personnes qui vous semblent d'int√©r√™t.

### Twitter
Le *hashtag* `#rstats` rassemble sur [Twitter](https://twitter.com/hashtag/rstats?src=hash) ce qui se tweete sur le sujet. On y retrouve les comptes de [R-bloggers](https://twitter.com/Rbloggers), [RStudio](https://twitter.com/rstudio) et [rOpenSci](https://twitter.com/rOpenSci). Certaines communaut√© y sont aussi actives, comme [R4DS online learning community](https://twitter.com/R4DScommunity), qui partage des nouvelles sur R, et [R-Ladies Global](https://twitter.com/RLadiesGlobal), qui vise √† amener davantage de diversit√© √† la communaut√© de R. Des comptes th√©matiques comme [Daily R Cheatsheets](https://twitter.com/daily_r_sheets) et [One R Package a Day](https://twitter.com/RLangPackage) permettent de d√©couvrir quotidiennement de nouvelles possibilit√©s. Enfin, plusieurs personnes contribuent positivement √† la communaut√© R. [Hadley Wickham](https://twitter.com/hadleywickham) brille parmi les √©toiles de R. Les comptes de [Mara Averick](https://twitter.com/dataandme), [Claus Wilke](https://twitter.com/ClausWilke) et [David Robinson](https://twitter.com/drob) sont aussi int√©ressants.

### Nouvelles
Le site d'aggr√©gation [R-bloggers](https://www.r-bloggers.com/), mis √† jour quotidiennement, republie des articles en anglais tir√©s d'un peu partout sur la toile. On y trouve principalement des tutoriels et des annonces de nouveaux d√©veloppement. Deux fois par mois, l'organisation [rOpenSci](https://news.ropensci.org/) offre un portrait de l'univ-R (üí©), ce que [R Weekely](https://rweekly.org/) offre de mani√®re hebdomadaire (l'information sera probablement redondante). Le tidyverse a quant √† lui son propre [blogue](https://www.tidyverse.org/articles/).

### Des questions?

Bien que davantage vou√©s √† la r√©solution de probl√®me qu' √† l'exploration de nouvelles opportunit√©s, [Stackoverflow](https://stackoverflow.com/questions/tagged/r) et [Cross Validated](https://stats.stackexchange.com/questions/tagged/r) sont des plate-forme pris√©es. De plus, la liste de courriels [r-sig-ecology](https://www.mail-archive.com/r-sig-ecology@r-project.org/info.html) permet des √©changes entre professionnels et novices en analyse de donn√©es √©cologiques avec R.

### Participer

R est un logiciel bas√© sur une communaut√© de d√©veloppement, d'utilisation et de vulgarisation. Des personnes offrent g√©n√©reusement du temps de support. Si vous vous sentez √† l'aise, offrez aussi le v√¥tre!

### Mise en garde

Les modules de R sont d√©velopp√©s par quiconque le veut bien: leur qualit√© n'est pas n√©cessairement audit√©e. Souvent, ils ne sont v√©rifi√©s que par une vigilance communautaire: dans ce cas, vous √™tes les cobailles. Ce qui n'est pas n√©cessairement une mauvaise chose, mais cela n√©cessite de prendre ses pr√©cautions. Dans sa conf√©rence [How to be a resilient R user](https://maelle.github.io/fluctuat_nec_mergitur), [Ma√´lle Salmon](https://twitter.com/ma_salmon) propose quelques guides pour juger de la qualit√© d'un module.

**1. Le module est-il activement d√©velopp√©?**

Bien!

![](images/06_2019-01-14-facebook-prophet.png)

Attention!

![](images/06_2019-01-14_mlammens_meteR.png)

**2. Le module est-il bien test√©?**

V√©rifiez si le module a fait l'objet d'une publication scientifique, s'il a √©t√© utilis√© avec succ√®s dans la lit√©rature ou dans des documents cr√©dibles.

**3. Le module est-il bien document√©?**

Un site internet d√©di√© est-il utilis√© pour documenter l'utilisation du module? Les fichiers d'aide sont-ils complets, et sont-ils de bonne qualit√©?

**4. Le module est-il largement utilis√©?**

Un module peu populaire n'est pas n√©cessessairement de mauvaise qualit√©: peut-√™tre est-il seulement destin√© √† des applications de niche. S'il n'est pas un indicateur √† lui seul de la solidit√© ou la validit√© d'un module, une masse critique indique que le module a pass√© sous la surveillance de plusieurs utilisateurs. Dans GitHub, ceci peut √™tre √©valu√© par le nombre d'√©toiles attribu√© au module (√©quivalent √† un J'aime).

![](images/06_peu-etoiles.png) ![](images/06_bcp-etoiles.png)

**5. Le module est-il d√©velopp√© par une personne ou une organisation cr√©dible?**

On peut affirmer sans trop se compromettre que l'√©quipe de RStudio d√©veloppe des modules de confiance. Tout comme il faudrait se m√©fier d'un module d√©velopp√© par une personne anonyme.

Le module [packagemetrics](https://github.com/ropenscilabs/packagemetrics) permet d'√©valuer ces crit√®res.

```{r expl-package_list_metrics, eval = FALSE}
# devtools::install_github("ropenscilabs/packagemetrics")
library("packagemetrics")
pm <- package_list_metrics(c("dplyr", "ggplot2", "vegan", "greta"))
metrics_table(pm)
```

### Prendre tout √ßa en note

Un logiciel de prise de notes (comme [Evernote](http://evernote.com/), [OneNote](http://onenote.com/), [Notion](http://notion.so), [Simplenote](https://simplenote.com), [Turtl](https://turtlapp.com/), etc.) pourrait vous √™tre utile pour retrouver l'information soutir√©e de vos flux d'information. Mais certaines personnes consignent simplement leurs informations dans un carnet ou un document de traitement de texte.

## R en chaire et en os

L'Universit√© Laval (institution aupr√®s de laquelle ce manuel est d√©velopp√©) sera haute en mai 2019 de la conf√©rence R √† Qu√©bec 2019. Des ateliers seront offerts pour les utilisateurs novices et avanc√©s.

[![R √† Qu√©bec](http://raquebec.ulaval.ca/2019/wp-content/uploads/2018/09/logo_r_a_quebec_2019_presente_par_ia_blanc.png)](http://raquebec.ulaval.ca/2019/)

## Quelques outils en √©cologie math√©matique avec R

### Pr√©traitement des donn√©es

Il arrive souvent ques les donn√©es brutes ne soient pas exprim√©es de mani√®re appropri√©e ou optimale pour l'analyse statistique ou la mod√©lisation. Vous devrez alors effectuer un pr√©traitement sur ces donn√©es. Lors du chapitre \@ref(chapitre-biostats), nous avons abord√© la mise √† l'√©chelle, o√π des variables num√©riques √©taient transform√©es pour avoir une moyenne de z√©ro et un √©cart-type de 1. Cette op√©ration permettait d'appr√©cier les coefficients et leur incertitude sur une m√™me √©chelle. L'encodage cat√©gorielle a quant √† lui permi d'utiliser des m√©thodes quantitatives sur des donn√©es qualitatives. Dans les deux cas, nous n'avons pas utilis√© le terme, mais il s'agissait d'un **pr√©traitement**, c'est-√†-dire une transformation des donn√©es pr√©alable √† l'analyse ou la mod√©lisation.

Un pr√©traitement peut consister simplement en une transformation logarithmique ou exponentielle. En particulier, si vos donn√©es forment une partie d'un tout (exprim√©es en pourcentages ou fractions), vous devriez probablement utiliser un pr√©traitement gr√¢ce aux outils de l'**analyse compositionnelle**. Avant de les aborder, nous allons traiter des transformations de base.

#### Standardisation

La standardisation consiste √† centrer vos donn√©es √† une moyenne de 0 et √† les √©chelonner √† une variance de 1, c'est-√†-dire

$$x_{standard} = \frac{x - \bar{x}}{\sigma}$$

o√π $\bar{x}$ est la moyenne du vecteur $x$ et o√π $\sigma$ est son √©cart-type.

Ce pr√©traitement des donn√©es peut s'av√©r√©r utile lorsque la mod√©lisation tient compte de l'√©chelle de vos mesures (par exemple, les param√®tres de r√©gression vus au chapitre \@ref(chapitre-biostats) ou les distances que nous verrons au chapitre \@ref(chapitre-ordination)). En effet, les pentes d'une r√©gression lin√©aire multiple ne pourront √™tre compar√©es entre elles que si elles sont une m√™me √©chelle. Par exemple, on veut mod√©liser la consommation en miles au gallon (`mpg`) de voitures en fonction de leur puissance (`hp`), le temps en secondes pour parcourir un quart de mile (`qsec`) et le nombre de cylindre.

```{r expl-scale1}
data("mtcars")
modl <- lm(mpg ~ hp + qsec + cyl, mtcars)
summary(modl)
```

Les pentes signifient que la distance parcourue par gallon d'essence diminue de 0.03552 miles au gallon pour chaque HP, de 0.89242 par seconde au quart de mile et de 2.2696 par cyclindre additionnel. L'interpr√©tation est conviviale √† cette √©chelle. Mais lequel de ces effets est le plus important? L `t value` indique que ce seraient les cylindres. Mais pour juger l'importance en terme de pente, il vaudrait mieux standardiser.

```{r expl-scale2}
library("tidyverse")
standardise <- function(x) (x-mean(x))/sd(x)
mtcars_sc <- mtcars %>%
  mutate_if(is.numeric, standardise) # ou bien scale(mtcars, center = TRUE, scale = TRUE)
modl_sc <- lm(mpg ~ hp + qsec + cyl, mtcars_sc)
summary(modl_sc)
```

Les valeurs des pentes ne peuvent plus √™tre interpr√©t√©es directement, mais peuvent maintenant √™tre compar√©es entre elles. Dans ce cas, le nombre de cilyndres a en effet une importance plus grande que la puissance et le temps pour parcourir un 1/4 de mile.

Les algorithmes bas√©s sur des distances auront, de m√™me, avantage √† √™tre standardis√©s.

#### √Ä l'√©chelle de la plage

Si vous d√©sirez pr√©server le z√©ro dans le cas de donn√©es positives ou plus g√©n√©ralement vous voulez que vos donn√©es pr√©trait√©es soient positives, vous pouvez les transformer √† l'√©chelle de la plage, c'est-√†-dire les forcer √† s'√©taler de 0 √† 1:

$$ x_{range01} = \frac{x - x_{min}}{x_{max} - x_{min}}  $$

Cette transformation est sensible aux valeurs aberrantes, et une fois le vecteur transform√© les valeurs aberrantes seront toutefois plus difficiles √† d√©tecter.

```{r expl-scale-range}
range_01 <- function(x) (x-min(x))/(max(x) - min(x))
mtcars %>%
  mutate_if(is.numeric, range_01) %>% # en fait, toutes les colonnes sont num√©riques, alors mutate_all aurait pu √™tre utilis√© au lieu de mutate_if
  sample_n(4)
```

#### Normaliser

Le terme *normaliser* est associer √† des op√©rations diff√©rentes dans la litt√©rature. Nous prendrons la nomenclature de [scikit-learn](https://scikit-learn.org/stable/modules/preprocessing.html#normalization), pour qui la normalisation consiste √† faire en sorte que la longueur du vecteur (sa norme, d'o√π *normaliser*) soit unitaire. Cette op√©ration est le plus souvent utilis√©e par observation (ligne), non pas par variable (colonne). Il existe plusieurs mani√®res de mesures la distance d'un vecteur, mais la plus commune est la distance euclidienne. La seule fois que j'ai eu √† utiliser ce pr√©traitement √©tait en analyse spectrale ([Chemometrics with R, Ron Wehrens, 2011, chapitre 3.5](https://www.springer.com/us/book/9783642178405#otherversion=9783642178412)). En R,

```{r expl-scale-norm}
library("pls")
data("gasoline")
spectro <- gasoline$NIR %>% unclass() %>% as_tibble()

normalise <- function(x) x/sqrt(sum(x^2))
spectro_norm <- spectro %>% 
  rowwise() %>% # diff√©rentes approches possibles pour les op√©rations sur les lignes
  normalise()
spectro_norm[1:4, 1:4]
```

#### Analyse compositionnelle en R

En 1898, le statisticien Karl Pearson nota que des corr√©lations √©taient induites lorsque l'on effectuait des ratios par rapport √† une variable commune.

![Karl Pearson, 1897](images/06_pearson1897.png)
Source [Karl Pearson, 1897. Mathematical contributions to the theory of evolution.‚Äîon a form of spurious correlation which may arise when indices are used in the measurement of organs. Proceedings of the royal society of London](https://royalsocietypublishing.org/doi/pdf/10.1098/rspl.1896.0076)

Faisons l'exercice! Nous g√©n√©rons au hasard 1000 donn√©es (comme le proposait Pearson) pour trois dimensions: le f√©mur, le tibia et l'hum√©rus. Ces dimensions ne sont pas g√©n√©r√©es par des distributions corr√©l√©es.

```{r expl-coda-bones}
set.seed(3570536)
n <- 1000
bones <- tibble(femur = rnorm(n, 10, 3),
                tibia = rnorm(n, 8, 2),
                humerus = rnorm(n, 6, 2))
plot(bones)
cor(bones)
```

Pourtant, si j'utilise des ratios allom√©triques avec l'hum√©rus comme base,

```{r expl-bones-allo}
bones_r <- bones %>% 
  transmute(fh = femur/humerus,
            th = tibia/humerus)
plot(bones_r)
text(30, 20, paste("corr√©lation =", round(cor(bones_r$fh, bones_r$th), 2)), col = "blue")
```

Nous avons induit ce que Pearson appelait une fausse corr√©lation (*spurious correlation*). En 1960, [Chayes](https://doi.org/10.1029/JZ065i012p04185) proposa que de telles fausses corr√©lations sont induites non seulement sur des ratios de valeurs absolues, mais aussi sur des ratios d'une somme totale. Par exemple, dans une composition simple de deux types d'utilisation du territoire, si une proportion augmente, l'autre doit n√©cessairement diminuer.


```{r expl-bones-corr}
n <- 100
tibble(A = runif(n, 0, 1)) %>% 
  mutate(B = 1 - A) %>% 
  ggplot(aes(x=A, y=B)) +
  geom_point()
```

Les variables exprim√©es relativement √† une somme totale sont dites *compositionnelles*. Elles poss√®dent les caract√©ristiques suivantes.

1. **Redondance d'information**. Un syst√®me de deux proportions ne contient qu'une seule variable du fait que l'on puisse d√©duire l'une en soutrayant l'autre de la somme totale. Un vecteur compositionnel contient de l'information redondante. Pourtant, effectuer des statistiques sur l'une plut√¥t que sur l'autre donnera des r√©sultats diff√©rents.
2. **D√©pendance d'√©chelle**. Les statistiques devraient √™tre ind√©pendantes de la somme totale utilis√©e. Pourtant, elles diff√©reront sur l'on utilise par exemple, une proportion des m√¢les d'une part et des femelles d'autre part, ou la proportion de la somme des deux, de m√™me que les r√©sultats d'un test sanguin diff√©rera si l'on utilise une base s√®che ou une base humide.
3. **Distribution th√©orique des donn√©es**. √âtant donn√©e que les proportions sont confin√©es entre 0 et 1 (ou 100%, ou une somme totale quelconque), la distribution normale (qui s'√©tend de -‚àû √† +‚àû) n'est souvent pas appropri√©e. On pourra utiliser la distribution de Dirichlet ou la distribution logitique-normale, mais d'autres approches sont souvent plus pratiques.

Pour illustrer l'effet de la distribution, voyons un diagramme ternaire incluant le sable, le limon et l'argile. En utilisant des √©cart-types univari√©s, nous obtenons l'ellipse en rouge, qui non seulement repr√©sente peu l'√©talement des donn√©es, mais elle d√©passe les bornes du triangle, admettant ainsi des proportions n√©gatives. En bleu, la distribution logistique normale (issue des m√©thodes pr√©sent√©es plus loin dans cette section) convient davantage.

![](images/06_ternaire-sd.png)

Les cons√©quences d'effectuer des statistiques lin√©aires sur des donn√©es compositionnelles brutes peuvent √™tre majeures. En outre, [Pawlowksy-Glahn et Egozcue (2006)](http://dx.doi.org/10.1144/GSL.SP.2006.264.01.01), s'appuyant en outre sur Rock (1988), note les probl√®mes suivants (exprim√©s en mes mots).

1. les r√©gressions, les regroupements et les analyses en composantes principales peuvent avoir peu ou pas de signification
2. les propri√©t√©s des distributions peuvent √™tre g√©n√©r√©es par l'op√©ration de fermeture de la composition (s'assurer que le total des proportions donne 100%)
3. les r√©sultats d'analyses discriminantes lin√©aires sont propices √† √™tre illusoires
4. tous les coefficients de corr√©lation seront affect√©s √† des degr√©s inconnus
5. les r√©sultats des tests d'hypoth√®ses seront intrins√®quement fauss√©s

Pour contourner ces probl√®mes, il faut d'abord aborder les donn√©es compositionnelles pour ce qu'elles sont: des donn√©es intrins√®quement multivari√©es. Elles sont un nuage de point, et non pas une collection de variables individuelles. Ceci qui n'emp√™che pas d'effectuer des analyses consciencieusement sous des angles particuliers. 

En R, on pourra ais√©ment rapporter une composition en somme unitaire gr√¢ce √† la fonction `apply`. Mais auparavant, chargeons le module `compositions` (n'oubliez pas de l'installer au pr√©alable) pour acc√©der √† des donn√©es fictives de proportions de sable, limon et argile dans des s√©diments.

```{r expl-coda-arcticlake, message=FALSE, warning=FALSE}
library("compositions")
data("ArcticLake")
ArcticLake <- ArcticLake %>% as_tibble()
head(ArcticLake)
```

```{r expl-coda-acomp1}
comp <- ArcticLake %>%
  select(-depth) %>%
  apply(., 1, function(x) x/sum(x)) %>% 
  t()
comp[1:5, ]
```

On pourra aussi utiliser la fonction `acomp` (pour Aitchison-composition) pour fermer la composition √† une somme de 1.

```{r expl-coda-acomp2}
comp <- ArcticLake %>%
  select(-depth) %>%
  acomp(.)
comp[1:5, ]
```

Cette strat√©gie a pour avantage d'attribuer √† la variable `comp` la classe `acomp`, qui automatise les op√©rations dans l'espace compositionnel (que l'on nomme aussi le *simplex*). La repr√©sentation ternaire est souvent utilis√©e pour pr√©senter des compositions. Toutefois, il est difficile d'interpr√©ter les compositions de plus de trois parties. La classe `acomp` automatise aussi la repr√©sentation teranaire.

```{r expl-coda-tern}
plot(comp)
```

Afin de transposer cet espace cl√¥t en un espace ouvert, on pourra diviser chaque proportion par une proportion de r√©f√©rence choisie parmi n'importe quelle proportion. Du coup, on retire une dimension redondante! Dans ce ratio, on choisit d'utiliser la proportion de r√©f√©rence au d√©nominateur, ce qui est arbitraire. En utilisant le log du ratio, l'inverse du ratio ne sera qu'un changement de signe, ce qui est pratique en statistiques lin√©aries. Cette solution, propos√©e par Aitchison (1986), s'applique non seulement sur les compositions √† deux composantes, mais sur toute composition. Il s'agit alors d'utiliser une composition de r√©f√©rence pour effecteur les ratios. Pour une composition de $A$, $B$, $C$, $D$ et $E$:

$$alr_A = log \left( \frac{A}{E} \right), alr_B = log \left( \frac{B}{E} \right), alr_C = log \left( \frac{C}{E} \right), alr_D = log \left( \frac{D}{E} \right)$$

Dans R, la colonne de r√©f√©rence est par d√©faut la **derni√®re colonne de la matrice des compositions**.

```{r expl-alr}
add_lr <- alr(comp)
```

Cette derni√®re strat√©gie se nomme les **log-ratios aditifs** ($alr$ pour *additive log-ratio*). Bien que valide pour effectuer des tests statistiques, cette strat√©gie a le d√©savantage de d√©pendre de la d√©cision arbitraire de la composante √† utiliser au num√©rateur. Deuxi√®me restriction des *alr*: les axes de l'espace des *alr* n'√©tant pas orthogonaux, ils ne peuvent pas √™tre utilis√©s pour effectuer des statistiques bas√©es sur les distances (que nous couvrirons au chapitre \@ref(chapitre-ordination)).

L'autre strat√©gie propos√©e par Aitchison √©tait d'effectuer un log-ratio entre chaque composante et la moyenne g√©om√©trique de toutes les composantes. Cette transformation se nomme le **log-ratio centr√©** ($clr$, pour *centered log-ratio*)

$$clr_i = log \left( \frac{x_i}{g \left( x \right)} \right)$$

En R,

```{r expl-clr}
cen_lr <- clr(comp)
```

Avec des CLRs, les distances sont valides. Mais... nous restons avec le probl√®me de la redondance d'information. En fait, la somme de chacunes des lignes d'une matrice de clr est de 0. Pas tr√®s pratique lorsque l'on effectue des statistiques incluant une inversion de la matrice de covariance (distance de Mahalanobis, g√©ostatistiques, etc.)

```
cen_lr %>% 
  cov() %>% 
  solve()
 Error in solve.default(.) : le syst√®me est num√©riquement singulier : conditionnement de la r√©ciproque = 4.44407e-17
```

Enfin, une autre m√©thode de transformation d√©velopp√©e par Egoscue et al. (2003), les log-ratios isom√©triques (ou *isometric log-ratios, ilr*) projette les compositions comprenant D composantes dans un espace restreint de D-1 dimensions orthonorm√©es. Ces dimensions doivent doivent √™tre pr√©alablement √©tablie dans un dendrogramme de bifurcation, o√π chaque composante ou groupe de composante est successivement divis√© en deux embranchement. La mani√®re d'arranger ces balances importe peu, mais on aura avantage √† cr√©er des balances interpr√©tables.

Le diagramme de balances peut √™tre encod√© dans une partition binaire s√©quentielle (ou *sequential bianry partition, sbp*). Une *sbp* est une matrice de contraste ou chaque ligne repr√©sente une partition entre deux variables ou groupes de variables. Une composante √©tiquett√©e `+1` correspondra au groupe du num√©rateur, une composante √©tiquett√©e `-1` au d√©nominateur et une composante √©tiquett√©e `0` sera exclue de la partition ([Parent et al., 2013](http://dx.doi.org/10.3389/fpls.2013.00039)). J'ai reformul√© la fonction CoDaDendrogram pour que l'on puisse ajouter des informations int√©ressantes sur les balants horizontaux. Cette fonction est disponible sur github.

```{r expl-sbp}
source("https://raw.githubusercontent.com/essicolo/AgFun/master/codadend2.R")

sbp <- matrix(c(1, 1,-1,
                1,-1, 0),
              byrow = TRUE,
              ncol = 3)

CoDaDendrogram2(comp, V = gsi.buildilrBase(t(sbp)), ylim = c(0, 1),
                equal.height = TRUE)

```

Si la SBP est plus imposante, il pourrait √™tre plus ais√© de monter dans un chiffrier, puis de l'importer dans R via un fichier csv.

Le calcul des ILRs est effectu√© comme suit.

$$ilr_j = \sqrt{\frac{n_j^+ n_j^-}{n_j^+ + n_j^-}} log \left( \frac{g \left( c_j^+ \right)}{g \left( c_j^+ \right)} \right)$$

ou, √† la ligne $j$ de la SBP, $n_j^+$ et $n_j^-$ sont respectivement le nombre de composantes au num√©rateur et au d√©nominateur, $g \left( c_j^+ \right)$ est la moyenne g√©om√©trique des composantes au num√©rateur et $g \left( c_j^- \right)$ est la moyenne g√©om√©trique des composantes au d√©nominateur.

Les balances sont conventionnellement not√©es `[A,B | C,D]`, ou les composantes `A` et `B` au d√©nominateur sont balanc√©es avec les composantes `C` and `D` au num√©rateur. Une balance positive signifie que la moyenne g√©om√©trique des concentrations au num√©rateur est sup√©rieur √† celle au d√©nominateur, et inversement, alors qu'une balance nulle signifie que les moyennes g√©om√©triques sont √©gales (√©quilibre). Ainsi, en mod√©lisation lin√©aire, un coefficient positif sur `[A,B | C,D]` signifie que l'augmentation de l'importance de `C` et `D` comparativement √† `A` et `B` est associ√© √† une augmentation de la variable r√©ponse du mod√®le.

En R,

```{r expl-ilr}
iso_lr <- ilr(comp, V = gsi.buildilrBase(t(sbp)))
```

Notez la forme `gsi.buildilrBase(t(sbp))` est une op√©ration pour obtenir la matrice d'orthonormalit√© √† partir de la SBP.

Les ILRs sont des balances multivari√©es sur lesquelles on pourra effectuer des statistiques lin√©aries. Bien que l'interpr√©tation des r√©sultats comme collection d'interpr√©tations sur des balances univari√©es pourra √™tre affect√©e par la structure de la SBP, ni les statistiques lin√©aires multivari√©es, ni la distance entre les points ne seront affect√©s. En effet, chaque variante de la SBP est une rotation (d'un facteur de 60¬∞) par rapport √† l'origine:

```{r expl-rotation}
source("lib/ilr-rotation-sbp.R")
```

![](images/06_ilr-rotation.png)

Pour les transformations inverses, vous pourrez utiliser les fonctions `alrInv`, `clrInv` et `ilrInv`. Dans tous les cas, si vous tenez √† garder la trace de vos donn√©es dans leur format original, vous aurez avantage √† ajouter √† votre vecteur compositionnel la valeur de remplissage, constitu√© d'un amalgame des composantes non mesur√©es. Par exemple,

```{r expl-npk}
pourc <- c(N = 0.03, P = 0.001, K = 0.01)
acomp(pourc) # vous perdez la trace des proportions originales
```

```{r expl-npk-acomp}
pourc <- c(N = 0.03, P = 0.001, K = 0.01)
Fv <- 1 - sum(pourc)
comp <- acomp(c(pourc, Fv = Fv))
comp
```

```{r expl-npk-ilr}
iso_lr <- ilr(comp) # avec une sbp par d√©faut
ilrInv(iso_lr)
```

Si vos donn√©es font partie d'un tout, je vous recommande chaudement d'utiliser des m√©thodes compositionnelles autant pour l'analyse que la mod√©lisation. Pour en savoir davantage, le livre *Compositional data analysis with R*, de van den Boogart et Tolosana-Delgado, est disponible en format √©lectronique √† la biblioth√®que de l'Universit√© Laval.

Pour aller plus loin, j'ai √©cri un billet √† ce sujet (auquel √† ce jour il manque toujours un cas d'√©tude): [We should use balances and machine learning to diagnose ionomes](https://www.authorea.com/users/23640/articles/281937-we-should-use-balances-and-machine-learning-to-diagnose-ionomes).

### Acqu√©rir des donn√©es m√©t√©o

Une t√¢che commune en √©cologie est de lier des observations √† la m√©t√©o... qui sont rarement collect√©s lors d'exp√©riences. [Environnement Canada](https://meteo.gc.ca/) poss√®de sont r√©seau de stations. Les donn√©es sont disponibles sur internet en libre acc√®s. Vous pouvez chercher des stations, effectuer des requ√™tes et t√©l√©charger des fichiers csv. Pour un petit tableau, la t√¢che est plut√¥t triviale. Mais √ßa devient rapidement laborieux √† mesure que l'on doit rechercher de nombreuses donn√©es.

Le module [weathercan](http://ropensci.github.io/weathercan/), d√©velopp√© par Steffi LaZerte, permet d'effectuer des requ√™tes rapidement √† partir des coordonn√©es de votre site exp√©rimental. Par exemple, si je cherche une station m√©t√©o sfournissant des donn√©es horaires situ√© √† moins de 20 km du sommet du Mont-Bellevue, √† Sherbrooke, aux coordonn√©es [latitude 45.35, longitude -71.90],

```{r expl-weathercan-stations}
library("weathercan")
station_site <- stations_search(coords = c(45.35, -71.90), dist = 20, interval = "hour")
station_site
```

Je prends en note l'identifiant de la station d√©sir√©e (ou des stations, disons 5397 et 48371), puis je lance une requ√™te pour obtenir la m√©t√©o horaire entre les dates d√©sir√©es.

```{r expl-weathercan-data}
mont_bellevue <- weather_dl(station_ids = c(5397, 48371),
                            start = "2019-02-01",
                            end = "2019-02-07",
                            interval = "hour",
                            verbose = TRUE, tz_disp = "Etc/GMT+5")
mont_bellevue %>% head(5)
```

Et voil√†.

```{r expl-weathercan-plot}
mont_bellevue %>% 
  ggplot(aes(x = time, y = temp)) +
  geom_line(aes(colour = station_name))
```

### P√©dom√©trie avec R

*Cette section a √©t√© √©crite par [Michael Leblanc](https://www.researchgate.net/profile/Michael_Leblanc7).*

Plusieurs fonctionnalit√©s ont √©t√© d√©velopp√©es sur R afin d'aider les *p√©dom√©triciens* √† visualiser, explorer et traiter les donn√©es num√©riques en science des sols. Voici quelques exemples.

#### Texture du sol

La texture du sol est d√©finie par sa composition granulom√©trique, habituellement repr√©sent√©e par trois fractions (sable, limon, argile), laquelle peut √™tre g√©n√©ralis√©e en classe texturale. La d√©finition des classes texturales diff√®re d'un syst√®me ou d'un pays √† l'autre comme en t√©moigne l'article [Perdus dans le triangle des textures (Richer de Forges et al. 2008)](http://www.afes.fr/wp-content/uploads/2017/10/EGS_15_2_richerdeforges.pdf). La d√©finition des fractions granulom√©triques peut √©galement diff√©rer selon le domaine d'√©tude (ing√©nierie, p√©dologie) ou le pays. Par exemple, le diam√®tre du limon est de 0,002 mm √† 0,05 mm dans le syst√®me canadien, am√©ricain et fran√ßais alors qu'il est de 0,002 mm √† 0,02 mm dans le syst√®me australien et de 0,002 mm √† 0,063 mm dans le syst√®me allemand. Il est donc important de v√©rifier la m√©thodologie et le syst√®me de classification utilis√©s pour interpr√©ter les donn√©es de texture du sol. Le module `soilTexture` propose des fonctions permettant d'aborder ces multiples d√©finitions.

```{r expl-sol-soiltexture}
library("soiltexture")
```

##### Les triangles texturaux

Avec la fonction `TT.plot`, vous pouvez pr√©senter vos donn√©es granulom√©triques dans un triangle textural tel que d√©fini par les diff√©rents syst√®mes nationaux. Auparavant, cr√©ons un objet comprenant des textures al√©atoires.

```{r expl-sol-data}
set.seed(848341) # random.org
rand_text <- TT.dataset(n=100, seed.val=29)
head(rand_text)
```

Avec le module soiltexture, les tableaux de texture doivent inclure les intitull√©s exactes CLAY, SILT et SAND (notez les majuscules). Les points des textures g√©n√©r√©es peuvent √™tre port√©s dans des diagrammes ternaires texturaux de diff√©rents syst√®mes de classification, par exemple le syst√®me canadioen et le syst√®me USDA.

```{r expl-sol-tern, fig.width = 16, fig.height = 8}
par(mfrow=c(1, 2))

TT.plot(class.sys = "CA.FR.TT", 
        tri.data = rand_text,
        col = "blue")
TT.plot(class.sys = "USDA.TT", 
        tri.data = rand_text,
        col = "blue")

```

Les param√®tres de la figure (titres, polices, style de la grille, etc.) peuvent √™tre personnalis√©s avec les [arguments TT.plot](https://www.rdocumentation.org/packages/soiltexture/versions/1.5.1/topics/TT.plot).

##### Les classes texturales

La fonction `TT.points.in.classes` est utile pour d√©signer la classe texturale √† partir des donn√©es granulom√©triques, en sp√©cifiant bien le syst√®me de classification d√©sir√©.

```{r expl-sol-classes}
TT.points.in.classes(
  tri.data = rand_text[1:10, ], # 
  class.sys = "CA.FR.TT",
  PiC.type = "t"
)
```

Plusieurs autres fonctions sont propos√©es par `soiltexture` afin de visualiser, classifier et transformer les donn√©es de texture du sol : [Functions in soiltexture](https://www.rdocumentation.org/packages/soiltexture/versions/1.5.1). Julien Moeys (2018) propose √©galement le tutoriel [*The soil texture wizard: a tutorial*](https://cran.r-project.org/web/packages/soiltexture/vignettes/soiltexture_vignette.pdf).

#### Profils de sols

Le profil de sols est une entit√© d√©crite par une s√©quence de couches ou d'horizons avec diff√©rentes caract√©ristiques morphologiques. Le module AQP, pour [*Algorithms for Quantitative Pedology*](http://ncss-tech.github.io/AQP/), propose des fonctions de visualisation, d'agr√©gation et de classification permettant d'aborder la complexit√© inh√©rente aux informations p√©dologiques.

##### La visualisation de profils

Vous devez d'abord structurer vos donn√©es dans un tableau (`data.frame`) incluant minimalement ces trois colonnes :

1. Identifiant unique du profil (groupes d'horizons) (`id`)
2. Limites sup√©rieures de l'horizon (`top`)
3. Limites inf√©rieures de l'horizon (`down`)

Vos donn√©es morphologiques, physico-chimiques, etc., sont incluses dans les autres colonnes. Chargeons un fichier p√©dologique √† titre d'exemple.

```{r expl-sol-pedometrie}
profils <- read_csv("data/06_pedometric-profile.csv")
head(profils)
```

La fonction `munsell2rgb` permet de convertir le code de couleur *Munsell* en format *RGB*.

```{r expl-sol-aqp}
library("aqp")
profils$soil_color <- with(profils, munsell2rgb(hue, value, chroma))
```

Pr√©alablement √† la visualisation, le tableau est transform√© en objet `SoilProfileCollection` par la fonction `depths`. Pour ce faire, le tableau doit √™tre un pur `data.frame`, non pas un `tibble`.

```{r expl-sol-profils}
profils <- profils %>% as.data.frame()
depths(profils) <- id ~ top + bottom
```

La fonction `plot` d√©tectera le type d'objet et appellera la fonction de visualisation en cons√©quence.

```{r expl-sol-profils-plot, fig.width = 12, fig.height = 4}
par(mfrow = c(1, 3))
plot(profils, name="horizon")
title('Couleur des horizons', cex.main=1)
plot(profils, name="horizon", color='C.CNS.pc', col.label='C total (%)')
plot(profils, name="horizon", color='pH.CaCl2', col.label='pH CaCl2')
```

De multiples figures th√©matiques peuvent √™tre g√©n√©r√©es afin de repr√©senter les particuliarit√©s des profils. Pour aller plus loin, consultez les guides [*Introduction to SoilProfileCollection Objects*](http://ncss-tech.github.io/AQP/aqp/aqp-intro.html) et [*Generating Sketches from SPC Objects*](http://ncss-tech.github.io/AQP/aqp/SPC-plotting-ideas.html).

##### Les plans verticaux (depth functions)

Les plans verticaux sont des diagrammes qui permettent d'interpr√©ter les donn√©es en fonction de la profondeur. La fonction `slab` permet le calcul de statistiques descriptives par intervalles de profondeur r√©guliers, lesquelles permettent de visualiser la variabilit√© verticale des propri√©t√©s des sols.

```{r expl-sol-slab}
agg <- slab(profils, fm = ~ C.CNS.pc + pH.CaCl2)
```

La visualisation est g√©n√©r√©e par le module graphique ggplot2

```{r expl-sol-agg-ggplot}
agg %>%
  ggplot(mapping = aes(x = -top, y = p.q50)) +
  facet_grid(. ~ variable, scale = "free") +
  geom_ribbon(aes(ymin =  p.q25, ymax = p.q75), fill = "grey75", alpha = 0.5) +
  geom_path() +
  labs(x = "Profondeur (cm)",
       y = "M√©diane bord√©e des 25e and 75e percentiles") +
  coord_flip()
```

##### Le regroupement de profils

Le calcul des distances de dissimilarit√© entre les profils avec `profile_compare` permet la construction de dendrogramme et le regroupement des profils. Notez que nous survolerons au chapitre \@ref(chapitre-ordination) les concepts de dissimilarit√© et de partitionnement.

```{r expl-sol-cluster}
library("cluster")
library("mvtnorm")
library("sharpshootR") # remotes::install_github("ncss-tech/sharpshootR")
d <- profile_compare(profils, vars=c('C.CNS.pc', 'pH.CaCl2'), k=0, max_d=40)
d_diana <- diana(d)
plotProfileDendrogram(profils, name="horizon", d_diana,
                      scaling.factor = 0.3, y.offset = 5,
                      color='pH.CaCl2',  col.label='pH CaCl2')
```

##### Diagramme de relations entre les horizons

Il est possible de visualiser les transitions d'horizon les plus probables dans un groupe de profils de sols.

```{r expl-sol-relations, fig.width = 10, fig.height = 5}
tp <- hzTransitionProbabilities(profils, name="horizon")
par(mar = c(0, 0, 0, 0), mfcol = c(1, 2))
plot(profils, name="horizon")
plotSoilRelationGraph(tp, graph.mode = "directed", edge.arrow.size = 0.5, edge.scaling.factor = 2, vertex.label.cex = 0.75, 
                      vertex.label.family = "sans")
```

Consultez [AQP project](http://ncss-tech.github.io/AQP/) pour des pr√©sentations, des tutoriels et des exemples de figures qui montrent les nombreuses possibilit√©s du package `AQP`.

### M√©ta-analyses en R

Je conseille les livres [*Introduction to Meta-Analysis*](https://www.wiley.com/en-us/Introduction+to+Meta+Analysis-p-9780470057247), [*Meta-analysis with R*](https://www.springer.com/us/book/9783319214153) et [*Handbook of Meta-analysis in Ecology and Evolution*](https://press.princeton.edu/titles/10045.html) pour les m√©ta-analyses sur des √©cosyst√®mes. Le module metafor est un ioncournable pour effectuer des m√©taanalyses en R. On ne passe pas tout √† fait √† c√¥t√© si l'on utilise le module meta, lui-m√™me bas√© en partie sur metafor. Le module meta a touttefois l'avantage d'√™tre simple d'utilisation. Par exemple, pour une m√©ta-analyse d'une r√©ponse continue,

```{r expl-meta-load}
library("meta")
meta_data <- read_csv("https://portal.uni-freiburg.de/imbi/_SUPPRESS_ACCESSRULE/lehre/lehrbuecher/meta-analysis-with-r/dataset02.csv")
meta_analyse <- metacont(n.e = Ne, mean.e = Me, sd.e = Se, n.c = Nc, mean.c = Mc, sd.c = Sc, data = meta_data, sm = "SMD")
meta_analyse
```

Et pour effectuer un *forest plot*,

```{r expl-meta-plot}
forest(meta_analyse)
```

### Cr√©er des applications avec R

RStudio vous permet de d√©ployer vos r√©sultats sous forme d'applications web gr√¢ce √† son module [shiny](https://www.rstudio.com/products/shiny/). Pour ce faire, le seul pr√©alable est de savoir programmer en R. En agen√ßant une interface avec des *inputs* (listes de s√©lection, des bo√Ætes de dialogue, des s√©lecteurs, des boutons, etc.) avec des mod√®les que vous d√©veloppez, vous pourrez cr√©er des interfaces int√©ractives.

Pour cr√©er une application shiny, vous devez cr√©er une partie pour l'interface (`ui`) et une autre pour le calcul (`server`). Je n'irai pas dans les d√©tails, √©tant donn√©e qu'il s'agit d'un sujet √† part enti√®re. Pour aller plus loin, visitez le site du projet [shiny](https://www.rstudio.com/products/shiny/).

```
library("shiny")

ui <- basicPage(
  sliderInput("A", "Asymptote:", min = 0, max = 100, value = 50),
  sliderInput("E", "Environnement:", min = -10, max = 100, value = 20),
  sliderInput("R", "Taux:", min = 0, max = 0.1, value = 0.035),
  sliderInput("prix_dose", "Prix dose:", min = 0, max = 5, value = 1),
  sliderInput("prix_vente", "Prix vente:", min = 0, max = 200, value = 100),
  sliderInput("dose", "Dose:", min = 0, max = 300, value = c(0, 200)),
  plotOutput("distPlot")
)

server <- function(input, output) {
  mitsch_f <- reactive({
    input$A * (1 - exp(-input$R * (seq(input$dose[1], input$dose[2], length = 100) + input$E)))
  })
  
  mitsch_opt <- reactive({
    (log((input$A * input$R * input$prix_vente) / input$prix_dose - input$E * input$R) / input$R )
  })
  
  
  output$distPlot <- renderPlot({
    plot(seq(input$dose[1], input$dose[2], length = 100), mitsch_f(), type = "l", ylim = c(0, 100))
    abline(v = mitsch_opt() )
    text(mitsch_opt(), 2, paste("Dose optimale:", round(mitsch_opt(), 0)))
  })
}

shinyApp(ui, server)
```

Une fois l'application cr√©√©e, il est possible de la d√©ployer sur le site shninyapps.io. D'abord cr√©er une application shiny dans RStudio: File > New File > Shiny Web App. √âcrivez votre code dans le fichier app.R (dans ce cas, ce peut √™tre un copier-coller), puis cliquez sur Run App en haut √† droite de la fen√™tre d'√©dition du code. Lorsque l'application fonctionne, vous pourrez la publier via RStudio en cliquant sur le bouton Publish dans la fen√™tre Viewer (vous devez au pr√©alable avoir un comte sur shinyapp.io).

Une application sera publique et sera ouverte. https://essicolo.shinyapps.io/Mitscherlich/

Pour d√©ployer en mode priv√©, vous devrez d√©bourser pour un forfait ou installer votre propre serveur.

### Travailler en Python

Le chapitre \@ref(chapitre-biostats-bayes) a pr√©sent√© un module pour les statistiques bay√©siennes n√©cessitant un environnement Python. Il s'agissait de faire fonctionner un module en R qui, √† l'interne, effectue ses calculs en Python. Rien ne vous emp√™che d'effectuer des calculs directement en Python √† m√™me l'interface de RStudio.

Il vous faudra d'abord installer Python et les modules de calcul que vous d√©sirez. Il existe plusieurs distributions de Python. Parmi elles, Anaconda est probablement la plus intuitive √† installer. Choisissez d'abord [Anaconda](https://www.anaconda.com/download) (~500 Mo) ou [Miniconda](https://conda.io/miniconda.html) pour une installation minimale (~60 Mo) - si vous installez Miniconda, vous devrez aussi installer les modules n√©cessaires pour le calcul. Installez aussi le module [**`reticulate`**](https://rstudio.github.io/reticulate/) de R, de sortte que vous puissiez communiquer avec Python. Anaconda fonctionne avec des environnements de calcul. Chaque environnement poss√®de sa propre version de Python et ses propres modules: cela vous permet d'isoler vos environnements et de contr√¥ler la version des modules. Vous pouvez connecter R √† l'environnement de base cr√©√© lors de l'installation d'Anaconda, ou bien en cr√©er un autre. Pour en cr√©er un nouveau, incluant une liste de modules de calcul,

```
library("reticulate")
conda_create(envname = "monprojet", packages = c("python", "numpy", "scipy", "matplotlib", "pandas", "scikit-learn"))
```

Connectez-vous √† votre environnement Python.

```{r expl-connecter-python}
library("reticulate")
use_condaenv("monprojet", required = TRUE)
# use_python("home/essi/anaconda3/bin/") # ou le chemin vers l'ex√©cutable pyhton, attention, non reproductible!
```

Supposons que vous travailliez en R markdown. Pour lancer un bloc de code en Python, indiquez `{python}` au lieu de `{r}` dans l'ent√™te.

```{python expl-py-matplotlib}
# ```{python}
import numpy as np
import matplotlib.pyplot as plt
a = np.linspace(0, 30, 101)
b = np.sin(a)
plt.plot(a, b)
plt.title("Un graphique en Matplotlib dans RStudio")
# ```
```

Pour r√©cup√©rer une variable Python en R, pr√©c√©dez la variable de `py$`.

```{r expl-importer-var-py}
plot(py$a, py$b, type = "l", main = "Un graphique en R avec \n des variables d√©finies en Python")
```

Idem, pour r√©cup√©rer un objet R en Python, `r data("iris")`.

```{python expl-importer-var-r}
r.iris.head(6)
```

Vous aurez ainsi acc√®s aux fonctionnalit√©s de Python et R dans un m√™me flux de travail.

```{r, include=FALSE}
rm(list = ls())
```

<!--chapter:end:07_explorer.Rmd-->

# Association, partitionnement et ordination {#chapitre-ordination}

***
Ô∏è\ **Objectifs sp√©cifiques**:

√Ä la fin de ce chapitre, vous

- serez en mesure d'effectuer des calculs permettant de mesurer des diff√©rence entre des observations, des groupes d'observation ou des variables observ√©es
- serez en mesure d'effection des analyses de partitionnement hi√©rarchiques et non-hi√©rarchiques
- serez en mesure d'effectuer des calculs d'ordination √† l'aide des techniques de r√©duction d'axe communes: analyse en composante principale, l'analyse de correspondance, l'analyse en coordonn√©es principales, analyse discriminante lin√©aire, l'analyse de redondance et l'analyse canonique des correspondances.

***

Les donn√©es √©cologiques incluent g√©n√©ralement plusieurs variables qui doivent √™tre analys√©es conjointement. Les techniques pour l'analyse multivari√©e de donn√©es √©cologiques ont grandi en nombre et en complexit√©, laissant √©merger l'√©cologie num√©rique, un nouveau domaine d'√©tude scientifique initi√© par Pierre Legendre et Louis Legendre dont l'ouvrage *Numerical Ecology*, aujourd'hui √† sa troisi√®me √©dition, reste un incontournable pour qui s'int√©resse aux math√©matiques sous-jacentes au domaine. Pour la r√©daction de ces notes, c'est toutefois le livre *Numerical ecology with R*, √©crit par Borcard et al. (2011) pour offrir un guide √† qui voudrait une approche plus appliqu√©e.

L'√©cologie num√©rique sera effleur√©e dans ce chapitre, qui introduit √† trois concepts.

1. Les **associations** permettent de quantifier la ressemblance ou la diff√©rence entre deux observation (√©chantillons) ou variables (descripteurs). Lorsque l'on a plus de deux variables ou plus de deux site, nous obtenons des matrices d'association.
2. Le **partitionnement** permet de regrouper des observations ou des variables selon des m√©triques d'association.
3. L'**ordination** vise par l'interm√©diaire de techniques de r√©duction d'axe √† mettre de l'ordre dans des donn√©es dont le nombre √©lev√© de variables peut amener √† des difficult√©s d'appr√©ciation et d'interpr√©taion. 

```{r}
library("tidyverse")
```


## Espaces d'analyse

### Abondance et occurence

L'abondance est le d√©compte d'esp√®ces observ√©es, tandis que l'occurence est la pr√©sence ou l'absence d'une esp√®ce. Le tableau suivant contient des donn√©es d'abondance.

```{r}
abundance <- tibble('Bruant familier' = c(1, 0, 0, 3),
                    'Citelle √† poitrine rousse' = c(1, 0, 0, 0),
                    'Colibri √† gorge rubis' = c(0, 1, 0, 0),
                    'Geai bleu' = c(3, 2, 0, 0),
                    'Bruant chanteur' = c(1, 0, 5, 2),
                    'Chardonneret' = c(0, 9, 6, 0),
                    'Bruant √† gorge blanche' = c(1, 0, 0, 0),
                    'M√©sange √† t√™te noire' = c(20, 1, 1, 0),
                    'Jaseur bor√©al' = c(66, 0, 0, 0))
```

Ce tableau peut √™tre rapidement transform√© en donn√©es d'occurence, qui ne comprennent que l'information bool√©enne de pr√©sence (not√© 1) et d'absence (not√© 0).

```{r}
occurence <- abundance %>%
  transmute_all(~if_else(. > 0, 1, 0))
```

L'**espace des esp√®ces** (ou des variables ou descripteurs) est celui o√π les esp√®ces forment les axes et o√π les sites sont positionn√©s dans cet espace. Il s'agit d'une perspective en *mode R*, qui permet principalement d'identifier quels esp√®ces se retrouvent plus courrament ensemble.


```{r}
abundance %>% 
  select("Bruant chanteur", "Chardonneret", "M√©sange √† t√™te noire")
```

Dans l'**espace des sites** (ou les √©chantillons ou objets), on transpose la matrice d'abondance. On passe ici en *mode Q*, o√π chaque point est une esp√®ce, et o√π l'on peut observer quels √©chantillons sont similaires.

```{r}
abundance %>% t()
```

### Environnement

L'**espace de l'environnement** comprend souvent un autre tableau contenant l'information sur l'environnement o√π se trouve les esp√®ces: les coordonn√©es et l'√©l√©vation, la pente, le pH du sol, la pluviom√©trie, etc.

## Analyse d'association

Nous utiliserons le terme *association* comme une **mesure pour quantifier la ressemblance ou la diff√©rence entre deux objets (√©chantillons) ou variables (descripteurs)**.

Alors que la corr√©lation et la covariance sont des mesures d'association entre des variables (analyse en *mode R*), la **similarit√©** et la **distance** sont deux types de une mesure d'association entre des objets (analyse en *mode Q*). Une distance de 0 est mesur√©e chez deux objets identiques. La distance augmente au fur et √† mesure que les objets sont dissoci√©s. Une similarit√© ayant une valeur de 0 indique aucune association, tandis qu'une valeur de 1 indique une association parfaite. √Ä l'oppos√©, la dissimilarit√© est √©gale √† 1-similarit√©.

La distance peut √™tre li√©e √† la similarit√© par la relation:

$$distance=\sqrt{1-similarit√©}$$

ou

$$distance=\sqrt{dissimilarit√©}$$

La racine carr√©e permet, pour certains indices de similarit√©, d'obtenir des propri√©t√©s eucl√©diennes. Pour plus de d√©tails, voyez le tableau 7.2 de [Legendre et Legendre (2012)](https://www.elsevier.com/books/numerical-ecology/legendre/978-0-444-53868-0).

Les matrices d'association sont g√©n√©ralement pr√©sent√©es comme des matrices carr√©es, dont les dimensions sont √©gales au nombre d'objets (*mode Q*) ou de vrariables (*mode R*) dans le tableau. Chaque √©l√©ment ("cellule") de la matrice est un indice d'association entre un objet (ou une variable) et un autre. Ainsi, la diagonale de la matrice est un vecteur nul (distance ou dissimilarit√©) ou unitaire (similarit√©), car elle correspond √† l'association entre un objet et lui-m√™me. 

Puisque l'association entre A et B est la m√™me qu'entre B et A, et puisque la diagonale retourne une valeur convenue, il est possible d'exprimer une matrice d'association en mode "compact", sous forme de vecteur. Le vecteur d'association entre des objets A, B et C contiendra toute l'information n√©cessaire en un vecteur de trois chiffres, `[AB, AC, BC]`, plut√¥t qu'une matrice de dimension $3 \times 3$. L'impact sur la m√©moire vive peut √™tre consid√©rable pour les calculs comprenant de nombreuses dimensions.

En R, les calculs de similarit√© et de distances peuvent √™tre effectu√©s avec le module vegan. La fonction `vegdist` permet de calculer les indices d'association en forme carr√©e.

Nous verons plus tard les m√©thodes de mesure de similarit√© et de distance plus loin. Pour l'instant, utilisons la m√©thode de *Jaccard* pour une d√©monstration sur des donn√©es d'occurence.

```{r}
library("vegan")
vegdist(occurence, method = "jaccard",
        diag = TRUE, upper = TRUE)
```

Remarquez que `vegdist` retourne une matrice dont la diagonale est de 0 (on l'affiche en sp√©cifiant `diag = TRUE`). La diagonale est l'association d'un objet avec lui-m√™me. Or la similarit√© d'un objet avec lui-m√™me devrait √™tre de 1! En fait, par convention `vegdist` retourne des dissimilarit√©s, non pas des similarit√©s. La matrice de distance serait donc calcul√©e en extrayant la racine carr√©e des √©l√©ments de la matrice de dissimilarit√©:

```{r}
dissimilarity <- vegdist(occurence, method = "jaccard",
                         diag = TRUE, upper = TRUE)
distance <- sqrt(dissimilarity)
distance
```

Dans le chapitre sur l'analyse compositionnelle, nous avons abord√© les significations diff√©rentes que peuvent prendre le z√©ro. L'information fournie par un z√©ro peut √™tre diff√©rente selon les circonstances. Dans le cas d'une variable continue, un z√©ro signifie g√©n√©ralement une mesure sous le seuil de d√©tection. Deux tissus dont la concentration en cuivre est nulle ont une afinit√© sous la perspective de la concentration en cuivre. Dans le cas de mesures d'abondance (d√©compte) ou d'occurence (pr√©sence-absence), on pourra d√©crire comme similaires deux niches √©cologiques o√π l'on retrouve une esp√®ce en particulier. Mais deux sites o√π l'on de retouve pas d'ours polaires ne correspondent pas n√©cessairement √† des niches similaires! En effet, il peut exister de nombreuses raisons √©cologiques et m√©thodologiques pour lesquelles l'esp√®ces ou les esp√®ces n'ont pas √©t√© observ√©es. C'est le probl√®me des **double-z√©ros** (esp√®ces non observ√©es √† deux sites), probl√®me qui est amplifi√© avec les grilles comprenant des esp√®ces rares.

La ressemblance entre des objets comprenant des donn√©es continues devrait √™tre calcul√©e gr√¢ce √† des indicateurs *sym√©triques*. Inversement, les affinit√©s entre les objets d√©crits par des donn√©es d'abondance ou d'occurence susceptibles de g√©n√©rer des probl√®mes de double-z√©ros devraient √™tre √©valu√©es gr√¢ce √† des indicateurs *asym√©triques*. Un d√©fi suppl√©mentaire arrive lorsque les donn√©es sont de type mixte.

Nous utiliserons la convention de `vegan` et nous calculerons la dissimilarit√©, non pas la similarit√©. Les mesures de dissimilarit√© sont calcul√©es sur des donn√©es d'abondance ou des donn√©es d'occurence. Notons qu'il existe beaucoup de confusion dans la litt√©rature sur la mani√®re de nommer les dissimilarit√©s (ce qui n'est pas le cas des distances, dont les noms sont reconnus). Dans les sections suivantes, nous noterons la dissimilarit√© avec un $d$ minuscule et la distance avec un $D$ majuscule.

### Association entre objets (mode Q)

#### Objets: Abondance

La **dissimilarit√© de Bray-Curtis** est asym√©trique. Elle est aussi appel√©e l'indice de Steinhaus, de Czekanowski ou de S√∏rensen. Il est important de s'assurer de bien s'entendre la m√©thode √† laquelle on fait r√©f√©rence. L'√©quation enl√®ve toute ambiguit√©. La dissimilarit√© de Bray-Curtis entre les points A et B est calcul√©e comme suit.

$$d_{AB} =  \frac {\sum \left| A_{i} - B_{i} \right| }{\sum \left(A_{i}+B_{i}\right)}$$

Utilisons `vegdist` pour g√©n√©rer les matrices d'association. Le format "liste" de R est pratique pour enregistrer la collection d'objets, dont les matrice d'association que nous allons cr√©er dans cette section.

```{r}
associations_abund <- list()
associations_abund[['BrayCurtis']] <- vegdist(abundance, method = "bray")
associations_abund[['BrayCurtis']]
```

La dissimilarit√© de Bray-Curtis est souvent utilis√©e dans la litt√©rature. Toutefois, la version originale de Bray-Curtis n'est pas tout √† fait m√©trique (semim√©trique). Cons√©quemment, la **dissimilarit√© de Ruzicka** (une variante de la dissimilarit√© de Jaccard pour les donn√©es d'abondance) est m√©trique, et devrait probablement √™tre pr√©f√©r√© √† Bary-Curtis ([Oksanen, 2006](http://ocw.um.es/ciencias/geobotanica/otros-recursos-1/documentos/vegantutorial.pdf)).

$$d_{AB, Ruzicka} =  \frac { 2 \times d_{AB, Bray-Curtis} }{1 + d_{AB, Bray-Curtis}}$$

```{r}
associations_abund[['Ruzicka']] <- associations_abund[['BrayCurtis']] * 2 / (1 + associations_abund[['BrayCurtis']])
```

La **dissimilarit√© de Kulczynski** (aussi √©crit Kulsinski) est asym√©trique et semim√©trique, tout comme celle de Bray-Curtis. Elle est calcul√©e comme suit.

$$d_{AB} = 1-\frac{1}{2} \times \left[ \frac{\sum min(A_i, B_i)}{\sum A_i} + \frac{\sum min(A_i, B_i)}{\sum B_i} \right]$$

```{r}
associations_abund[['Kulczynski']] <- vegdist(abundance, method = "kulczynski")
```

Une approche commune pour mesurer l'association entre sites d√©crits par des donn√©es d'abondance est la **distance de Hellinger**. Notez qu'il s'agit ici d'une distance, non pas d'une dissimilarit√©. Pour l'obtenir, on doit d'abord diviser chaque donn√©e d'abondance par l'abondance totale pour chaque site pour obtenir les esp√®ces en tant que proportions, puis on extrait la racine carr√©e de chaque √©l√©ment. Enfin, on calcule la distance euclidienne entre les proportions de chaque site. Pour rappel, une distance euclidienne est la g√©n√©ralisation en plusieurs dimensions du th√©or√®me de Pythagore, $c = \sqrt{a^2 + b^2}$.

$$D_{AB} = \sqrt {\sum \left( \frac{A_i}{\sum A_i} - \frac{B_i}{\sum B_i} \right)^2}$$

------------------ -----------------------------------------------
üò±\ **Attention**   La distance d'Hellinger h√©rite des biais li√©es aux donn√©es compositionnelles. Elle peut √™tre substiti√©e par une matrice de distances d'Aitchison.

------------------------------------------------------------------

```{r}
associations_abund[['Hellinger']] <- dist(decostand(abundance, method="hellinger"))
```

Toute comme la distance d'Hellinger, la **distance de chord** est calcul√©e par une distance euclidienne sur des donn√©es d'abondance transform√©es de sorte que chaque ligne ait une longueur (norme) de 1.

```{r}
associations_abund[['Chord']] <- dist(decostand(abundance, method="normalize"))
```

La **m√©trique du chi-carr√©**, ou $\chi$-carr√©, ou chi-square, donne davantage de poids aux esp√®ces rares qu'aux esp√®ces communes. Son utilisation est recommand√©e lorsque les esp√®ces rares sont de bons indicateurs de conditions √©cologiques particuli√®res ([Legendre et Legendre, 2012](https://www.elsevier.com/books/numerical-ecology/legendre/978-0-444-53868-0), p. 308).

$$  d_{AB} = \sqrt{\sum _j \frac{1}{\sum y_j} \left( \frac{A_j}{\sum A} - \frac{B_j}{\sum B} \right)^2 }  $$

La m√©trique peut √™tre transform√©e en distance en la multipliant par la racine carr√©e de la somme totale des esp√®ces dans la matric d'abondance ($X$).

$$ D_{AB} = \sqrt{\sum X} \times d_{AB} $$


```{r}
associations_abund[['ChiSquare']] <- dist(decostand(abundance, method="chi.square"))
```

Une manni√®re visuellement plus int√©ressante de pr√©senter une matrice d'association est un graphique de type *heatmap*.

```{r}
associations_abund_df <- list()

for (i in 1:length(associations_abund)) {
  associations_abund_df[[i]] <- data.frame(as.matrix(associations_abund[[i]]))
  colnames(associations_abund_df[[i]]) <- rownames(associations_abund_df[[i]])
  associations_abund_df[[i]]$row <- rownames(associations_abund_df[[i]])
  associations_abund_df[[i]] <- associations_abund_df[[i]] %>% gather(key=row)
  associations_abund_df[[i]]$column = rep(1:4, 4)
  associations_abund_df[[i]]$dist <- names(associations_abund)[i]
}
associations_abund_df <- do.call(rbind, associations_abund_df)

ggplot(associations_abund_df, aes(x=row, y=column)) +
  facet_wrap(. ~ dist, nrow = 2) +
  geom_tile(aes(fill = value)) +
  geom_text(aes(label = round(value, 2))) +
  scale_fill_gradient2(low = "#00ccff", mid = "#aad400", high = "#ff0066", midpoint = 2) +
  labs(x="Site", y="Site")
```

Peu importe le type d'association utilis√©e, les *heatmaps* montrent les m√™mes tendances. Les assocaitions de dissimilarit√© (Bray-Curtis, Kulczynski et Ruzicka) s'√©talent de 0 √† 1, tandis que les distances (Chi-Square, Chord et Hellinger) partent de z√©ro, mais n'ont pas de limite sup√©rieure. On note les plus grandes diff√©rences entre les sites 2 et 4, tandis que les sites 2 et 3 sont les plus semblables pour toutes les mesures d'association √† l'exception de la dissimilarit√© de Kulczynski.

#### Objets: Occurence (pr√©sence-absence)

Des indices d'association diff√©rents devraient √™tre utilis√©s lorsque des donn√©es sont compil√©es sous forme bool√©enne. En g√©n√©ral, les tableaux de donn√©es d'occurence seront compil√©s avec des 1 (pr√©sence) et des 0 (absence).

La **similarit√© de Jaccard** entre le site A et le site B est la proportion de double 1 (pr√©sences de 1 dans A et B) parmi les esp√®ces. La dissimilari√© est la proportion compl√©mentaire (comprenant [1, 0], [0, 1] et [0, 0]). La distance de Jaccard est la racine carr√©e de la dissimilarit√©.

```{r}
associations_occ <- list()
associations_occ[['Jaccard']] <- vegdist(occurence, method = "jaccard")
```

Les **distances d'Hellinger, de chord et de chi-carr√©** sont aussi appropri√©es pour les calculs de distances sur des tableaux d'occurence.


```{r}
associations_occ[['Hellinger']] <- dist(decostand(occurence, method="hellinger"))
associations_occ[['Chord']] <- dist(decostand(occurence, method="normalize"))
associations_occ[['ChiSquare']] <- dist(decostand(occurence, method="chi.square"))
```

Graphiquement,

```{r}
associations_occ_df <- list()

for (i in 1:length(associations_occ)) {
  associations_occ_df[[i]] <- data.frame(as.matrix(associations_occ[[i]]))
  colnames(associations_occ_df[[i]]) <- rownames(associations_occ_df[[i]])
  associations_occ_df[[i]]$row <- rownames(associations_occ_df[[i]])
  associations_occ_df[[i]] <- associations_occ_df[[i]] %>% gather(key=row)
  associations_occ_df[[i]]$column = rep(1:4, 4)
  associations_occ_df[[i]]$dist <- names(associations_occ)[i]
}
associations_occ_df <- do.call(rbind, associations_occ_df)

ggplot(associations_occ_df, aes(x=row, y=column)) +
  facet_wrap(. ~ dist) +
  geom_tile(aes(fill = value)) +
  geom_text(aes(label = round(value, 2))) +
  scale_fill_gradient2(low = "#00ccff", mid = "#aad400", high = "#ff0066", midpoint = 1) +
  labs(x="Site", y="Site")

```

Il est attendu que les matrices d'association sur l'occurence sont semblables √† celles sur l'abondance. Dans ce cas-ci, la distance d'Hellinger donne des r√©sultats semblables √† la dissimilarit√© de Jaccard.

#### Objets: Donn√©es quantitatives

Les donn√©es quantitative en √©cologie peuvent d√©crire l'√©tat de l'environnement: le climat, l'hydrologie, l'hydrog√©ochimie, la p√©dologie, etc. En r√®gle g√©n√©rale, les coordonn√©es des sites ne sot pas des variables environnementales, √† que l'on soup√ßonne la coordonn√©e elle-m√™me d'√™tre responsable d'effets sur notre syst√®me: mais il s'agira la plupart du temps d'effets confondants (par exemple, on peut mesurer un effet de lattitude sur le rendement des agrumes, mais il s'agira probablement avant tout d'effets dus aux conditions climatiques, qui elles changent en fonction de la lattitude). D'autre types de donn√©es quantitative pouvant √™tre appr√©hend√©es par des distances sont les traits ph√©nologiques, les ionomes, les g√©nomes, etc.

La **distance euclidienne** est la racine carr√©e de la somme des carr√©s des distances sur tous les axes. Il s'agit d'une application multidimensionnelle du th√©or√®me de Pythagore. La **distance d'Aitchison**, couverte dans le chapitre \@ref(chapitre-explorer), est une distance euclidienne calcul√©e sur des donn√©es compositionnelles pr√©alablement transform√©es. La distance euclidienne est sensible aux unit√©s utilis√©s: utiliser des milim√®tres plut√¥t que des m√®tres enflera la distance euclidienne. Il est recommand√© de porter une attention particuli√®re aux unit√©s, et de standardiser les donn√©es au besoin (par exemple, en centrant la moyenne √† z√©ro et en fixant l'√©cart-type √† 1).

On pourrait, par exemple, mesurer la distance entre des observations des dimensions de diff√©rentes esp√®ces d'iris. Ce tableau est inclu dans R par d√©faut.

```{r}
data(iris)
iris %>% sample_n(5)
```

Les mesures du tableau sont en centim√®tres. Pour √©viter de donner davantage de poids aux longueur des s√©pales et en m√™me temps de n√©gliger la largeur des p√©tales, nous allons standardiser le tableau.

```{r}
iris_sc <- iris %>%
  select(-Species) %>% 
  scale(.)%>% 
  as_tibble(.) %>% 
  mutate(Species = iris$Species) 
iris_sc
```

Pour les comparaisons des dimensions, prenons la moyenne des dimensions (mises √† l'√©chelle) par esp√®ce.

```{r}
iris_means <- iris_sc %>%
  group_by(Species) %>%
  summarise_all(mean) %>%
  select(-Species)
iris_means
```

Nous pouvons utiliser la distance euclidienne, commune en g√©om√©trie, pour comparer les esp√®ces. La distance euclidienne est calcul√©e comme suit.


$$ \mathcal{E} = \sqrt{\Sigma_i \left( A_i - B_i \right) ^2 } $$

```{r}
associations_cont = list()
associations_cont[['Euclidean']] <- dist(iris_sc %>% select(-Species), method="euclidean")
```

La **distance de Mahalanobis** est semblable √† la distance euclidienne, mais qui tient compte de la covariance de la matrice des objets. Cette covariance peut √™tre utilis√©e pour d√©crire la structure d'un nuage de points. La diastance de Mahalanobis se calcule comme suit.

$$\mathcal{M} = \sqrt{(A - B)^T S^{-1} (A-B)}$$

Notez qu'il s'agit d'une g√©n√©ralisation de la distance euclidienne, qui √©quivaut √† une distance de Mahalanobis dont la matrice de covariance est une matrice identit√©.

La distance de Mahalanobis permet de repr√©senter des distances dans un espace fortement corr√©l√©. Elle est courramment utilis√©e pour d√©tecter les valeurs aberrantes selon des crit√®res de distance √† partir du centre d'un jeu de donn√©es multivari√©es.

```{r}
associations_cont[['Mahalanobis']] <- vegdist(iris_sc %>% select(-Species), 'mahalanobis')
```

La **distance de Manhattan** porte aussi le nom de distance de cityblock ou de taxi. C'est la distance que vous devrez parcourir pour vous rendre du point A au point B √† Manhattan, c'est-√†-dire selon une s√©quence de tron√ßons perpendiculaires.

$$ D_{AB} = \sum _i \left| A_i - B_i \right| $$

La distance de Manhattan est appropri√©e lorsque les gradients (changements d'un √©tat √† l'autre ou d'une r√©gion √† l'autre) ne permettent pas des changements simultan√©s. Mieux vaut standardiser les variables pour √©viter qu'une dimension soit pr√©pond√©rante.

```{r}
associations_cont[['Manhattan']] <- vegdist(iris_sc %>% select(-Species), 'manhattan')
```

Avant de pr√©senter les r√©sultats des esp√®ces d'iris, voici une repr√©sentation des distances euclidiennes (rouge), de Mahalanobis (bleu) et de Manhattan (vert), chacune de 1 et 2 unit√©s √† partir du centre et, pour ce qui est de la distance de Mahalanobis, selon la covariance.

```{r, fig.height=5, fig.width=5}
library("car")
library("MASS")
select <- dplyr::select # √©viter les conflits de fonctions entre MASS et dplyr
filter <- dplyr::filter

sigma <- matrix(c(1, 0.6, 0.6, 1), ncol = 2) # matrice de covariance
mu <- c(0, 0) # centre
data <- mvrnorm(n = 100, mu, sigma) # g√©n√©rer des donn√©es

plot(data, ylim = c(-2, 2), xlim = c(-2, 2), asp = 1)

## cercles
t <- seq(0,2*pi,length=100)
c1 <- t(rbind(mu[2] + sin(t)*1, mu[1] + cos(t)*1))
c2 <- t(rbind(mu[2] + sin(t)*2, mu[1] + cos(t)*2))
lines(c1, lwd = 2, col = "red")
lines(c2, lwd = 2, col = "red")


## ellipses
e1 <- ellipse(mu, sigma, radius=1, add=TRUE)
e2 <- ellipse(mu, sigma, radius=2, add=TRUE)

## carr√©s
lines(c(1, 0, -1, 0, 1), c(0, 1, 0, -1, 0), lwd = 2, col = "green")
lines(c(2, 0, -2, 0, 2), c(0, 2, 0, -2, 0), lwd = 2, col = "green")
```

Et, graphiquement, les r√©sultats des distances des iris.

```{r}
associations_cont_df <- list()

for (i in 1:length(associations_cont)) {
  associations_cont_df[[i]] <- data.frame(as.matrix(associations_cont[[i]]))
  colnames(associations_cont_df[[i]]) <- rownames(associations_cont_df[[i]])
  associations_cont_df[[i]]$row <- rownames(associations_cont_df[[i]])
  associations_cont_df[[i]] <- associations_cont_df[[i]] %>% gather(key=row)
  associations_cont_df[[i]]$column = rep(1:nrow(iris), nrow(iris))
  associations_cont_df[[i]]$dist <- names(associations_cont)[i]
}
associations_cont_df <- do.call(rbind, associations_cont_df)

ggplot(associations_cont_df, aes(x=row, y=column)) +
  facet_wrap(. ~ dist) +
  geom_tile(aes(fill = value), colour = NA) +
  #geom_text(aes(label = round(value, 2))) +
  scale_fill_gradient2(low = "#00ccff", mid = "#aad400", high = "#ff0066", midpoint = 5) +
  labs(x="Site", y="Site")
```


Le tableau `iris` est ordonn√© par esp√®ce. Les distances euclidienne et de Manhattan permettent ais√©ment de distinguer les esp√®ces selon les dimensions des p√©tales et des s√©pales. Toutefois, l'utilsation de la covariance avec la distance de Mahalanobis cr√©e des distinction moins tranch√©es.

#### Objets: Donn√©es mixtes

Les donn√©es cat√©gorielles ordinales peuvent √™tre transform√©es en donn√©es continues par gradations lin√©aires ou quadratiques. Les donn√©es cat√©gorielles nominales, quant √† elles, peuvent √™tre encod√©es (*encodage cat√©goriel*) en donn√©es similaires √† des occurences. Attention toutefois: contrairement √† la r√©gression lin√©aire qui demande d'exclure une cat√©gorie, l'encodage cat√©goriel doit inclure toutes les cat√©gories. Le comportement par d√©faut de la fonction `model.matrix` est d'exclure la cat√©gorie de r√©f√©rence: on doit sp√©cifier que l'intercept est de z√©ro, c'est-√†-dire `model.matrix(~ + categorie)`.

La **similarit√© de Gower** a √©t√© d√©velopp√©e pour mesurer des associations entre des objets dont les donn√©es sont mixtes: bool√©ennes, cat√©gorielles et continues. La similarit√© de Gower est calcul√©e en additionnant les distances calcul√©es par colonne, individuellement. Si la colonne est bool√©enne, on utilise les distances de Jaccard (qui exclue les double-z√©ro) de mani√®re univari√©e: une variable √† la fois. Pour les variables continues, on utilise la distance de Manhattan divis√©e par la plage de valeurs de la variable (pour fin de standardisation). Puisqu'elle h√©rite de la particularit√© de la distance de Manhattan et de la similarit√© de Jaccard univari√©e, la **similarit√© de Gower** reste une combinaison lin√©aire de distances univari√©es.


```{r}
X <- tibble(ID = 1:8,
            age = c(21, 21, 19, 30, 21, 21, 19, 30),
            gender = c('M','M','N','M','F','F','F','F'),
            civil_status = c('MARRIED','SINGLE','SINGLE','SINGLE','MARRIED','SINGLE','WIDOW','DIVORCED'),
            salary = c(3000.0,1200.0 ,32000.0,1800.0 ,2900.0 ,1100.0 ,10000.0,1500.0),
            children = c(TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE),
            available_credit = c(2200,100,22000,1100,2000,100,6000,2200))
X
```

Il faut pr√©alablement proc√©der √† l'encodage cat√©goriel pour les variables cat√©gorielles nominales.

```{r}
X_dum <- model.matrix(~ 0 + ., X[, -1])
X_dum
```

Calculons la dissimilarit√© de Gower (cette fois le graphique est fait avec `pheatmap`).

```{r}
library("pheatmap")
d_gow <- as.matrix(vegdist(X_dum, 'gower'))
colnames(d_gow) <- rownames(d_gow) <- X$ID
pheatmap(d_gow)
```

Les dendrogrammes apparaissants sur les axes du graphique sont issus d'un processus de partitionnement bas√© sur la distance, que nous verrons plus loin dans ce chapiter. Les profils des clients 4 et 7, ainsi que ceux des clients 3 et 7 diff√®rent le plus. Les profils 3 et 4 sont n√©anmoins plut√¥t diff√©rents.

### Associations entre variables (mode R)

Il existe de nombreuses approches pour mesurer les associations entre variables. La plus connue est la corr√©lation. Mais les donn√©es d'abondance et d'occurence demandent des approches diff√©rentes.

#### Variables: Abondance

La distance du chi-carr√© est sugg√©r√©e par [Borcard et al. (2011)](http://www.springer.com/us/book/9781441979759).

```{r}
abundance_r <- t(abundance)
D_chisq_R <- as.matrix(dist(decostand(abundance_r, method="chi.square")))
pheatmap(D_chisq_R, display_numbers = round(D_chisq_R, 2))
```

Des coabondances sont notables pour la m√©sange √† t√™te noire, le jaseur bor√©al, la citelle √† poitrine rousse et le bruant √† gorge blanche (tache bleu au centre).

#### Variables: Occurence

La dissimilarit√© de Jaccard peut √™tre utilis√©e.

```{r}
occurence_r <- t(occurence)
D_jacc_R <- as.matrix(vegdist(occurence_r, method = "jaccard"))
pheatmap(D_jacc_R, display_numbers = round(D_jacc_R, 2))
```

Des cooccurences sont notables pour le jaseur bor√©al, la citelle √† poitrine rousse et le bruant √† gorge blanche (tache bleu au centre).

#### Variables: Quantit√©s

La matrice des corr√©lations de Pearson peut √™tre utilis√©e pour les donn√©es continues. Quant aux variables ordinales, elles devraient id√©alement √™tre li√©es lin√©airement ou quadratiquement. Si ce n'est pas le cas, c'est-√†-dire que les cat√©gories sont ordonn√©es par rang seulement, vous pourrez avoir recours aux coefficients de corr√©lation de Spearman ou de Kendall.

```{r}
iris_cor <- iris %>%
  select(-Species) %>%
  cor(.)
pheatmap(cor(iris[, -5]), cluster_rows = FALSE, cluster_cols = FALSE,
         display_numbers = round(iris_cor, 2))
```

### Conclusion sur les associations

Il n'existe pas de r√®gle claire pour d√©terminer quelle technique d'association utiliser. Cela d√©pend en premier lieu de vos donn√©es. Vous s√©lectionnerez votre m√©thode d'association selon le type de donn√©es que vous abordez, la question √† laquelle vous d√©sirez r√©pondre ainsi l'exp√©rience dans la litt√©rature comme celle de vos coll√®gues scientifiques. S'il n'existe pas de r√®gle clair, c'est qu'il existe des dizaines de m√©thodes diff√©rentes, et la plupart d'entre elles vous donneront une perspective juste et valide. Il faut n√©anmoins faire attention pour √©viter de s√©lectionner les m√©thodes qui ne sont pas appropri√©es. 

## Partitionnement

Les donn√©es suivantes ont √©t√© g√©n√©r√©es par [Leland McInnes](https://github.com/scikit-learn-contrib/hdbscan/blob/master/notebooks/clusterable_data.npy) (Tutte institute of mathematics, Ottawa). √ätes-vous en mesure d'identifier des groupes? Combien en trouvez-vous?

```{r}
df_mcinnes <- read_csv("data/clusterable_data.csv", col_names = c("x", "y"), skip = 1)
ggplot(df_mcinnes, aes(x=x, y=y)) + geom_point() + coord_fixed()
```

En 2D, l'oeil humain peut facilement d√©tecter les groupes. En 3D, c'est toujours possible, mais au-del√† de 3D, le partitionnement cognitive devient rapidement maladroite. Les algorithmes sont alors d'une aide pr√©cieuse. Mais ils transportent en pratique tout un baggage de limitations. Quel est le crit√®re d'association entre les groupes? Combien de groupe devrions-nous cr√©er? Comment distinguer une donn√©e trop bruit√©e pour √™tre classifi√©e?

Le partitionnement de donn√©es (*clustering* en anglais), et inversement leur regroupement, permet de cr√©er des ensembles selon des crit√®res d'association. On suppose donc que Le partitionnement permet de cr√©er des groupes selon l'information que l'on fait √©merger des donn√©es. Il est cons√©quemment entendu que les donn√©es ne sont pas cat√©goris√©es √† priori: **il ne s'agit pas de pr√©dire la cat√©gorie d'un objet, mais bien de cr√©er des cat√©gories √† partir des objets** par exemple selon leurs dimensions, leurs couleurs, leurs signature chimique, leurs comportements, leurs g√®nes, etc. 

Plusieurs m√©thodes sont aujourd'hui offertes aux analystes pour partitionner leurs donn√©es. Dans le cadre de ce manuel, nous couvrirons ici deux grandes tendances dans les algorithmes.

1. *M√©thodes hi√©rarchique et non hi√©rarchiques*. Dans un partitionnement hi√©rarchique, l'ensemble des objets forme un groupe, comprenant des sous-regroupements, des sous-sous-regroupements, etc., dont les objets forment l'ultime partitionnement. On pourra alors identifier comment se d√©cline un partitionnement. √Ä l'inverse, un partitionnement non-hi√©rarchique des algorhitmes permettent de cr√©er les groupes non hi√©rarchis√©s les plus diff√©rents que possible.

2. *Membership exclusif ou flou*. Certaines techniques attribuent √† chaque objet une classe unique: l'appartenance sera indiqu√©e par un 1 et la non appartenance par un 0. D'autres techniques vont attribuer un membership flou o√π le degr√© d'appartenance est une variable continue de 0 √† 1. Parmi les m√©thodes floues, on retrouve les m√©thodes probabilistes.

### √âvaluation d'un partitionnement

Le choix d'une technique de partitionnement parmi de nombreuses disponibles, ainsi que le choix des param√®tres gouvernant chacune d'entre elles, est avant tout bas√© sur ce que l'on d√©sire d√©finir comme √©tant un groupe, ainsi que la mani√®re d'interpr√©ter les groupes. En outre, **le nombre de groupe √† d√©partager est *toujours* une d√©cision de l'analyste**. N√©anmoins, on peut se fier [des indicateurs de performance de partitionnement](http://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation). Parmis ceux-ci, retenons le score [silouhette](https://rdrr.io/cran/cluster/man/silhouette.html) ainsi que l'[indice de Calinski-Harabaz](https://www.tandfonline.com/doi/abs/10.1080/03610927408827101).

#### Score silouhette

En anglais, le *h* dans silouhette se trouve apr√®s le *l*: on parle donc de *silhouette coefficient* pour d√©signer le score de chacun des objets dans le partitionnement. Pour chaque objet, on calcule la distance moyenne qui le s√©pare des autres points de son groupe ($a$) ainsi que la distance moyenne  qui le s√©pare des points du groupe le plus rapproch√©.

$$s = \frac{b-a}{max \left(a, b \right)}$$

Un coefficient de -1 indique le pire classement, tandis qu'un coefficient de 1 indique le meilleur classement. La moyenne des coefficients silouhette est le score silouhette.

#### Indice de Calinski-Harabaz

L'indice de Calinski-Harabaz est proportionnel au ratio des dispersions intra-groupe et la moyenne des dispersions inter-groupes. Plus l'indice est √©lev√©, mieux les groupes sont d√©finis. La math√©matique est d√©crite [dans la documentation de scikit-learn](http://scikit-learn.org/stable/modules/clustering.html#calinski-harabaz-index), un module d'analyse et autoapprentissage sur Python.

**Note**. Les coefficients silouhette et l'indice de Calinski-Harabaz sont plus appropri√©s pour les formes de groupes convexes (cercles, sph√®res, hypersph√®res) que pour les formes irr√©guli√®res (notamment celles obtenues par la DBSCAN, discut√©e ci-desssous).

### Partitionnement non hi√©rarchique

Il peut arriver que vous n'ayez pas besoin de comprendre la structure d'agglom√©ration des objets (ou variables).  Plusieurs techniques de partitionnement non hi√©rarchique sont disponibles sur R. On s'int√©ressera en particulier aux *k-means* et au *dbscan*.

#### Kmeans

L'objectif des kmeans est de minimiser la distance eucl√©dienne entre un nombre pr√©d√©fini de *k* groupes exclusifs.

1. L'algorhitme commence par placer une nombre *k* de centroides au hasard dans l'espace d'un nombre *p* de variables (vous devez fixer *k*, et *p* est le nombre de colonnes de vos donn√©es).
2. Ensuite, chaque objet est √©tiquett√© comme appartenant au groupe du centroid le plus pr√®s.
3. La position du centroide est d√©plac√©e √† la moyenne de chaque groupe.
4. Recommencer √† partir de l'√©tape 2 jusqu'√† ce que l'assignation des objets aux groupes ne change plus.

![](https://media.giphy.com/media/12vVAGkaqHUqCQ/giphy.gif)
<center>Source: [David Sheehan](https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/)</center>

La technique des kmeans suppose que les groupes ont des distributions multinormales - repr√©sent√©es par des cercles en 2D, des sph√®res en 3D, des hypersph√®res en plus de 3D. Cette limitation est probl√©matique lorsque les groupes se pr√©sentent sous des formes irr√©guli√®res, comme celles du nuage de points de Leland McInnes, pr√©sent√© plus haut. De plus, la technique classique des kmeans est bas√©e sur des distances euclidiennes: l'utilisation des kmeans n'est appropri√©e pour les donn√©es comprenant beaucoup de z√©ros, comme les donn√©es d'abondance, qui devraient pr√©alablement √™tre transform√©es en variables centr√©es et r√©duites ([Legendre et Legendre, 2012](https://www.elsevier.com/books/numerical-ecology/legendre/978-0-444-53868-0)). La technique des **mixtures gaussiennes** ([*gaussian mixtures*](https://www.stat.washington.edu/mclust/)) est une g√©n√©ralisation des kmeans permettant d'int√©grer la covariance des groupes. Les groupes ne sont plus des hyper-sph√®res, mais des hyper-ellipso√Ødes.

##### Application

Nous pouvons utilis√© la fonction `kmeans` de R. Toutefois, puisque l'on d√©sire ici effectuer des tests de partitionnement pour plusieurs nombres de groupes, nous utiliserons `cascadeKM`, du module vegan. Notez que de nombreux param√®tres par d√©faut sont utilis√©s dans les ex√©cutions ci-dessous. Ces notes de cours ne forment pas un travail de recherche scientifique. Lors de travaux de recherche, l'utilsation d'un argument ou d'un autre dans une fonction doit √™tre justifi√©: qu'un param√®tre soit utilis√© par d√©faut dans une fonction n'est a priori pas une justification convainquante.

Pour les kmeans, on doit fixer le nombre de groupes. Le graphique des donn√©es de Leland McInnes montrent 6 groupes. Toutefois, il est rare que l'on puisse visualiser des d√©marquations aussi tranch√©es que celles de l'exemple, qui plus est dans des cas o√π l'on doit traiter de plus de deux dimensions. Je vais donc lancer le partitionnement en boucle pour plusieurs nombres de groupes, de 3 √† 10 et pour chaque groupe, √©valuer le score silouhette et de Calinski-Habaraz. J'utilise un argument random_state pour m'assurer que les groupes seront les m√™mes √† chaque fois que la cellule sera lanc√©e.

```{r}
library("vegan")
mcinnes_kmeans <- cascadeKM(df_mcinnes, inf.gr = 3, sup.gr = 10, criterion = "calinski")
str(mcinnes_kmeans)
```

L'objet `mcinnes_kmeans`, de type `cascadeKM`, peut √™tre visualis√© directement avec la fonction `plot`.

```{r}
plot(mcinnes_kmeans)
```

On obtient un maximum de Calinski √† 4 groupes, qui correspons √† la deuxi√®me simulation effectu√©e de 3 √† 10.

Examinons les scores silouhette (module: cluster).

```{r}
library("cluster")
asw <- c()
for (i in 1:ncol(mcinnes_kmeans$partition)) {
  mcinnes_kmeans_silhouette <- silhouette(mcinnes_kmeans$partition[, i], dist = vegdist(df_mcinnes, method = "euclidean"))
  asw[i] <- summary(mcinnes_kmeans_silhouette)$avg.width
}
plot(3:10, asw, type = 'b')
````

Le score silouhette maximum est √† 3 groupes. La forme des groupes n'√©tant pas convexe, il fallait s'attendre √† ce que indicateurs maximaux pour les deux indicateurs soient diff√©rents. C'est d'ailleurs souvent le cas. Cet exemple supporte que le choix du nombre de groupe √† d√©partager repose sur l'analyste, non pas uniquement sur les indicateurs de performance. Choisissons 6 groupes, puisque que c'est visuellement ce que l'on devrait chercher pour ce cas d'√©tude.

```{r}
kmeans_group <- mcinnes_kmeans$partition[, 4]
mcinnes_kmeans$partition %>% head(3)
df_mcinnes %>% 
  mutate(kmeans_group = kmeans_group) %>% # ajouter une colonne de regoupement
  ggplot(aes(x=x, y=y)) +
  geom_point(aes(colour = factor(kmeans_group))) +
  coord_fixed()

```

L'algorithme kmeans est loin d'√™tre statisfaisant. Cela est attendu, puisque les kmeans recherchent des distribution gaussiennes sur des groupes vraisemblablement non-gaussiens.

Nous pouvons cr√©er un graphique silouhette pour nos 6 groupes. Notez qu'√† cause d'un bogue, il n'est pas possible de pr√©senter les donn√©es clairement lorsqu'elles sont nombreuses.

```{r, fig.height=30, fig.width=8}
sil <- silhouette(mcinnes_kmeans$partition[, 6],
                  dist = vegdist(df_mcinnes[, ], method = "euclidean"))
sil <- sortSilhouette(sil)
plot(sil, col = 'black')
```

#### DBSCAN

La technique DBSCAN (* **D**ensity-**B**ased **S**patial **C**lustering of **A**pplications with **N**oise*) sousentend que les groupes sont compos√©s de zones o√π l'on retrouve plus de points (zones denses) s√©par√©es par des zones de faible densit√©. Pour lancer l'algorithme, nous devons sp√©cifier une mesure d'association critique (distance ou dissimilarit√©) *d* ainsi qu'un nombre de point critique *k* dans le voisinage de cette distance.

1. L'algorithme commence par √©tiqueter chaque point selon l'une de ces cat√©gories:

- *Noyau*: le point a au moins *k* points dans son voisinage, c'est-√†-dire √† une distance inf√©rieure ou √©gale √† *d*.
- *Bordure*: le point a moins de *k* points dans son voisinage, mais l'un de des points voisins est un *noyau*.
- *Bruit*: le cas √©ch√©ant. Ces points sont consid√©r√©s comme des outliers.

<img src="images/07_dbscan_1.svg" width=600>

2. Les noyaux distanc√©s de *d* ou moins sont connect√©s entre eux en englobant les bordures.

<img src="images/07_dbscan_2.svg" width=600>

Le nombre de groupes est prescrit par l'algorithme DBSCAN, qui permet du coup de d√©tecter des donn√©es trop bruit√©es pour √™tre class√©es.

[Damiani et al. (2014)](https://doi.org/10.1145/2666310.2666417) a d√©velopp√© une approche utilisant la technique DBSCAN pour partitionner des zones d'escale pour les flux de populations migratoires.

##### Application

La technique **DBSCAN** n'est pas bas√©e sur le nombre de groupe, mais sur la densit√© des points. L'argument `x` ne constitue pas les donn√©es, mais une matrice d'association. L'argument minPts sp√©cifie le nombre minimal de points qui l'on doit retrouver √† une distance critique d* pour la formation des *noyaux et la propagation des groupes, sp√©cifi√©e dans l'argument eps. La distance d peut √™tre estim√©e en prenant une fraction de la moyenne, mais on aura volontiers recours √† sont bon jugement.

```{r}
library("dbscan")
mcinnes_dbscan <- dbscan(x = vegdist(df_mcinnes[, ], method = "euclidean"),
                         eps = 0.03, minPts = 10)
dbscan_group <- mcinnes_dbscan$cluster
unique(dbscan_group)
```

Les param√®tres sp√©cifi√©s donnent 5 groupes (`1, 2, ..., 5`) et des points trop bruit√©s pour √™tre classifi√©s (√©tiquet√©s `0`). Voyons comment les groupes ont √©t√© form√©s.

```{r}
df_mcinnes %>% 
  mutate(dbscan_group = dbscan_group) %>% # ajouter une colonne de regoupement
  ggplot(aes(x=x, y=y)) +
  geom_point(aes(colour = factor(dbscan_group))) +
  coord_fixed()
```

Le partitionnement semble plus conforme √† ce que l'on recherche. N√©anmoins, DBSCAN cr√© quelques petits groupes ind√©sirables (groupe 6,  en rose) ainsi qu'un grand groupe (violet) qui auraient lieu d'√™tre partitionn√©. Ces d√©faut pourraient √™tre r√©gl√©s en jouant sur les param√®tres `eps` et `minPts`.

### Partitionnement hi√©rarchique

Les techniques de partitionnement hi√©rarchique sont bas√©es sur les matrices d'association. La technique pour mesurer l'association (entre objets ou variables) d√©terminera en grande partie le paritionnement des donn√©es. Les partitionnements hi√©rarchiques ont l'avantage de pouvoir √™tre repr√©sent√©s sous forme de dendrogramme (ou arbre) de partition. Un tel dendrogramme pr√©sente des sous-groupes qui se joignent en groupes jusqu'√† former un seul ensemble.

Le partitionnement hi√©rarchique est abondamment utilis√© en phylog√©nie, pour √©tudier les relations de parent√© entre organismes vivants, populations d'organismes et esp√®ces. La ph√©n√©tique, branche empirique de la phylog√©n√®se intersp√©cifique, fait usage du partitionnement hi√©rarchique √† partir d'associations g√©n√©tiques entre unit√©s taxonomiques. On retrouve de nombreuses ressources acad√©miques en phylog√©n√©tique ainsi que des outils pour [R](https://www.springer.com/us/book/9781461495413) et [Python](https://academic.oup.com/bioinformatics/article/26/12/1569/287181/DendroPy-a-Python-library-for-phylogenetic). Toutefois, la phylog√©n√©tique en particulier ne fait pas partie de la pr√©sente itt√©ration de ce manuel.

#### Techniques de partitionnement hi√©rarchique

Le partitionnement hi√©rarchique est typiquement effectu√© avec une des quatres m√©thodes suivantes, dont chacune poss√®de ses particularit√©s, mais sont toutes agglom√©ratives: √† chaque √©tape d'agglom√©ration, on fusionne les deux groupes ayant le plus d'affinit√© sur la base des deux sous-groupes les plus rapproch√©s.

**Single link** (`single`). Les groupes sont agglom√©r√©s sur la base des deux points parmi les groupes, qui sont les plus proches.

**Complete link** (`complete`). √Ä la diff√©rence de la m√©thode *single*, on consid√®re comme crit√®re d'agglom√©ration les √©l√©ments les plus √©loign√©s de chaque groupe.

**Agglom√©ration centrale**. Il s'agit d'une fammlle de m√©thode bas√©es sur les diff√©rences entre les tendances centrales des objets ou des groupes.

- **Average** (`average`). Appel√©e UPGMA (Unweighted Pair-Group Method unsing Average), les groupes sont agglom√©r√©s selon un centre calcul√©s par la moyenne et le nombre d'objet pond√®re l'agglom√©ration (le poids des groupes est retir√©). Cette technique est historiquement utilis√©e en bioinformatique pour partitionner des groupes phylog√©n√©tiques ([Sneath et Sokal, 1973](https://www.cabdirect.org/cabdirect/abstract/19730310919)).
- **Weighted** (`weighted`). La version de average, mais non pond√©r√©e (WPGMA).
- **Centroid** (`centroid`). Tout comme average, mais le centro√Øde (centre g√©om√©trique) est utilis√© au lieu de la moyenne. Accronyme: UPGMC.
- **Median** (`median`). Appel√©e WPGMC. Devinez! ;)

**Ward** (`ward`). L'optimisation vise √† minimiser les sommes des carr√©s par regroupement.

#### Quel outil de partitionnement hi√©rarchique utiliser?

Alors que le choix de la matrice d'association d√©pend des donn√©es et de leur contexte, la technique de partitionnement hi√©rarchique peut, quant √† elle, √™tre bas√©e sur un crit√®re num√©rique. Il en existe plusieurs, mais le crit√®re recommand√© pour le choix d'une technique de partitionnement hi√©rarchique est la **corr√©lation coph√©n√©tique**. La distance coph√©n√©tique est la distance √† laquelle deux objets ou deux sous-groupes deviennent membres d'un m√™me groupe. La corr√©lation coph√©n√©tique est la corr√©lation de Pearson entre le vecteur d'association des objets et le vecteur de distances coph√©n√©tiques.

#### Application

Les techniques de partitionnement hi√©rarchique pr√©sent√©es ci-dessus sont disponibles dans le module `stats` de R, qui est charg√© automatiquement lors de l'ouversture de R. Nous allons classifier les dimensions des iris gr√¢ce √† la distance de Manhattan.

```{r}
mcinnes_hclust_distmat <- vegdist(df_mcinnes, method = "manhattan")

clustering_methods <- c('single', 'complete', 'average', 'centroid', 'ward')

clust_l <- list()
coph_corr_l <- c()

for (i in seq_along(clustering_methods)) {
  clust_l[[i]] <- hclust(mcinnes_hclust_distmat, method = clustering_methods[i])
  coph_corr_l[i] <- cor(mcinnes_hclust_distmat, cophenetic(clust_l[[i]]))
}

tibble(clustering_methods, coph_corr = coph_corr_l) %>% 
  ggplot(aes(x = fct_reorder(clustering_methods, -coph_corr), y = coph_corr)) +
  geom_col() +
  labs(x = "M√©thode de partitionnement", y = "Corr√©lation coph√©n√©tique")

```

La m√©thode `average` retourne la corr√©lation la plus √©lev√©e. Pour plus de flexibilit√©, ench√¢ssons le nom de la m√©thode dans une variable. Ainsi, en chageant le nom de cette variable, le reste du code sera cons√©quent.

```{r}
names(clust_l) <- clustering_methods
best_method <- "average"
```

Le partitionnement hi√©rarchique peut √™tre visualis√© par un dendrogramme.

```{r, fig.width=15, fig.height=5}
plot(clust_l[[best_method]])
```

#### Combien de groupes utiliser?

La longueur des lignes verticales est la distance s√©parant les groupes enfants. Bien que la s√©lection du nombre de groupe soit avant tout bas√©e sur les besoins du probl√®me, nous pouvons nous appuyer sur certains outils. La hauteur totale peut servir de crit√®re pour d√©finir un nombre de groupes ad√©quat. On pourra s√©lectionner le nombre de groupe o√π la hauteur se stabilise en fonction du nombre de groupe. On pourra aussi utiliser le *graphique silhouette*, comprenant une collection de *largeurs de silouhette*, repr√©sentant le degr√© d'appartenance √† son groupe. La fonction `sklearn.metrics.silhouette_score`, du module scikit-learn, s'en occupe.



```{r}
asw <- c()
num_groups <- 3:10
for(i in seq_along(num_groups)) {
  sil <- silhouette(cutree(clust_l[[best_method]], k = num_groups[i]), mcinnes_hclust_distmat)
  asw[i] <- summary(sil)$avg.width
}
plot(num_groups, asw, type = "b")
```

Le nombre optimal de groupes serait de 5. Coupons le dendrorgamme √† la hauteur correspondant √† 5 groupes avec la fonction `cutree`.

```{r, fig.width=15, fig.height=5}
k_opt <- num_groups[which.max(asw)]
hclust_group <- cutree(clust_l[[best_method]], k = k_opt)
plot(clust_l[[best_method]])
rect.hclust(clust_l[[best_method]], k = k_opt)
```

La classification hi√©rarchique, uniquement bas√©e sur la distance, peut √™tre inappropri√©e pour d√©finir des formes complexes.

```{r}
df_mcinnes %>% 
  mutate(hclust_group = hclust_group) %>% # ajouter une colonne de regoupement
  ggplot(aes(x=x, y=y)) +
  geom_point(aes(colour = factor(hclust_group))) +
  coord_fixed()
```

### Partitionnement hi√©rarchique bas√©e sur la densit√© des points

La tecchinque HDBSCAN, dont l'algorithme est relativement r√©cent ([Campello et al., 2013](https://link.springer.com/chapter/10.1007%2F978-3-642-37456-2_14)), permet une partitionnement hi√©rarchique sur le m√™me principe des zones de densit√© de la technique DBSCAN. Le HDBSCAN a √©t√© utilis√©e pour partitionner les lieux d'escale d'oiseaux migrateurs en Chine ([Xu et al., 2013](https://www.jstage.jst.go.jp/article/dsj/12/0/12_WDS-027/_article)).

Avec DBSCAN, un rayon est fix√© dans une m√©trique appropri√©e. Pour chaque point, on compte le nombre de point voisins, c'est √† dire le nombre de point se situant √† une distance (ou une dissimilarit√©) √©gale ou inf√©rieure au rayon fix√©. Avec HDBSCAN, on sp√©cifie le nombre de points devant √™tre recouverts et on calcule le rayon n√©cessaire pour les recouvrir. Ainsi, chaque point est associ√© √† un rayon critique que l'on nommera $d_{noyau}$. La m√©trique initiale est ensuite alt√©r√©e: on remplace les associations entre deux objets A et B par la valeur maximale entre cette association, le rayon critique de A et le rayon critique de B. Cette nouvelle distance est appel√©e la *distance d'atteinte mutuelle*: elle accentue les distances pour les points se trouvant dans des zones peu denses. On applique par la suite un algorithme semblable √† la partition hi√©rarchique *single link*: En s'√©largissant, les rayons se superposent, chaque superposition de rayon forment graduellement des groupes qui s'agglom√®rent ainsi de mani√®re hi√©rarchique. Au lieu d'effectuer une tranche √† une hauteur donn√©e dans un dendrogramme de partitionnement, la technique HDBSCAN se base sur un dendrogramme condens√© qui discarte les sous-groupes comprenant moins de *n* objets ($n_{gr min}$). Dans nouveau dendrogramme, on recherche des groupes qui occupent bien l'espace d'analyse. Pour ce faitre, on utilise l'inverse de la distance pour cr√©er un indicateur de *persistance* (semblable √† la similarit√©), $\lambda$. Pour chaque groupe hi√©rarchique dans le dendrogramme condens√©, on peut calculer la persistance o√π le groupe prend naissance. De plus, pour chaque objet d'un groupe, on peut aussi calculer une distance √† laquelle il quitte le groupe. La *stabilit√©* d'un groupe est la domme des diff√©rences de persistance entre la persistance √† la naissance et les persistances des objets. On descend dans le dendrogramme. Si la somme des stabilit√© des groupes enfants est plus grande que la stabilit√© du groupe parent, on accepte la division. Sinon, le parent forme le groupe. La [documentation du module `hdbscan`](http://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html) pour Python offre une description intuitive et plus exhaustive des principes et algorithme de HDBSCAN.

#### Param√®tres

Outre la m√©trique d'association dont nous avons discut√©, HDBSCAN demande d'√™tre nourri avec [quelques param√®tres importants](https://www.rdocumentation.org/packages/dbscan/versions/1.1-3/topics/hdbscan). En particulier, le **nombre minimum d'objets par groupe**, $n_{gr min}$ d√©pend de la quantit√© de donn√©es que vous avez √† votre disposition, ainsi que de la quantit√© d'objets que vous jugez suffisante pour cr√©er des groupes. Nous utiliserons l'impl√©mentation de HDBSCAN du module dbscan. Si vous d√©sirez davantage d'options, vous pr√©f√©rerez probablement l'[impl√©mentation du module largeVis](https://www.rdocumentation.org/packages/largeVis/versions/0.2.1.1/topics/hdbscan).

```{r, fig.width=15, fig.height=7}
mcinnes_hdbscan <- hdbscan(x = vegdist(df_mcinnes, method = "euclidean"),
                           minPts = 20,
                           gen_hdbscan_tree = TRUE,
                           gen_simplified_tree = FALSE)
hdbscan_group <- mcinnes_hdbscan$cluster
unique(hdbscan_group)
```

Nous avons 6 groupes, num√©rot√©s de 1 √† 6, ainsi que des √©tiquettes identifiant des objets d√©sign√©s comme √©tant du bruit de fond, num√©rot√© 0. Le dendrogramme non condens√© peu √™tre produit.

```{r}
plot(mcinnes_hdbscan$hdbscan_tree)
```

Difficile d'y voir clair avec autant d'objets. L'objet `mcinnes_hdbscan` a un nombre minimum d'objets par groupe de 20. Ce qui permet de pr√©senter le dendrogramme de mani√®re condens√©e.

```{r, fig.width=15, fig.height=7}
plot(mcinnes_hdbscan)
```

Enfin, un aper√ßu des strat√©gies de partitionnement utilis√©s jusqu'ici.

```{r, fig.width=8, fig.height=8}
clustering_group <- df_mcinnes %>% 
  mutate(kmeans_group,
         hclust_group,
         dbscan_group,
         hdbscan_group) %>% 
  gather(-x, -y, key = "method", value = "cluster")
clustering_group$cluster <- factor(clustering_group$cluster)
clustering_group %>% 
  ggplot(aes(x = x, y = y)) +
  geom_point(aes(colour = cluster)) +
  facet_wrap(~method, ncol = 2) +
  coord_equal() +
  theme_bw()
```

Clairement, le partitionnement avec HDBSCAN donne les meilleurs r√©sultats.

### Conclusion sur le partitionnement

Au chapitre \@ref(chapitre-visualisation), nous avons vu avec le jeu de donn√©es "datasaurus" que la visualisation peut permettre de d√©tecter des structures en segmentant les donn√©es selon des groupes.

<img src="images/07_datasaurus_mix.png" width=400>

<img src="images/07_datasaurus_facet.png">

Or, si les donn√©es n'√©taient pas √©tiquet√©es, leur structure serait ind√©tectable avec les algorithmes disponibles actuellement. Le partitionnement permet d'explorer des donn√©es, de d√©tecter des tendances et de d√©gager des groupes permettant la prise de d√©cision.

Plusieurs techniques de partitionnement ont √©t√© pr√©sent√©es. Le choix de la technique sera d√©terminante sur la mani√®re dont les groupes seront partitionn√©s. La d√©finition d'un groupe variant d'un cas √† l'autre, il n'existe pas de r√®gle pour prescrire une m√©thode ou une autre. La partitionnement hi√©rarchique a l'avantage de permetre de visualiser comment les groupes s'agglom√®rent. Parmi les m√©thodes de partitionnement hi√©rarchique disponibles, les m√©thodes bas√©es sur la densit√© permettent une grande flexibilit√©, ainsi qu'une d√©tection d'observations ne faisant partie d'aucun goupe.

## Ordination

En √©cologie, biologie, agronommie comme en foresterie, la plupart des tableaux de donn√©es comprennent de nombreuses variables: pH, nutriments, climat, esp√®ces ou cultivars, etc. L'ordination vise √† mettre de l'ordre dans des donn√©es dont le nombre √©lev√© de variables peut amener √† des difficult√©s d'appr√©ciation et d'interpr√©taion ([Legendre et Legendre, 2012](https://www.elsevier.com/books/numerical-ecology/legendre/978-0-444-53868-0)). Plus pr√©cis√©ment, le terme ordination est utilis√© en √©cologie pour d√©signer les techniques de r√©duction d'axe. L'analyse en composante principale est probablement la plus connue de ces techniques. Mais de nombreuses techniques d'ordination ont √©t√© d√©velopp√©es au cours des derni√®res ann√©es, chacune ayant ses domaines d'application.

Les techniques de r√©duction d'axe permettent de d√©gager l'information la plus importante en projetant une synth√®se des relations entre les observations et entre les variables. Les techniques ne supposant aucune structure *a priori* sont dites *non-contraignantes*: elles ne comprennent pas de tests statistiques. √Ä l'inverse, les ordinations contraignantes lient des variables descriptives avec une ou plusieurs variables pr√©dictives.

La r√©f√©rence en la mati√®re est indiscutablement ([Legendre et Legendre, 2012](https://www.elsevier.com/books/numerical-ecology/legendre/978-0-444-53868-0)). Cette section en couvrira quelques unes et vous guidera vers la technique la plus appropri√©e pour vos donn√©es.

### Ordination non contraignante

Cette section couvrira l'**analyse en composantes principales** (ACP), l'**analyse de correspondance** (AC), **l'analyse factorielle** (AF) ainsi que l'**analyse en coordonn√©es principales** (ACoP).

| M√©thode | Distance pr√©serv√©e | Variables |
|---|---|---|
| Analyse en composantes principales (ACP) | Distance euclidienne | Donn√©es quantitatives, relations lin√©aires (attention aux double-z√©ros) |
| Analyse de correspondance (AC) | Distance de $\chi^2$ | Donn√©es non-n√©gatives, dimentionnellement homog√®nes ou binaires, abondance ou occurence |
| Positionnement multidimensionnel (PoMd) | Toute mesure de dissimilarit√© | Donn√©es quantitatives, qualitatives nominales/ordinales ou mixtes |

Source: Adapt√© de ([Legendre et Legendre, 2012](https://www.elsevier.com/books/numerical-ecology/legendre/978-0-444-53868-0), chapitre 9)


#### Analyse en composantes principales

L'objectif d'une ACP est de repr√©senter les donn√©es dans un nombre r√©duit de dimensions repr√©sentant le plus possible la variation d'un tableau de donn√©es: elle permet de projetter les donn√©es dans un espace o√π les variables sont combin√©es en axes orthogonaux dont le premier axe capte le maximum de variance. L'ACP peut par exemple √™tre utilis√©e pour analyser des corr√©lations entre variables ou d√©gager l'information la plus pertinente d'un tableau de donn√©es m√©t√©o ou de signal en un nombre plus retreint de variables.

L'ACP effectue une rotation des axes √† partir du centre (moyenne) du nuage de points effectu√©e de mani√®re √† ce que le premier axe d√©finisse la direction o√π l'on retrouve la variance maximale. Ce premier axe est une combinaison lin√©aire des variables et forme la premi√®re composante principale. Une fois cet axe d√©finit, on trouve de deuxi√®me axe, orthogonal au premier, o√π l'on retouve la variance maximale - cet axe forme la deuxi√®me composante principale, et ainsi de suite jusqu'√† ce que le nombre d'axe corresponde au nombre de variables. Les projections des observations sur ces axes principaux sont appel√©s les **scores**. Les projections des variables sur les axes principaux sont les **vecteurs propres**  (*eigenvectors*, ou *loadings*). La variance des composantes principales diminue de la premi√®re √† la derni√®re, et peut √™tre calcul√©e comme une proportion de la variance totale: c'est le **pourcentage d'inertie**. Par convention, on utilise les **valeurs propres** (*eigenvalues*) pour mesurer l'importance des axes. Si la premi√®re composante principale a une inertie de 50% et la deuxi√®me a une intertie de 30%, la repr√©sentation en 2D des projection repr√©sentera 80% de la variance du nuage de points.

L'h√©t√©rog√©n√©it√© des √©chelles de mesure peut avoir une grande importance sur les r√©sultats d'une ACP (les donn√©es doivent √™tre dimensionnellement homog√®nes). En effet, la hauteur d'un ceriser aura une variance plus grande que le diam√®tre d'une cerise exprim√© dans les m√™mes unit√©s, et cette derni√®re aura plus de variance que la teneur en cuivre d'une feuille. Il est cons√©quemment avis√© de mettre les donn√©es √† l'√©chelle en centrant la moyenne √† z√©ro et l'√©cart-type √† 1 avant de proc√©der √† une ACP.

L'ACP a √©t√© con√ßue pour projetter en un nombre moindre de dimensions des observations dont les distributions sont multinormales. Bien que l'ACP soit une technique robuste, il est pr√©f√©rable de transformer pr√©alablement les variables dont la distribution est particuli√®rement asym√©triques ([Legendre et Legendre, 2012, p. 450](https://www.elsevier.com/books/numerical-ecology/legendre/978-0-444-53868-0)). Le cas √©ch√©ant, les valeurs extr√™mes pourraient faire d√©vier les vecteurs propres et biaiser l'analyse. En particulier, les donn√©es ACP men√©es sur des donn√©es compositionnelles sont r√©put√©es pour g√©n√©rer des analyses biais√©es ([Pawlowsky-Glahn and Egozcue, 2006](http://sp.lyellcollection.org/content/specpubgsl/264/1/1.full.pdf)). Le test de Mardia ([Korkmaz, 2014](https://journal.r-project.org/archive/2014-2/korkmaz-goksuluk-zararsiz.pdf)) peut √™tre utilis√© pour tester la multinormalit√©. Une distribution multinormale devrait g√©n√©rer des scores en forme d'hypersph√®re (en forme de cercle sur un biplot: voir plus loin).

##### Vecteurs propres et valeurs propres

Une matrice carr√©e (comme une matrice de covariance $\Sigma$) multipli√©e par un vecteur propre $e$ est √©gale aux valeurs propres $\lambda$ multipli√©es par les vecteurs propres $e$.

$$ \Sigma e = \lambda e $$

De mani√®re intuitive, les vecteurs propres indiquent l'orientation de la covariance, et les valeurs propres indique la longueur associ√©e √† cette direction. L'ACP est bas√©e sur le calcul des vecteurs propres et des valeurs propres de la matrice de covariance des variables. Pour d'abord obtenir les valeurs propres $\lambda$, il faut r√©soudre l'√©quation

$$ det(cov(X) - \lambda I) = 0 $$, 

o√π $det$ est l'op√©ration permettant de calculer le d√©terminant, $cov$ est l'op√©ration pour calculer la covariance, $X$ est la matrice de donn√©es, $\lambda$ sont les valeurs propres et $I$ est une matrice d'identit√©.

Pour $p$ variables dans votre tableau $X$, vous obtiendrex $p$ valeurs propres. Ensuite, on trouve les vecteurs propres en r√©solvant l'√©quation $ \Sigma e = \lambda e $.

Bien qu'il soit possible d'[effectuer cette op√©ration √† la main](https://www.youtube.com/watch?v=2fCBE7DWgd0&list=PLBv09BD7ez_5_yapAg86Od6JeeypkS4YM) pour des cas tr√®s simples, vous aurez avantage √† utiliser un langage de programmation.

Chargeons les donn√©es d'iris, puis isolons seulement les deux dimensions des s√©pales l'esp√®ce *setosa*.

```{r}
data("iris")
setosa_sepal <- iris %>% 
  filter(Species == "setosa") %>% 
  select(starts_with("Sepal"))
setosa_sepal
```

```{r}
library("MVN")
setosa_sepal_mvn <- mvn(setosa_sepal, mvnTest = "mardia")
setosa_sepal_mvn$multivariateNormality
```

Pour consid√©rer la distribution comme multinormale, la p-value de la distortion (`Mardia Skewness`) **et** la statistique de Kurtosis (`Mardia Kurtosis`) doit √™tre √©gale ou plus √©lev√©e que 0.05 ([Kormaz, 2019, fiche d'aide de la fonction `mvn` de R](https://www.rdocumentation.org/packages/MVN/versions/5.6/topics/mvn)). C'est bien le cas pour les donn√©es du tableau `setosa_sepal`.

Retirons de la matrice de covariance les valeurs et vecteurs propres avec la fonction `eigen`.

```{r}
setosa_eigen <- eigen(cov(setosa_sepal))
setosa_eigenval <- setosa_eigen$values
setosa_eigenvec <- setosa_eigen$vectors
```

Le premier vecteur propre correspond √† la premi√®re colonne, et le second √† la deuxi√®me. Les coordonn√©es x et y sont les premi√®res et deuxi√®mes lignes. Les vecteurs propres ont une longueur unitaire (norme de 1). Ils peuvent √™tre mis √† l'√©chelles √† la racine carr√©e des valeurs propres.

```{r}
setosa_eigenvec_sc <- setosa_eigenvec %*% diag(sqrt(setosa_eigen$values))
```

Pour effectuer une translation des vecteurs propres au centre du nuage de point, nous avons besoin du centro√Øde.

```{r}
centroid <- setosa_sepal %>% apply(., 2, mean)
```



```{r}
plot(setosa_sepal, asp = 1)

# vecteurs propres brutes
lines(x=c(centroid[1], centroid[1] + setosa_eigenvec[1, 1]),
      y=c(centroid[2], centroid[2] + setosa_eigenvec[2, 1]), col = "green", lwd = 3) # vecteur propre 1
lines(x=c(centroid[1], centroid[1] + setosa_eigenvec[1, 2]),
      y=c(centroid[2], centroid[2] + setosa_eigenvec[2, 2]), col = "green", lwd = 3) # vecteur propre 1

# vecteurs propres √† l'√©chelle
lines(x=c(centroid[1], centroid[1] + setosa_eigenvec_sc[1, 1]),
      y=c(centroid[2], centroid[2] + setosa_eigenvec_sc[2, 1]), col = "red", lwd = 4) # vecteur propre 1
lines(x=c(centroid[1], centroid[1] + setosa_eigenvec_sc[1, 2]),
      y=c(centroid[2], centroid[2] + setosa_eigenvec_sc[2, 2]), col = "red", lwd = 4) # vecteur propre 1

points(x=centroid[1], y=centroid[2], pch = 16, cex = 2, col  ="blue") # centroid
```

On peut observer que, comme je l'ai mentionn√© plus haut, **les vecteurs propres indiquent l'orientation de la covariance, et les valeurs propres indique la longueur associ√©e √† cette direction**.

##### Biplot

Imaginez un nuage de points en 3D, axes y compris. Vous tournez votre nuage de points pour trouver la perspective en 2D qui fera en sorte que vos donn√©es soient les plus dispers√©es possibles. Avec une lampe de poche, vous illuminez votre nuage de points dans l'axe de cette perspective: vous venez d'effectuer une analyse en composantes principales, et l'ombre des points et des axes sur le mur formera votre biplot.

Pour cr√©er un biplot, on juxtapose les descripteurs (variables) en tant que vecteurs propres, repr√©sent√©s par des fl√®ches, et les objets (observations) en tant que scores, repr√©sent√©s par des points. Les r√©sultats d'une ordination peuvent √™tre pr√©sent√©s selon deux types de biplots ([Legendre et Legendre, 2012](https://www.elsevier.com/books/numerical-ecology/legendre/978-0-444-53868-0)).

![](images/07_bipolot-meteo-sc2.svg)

<center>Biplot de corr√©lation permettant de visualiser les corr√©lations entre des variables m√©t√©orologiques. Source: [Parent, 2017]()</center>

Deux types de projection sont courramment utilis√©s.

**Biplot de distance**. Ce type de projection permet de visualiser la position des objets entre eux et par rapport aux descripteurs et d'appr√©cier la contribution des descripteurs pour cr√©er les composantes principales. Pour cr√©er un biplot de distance, on projette directement les vecteurs propres ($U$) en guise de descripteurs. Pour ce qui est des objets, on utilise les scores de l'ACP ($F$). De cette mani√®re, 

1. les distances euclidiennes entre les scores sont des approximations des distances euclidiennes dans l'espace multidimentionnel,
2. la projection d'un objet sur un descripteur perpendiculairement √† ce dernier est une approximation de la position de l'objet sur le descripteur et
3. la projection d'un descripteur sur un axe principal est proportionnelle √† sa contribution pour g√©n√©rer l'axe.

**Biplot de corr√©lation**. Cette projection permet d'appr√©cier les corr√©lations entre les descripteurs. Pour ce faire, les objets et les valeurs propres doivent √™tre transform√©s. Pour g√©n√©rer les descripteurs, les vecteurs propres ($U$) doivent √™tre multipli√©s par la matrice diagonalis√©e de la racine carr√©e des valeurs propres ($\Lambda$), c'est-√†-dire $U \Lambda ^{\frac{1}{2}}$. En ce qui a trait aux objets, on multiplie les scores par ($F$) par la racine carr√©e n√©gative des valeurs propres diagonalis√©es, c'est-√†-dire $F \Lambda ^{- \frac{1}{2}}$. De cette mani√®re, 

1. tout comme c'est le cas pour le biplot de distance, la projection d'un objet sur un descripteur perpendiculairement √† ce dernier est une approximation de la position de l'objet sur le descripteur,
2. la projection d'un descripteur sur un axe principal est proportionnelle √† son √©cart-type et
3. les **angles** entre les descripteurs sont proportionnelles √† leur corr√©lation (et non pas leur proximit√©).

En d'autres mots, le bilot de distances devrait √™tre utilis√© pour appr√©cier la distance entre les objets et le biplot de corr√©lation devrait √™tre utilis√© pour appr√©cier les corr√©lations entre les descripteurs. Mais dans tous les cas, **le type de biplot utilis√© doit √™tre indiqu√©**.

Le *triplot* est une forme apparent√©e au biplot, auquel on ajoute des variables pr√©dictives. Le triplot est utile pour repr√©senter les r√©sultats des ordinations contraignantes comme les analyses de redondance et les analyse de correspondance canoniques.

##### Application

Bien que l'ACP puisse √™tre effectu√©e gr√¢ce √† des modules de base de R, nous utiliserons le module vegan. Le tableau [`varechem`](https://rdrr.io/rforge/vegan/man/varechem.html) comprend des donn√©es issues d'analyse de sols identifi√©s par leur composition chimique, leur pH, leur profondeur totale et la profondeur de l'humus publi√©es dans [V√§re et al. (1995)](http://onlinelibrary.wiley.com/doi/10.2307/3236351/abstract) et export√©es du module [vegan](https://rdrr.io/rforge/vegan/).

```{r}
library("vegan")
data("varechem")
varechem %>% 
  sample_n(5)
```

Comme nous l'avons vu pr√©cdemment, les donn√©es de concentration sont de type *compositionnelles*. Les donn√©es compositionnelles du tableau `varechem` m√©riteraient d'√™tre transform√©es ([Aitchison et Greenacre, 2002](http://doi.wiley.com/10.1111/1467-9876.00275)). Utilisons les log-ratios centr√©s (*clr*).

```{r}
library("compositions")
varecomp <- varechem %>%
  select(-Baresoil, -Humdepth, -pH) %>% 
  mutate(Fv = apply(., 1, function(x) 1e6 - sum(x)))
vareclr <- varecomp %>%
  acomp(.) %>%
  clr(.) %>% 
  as_tibble() %>% 
  bind_cols(varechem %>%
              select(Baresoil, Humdepth, pH))
vareclr %>% 
  sample_n(5)
```

Effectuons l'ACP. Pour cet exemple, nous standardiserons les donn√©es √©tant donn√©es que les colonnes Baresoil, Humedepth et pH ne sont pas √† la m√™me √©chelle que les colonnes des clr.

```{r}
vareclr_sc <- scale(vareclr)
vare_pca <- rda(vareclr_sc) # ou bien rda(vareclr, scale = TRUE, mais la mise √† l'√©chelle pr√©alable est plus explicite)
```

L'objet `vareclr_pca` contient l'information n√©cessaire pour mener notre ACP.

```{r}
summary(vare_pca, scaling = 2) # scaling = 2 pour obtenir les infos pour les biplots de corr√©lation
```

La deuxi√®me ligne de `Importance of components`, `Proportion Explained`, indique la proportion de la variance totale capt√©e successivement par les axes principaux. Le premier axe principal comporte 47.68% de la variance. Le deuxi√®me axe principal ajoutant une proportion de 16,51%, une repr√©sentation en deux axes principaux pr√©sentent 64.19 % de la variance.

```{r}
prop_expl <- vare_pca$CA$eig / sum(vare_pca$CA$eig)
prop_expl
```


La d√©cision du nombre d'axes principaux √† retenir est arbitraire. Elle peut d√©pendre d'un nombre maximal de param√®tre √† retenir pour √©viter de surdimensionner un mod√®le (*curse of dimensionality*, section 11) ou d'un seuil de pourcentage de variance minimal √† retenir, par exemple 75%. Ou bien, vous retiendrez deux composantes principales si vous d√©sirez pr√©senter un seul biplot. 

L'approche de *Kaiser-Guttmann* ([Borcard et al., 2011](http://www.springer.com/us/book/9781441979759)) consiste √† s√©lectionner les composantes principales dont la valeur propre est sup√©rieure √† leur moyenne.

```{r}
plot(x = 1:length(vare_pca$CA$eig),
     y = vare_pca$CA$eig,
     type = "b",
     xlab = "Rang de la valeur propre",
     ylab = "Valeur propre")
abline(h = mean(vare_pca$CA$eig), col = "red", lty = 2)
```

L'approche du *broken stick* consiste √† couper un b√¢ton d'une longueur de 1 en n tranches. La premi√®re tranche est de longueur $\frac{1}{n}$. La tranche suivante est d'une longueur de la tranche pr√©c√©dente √† laquelle on aditionne  $\frac{1}{longueur~restante}$. Puis on place les longueurs en ordre d√©croissant. On retient les composantes principales dont les valeurs propres cumul√©es sont plus grandes que le broken stick.


```{r}
broken_stick <- function(x) {
  bsm <- vector("numeric", length = x)
  bsm[1] <- 1/x
  for (i in 2:x) {
    bsm[i] <- bsm[i-1] + 1/(x+1-i)
  }
  bsm <- rev(bsm/x)
  return(bsm)
}

```

Le graphique du *broken stick*:

```{r}
plot(x = 1:length(vare_pca$CA$eig),
     y = prop_expl,
     type = "b",
     xlab = "Rang de la valeur propre",
     ylab = "Valeur propre")
lines(x = 1:length(vare_pca$CA$eig),
      y = broken_stick(length(vare_pca$CA$eig)),
      col = "red",
      lty = 2)
```

Les approches *Kaiser-Guttmann* et *broken stick* sugg√®rent que les trois premi√®res composantes sont suffisantes pour d√©crire la dispersion des donn√©es.

Examinons les loadings (vecteurs propres) plus en particulier. Dans le langage du module vegan, les vecteurs propres sont les esp√®ces (`species`) et les scores sont les `sites`.

```{r}
vare_eigenvec <- vegan::scores(vare_pca, scaling = 2, display = "species", choices = 1:(ncol(vareclr)-1))
vare_eigenvec
```

L'ordre d'importance des vecteurs propres est √©tabli en ordre croissant des √©l√©ment des vecteurs propres associ√©es. Un vecteur propre est une combinaison lin√©aire des variables. Par exemple, le premier vecteur propre pointe surtout dans la direction du Fe (-1.497) et de l'Al (-1.463). Le deuxi√®me pointe surtout vers le Mo (2.145). Les vecteurs (*loadings*) d'un biplot de distance pr√©sentant les des deux premi√®res composantes principales prendront les coordonn√©es des deux premi√®res colonnes. Le vecteur Al aura la coordonn√©e [-1.463 ; -0.601], le vecteur de Fe sera plac√© √† [-1.497 ; -0.606] et le vecteur Mo √† [-0.312 ; 2.145]. Il existe diff√©rentes fonctions d'affichage des biplots. Notez que leur longueur peut √™tre magnifi√©e pour am√©liorer la visualisation.

Lan√ßons la fonction `biplot` pour cr√©er un biplot de distance et un autre de corr√©lation.

```{r}
par(mfrow = c(1, 2))
biplot(vare_pca, scaling = 1, main = "Biplot de distance")
biplot(vare_pca, scaling = 2, main = "Biplot de corr√©lation")
```

Le biplot de distance permet de d√©gager les variables qui expliquent davantage la variabilit√© dans notre tableau: les clr du Fe et de l'Al forment en grande partie le premier axe principal, alors que le clr du Mo forme en grande partie le second axe. Le biplot de corr√©lation montre que les clr du Fe et du Al sont corr√©l√©s dans le m√™me sens, mais das le sens contraire du clr du Mn. L'information sur la teneur en Fe et celle de l'Al est en grande partie redondante. Toutefois, le clr du Mo est presque ind√©pendant du clr du Fe, ceux-ci √©tant √† angle presque droit (~90¬∞). Ces relations peuvent √™tre explor√©es directement.

```{r}
par(mfrow = c(1, 2))
plot(vareclr$Al, vareclr$Fe)
plot(vareclr$Mo, vareclr$Fe)
```

Nous avons mentionn√© que l'ACP est une rotation. Prenons un second exemple pour bien en saisir les tenants et aboutissants. Le tableau de donn√©es que nous chargerons provient d'un infographie d'un dauphin, intitull√©e *Bottlenose Dolphin*, con√ßu par l'artiste [Tarnyloo](https://www.blendswap.com/blends/view/83681). Les points correspondent √† la surface d'un dauphin. J'ai ajout√© une colonne `anatomy`, qui indique √† quelle partie anatomique le point appartient.

```{r}
dolphin <- read_csv("data/07_dolphin.csv")
dolphin %>% sample_n(5)
```

Voici en vue isom√©trique ce en quoi consiste ce nuage de points.

```{r}
library("scatterplot3d")
scatterplot3d(x = dolphin$x, y = dolphin$y, z = dolphin$z, pch = 16, cex.symbols = 0.2)
```

Effectuons l'ACP sur le dauphin.

```{r}
dolph_pca <- rda(dolphin %>% select(x, y, z), scale = FALSE)
biplot(dolph_pca, scaling = 2)
```

On n'y voit pas grand chose, mais si l'on extrait les scores et que l'on raccourcit les vecteurs:

```{r}
dolph_scores <- vegan::scores(dolph_pca, display = "sites")
dolph_loads <- vegan::scores(dolph_pca, display = "species")
dolph_loads

plot(dolph_scores, pch = 16, cex = 0.24, asp = 1, col = factor(dolphin$anatomy))
segments(x0 = rep(0, 3), y0 = rep(0, 3),
         x = dolph_loads[, 1]/50,
         y = dolph_loads[, 2]/50,
         col = "chocolate", lwd = 4)
```

La meilleure repr√©sentation du dauphin en 2D, selon la variance, est son profil - en effet, il est plus long et haut que large.

<div class="alert alert-block alert-info">**Note**. Une ACP effectue seulement une rotation des points. Les distances euclidiennes entre les points sont maintenues.</div>

<div class="alert alert-block alert-info">**Note**. L'ACP a √©t√© con√ßue pour projetter en un nombre moindre de dimensions des observations dont les distributions sont multinormales (ce n'est √©videmment pas le cas du dauphin).</div>

<div class="alert alert-block alert-info">**Note**. Les axes principaux d'une ACP sont des variables al√©atoires. Elles peuvent √™tre assujetties √† des tests ststistiques, des mod√®les, du partitionnement de donn√©es, etc.</div>

<div class="alert alert-block alert-success">**Excercice**. Effectuez maintenant une ACP avec les donn√©es d'iris.</div>

#### Analyse de correspondance (AC)

L'analyse de correspondance (AC) est particuli√®rement appropri√©e pour traiter des donn√©es d'abondance et d'occurence. Tout comme l'analyse en composantes principales, les donn√©es apport√©s vers une AC doivent √™tre dimensionnellement homog√®nes, c'est-√†-dire que chaque variable doit √™tre de m√™me m√©trique: pour des donn√©es d'abondance, cela signifie que les d√©comptes r√©f√®rent tous au m√™me concept: individus, colonies, surfaces occup√©es, etc. Alors que la distance euclidienne est pr√©serv√©e avec l'ACP, l'AC pr√©serve la distance du $\chi^2$, qui est insensible aux double-z√©ros.

L'AC produit $min(n,p)-1$ axes principaux orthogonaux qui captent non pas le maximum de variance, mais la proportion de mesures aux carr√© par rapport √† la somme des carr√©s de la matrice. Le biplot obtenu peut √™tre pr√©sent√© sous forme de biplot de site (*scaling 1*), o√π la distance du $\chi^2$ est pr√©serv√©e entre les sites ou biplot d'esp√®ces (*scaling 2*), ou la distance du $\chi^2$ est pr√©serv√©e entre les esp√®ces. L'AC h√©rite du coup une propri√©t√© importate de la distance du $\chi^2$, qui accorde davantage de distance entre un compte de 0 et de 1 qu'entre 1 et 2, et davantage entre 1 et 2 qu'entre 2 et 3.

Par exemple, sur ces trois sites, on a compt√© un individu A de moins que d'individu B.

```{r}
abundance_0123 = tibble(Site = c("Site 1", "Site 2", "Site 3"),
                        A = c(0, 1, 9),
                        B = c(1, 2, 10))
abundance_0123
```

Pourtant, la distance du $\chi^2$ est plus √©lev√©e entre le site 1 et le site 2 qu'entre le site 2 et le site 3.

```{r}
dist(decostand(abundance_0123 %>% select(-Site), method="chi.square"))
```

La distance du $\chi^2$ donne davantage d'importance aux esp√®ces rares, ce dont une analyse doit tenir compte. Il pourrait √™tre envisageable de retirer d'un tableau des esp√®ces rare, ou bien pr√©transformer des donn√©es d'abondance par une transformation de chord ou de Hellinger (tel que discut√© au chapitre 6), puis proc√©der √† une ACP sur ces donn√©es ([Legendre et Gallagher, 2001](https://doi.org/10.1007/s004420100716)).

##### Application

Le tableau [`varespec`](https://rdrr.io/rforge/vegan/man/varechem.html) comprend des donn√©es de surface de couverture de 44 esp√®ces de plantes en lien avec les donn√©es environnementales du tableau `varechem`. Ces donn√©es ont √©t√© publi√©es dans [V√§re et al. (1995)](http://onlinelibrary.wiley.com/doi/10.2307/3236351/abstract) et export√©es du module [vegan](https://rdrr.io/rforge/vegan/).

```{r}
data("varespec")
varespec %>%sample_n(5)
```

Pour effectuer l'AC, nous utiliserons, comme pour l'ACP, le module vegan mais cette fois-ci avec la fonction `cca`. L'AC en *scaling 1* est effectu√©e sur le tableau des abondances avec les esp√®ces comme colonnes et les sites comme lignes. Les matrices d'abondance transpos√©es indique les sites o√π chque esp√®ce ont √©t√© d√©nombr√©es: pour une analyse en *scaling 2*, on effectue une analyse de correspondance sur la matrice d'abondance (ou d'occurence) transpos√©e.

Pour chacune des AC, je filtre pour m'assurer que toutes les lignes contiennent au moins une observation. Ce n'est pas n√©cessaire dans notre cas, mais je le laisse pour l'exemple.

```{r}
vare_cca <- cca(varespec %>% filter(rowSums(.) > 0))
summary(vare_cca, scaling = 1)
```

```{r}
varespec_eigenval <- eigenvals(vare_cca, scaling = 1)

prop_expl <- varespec_eigenval / sum(varespec_eigenval)

par(mfrow = c(1, 2))
plot(x = 1:length(varespec_eigenval),
     y = vare_cca$CA$eig,
     type = "b",
     xlab = "Rang de la valeur propre",
     ylab = "Valeur propre")
abline(h = mean(varespec_eigenval), col = "red", lty = 2)

plot(x = 1:length(varespec_eigenval),
     y = prop_expl,
     type = "b",
     xlab = "Rang de la valeur propre",
     ylab = "Valeur propre")
lines(x = 1:length(varespec_eigenval),
      y = broken_stick(length(varespec_eigenval)),
      col = "red",
      lty = 2)
```

Cr√©ons les biplots.

```{r, fig.height=5, fig.width=10}
par(mfrow = c(1, 2))
plot(vare_cca, scaling = 1, main = "Biplot des esp√®ces")
plot(vare_cca, scaling = 2, main = "Biplot des sites")
```

Le **biplot des esp√®ces**, √† gauche (`scaling = 1`), montre la distribution des sites selon les esp√®ces. Les emplacements des scores (en noir) montrent les contrastes entre sites selon les esp√®ces qui les recouvrent. Les sites 14 et 15, par exemple, contrastent les sites 19, 20, 21 et 22 selon le 2i√®me axe principal. Par ailleurs, les axes principaux sont form√© de plusieurs esp√®ces dont aucune ne domine clairement. 

Le **biplot des sites**, √† droite (`scaling = 2`), montre la distribution des recouvrements d'esp√®ces selon les sites. Par exemple, les esp√®ces Betupube ([*Betula pubescens*](https://fr.wikipedia.org/wiki/Betula_pubescens)) et Barbhatc ([*Barbilophozia hatcheri *](https://en.wikipedia.org/wiki/Marchantiophyta)) se recouvrent en particulier le site 24. Le site 1 est difficile √† identifier, car il est couvert par plusieurs noms d'esp√®ces, au bas au centre. Les sites 3 et 13 se confondent avec Dicrsp (une esp√®ce de [*Dicranum*](https://en.wikipedia.org/wiki/Dicranum)) qui le recouvre amplement.

Pour les deux types de biplot, les sites o√π les esp√®ces situ√©s pr√®s de l'origine, car ils peuvent √™tre soit pr√®s de la moyenne, soit distribu√©s uniform√©ment.

Le nombre de composantes √† retenir peut √™tre √©valu√© par les approches *Kaiser-Guttmann* et *broken-stick*.

```{r, fig.width=10, fig.height=4}
scaling <- 1
varespec_eigenval <- eigenvals(vare_cca, scaling = scaling) # peut √™tre effectu√© sur les deux types de scaling

prop_expl <- varespec_eigenval / sum(varespec_eigenval)

par(mfrow = c(1, 2))
plot(x = 1:length(varespec_eigenval),
     y = vare_cca$CA$eig,
     type = "b",
     xlab = "Rang de la valeur propre",
     ylab = "Valeur propre",
     main = paste("Eigenvalue - Kaiser-Guttmann, scaling =", scaling))
abline(h = mean(varespec_eigenval), col = "red", lty = 2)

plot(x = 1:length(varespec_eigenval),
     y = prop_expl,
     type = "b",
     xlab = "Rang de la valeur propre",
     ylab = "Valeur propre",
     main = paste("Proportion - broken stick, scaling =", scaling))
lines(x = 1:length(varespec_eigenval),
      y = broken_stick(length(varespec_eigenval)),
      col = "red",
      lty = 2)
```

Pour les deux scalings, l'approche *Kaiser-Guttmann* propose 7 axes, tandis que l'approche *broken-stick* en propose 5.

Les repr√©sentations biplot d'analyse de correspondance peuvent prendre la forme d'un boomerang, en particulier celles qui sont bas√©es sur des donn√©es d'occurence. Le tableau suivant initialement de [Chessel et al. (1987)](http://pbil.univ-lyon1.fr/R/pdf/pps047.pdf) et est distribu√© dans le module ade4.

```{r}
library("ade4")
data("doubs")
fish <- doubs$fish
doubs_cca <- cca(fish %>% filter(rowSums(.) > 0))
plot(doubs_cca, scaling = 2)
```

Les num√©ros de sites correspondent √† la position dans une rivi√®re, 1 √©tant en amont et 30 en aval. Le premier axe discrimine l'amont et l'aval, tandis que le deuxi√®me montre deux niches en amont. Bien que l'on observe une discontinuit√© dans le cours d'eau, il y a une continuit√© dans les abondances. Cet effet peut √™tre corrig√© en retirant la tendance de l'analyse de correspondance par une *detrended correspondance analysis*. Pour cela, il faudra utiliser la fonction `decorana`, ce qui ne sera pas couvert ici.

L'*analyse des correspondances multiples* (ACM) est utile pour l'ordination des donn√©es cat√©gorielles. Le module [*ade4*](https://larmarange.github.io/analyse-R/analyse-des-correspondances-multiples.html#acm-avec-ade4) est en mesure d'effectuer des AMC, mais n'est pas couvert dans ce manuel.

<div class="alert alert-block alert-success">**Excercice**. Effectuez et analysez une AC avec les donn√©es de recouvrement `varespec`.</div>

#### Positionnement multidimensionnel (PoMd)

Le positionnement multidimensionnel (PoMd), ou [*manifold analysis*](http://scikit-learn.org/stable/modules/manifold.html), se base sur les assiciations entre les objets (mode Q) ou les variables (mode R) pour en r√©duire les dimensions. Alors que l'analyse en composantes principales conserve la distance euclidienne et que l'analyse de correspondance conserve la distance du $\chi^2$, le PoMd conserve l'association que vous s√©lectionnerez √† votre convenance. Le PoMd vise √† repr√©senter en un nombre limit√© de dimensions (souvent 2) la distance (ou dissimilarit√©) qu'ont les objets (ou des variables) les uns par rapport aux autres dans l'espace multidimensionnel.

Il existe deux types d'AEM. Le **PoMd-m√©trique** (*metric multidimentional scaling* MMDS, parfois le *metric* est retir√©, MDS, et parfois l'on parle de *classic MDS*) vise √† repr√©senter fid√®lement la distance entre les objets ou les variables. Le PoMd-m√©trique ne devrait √™tre utilis√©e que lorsque la m√©trique n'est ni euclidienne, ni de $\chi^2$ et que l'on d√©sire pr√©server les distances entre les objets. L'PoMd-m√©trique aussi appel√©e *analyse en coordonn√©es principales* (ACoP ou de l'anglais *PCoA*) .

Le **PoMd-non-m√©trique** (*nonmetric multidimentional scaling*, NMDS) vise quant √† lui √† repr√©senter l'ordre des distances entre les objets ou les variables. C'est une approche par rang: le PoMd-non-m√©trique vise repr√©senter les objets sont plus proches ou plus √©loign√©es les uns des autres plut√¥t que de repr√©senter leur similarit√© dans l'espace multidimentionnelle.

L'**IsoMap**, pour *isometric feature mapping*, est une extension du PoMd qui recontruit les distances selon les points retrouv√©s dans le voisinage. Les isomaps sont en mesure d'applatir des donn√©es ayant des formes complexes.

Nous ne traitons pour l'instant que de l'PoMd-m√©trique (fonction `vegan::cmdscale`) et des PoMd-non-m√©trique (fonction `vegan::metaMDS`).

##### Application

Utilisons les donn√©es d'abondance que nous avions au tout d√©but de ce chapitre. La matrice d'association de Bray-Curtis sera utilis√©e. 

```{r, fig.width=3, fig.height=2.4}
assoc_mat <- vegdist(abundance, method = "bray")
pheatmap(assoc_mat %>% as.matrix(), cluster_rows = FALSE, cluster_cols = FALSE,
         display_numbers = round(assoc_mat %>% as.matrix(), 2))
```

Les sites 2 et 3 devraient √™tre plus pr√®s l'un et l'autre, puis les sites 3 et 4. Les autres associations sont √©loign√©s d'environ la m√™me distance. Lan√ßons le calcul de la PoMd-m√©trique.

```{r}
pcoa <- cmdscale(assoc_mat, k = nrow(abundance)-1, eig = TRUE)
spec_scores <- wascores(pcoa$points, abundance)
ordiplot(vegan::scores(pcoa), type = 't', cex = 1.5)
text(spec_scores, row.names(spec_scores), col = "red", cex = 0.75)
```

On observe en effet que les sites 2 et 3 sont les plus pr√®s. Les sites 3 et 4sont plus √©loign√©s. Les sites 1, 2 et 4 font √† peu pr√®s un triangle √©quilat√©ral, ce qui correspond √† ce √† quoi on devrait s'attendre. Les wa-scores permettent de juxtaposer les esp√®ces sur les sites, pour r√©f√©rence. Le colibri n'est pr√©sent que sur le site 2. Le site 1 est popul√© par des jaseurs et des m√©sanges, et c'est le seul site o√π l'on a observ√© une citelle. On a observ√© des chardonnerets sur les sites 2 et 3. Sur le site 4, on n'a observ√© que des bruants, que l'on a aussi observ√© ailleurs, sauf au site 2.

Le PoMd-non-m√©trique (*non metric dimensional scaling, NMDS*) fonctionne de la m√™me mani√®re que la PoMd-m√©trique, √† la diff√©rence que la distance est bas√©e sur les rangs. √Ä cet √©gard, le site 4 √† une distance de 0.76 du site 3, mais plut√¥t le deuxi√®me plus loin, apr√®s le site 2 et avant le site 1. Utilisons la fonction `metaMDS`.

```{r}
nmds <- metaMDS(assoc_mat, k = nrow(abundance)-1, eig = TRUE)
spec_scores <- wascores(nmds$points, abundance)
ordiplot(vegan::scores(nmds), type = 't', cex = 1.5)
text(spec_scores, row.names(spec_scores), col = "red", cex = 0.75)
```

Dans ce cas, entre PoMd-m√©trique et non-m√©trique, les r√©sultats peuvent √™tre interpr√©t√©s de mani√®re similaire.

En ce qui a trait au dauphin,

```{r, echo=FALSE, warning=FALSE, message=FALSE, results='hide', fig.keep='all'}
dolphin_sample <- dolphin %>% sample_n(300)
pcoa_dolphin <- cmdscale(dolphin_sample %>% select(-anatomy) %>% vegdist(method = "euclidean"))
nmds_dolphin <- metaMDS(dolphin_sample %>% select(-anatomy) %>% vegdist(method = "euclidean"), k = 2)

par(mfrow = c(1, 2))
plot(vegan::scores(pcoa_dolphin), pch = 16, col = factor(dolphin_sample$anatomy))
plot(vegan::scores(nmds_dolphin), pch = 16, col = factor(dolphin_sample$anatomy))
```

Pour plus de d√©tails, je vous invite √† vous r√©f√©rer √† [Borcard et al. (2011)](http://www.springer.com/us/book/9781441979759)) ou de consulter l'excellent site [GUSTA ME](https://mb3is.megx.net/gustame/dissimilarity-based-methods/nmds).

#### Conclusion sur l'ordination non contraignante

Lorsque les donn√©es sont euclidiennes, l'analyse en composantes principales (ACP) dervait √™tre utilis√©e. Lorsque la m√©trique est celle du $\chi^2$, on pr√©f√©rera l'analyse de correspondance (AC). Si la m√©trique est autre, le positionnement multidimensionel (PoMd) est pr√©f√©rable. Dans ce dernier cas, si l'on recherche une repr√©sentation simplifi√©e de la distance entre les objets ou variables, on utilisera un PoMd-m√©trique. √Ä l'inverse, si l'on d√©sire une repr√©sentation plus fid√®le au rang des distances, on pr√©f√©rera l'PoMd-non-m√©trique.

### Ordination contraignante

Alors que l'ordination non contraignante vous permet de dresser un protrait de vos variables, l'ordination contraignante (ou canonique) permet de tester statistiquement ainsi que de repr√©senter la relation entre plusieurs variables explicatives (par exemple, des conditions environnementales) et une ou plusieurs variables r√©ponses (par exemple, les esp√®ces observ√©es). 

- L'analyse discriminante n'a fondamentalement qu'une seulement variable r√©ponse, et celle-ci doit d√©crire l'appartenance √† une cat√©gorie.
- L'analyse de redondance sera pr√©f√©r√©e lorsque le nombre de variable est plus restreint (variables ionomiques et indicateurs de performance des cultures). Les d√©tails, ainsi que les tenants et aboutissants de ces m√©thodes, sont pr√©sent√©s dans [Numerical Ecology (Legendre et Legendre, 2012)](https://www.elsevier.com/books/numerical-ecology/legendre/978-0-444-53868-0).
- L'analyse canonique des corr√©lations sera pr√©f√©r√©e lorsque les variables sont parsem√©es (beaucoup de colonnes avec beaucoup de z√©ros, comme les variables d'abondance).

#### Analyse discriminante

Alors que l'analyse en composante principale vise √† pr√©senter la perspective (les axes) selon laquelle les points sont les plus √©clat√©es, l'analyse discriminante, le plus souvent utilis√© dans sa forme lin√©aire (ADL) et quadratique (ADQ), vise √† pr√©senter la perspective selon laquelle les groupes sont les plus √©clat√©s, les groupes formant la variable contraignante. Ces groupes peuvent √™tre connus (e.g. cultivar, r√©gion g√©ographique) ou attribu√©s (exemple: par partitionnement). L'ADL est parfois nomm√©e *analyse canonique de la variance*.

L'AD vise √† repr√©senter des diff√©rences entre des groupes aux moyens de combinaisons lin√©aires (ADL) ou quadratique (ADQ) de variables mesur√©es. Sa repr√©sentation sous forme de biplot permet d'appr√©cier les diff√©rences entre les groupes d'identifier les variables qui sont responsables de la discrimination.

<img src="images/07_ionome-revisited-figure4d.png" alt="" style="width: 600px;"/>

<center>Biplot de distance de l'analyse discriminante des ionomes d'esp√®ces de plantes √† fruits cultiv√©es sauvages et domestiqu√©es,  Source: [Parent et al. (2013)](https://doi.org/10.3389/fpls.2013.00039)</center>

L'ADL a √©t√© d√©velopp√©e par [Fisher (1936)](http://onlinelibrary.wiley.com/doi/10.1111/j.1469-1809.1936.tb02137.x/abstract), qui √† titre d'exemple d'application a utilis√© un jeu de donn√©es de dimensions d'iris collect√©es par Edgar Anderson, du Jardin botanique du Missouri, sur 150 sp√©cimens d'iris collect√©s en Gasp√©sie (Est du Qu√©bec), ma r√©gion natale (suis-je assez chauvin?). Ce jeu de donn√©es est amplement utilis√© √† titre d'exemple en analyse multivari√©e.

[Williams (1983)](http://www.jstor.org/stable/1937836) a pr√©sent√© les tenants et aboutissants de l'ADL en √©cologie. Tout comme les donn√©es passant pas une ACP doivent suivre une distribution multinormale pour √™tre statistiquement valide, les distributions des groupes dans une ADL doivent √™tre multinormales et les variances des points par groupe doivent √™tre homog√®nes... ce qui est rarement le cas en science. N√©anmoins:

> Heureusement, il y a des √©vidences dans la litt√©rature que certaines d'entre [ces r√®gles] peuvent √™tre transgress√©es mod√©r√©ment sans de grands changement dans les taux de classification. Cette conclusion d√©pends, toutefois, de la s√©v√©rit√© des transgressions, et de facteurs structueaux comme la position relative des moyennes des populations et de la nature des dispersions. - [Williams (1983)](http://www.jstor.org/stable/1937836)

L'ADL peut servir autant d'outil d'interpr√©tation que d'outil de classification, c'est √† dire de pr√©dire une cat√©gorie selon les variables (chapitre \@ref(chapitre-ml)). Dans les deux cas, lorsque le nombre de variables approchent le nombre d'observation, les r√©sultats d'une ADL risque d'√™tre difficilement interpr√©tables. Le test appropri√© pour √©valuer l'homod√©n√©it√© de la covariance est le M-test de Box. Ce test est peu document√© dans la litt√©rature, est rarement utilis√© mais a la [r√©putation](https://en.wikiversity.org/wiki/Box%27s_M) d'√™tre particuli√®rement s√©v√®re. 

Il est rare que des donn√©es √©cologiques aient des dispersions (covariances) homog√®nes. Contrairement √† l'ADL, l'ADQ ne demande pas √† ce que les dispersions (covariances) soient homog√®nes. N√©anmoins, l'ADQ ne g√©n√®re ni de scores, ni de loadings: il s'agit d'un outil pour pr√©dire des cat√©gories (classification), non pas d'un outil d'ordination.

##### Application

Utilisons les donn√©es d'iris.

```{r}
data("iris")
```

Testons la multinormalit√© par groupe. Rappelons-nous que pour consid√©rer la distribution comme multinormale, la p-value de la distortion ainsi que la statistique de Kurtosis doivent √™tre √©gale ou plus √©lev√©e que 0.05. La fonction `split` s√©pare le tableau en listes et la fonction `map` applique la fonction sp√©cifi√©e √† chaque √©l√©ment de la liste. Cela permet d'effectuer des tests de multinormalit√© sur chacune des esp√®ces d'iris.

```{r}
iris %>% 
  split(.$Species) %>% 
  map(~ mvn(.x %>% select(-Species),
            mvnTest = "mardia")$multivariateNormality)
```

Le test est pass√© pour toutes les esp√®ces. Voyons maintenant l'homog√©n√©it√© de la covariance. Pour ce faire, nous aurons besoin de la fonction `boxM`, disponible avec le module `biotools`. Pour que les covariances soient consid√©r√©es comme √©gales, la p-vaule doit √™tre sup√©rieure √† 0.05.

```{r}
library("heplots")
boxM(iris %>% select(-Species),
     group = iris$Species)
```

On est loin d'un cas o√π les distributions sont homog√®nes. Nous allons n√©anmoins proc√©der √† l'analyse discriminante avec le module ade4. Nous aurons d'abord besoin d'effectuer une ACP avec la fonction `dudi.pca` de ade4 (en sp√©cifiant une mise √† l'√©chelle), que nous projeterons en ADL avec `discrimin`.

```{r}
library("ade4")
iris_pca <- dudi.pca(df = iris %>% select(-Species),
                     scannf = FALSE, # ne pas g√©n√©rer de graphique
                     scale = TRUE)
iris_lda <- discrimin(dudi = iris_pca,
                      fac = iris$Species,
                      scannf = FALSE)
```

La visualisation peut √™tre effectu√©e directement sur l'objet issu de la fonction `discrimin`.

```{r}
plot(iris_lda)
```

Il s'agit toutefois d'une visualisation pour le diagnostic davantage que pour la publication. Si l'objectif est la pubilcation, vous pourriez utiliser la fonction `plotDA` que j'ai con√ßue √† cet effet. J'ai aussi con√ßu une [fonction similaire qui utilise le module graphique de base de R](https://raw.githubusercontent.com/essicolo/AgFun/master/plotDA_trad.R).

```{r}
source("https://raw.githubusercontent.com/essicolo/AgFun/master/plotDA_gg.R")
plotDA(scores = iris_lda$li,
       loadings = iris_lda$fa,
       fac = iris$Species,
       level=0.95,
       facname = "Species",
       propLoadings = 1) 
```

√Ä la diff√©rence de l'ACP, l'ADL maximise la s√©patation des groupes. Nous avions not√© avec l'ACP que les dimensions des p√©tales distingaient les groupes. Puisque nous avions justement des informations sur les groupes, nous aurions pu proc√©der directement √† un ADL pour obtenir des conclusions plus directes. Si la longueur des p√©tales permet de distinguer l'esp√®ce *setosa* des deux autres, la largeur des p√©tales permet de distinguer *virginica* et *versicolor*, bien que les nuages de points se superposent. De mani√®re bivari√©e, les r√©gions de confiance des moyennes des scores discriminants (petites ellipses) montrent des diff√©rence significatives au seuil 0.05. 

<div class="alert alert-block alert-success">**Excercice**. Si l'on effectuait l'ADL sur notre dauphin, avec la colonne `anatomy` comme  variable de regroupement, qu'obtiendrions-nous? Si l'on consi√®re la nageoire codale (queue) comme faisant partie du corps? Quelles sont les limitations?</div>

#### Analyse de redondance (RDA)

En anglais, on la nomme *redundancy analysis*, souvent abr√©g√©e RDA. Elle est utilis√©e pour r√©sumer les relations lin√©aires entre des variables r√©ponse et des variables explicatives. La "redondance" se situe dans l'utilisation de deux tableaux de donn√©es contenant de l'information concordante. L'analyse de redondance est une mani√®re √©l√©gante d'effectuer une r√©gresssion lin√©aire multiple, o√π la matrice de valeurs pr√©dites par la r√©gression est assujettie √† une analyse en composantes principales. Il est ainsi possible de superposer les scores des variables explicatives √† ceux des variables r√©ponse.

Plus pr√©cis√©ment, une RDA effectue les √©tapes suivantes ([Borcard et al. (2011)](http://www.springer.com/us/book/9781441979759)) entre une matrice de variables ind√©pendantes (explicatives) $X$ et une matrice de variables d√©pendantes (r√©ponse) $Y$.

##### 1. R√©gression entre $Y$ et $X$
Pour chacune des variables r√©ponse de $Y$ ($y_1$, $y_2$, , $y_j$), effectuer une r√©gression lin√©aire sur les variables explicatives $X$. 

$$\hat{y}_j = b_j + m_{1, j} \times x_1 + m_{2, j} \times x_2 + ... + m_{i, j} \times x_i$$

$$\hat{y}_j = y_j + y_{res, j}$$

Pour chaque observation ($n$), nous obtenons une s√©rie de valeurs de $\hat{y}_j$ et de $y_{res, j}$. Donc chaque cellule de la matrice $Y$ a ses pendant $\hat{y}$ et $y_{res}$. Nous obtenons ainsi une matrice de pr√©diction $\hat{Y}$ et une matrice des r√©sidus $Y_{res} = Y - \hat{Y}$.

##### 2. Analyse en composantes principales

Ensuite, on effectue une analyse en composantes principales (ACP) sur la matrice des pr√©dictions $\hat{Y}$. On obtient ainsi ses valeurs et vecteurs propres. Nommons $U$ ses vecteurs propres. Les fonctions de RDA mettent souvent ces veceturs √† l'√©chelle avant de les retourner √† l'utilisateur. En ordination √©cologique, ces vecteurs mis √† l'√©chelle sont souvent appel√©s les *scores des esp√®ces*, bien qu'il ne s'agisse pas n√©cessairement d'esp√®ces, mais plus g√©n√©ralement des variables de la matrice d√©pendante $Y$.

Il est aussi possible d'effectuer une ACP sur $Y_{res}$.

##### 3. Calculer les scores

Les vecteurs propres $U$ sont utilis√©s pour calculer les *scores des sites*, $Y \times U$, ainsi que les *contraintes de site*  $\hat{Y} \times U$.

##### Application

Nous allons utiliser la fonction `rda` du module vegan. En ce qui a trait aux donn√©es, utilisons les donn√©es varespec (matrice Y) et varechem (matrice X). La fonction `rda` peut fonctionner avec l'interface-formule de R, o√π √† gauche du `~` on retrouve le Y (la matrice de la communaut√© √©cologique, i.e. les abondances d'esp√®ces) contre le X (l), √† gauche, ce qui peut √™tre pratique pour l'analyse d'int√©ractions. Mais pour comparer deux matrices, nous pouvons d√©finir X et Y. Ce qui est m√©langeant, c'est que vegan, contrairement aux conventions, d√©fini X comme √©tant la matrice r√©ponse et Y comme √©tant la matrice explicative.

```{r, fig.height=6, fig.width=10}
vare_rda <- rda(X = varespec, Y = vareclr, scale = FALSE)
par(mfrow = c(1, 2))
ordiplot(vare_rda, scaling = 1, type = "text", main = "Scaling 1: triplot de distance")
ordiplot(vare_rda, scaling = 2, type = "text", main = "Scaling 2: triplot de corr√©lation")
```

La fonction `ordiplot` permet de cr√©er un triplot de base. La repr√©sentation des wascores est r√©put√©e plus robuste (moins susceptible d'√™tre bruit√©e), mais leur interpr√©tation porte √† confusion ([Borcard et al. (2011)](http://www.springer.com/us/book/9781441979759)).

**Triplot de distance (scaling 1)**. Les angles entre les variables explicatives repr√©sentent leur corr√©lation (non pas les variables r√©ponse). 

**Triplot de corr√©lation (scaling 2)**. Les angles entre les variables repr√©sentent leurs corr√©lation, que les variables soient r√©ponse ou explicative, ou entre variables r√©ponses et variables explicatives. Les distances entre les objets sur le triplot ne sont pas des approximation de leur distance euclidienne.

Les triplots montrent que les variables ont toutes un r√¥le important sur la dispersion des sites autours des axeds principaux. Le premier axe principal est compos√© de mani√®re plus marqu√©e par le clr de l'Al et celui du Fe. Le deuxi√®me axe principal est compos√© de mani√®re plus marqu√©e par le clr du S, du P et du K. Le triplot de corr√©lation ne pr√©sente pas de tendance appr√©ciable pour la plupart des esp√®ces, qui ne poss√®dent pas de niche particuli√®re. Toutefois, l'esp√®ce Cladstel, pr√©sente surtout dans les sites 9 et 10, est li√©e √† de basses teneurs en N et √† de faibles valeurs de Baresoil (sol nu). L'esp√®ce Pleuschr est li√©e √† des sols o√π l'on retrouve une grande √©paisseur d'humus, ainsi que des teneurs √©lev√©es en nutriment K, P, S, Ca, Mg et Zn. Elle semble appr√©cier les sols √† bas pH, mais √† faible teneur en Fe et Al. La teneur en N lui semble plus indiff√©rente (son vecteur √©tant presque perpendiculaire).

On pourra personnaliser les graphiques en extrayant les scores.

```{r, fig.height=6, fig.width=6}
scaling <- 2
sites <- vegan::scores(vare_rda, display = "wa", scaling = scaling)
species <- vegan::scores(vare_rda, display = "species", scaling = scaling)
env <- vegan::scores(vare_rda, display = "reg", scaling = scaling)

plot(0, 0, type = "n", xlim = c(-3, 5), ylim = c(-3, 4), asp = 1)
abline(h=0, v = 0, col = "grey80")
text(sites/2, labels = rownames(sites), cex = 0.7, col = "grey50")
text(species/2, labels = rownames(species), col = "green", cex = 0.7)
segments(x0 = 0, y0 = 0, x = env[, 1], y = env[, 2], col = "blue")
text(env, labels = rownames(env), col = "blue", cex = 1)

```

On pourra effectuer une analyse de Kaiser-Guttmann ou de broken-stick de la m√™me mani√®re que pr√©c√©demment. √âtant une collection de r√©gressions, une RDA est en mesure d'effectuer des tests statistiques sur les coefficients de la r√©gression en utilisant des permutations pour tester la signification des coefficients et des axes d'une RDA. On doit n√©anmoins obligatoirement effectuer la RDA avec l'interface formule. L'a variable de gauche'objet √† gauche du `~` peut √™tre une matrice ou un tableau, et celui de droite est d√©fini dans `data`. Le `.` dans l'interface formule signifie "une combinaison lin√©aire de toutes les variables, sans int√©raction".

```{r}
vare_rda <- rda(varespec ~ ., data = vareclr, scale = FALSE)
perm_test_term <- anova(vare_rda, by = "term")
#perm_test_axis <- anova(vare_rda, by = "axis")
```

La signification des axes est difficile √† interpr√©ter. Toutefois, celui des variables pr√©sente un int√©r√™t.

```{r}
perm_test_term
```

La p-value est la probabilit√© que les pentes calcul√©es pour les variables √©mergent de distributions dont la moyenne est nulle. Au seuil 0.05, les variables significatives sont (les clr de) l'azote, le phosphore, le potassium et l'aluminium.

Dans le cas des matrices d'abondance (ce n'est pas le cas de varespec, constitu√©e de donn√©es de recouvrement), il est pr√©f√©rable avec les RDA de les transformer pr√©alablement avec la transformation compositionnelle, de chord ou de Hellinger (chapitre \@ref(chapitre-explorer)). Une autre option est d'effectuer une RDA sur des matrices d'association en passant par une analyse en coordonn√©es principales ([Legendre et Anderson, 1999](http://www.jstor.org/stable/2657192 )). Enfin, les donn√©es d'abondance √† l'√©tat brutes devraient plut√¥t passer utiliser une analyse canonique des corr√©lations.

#### Analyse canonique des correspondances (ACC)

L'analyse canonique des correspondances (*Canonical correspondance analysis*), ACC, a √©t√© √† l'origine con√ßue pour √©tudier les liens entre des variables environnementales et l'abondance (d√©compte) ou l'occurence (pr√©sence-absence) d'esp√®ces ([ter Braak, 1986](https://www.ohio.edu/plantbio/staff/mccarthy/multivariate/terBraak1986.pdf)). L'ACC est √† la RDA ce que la CA est √† l'ACP. Alors que la RDA pr√©serve les distance euclidiennes entre variables d√©pendantes et indpendantes, l'ACC pr√©serve les distances du $\chi^2$. Tout comme l'AC, elle h√©rite du coup une propri√©t√© importate de la distance du $\chi^2$: il y a davantage davantage d'importance aux esp√®ces rares.

L'analyse des correspondances canoniques est souvent utilis√©e dans la litt√©rature, mais dans bien des cas une RDA sur des donn√©es d'abondance transform√©es donnera des r√©sultats davantage int√©rpr√©tables ([Legendre et Gallagher, 2001](https://doi.org/10.1007/s004420100716)).

##### Application

Cet exemple d'application concerne des donn√©es d'abondance. Nous allons cons√©quemment utiliser une CCA avec la fonction `cca`, toujours avec le module vegan.

Les tableaux [`doubs_fish` et `doubs_env`](https://rdrr.io/cran/ade4/man/doubs.html) comprennent respectivement des donn√©es d'abondance d'esp√®ces de poissons et dans diff√©rents environnements de la rivi√®re Doubs (Europe) publi√©es dans [Verneaux. (1973)](https://www.worldcat.org/title/cours-deau-de-franche-comte-massif-du-jura-recherches-ecologiques-sur-le-reseau-hydrographique-du-doubs-essai-de-biotypologie/oclc/496763306) et export√©es du module [`ade4`](https://rdrr.io/rforge/ade4/).

```{r}
data("doubs")
doubs_fish <- doubs$fish
doubs_env <- doubs$env
```

Sur le site no 8, aucun poisson n'a pas √©t√© observ√©. Les observations ne comprenant que des z√©ro doivent √™tre pr√©alablement retir√©es.

```{r}
tot_spec <- doubs_fish %>%
  transmute(tot_spec = apply(., 1, sum))
doubs_fish <- doubs_fish %>%
  filter(tot_spec != 0)
doubs_env <- doubs_env %>%
  filter(tot_spec != 0)
```

De la m√™me mani√®re qu'avec la fonction `rda` de vegan, nous utilisons `cca` pour l'ACC.

```{r}
doubs_cca <- cca(doubs_fish ~ ., data = doubs_env, scale = FALSE)
```

Comparons les r√©sultats

```{r, fig.width=10, fig.height=6}
par(mfrow = c(1, 2))
ordiplot(doubs_cca, scaling = 1, type = "text", main = "CCA - Scaling 1 - Triplot de distance")
ordiplot(doubs_cca, scaling = 2, type = "text", main = "CCA - Scaling 2 - Triplot de corr√©lation")
```

**Triplot de distance (scaling 1)**.

> (1) La projection des variables r√©ponse √† angle droit sur les variables explicatives est une approximation de la r√©ponse sur l'explication. (2) Un objet (site ou r√©ponse) situ√© pr√®s d'une variable explicative est plus susceptible d'avoir le d√©compte `1`. (3) Les distances entre les variables (r√©ponse et explicatives) approximent la distance du $\chi^2$ (traduction adapt√©e de [Borcard et al. (2011)](http://www.springer.com/us/book/9781441979759)).

**Triplot de corr√©lation (scaling 2)**. 

> (1) La valeur optmiale de l'esp√®ce sur une variable environnementale quantitative peut √™tre obtenue en projetant l'esp√®ce √† angle droit sur la variable. (2) Une esp√®ce se trouvant pr√®s d'une variable environnementale est susceptible de se trouver en plus grande abondance aux sites de statut `1` pour cette variable. (3) Les distances n'approximent pas la distance du $\chi^2$ (traduction adapt√©e de [Borcard et al. (2011)](http://www.springer.com/us/book/9781441979759)).

```{r, include=FALSE}
rm(list = ls())
```

<!--chapter:end:08_association-partition-ordination.Rmd-->

# D√©tection de valeurs aberrantes et imputation de donn√©es manquantes {#chapitre-outliers}

***
Ô∏è\ **Objectifs sp√©cifiques**:

√Ä la fin de ce chapitre, vous

- saurez comment proc√©der √† l'imputation de valeurs manquantes en mode univari√© et multivari√©
- saurez comment d√©tecter des valeurs aberrantes en mode univari√© et multivari√©

***

> **Note**. Ce chapitre a √©t√© initialement r√©dig√© par Zonlehoua Coulibali, qui a gracieusement accept√© de contribuer √† ces notes de cours. Le texte a √©t√© adapt√© au format du manuel par Serge-√âtienne Parent.

Les donn√©es √©cologiques sont g√©n√©ralement recueillies √† diff√©rentes √©chelles, concernent plusieurs sites et plusieurs variables (corr√©l√©es ou non), impliquent diff√©rents individus de diff√©rentes agences et peuvent s'√©tendre sur plusieurs ann√©es ([Alameddine et al., 2010](https://ascelibrary.org/doi/10.1061/%28ASCE%29EE.1943-7870.0000271); [Lokupitiya et al., 2006](https://doi.org/10.1002/env.773)). De ce fait, la plupart de ces bases de donn√©es contiennent des valeurs manquantes et/ou aberrantes li√©es √† diff√©rentes sources d'erreurs, pouvant parfois limiter l'utilit√© des inf√©rences statistiques ([Collins et al., 2001](https://europepmc.org/abstract/MED/11778676); [Glasson-Cicognani et Berchtold, 2010](https://hal.inria.fr/inria-00494698/document)). Il convient alors de les traiter correctement avant d'effectuer les analyses statistiques car les ignorer peut entra√Æner, outre une perte de pr√©cision, de forts biais dans les mod√®les d'analyse ([Alameddine et al., 2010](https://ascelibrary.org/doi/10.1061/%28ASCE%29EE.1943-7870.0000271); [Filzmoser et al., 2008](https://www.researchgate.net/publication/222423330_Outlier_identification_in_high_dimension); [Glasson-Cicognani et Berchtold, 2010](https://hal.inria.fr/inria-00494698/document)).

## Donn√©es manquantes: d√©finition, origine, typologie et traitement

### D√©finition

Les tableaux de donn√©es sont organis√©s en lignes et colonnes. Les lignes repr√©sentent les observations, les unit√©s, les sujets ou les cas √©tudi√©s selon le contexte, et les colonnes repr√©sentent les variables mesur√©es pour chaque observation. Les entr√©es qui sont les valeurs (ou contenus) des cellules ou encore les valeurs observ√©es, peuvent √™tre des valeurs continues, ou des valeurs cat√©goriales ([Little et Rubin, 2002](https://onlinelibrary.wiley.com/doi/book/10.1002/9781119013563)). Consid√©rant une variable al√©atoire $X$ quelconque, une donn√©e manquante $x_m$, est une donn√©e pour laquelle la valeur  de la variable $X$ est inconnue (ou absente). En d'autres termes, on ne dispose pas de la valeur de $X$ pour le sujet $i$ donn√©. C'est une donn√©e non disponible qui serait utile pour l'analyse si elle √©tait observ√©e ([Ware  et al., 2012](https://www.nejm.org/doi/full/10.1056/NEJMsm1210043)).

La litt√©rature sur les donn√©es manquantes est plus abondante dans les domaines des sciences sociales sur les donn√©es d'enqu√™tes, et des sciences m√©dicales ([Davey et al., 2001](https://www.jstor.org/stable/3069628?seq=1/subjects); [Graham, 2012](https://www.springer.com/us/book/9781461440178)). Pour repr√©senter leur r√©partition dans la table de donn√©es, une matrice indicatrice des valeurs manquantes $M = (m_{ij})$ est g√©n√©ralement utilis√©e o√π $m_{ij}$ est une variable binaire qui prend la valeur 1 si la valeur de la variable ($X$) est observ√©e et 0 si $x$ est absent ([Collins et al., 2001](https://europepmc.org/abstract/MED/11778676); [Graham, 2012](https://www.springer.com/us/book/9781461440178); [Little et Rubin, 2002](https://onlinelibrary.wiley.com/doi/book/10.1002/9781119013563)).

### Origines des donn√©es manquantes

Les donn√©es manquantes ont des origines mat√©rielles diverses. Des valeurs peuvent √™tre absentes soit parce qu'elles n'ont pas √©t√© observ√©es, ou qu'elles ont √©t√© perdues ou √©taient incoh√©rentes ([Glasson-Cicognani et Berchtold, 2010]([Glasson-Cicognani et Berchtold, 2010](https://hal.inria.fr/inria-00494698/document)). La donn√©e peut avoir √©t√©

- perdue lors de la collecte ou du processus d'enregistrement des donn√©es,
- non mesur√©e en raison du dysfonctionnement d'un √©quipement,
- non mesurable en raison de la disparition du sujet d'√©tude (mort, fugue, champ non r√©colt√©, etc.),
- √©cart√©e en raison d'une contamination,
- oubli√©e,
- non √©tudi√©e,
- etc.

### Profils des donn√©es manquantes

Les auteurs traitant des donn√©es manquantes distinguent des formes de r√©partition des donn√©es manquantes et des m√©canismes conduisant √† ces derni√®res. La r√©partition des donn√©es manquantes d√©crit les dispositions des valeurs pr√©sentes et celles qui sont manquantes dans la matrice indicatrice. Les m√©canismes √† l'origine des donn√©es manquantes d√©crivent la relation probabiliste entre les valeurs observ√©es et les valeurs manquantes de la table de donn√©es.

#### R√©partition des donn√©es manquantes

Les donn√©es manquantes se r√©partissent selon diff√©rents cas de figures ([Graham, 2012](https://www.springer.com/us/book/9781461440178); [Little et Rubin, 2002](https://onlinelibrary.wiley.com/doi/book/10.1002/9781119013563)) dont les trois principaux sont

- les valeurs manquantes univari√©es,
- les valeurs manquantes monotones et
- celles non monotones ou arbitraires.

Cette distinction est fonction de la matrice indicatrice des valeurs manquantes. Cette matrice est dite √† **valeurs manquantes univari√©es** ou de non-r√©ponse univari√©e, lorsque pour une variable donn√©e, si une observation est absente, alors toutes les observations suivantes pour cette variable sont absentes (figure \@ref(fig:mv-types)a). En exp√©rimentation agricole, ce cas de figure est qualifi√© de probl√®me de la parcelle manquante o√π, pour une raison quelconque (par exemple : une absence de germination, une destruction accidentelle d'une parcelle ou des enregistrements incorrects), un facteur √† l'√©tude est non disponible. Les **valeurs manquantes monotones** surviennent lorsque la valeur d'une variable $Y_j$ manquante pour un individu $i$ implique que toutes les variables suivantes $Y_k$ ($k > j$) sont manquantes pour cet individu (figure \@ref(fig:mv-types)b). Les **valeurs manquantes arbitraires** ou non monotones ou encore g√©n√©rales, surviennent lorsque la matrice ne dessine sp√©cifiquement aucune des formes pr√©c√©dentes (figure \@ref(fig:mv-types)c).

```{r mv-types, out.width='100%', fig.align='center', fig.cap="Exemple de profils de donn√©es manquantes", echo = FALSE}
knitr::include_graphics('images/08_mv-types.png')
```

Le module VIM permet de visualiser la structure des donn√©es manquantes.

```{r echo = FALSE}
library("VIM")
library("tidyverse")
```

Pour l'exemple, prenons le tableau `iris` puis rempla√ßons au hasard des donn√©es par des valeurs manquantes (`NA`), puis v√©rifions les proportions de donn√©es manquantes et les proportions de combinaisons de donn√©es manquantes.

```{r}
set.seed(2868374)

data("iris")
iris_NA <- iris
n_NA <- 20
row_NA <- sample(1:nrow(iris), n_NA, replace = TRUE)
col_NA <- sample(1:ncol(iris), n_NA, replace = TRUE)
for (i in 1:n_NA) iris_NA[row_NA[i], col_NA[i]] <- NA

summary(aggr(iris_NA, sortVar = TRUE))
```

Avec la fonction `matrixplot`, il est possible de visualiser les donn√©es manquantes en rouge, tandis que les donn√©es pr√©sentes prennent un niveau de gris selon leur valeur.

```{r fig.width=2, fig.height=5}
matrixplot(iris_NA)
```

#### M√©canismes conduisant aux donn√©es manquantes

Les m√©canismes conduisant aux donn√©es manquantes d√©crivent la relation entre les valeurs manquantes et celles observ√©es des variables de la table (Collins et al., 2001; [Graham, 2012](https://www.springer.com/us/book/9781461440178); [Little et Rubin, 2002](https://onlinelibrary.wiley.com/doi/book/10.1002/9781119013563)). En consid√©rant la table de donn√©e $Y = \{O,M\}$ o√π $O = \left[ o_{i, j} \right]$ repr√©sente les donn√©es observ√©es et $M = \left[ m_{i, j} \right]$ la matrice indicatrice des donn√©es manquantes, le m√©canisme √† l'origine des donn√©es manquantes est d√©fini par la distribution conditionnelle de $M$ sachant $Y$.

Lorsque la probabilit√© qu'une valeur soit manquante ne d√©pend ni des valeurs observ√©es, ni de celles manquantes, les donn√©es sont dites **manquantes compl√®tement au hasard** (* **MCAR**, missing completely at random*). La probabilit√© d'absence est donc la m√™me pour toutes les observations et elle ne d√©pend que de param√®tres ext√©rieurs ind√©pendants de cette variable (Collins et al., 2001; [Graham, 2012](https://www.springer.com/us/book/9781461440178); [Heitjan, 1997](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1380829/); [Little et Rubin, 2002](https://onlinelibrary.wiley.com/doi/book/10.1002/9781119013563); [Rubin, 1976](https://www.jstor.org/stable/2335739?seq=1#page_scan_tab_contents)). Avec de telles donn√©es (MCAR), les r√©gressions qui n'utilisent que les enregistrements complets, les moyennes des cas disponibles, les tests non-param√©triques et les m√©thodes bas√©es sur les "moments", sont toutes valides ([Heitjan, 1997](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1380829/)). Toutefois, une perte de pr√©cision est √† pr√©voir dans les r√©sultats ([Collins et al., 2001](https://europepmc.org/abstract/MED/11778676)).

Selon les m√™mes auteurs, lorsque la probabilit√© qu'une valeur soit manquante d√©pend uniquement de la composante observ√©e "O" (une ou plusieurs variables observ√©es) mais pas des valeurs manquantes elles-m√™mes, les donn√©es sont dites **manquantes au hasard** (* **MAR**: missing at random*). Dans ce cas, les m√©thodes du maximum de vraisemblance sont valides pour estimer les param√®tres du mod√®le. Les proc√©dures d'imputation multiples utilisent implicitement le m√©canisme MAR ([Collins et al., 2001](https://europepmc.org/abstract/MED/11778676); [Heitjan, 1997](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1380829/)).

Lorsque la probabilit√© qu'une valeur manque d√©pend de la valeur non observ√©e de la variable elle-m√™me ($M$), les donn√©es ne manquent pas au hasard (* **MNAR**: missing not at random*). Ce type de donn√©es ne doit pas √™tre ignor√© dans l'ajustement de mod√®les car elles induisent une perte de pr√©cision (inh√©rente √† tout cas de donn√©es manquantes) mais aussi un biais dans l'estimation des param√®tres ([Collins et al., 2001](https://europepmc.org/abstract/MED/11778676); [Heitjan, 1997](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1380829/)).

### Traitement des donn√©es manquantes

La pr√©sence de donn√©es manquantes dans une analyse peut conduire √† des estim√©s de param√®tres biais√©s, gonfler les erreurs de type I et II, baisser les performances des intervalles de confiance ([Collins et al., 2001](https://europepmc.org/abstract/MED/11778676)) et entacher la g√©n√©ralisation des r√©sultats ([Taylor et al., 2002](https://www.ncbi.nlm.nih.gov/pubmed/12370166)). Plusieurs m√©thodes existent pour calculer des estim√©s de param√®tres de mod√®les approximativement sans biais, en pr√©sence de donn√©es manquantes.

#### L'analyse des cas complets

Cette m√©thode consiste √† exclure du fichier de donn√©es tous les individus ayant au moins une donn√©e manquante ([Glasson-Cicognani et Berchtold, 2010]([Glasson-Cicognani et Berchtold, 2010](https://hal.inria.fr/inria-00494698/document)). Elle serait la plus utilis√©e pour traiter les valeurs manquantes mais n'est efficace que pour les cas de donn√©es manquant compl√®tement au hasard (MCAR) lorsque le nombre de d'observations √† √©liminer n'est pas trop important ([Davey et al., 2001](https://www.jstor.org/stable/3069628?seq=1/subjects)).

En R, de mani√®re g√©n√©rique, il est possible d'identifier une donn√©e manquante dans un tableau, une matrice ou un vecteur avec `is.na`, qui retourne un objet bool√©en (`TRUE` / `FALSE`). La fonction `any` permet d'identifier si au moins une valeur est vraie ou fausse dans un objet, alors que la fonction `all` permet d'identifier si toutes les valeurs sont vraies. On pourra v√©rifier si une ligne contient une valeur manquante avec la fonction `apply`, dans l'axe des lignes. Il faudra toutefois inverser le r√©sultat bool√©en avec un `!` pour faire en sorte que l'on √©carte les valeurs manquantes.

```{r}
row_missing <- iris_NA %>%
        filter(apply(., 1, function(x) any(is.na(x))))
row_complete <- iris_NA %>%
        filter(!apply(., 1, function(x) any(is.na(x))))
row_missing
```

Au lieu de `apply`, R fournit la fontion raccourci `complete.cases`.

```{r}
row_missing <- iris_NA %>%
        filter(complete.cases(.))
```

Le module tidyr (inclus dans tidyverse) nous facilite la vie avec la fonction `tidyr::drop_na`, qui retire toutes les lignes contenant au moins une valeur manquante.

```{r}
row_complete <- iris_NA %>%
        drop_na()
```

De m√™me, on pourra √©valuer la proportion de donn√©es manquantes.

```{r}
nrow(row_complete) / nrow(iris)
```

Ou bien, √©valuer la proportion de donn√©e manquante par groupe.

```{r}
iris_NA %>%
        group_by(Species) %>%
        summarise_each(funs(sum(is.na(.))/length(.)))
```

Pour terminer cette section, il est possible que certaines variables soient peu mesur√©es dans une √©tude. Au jugement, on pourra sacrifier une colonne contenant plusieurs donn√©es manquantes en vue de conserver des lignes.

#### L'imputation

L'imputation permet de cr√©er des bases de donn√©es compl√®tes ([Donz√©, 2001](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1012.8980&rep=rep1&type=pdf)). Elle corrige la non-r√©ponse partielle en substituant une "valeur artificielle" √† la valeur manquante. Les auteurs distinguent l'imputation unique et l'imputation multiple.

##### L'imputation unique

*L'imputation unique consiste √† remplacer chaque donn√©e manquante par une seule valeur plausible telle que la moyenne calcul√©e sur les donn√©es r√©ellement observ√©es, l'imputation par le ou les plus proche(s) voisin(s)* (la technique des plus proches voisins est couverte au chapitre \@ref(chapitre-ml)). Cette derni√®re remplace les donn√©es manquantes par des valeurs provenant d'individus similaires pour lesquels toute l'information a √©t√© observ√©e. L'imputation peut aussi se faire par r√©gression en rempla√ßant les valeurs manquantes par des valeurs pr√©dites selon un mod√®le de r√©gression ou des m√©thodes bay√©siennes plus sophistiqu√©es. L'imputation unique est valide en pr√©sence de donn√©es manquantes de type MAR ([Davey et al., 2001](https://www.jstor.org/stable/3069628?seq=1/subjects); [Donz√©, 2001](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1012.8980&rep=rep1&type=pdf); [Glasson-Cicognani et Berchtold, 2010]([Glasson-Cicognani et Berchtold, 2010](https://hal.inria.fr/inria-00494698/document)).

Selon [Heitjan (1997)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1380829/), il n'existe pas de r√®gles strictes pour d√©cider quand il faut entreprendre une imputation multiple. N√©anmoins, si la fraction des observations avec des donn√©es manquantes est inf√©rieure √† par exemple 5%, et le m√©canisme est ignorable (MCAR ou MAR), les analyses les plus simples sont satisfaisantes.

Bien que con√ßu principalement pour l'imputation multiple (on y arrive bient√¥t), le module mice permet l'imputation univari√©e. Nous allons tester l'imputation par la moyenne. Voyons par exemple la moyenne des longueurs des s√©pales.

```{r}
mean(iris_NA$Sepal.Length[!complete.cases(iris_NA)], na.rm = TRUE)
```

Lan√ßons l'imputation par la fonction `mice`, puis la pr√©diction du tableau imput√© par la fonction `complete`.

```{r message=FALSE, warning=FALSE, results=FALSE}
library("mice")
iris_mice <- mice(iris_NA, method = "mean")
iris_imp <- complete(iris_mice)
```

Le tableau original peut √™tre compar√© au tableau imput√©.

```{r}
iris_NA[!complete.cases(iris_NA), ]
iris[!complete.cases(iris_NA), ]
iris_imp[!complete.cases(iris_NA), ]
```

Dans la colonne `Sepal.Length`, toutes les valeurs manquantes ont √©t√© remplac√©es par ~5.862.

**Exercice**. Pourquoi la pr√©diction diff√®re-t-elle de la moyenne?

----

üò± **Attention**. Lorsque les valeurs sont syst√©matiquement manquantes chez une cat√©gorie, les estimateurs seront biais√©s.

```{r}
iris_NA_biais_1 <- tibble(Sepal.Length = c(5.3, NA, 4.9, NA, 4.7, NA),
                          Species = c("setosa", "versicolor", "setosa", "versicolor", "setosa", "versicolor"))
mean(iris_NA_biais_1$Sepal.Length, na.rm = TRUE)

iris_NA_biais_2 <- tibble(Sepal.Length = c(5.3, 7.0, 4.6, 6.4, 4.8, 6.9),
                          Species = c("setosa", "versicolor", "setosa", "versicolor", "setosa", "versicolor"))
mean(iris_NA_biais_2$Sepal.Length, na.rm = TRUE)
```

Dans l'exemple pr√©c√©dent, les donn√©es sont syst√©matiquement manquantes chez l'esp√®ce *versicolor*. La moyenne de la longueur des s√©pales est donc biais√©e, et l'imputation par la moyenne de sera tout autant. L'imputation par la moyenne est jug√©e non recommandable par plusieurs statisticiens. Dans la mesure du possible, **l'imputation multiple devrait √™tre favoris√©e √† l'imputation univari√©e**.

----

##### L'imputation multiple

L'imputation multiple consiste √† imputer plusieurs fois les valeurs manquantes et √† combiner les r√©sultats pour diminuer l'erreur caus√©e par la compl√©tion ([Davey et al., 2001](https://www.jstor.org/stable/3069628?seq=1/subjects)). Les valeurs manquantes sont remplac√©es par $M$ ($M > 1$) ensembles de valeurs simul√©es donnant lieu √† $M$ versions plausibles mais diff√©rentes des donn√©es compl√®tes ([Collins et al., 2001](https://europepmc.org/abstract/MED/11778676); [Taylor et al., 2002](https://www.ncbi.nlm.nih.gov/pubmed/12370166)). En pratique, seulement $M$ allant de 5 √† 10 (imputations) est suffisant pour produire des bonnes inf√©rences ([Collins et al., 2001](https://europepmc.org/abstract/MED/11778676); [Donz√©, 2001](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1012.8980&rep=rep1&type=pdf)). Chacun des $M$ ensembles de donn√©es est analys√© de la m√™me mani√®re par des m√©thodes standards d'analyse de donn√©es compl√®tes, et les r√©sultats sont combin√©s en utilisant une arithm√©tique simple: les moyennes des param√®tres estim√©s sont calcul√©es, les erreurs standards sont combin√©es pour refleter l'incertitude des donn√©es manquantes et l'erreur d'√©chantillonnage.

L'imputation multiple est une proc√©dure bas√©e sur un mod√®le (model-based). L'utilisateur doit sp√©cifier un mod√®le de probabilit√© conjointe pour les donn√©es observ√©es et manquantes ([Collins et al., 2001](https://europepmc.org/abstract/MED/11778676); [Taylor et al., 2002](https://www.ncbi.nlm.nih.gov/pubmed/12370166)).

Le module mice donne acc√®s √† plusieurs types de mod√®les (argument `method`). Les mod√®les `cart` et `rf` tombent la la cat√©gorie de l'autoapprentissage (couvert au chapitre \@ref(chapitre-ml)). Ils ont l'avantage important d'√™tre applicables autant pour tout type de variable.

```{r message=FALSE, warning=FALSE, results=FALSE}
iris_mice <- mice(iris_NA, method = "rf")
iris_imp <- complete(iris_mice)
```

De m√™me que pr√©c√©demment, le tableau original peut √™tre compar√© au tableau imput√©.

```{r}
iris_NA[!complete.cases(iris_NA), ]
iris[!complete.cases(iris_NA), ]
iris_imp[!complete.cases(iris_NA), ]
```

Mieux vauit √©viter d'imputer des donn√©es compositionnelles transform√©es (alr, clr ou ilr), car l'imputation d'une dimension transform√©e aura un impact sur tout le vecteur. Dans ce cas, vous pourriez pr√©f√©rablemen utiliser la fonction `robCompositions::impCoda`.

## Valeurs et √©chantillons aberrants: d√©finition, origines, m√©thodes de d√©tection et traitement

### D√©finitions

En analyse univari√©e, une valeur aberrante est une "donn√©e observ√©e" pour une variable qui semble anormale au regard des valeurs dont on dispose pour les autres observations de l'√©chantillon ([Planchon, 2005](https://www.researchgate.net/publication/26406403_Traitement_des_valeurs_aberrantes_concepts_actuels_et_tendances_generales)). En analyse multivari√©e, l'√©chantillon aberrant r√©sulte d'une erreur importante se trouvant dans un des composants du vecteur de r√©ponse, ou de petites erreurs syst√©matiques dans chacun de ses composants, et qui de ce fait, ne partage pas les relations entre les variables de la population ([Planchon, 2005](https://www.researchgate.net/publication/26406403_Traitement_des_valeurs_aberrantes_concepts_actuels_et_tendances_generales)).

La valeur ou l'observation aberrante est statistiquement discordante dans le contexte d'un mod√®le de probabilit√© suppos√© connu ([Barnett et Lewis, 1994](https://www.wiley.com/en-us/Outliers+in+Statistical+Data%2C+3rd+Edition-p-9780471930945); [Grubbs, 1969](https://www.tandfonline.com/doi/abs/10.1080/00401706.1969.10490657); [Munoz-Garcia et al., 1990](https://www.jstor.org/stable/1403805?seq=1#page_scan_tab_contents); [Pires et Santos-Pereira, 2005](https://www.researchgate.net/publication/239850370_Using_Clustering_and_Robust_Estimators_to_Detect_Outliers_in_Multivariate_Data)). Leur pr√©sence dans les donn√©es peut conduire √† des estimateurs de param√®tres biais√©s et, suite √† la r√©alisation de tests statistiques, √† une interpr√©tation des r√©sultats erron√©e ([Planchon, 2005](https://www.researchgate.net/publication/26406403_Traitement_des_valeurs_aberrantes_concepts_actuels_et_tendances_generales)).

### Origines

Dans une collecte de donn√©es, plusieurs sources de variabilit√© peuvent mener √† des donn√©es aberrantes: la variabilit√© inh√©rente mais inusit√©e ou erreur syst√©matique, l'erreur de mesure et l'erreur d'ex√©cution (figure \@ref(fig:va-origine)) ([Barnett et Lewis, 1994](https://www.wiley.com/en-us/Outliers+in+Statistical+Data%2C+3rd+Edition-p-9780471930945); [Planchon, 2005](https://www.researchgate.net/publication/26406403_Traitement_des_valeurs_aberrantes_concepts_actuels_et_tendances_generales)).

```{r va-origine, out.width='100%', fig.align='center', fig.cap="Sch√©ma g√©n√©ral de traitement des valeurs aberrantes - adapt√© de Barnett et Lewis, 1994", echo = FALSE}
knitr::include_graphics('images/08_origine-va.png')
```

La variabilit√© inh√©rente est celle par laquelle les observations varient naturellement de mani√®re al√©atoire √† travers la population. L'erreur de mesure renferme les inad√©quations au niveau de la m√©thode de mesure, des instruments de mesure, l'arrondi des valeurs obtenues ou les erreurs d'enregistrement. Cette erreur est donc li√©e √† des circonstances bien d√©termin√©es. Les erreurs d'ex√©cution interviennent √©galement dans des circonstances bien d√©termin√©es. Ce sont les erreurs de manipulation, les erreurs commises dans l'assemblage des donn√©es, ou lors du traitement informatique.

L'examen des valeurs aberrantes dans une base de donn√©es a pour objectif de les identifier pour soit les supprimer, soit les conserver, ou les corriger avant d'ajuster des mod√®les non robustes ([Filzmoser et al., 2008](https://www.researchgate.net/publication/222423330_Outlier_identification_in_high_dimension); [Planchon, 2005](https://www.researchgate.net/publication/26406403_Traitement_des_valeurs_aberrantes_concepts_actuels_et_tendances_generales)). La valeur extr√™me peut √™tre li√©e √† un √©v√©nement atypique, mais n√©anmoins connu et int√©ressant √† √©tudier. Dans ce cas elle est importante √† conserver. La correction (ou accommodation) √©vite le rejet des observations aberrantes et consiste √† estimer les valeurs des param√®tres de la distribution de base de fa√ßon relativement libre sans d√©formation des r√©sultats li√©s √† leur pr√©sence ([Barnett et Lewis, 1994](https://www.wiley.com/en-us/Outliers+in+Statistical+Data%2C+3rd+Edition-p-9780471930945)).

### D√©tection et traitement des √©chantillons aberrants multivari√©s

L'approche d'identification des observations aberrantes selon [Davies et Gather (1993)](https://www.tandfonline.com/doi/abs/10.1080/01621459.1993.10476339) est de supposer qu'elles ont une distribution diff√©rente de celle du reste des observations. [Reimann et al. (2005)](https://doi.org/10.1016/j.scitotenv.2004.11.023) les distinguent ainsi des valeurs extr√™mes qui, bien qu'√©loign√©es du centre du nuage, appartiennent √† la m√™me distribution que les autres observations.

En analyse univari√©e, les m√©thodes graphiques telles que le diagramme de dispersion des observations class√©es en fonction de leur rang, les boxplots, les graphiques des quantiles de valeurs brutes ou des r√©sidus, permettent de signaler la pr√©sence de valeurs aberrantes ([Planchon, 2005](https://www.researchgate.net/publication/26406403_Traitement_des_valeurs_aberrantes_concepts_actuels_et_tendances_generales)). En analyse multivari√©e, il existe deux approches fondamentales d'identification des valeurs aberrantes: celles bas√©es sur le calcul de distances et les m√©thodes par projection ([Filzmoser et al., 2008](https://www.researchgate.net/publication/222423330_Outlier_identification_in_high_dimension); Hadi et al., 2009).

#### Approches bas√©es sur les distances

##### La distance de Mahalanobis

Les m√©thodes bas√©es sur la distance d√©tectent les valeurs aberrantes en calculant la distance, g√©n√©ralement la distance de Mahalanobis (vue au chapitre \@ref(chapitre-ordination)) entre un point particulier et le centre des donn√©es ([Filzmoser et al., 2008](https://www.researchgate.net/publication/222423330_Outlier_identification_in_high_dimension); [Pires et Santos-Pereira, 2005](https://www.researchgate.net/publication/239850370_Using_Clustering_and_Robust_Estimators_to_Detect_Outliers_in_Multivariate_Data)). Pour un √©chantillon $x$ multivari√©, la distance de Mahalanobis est calcul√©e comme:

$$ \mathscr{M} = \sqrt{(\vec{x}-\vec{\mu})^T S^{-1} (\vec{x}-\vec{\mu})}.\ $$
o√π $\vec{\mu}$ est la moyenne arithm√©tique multivari√©e (le centro√Øde) et $S$ la matrice de variance-covariances de l'√©chantillon, qui doit √™tre invers√©e.

Cette distance indique √† quel point chaque observation est √©loign√©e du centre du nuage multivari√© cr√©√© par les donn√©es ([Alameddine et al., 2010](https://ascelibrary.org/doi/10.1061/%28ASCE%29EE.1943-7870.0000271); [Davies et Gather, 1993](https://www.tandfonline.com/doi/abs/10.1080/01621459.1993.10476339)). D'apr√®s [Alameddine et al. (2010)](https://ascelibrary.org/doi/10.1061/%28ASCE%29EE.1943-7870.0000271), lorsque les donn√©es sont suppos√©es suivre une distribution normale, les carr√©s des distances $\mathscr{M}$ calcul√©es peuvent √™tre consid√©r√©s comme suivant une distribution du $\chi^2$. Par convention, tout point qui a une  d√©passant un quantile donn√© de la distribution du $\chi^2$ (par exemple, $\chi^2_{df = p ; 0.975}$, le quantile 97,5% avec $p$ (le nombre de variables) degr√©s de libert√©), est consid√©r√© comme atypique et identifi√© comme une valeur aberrante ([Filzmoser et al., 2005](https://dl.acm.org/citation.cfm?id=1650448)). Les observations aberrantes multivari√©es peuvent ainsi √™tre d√©finies comme des observations ayant une grande distance de Mahalanobis ($\mathscr{M}^2$).

L'inconv√©nient avec les m√©thodes bas√©es sur les distances r√©side dans la difficult√© d'obtenir des estim√©s robuste de la moyenne $\mu$ et de la matrice de variance-covariances $S$, puisque la distance de Mahalanobis est elle-m√™me sensible aux donn√©es extr√™mes. De plus, il serait difficile de fixer la valeur critique id√©ale de $\mathscr{M}$ permettant de s√©parer les valeurs aberrantes des points r√©guliers ([Filzmoser et al., 2005](https://dl.acm.org/citation.cfm?id=1650448); [Filzmoser et al., 2008](https://www.researchgate.net/publication/222423330_Outlier_identification_in_high_dimension)).

La fonction `sign1` du module mvoutlier d√©tecte les valeurs aberrantes selon un seuil du $\chi^2_{df = 3 ; 0.975}$ pour les transformations en log-ratio isom√©triques de Al, Fe et K dans un humus (l'inverse de la matrice de covariance des les log-ratio centr√©s est singuli√®re).

```{r message=FALSE}
library("mvoutlier")
library("compositions")
data("humus")
sbp <- matrix(c(1, 1,-1,-1,
                1,-1, 0, 0,
                0, 0, 1,-1), ncol = 4, byrow = TRUE)
ilr_elements <- humus %>%
        dplyr::select(Al, Fe, K, Na) %>%
        ilr(., V = gsi.buildilrBase(t(sbp))) %>%
        as_tibble(.) %>%
        dplyr::rename(AlFe_KNa = V1,
                      Al_Fe = V2,
                      K_Na = V3)
is_out <- sign1(ilr_elements, qcrit = 0.975)$wfinal01
plot(ilr_elements, col = is_out + 2)
```

La proportion de valeurs aberrantes:

```{r}
sum(is_out == 0) / length(is_out)
```

Diff√©rentes m√©thodes *robustes* (qui s'accommodent de la pr√©sence de points extr√™mes) de d√©tection des valeurs aberrantes sont pr√©sent√©es dans la litt√©rature telles que la m√©thode du volume minimum de l'ellipso√Øde (**MVE**, *minimum volume ellipsoid*), du d√©terminant minimum de la matrice de covariance (MCD, *minimum Covariance matrix determinant*), et les estimateurs de type maximum de vraisemblance (M-estimators) ([Alameddine et al., 2010](https://ascelibrary.org/doi/10.1061/%28ASCE%29EE.1943-7870.0000271); [Filzmoser et al., 2008](https://www.researchgate.net/publication/222423330_Outlier_identification_in_high_dimension)). Ces m√©thodes calculent des distances robustes similaires aux distances de Mahalanobis, mais remplacent les matrices des moyennes et des covariances respectivement par un seuil critique multivari√© robuste (sur $\mu$) et un estimateur d'√©chelle (sur $S$) qui ne sont pas influenc√©s par les valeurs aberrantes ([Alameddine et al., 2010](https://ascelibrary.org/doi/10.1061/%28ASCE%29EE.1943-7870.0000271)).

##### La m√©thode du volume minimum de l'ellipso√Øde (MVE)

Le volume minimum de l'ellipso√Øde est le plus petit ellipso√Øde r√©gulier couvrant au moins $h$ √©l√©ments de l'ensemble des donn√©es $X = \{x_1, x_2, ..., x_n \}$ o√π l'estimateur de localisation est le centre de cet ellipso√Øde et l'estimateur de dispersion correspond √† sa matrice de covariance. $h$ est fix√© √† priori sup√©rieur ou √©gal √† $\frac{n}{2}+1$, o√π $n$ est le nombre total de points du nuage de donn√©es. Le seuil de d√©tection qui est la fraction des valeurs aberrantes qui, lorsqu'elle est d√©pass√©e entra√Æne des estim√©s totalement biais√©s est de l'ordre de 50% √† mesure que $n$ augmente ([Alameddine et al., 2010](https://ascelibrary.org/doi/10.1061/%28ASCE%29EE.1943-7870.0000271); Croux et al., 2002; [Filzmoser et al., 2005](https://dl.acm.org/citation.cfm?id=1650448); Van Aelst et Rousseeuw, 2009).

L'algorithme MVE est initi√© en choisissant au hasard un ensemble de $p+1$ points de donn√©es pour estimer le mod√®le majoritaire, o√π $p$ est le nombre de variables. Cet ensemble initial est alors augment√© pour contenir les $h$ points de donn√©es. L'algorithme passe par plusieurs it√©rations avant de converger sur l'ensemble des points les plus rapproch√©s qui auront le plus petit volume d'ellipso√Øde ([Alameddine et al., 2010](https://ascelibrary.org/doi/10.1061/%28ASCE%29EE.1943-7870.0000271)).

Le module MASS comprend la fonction `cov.mve` √† cet effet. Cette fonction demande le nombre minimal de points que l'on d√©sire conserver, en absolu. Il s'agit d'un nombre entier, alors si l'on d√©sire en utiliser une fraction (ici, 90%), il faut l'arrondir. Parmi les sorties de la fonction `cov.mve`, on retrouve les num√©ros de ligne qui se trouvent √† l'int√©rieur de l'ellipsoide.

```{r message=FALSE}
library("MASS")
select <- dplyr::select # pour √©viter que la fonction select du module MASS remplace celle de dplyr
min_in <- round(0.9 * nrow(ilr_elements)) # le minimum de points √† garder, 90% du total
id_in <- cov.mve(ilr_elements, quantile.used = min_in)$best
is_in <- 1:nrow(ilr_elements) %in% id_in
plot(ilr_elements, col = is_in + 2)
```

La proportion de valeurs aberrantes:

```{r}
sum(!is_in) / length(is_in)
```

##### La m√©thode du d√©terminant minimum de la matrice de covariance (MCD)

La m√©thode du d√©terminant minimum de la matrice de covariance a pour objectif de trouver $h$ ($h > n$) observations de l'ensemble de donn√©es $X = \{x_1, x_2, ..., x_n \}$,  dont la matrice de covariance a le plus petit d√©terminant. Comme avec la m√©thode MVE, l'estimateur de localisation est la moyenne de ces $h$ points et celui de la dispersion est proportionnel √† la matrice de covariance ([Filzmoser et al., 2005](https://dl.acm.org/citation.cfm?id=1650448); [Hubert et al., 2018](https://onlinelibrary.wiley.com/doi/full/10.1002/wics.1421); [Rousseeuw et Van Driessen, 1999](https://www.tandfonline.com/doi/abs/10.1080/00401706.1999.10485670)).

```{r message=FALSE}
id_in <- cov.mcd(ilr_elements, quantile.used = min_in)$best
is_in <- 1:nrow(ilr_elements) %in% id_in
plot(ilr_elements, col = is_in + 2)
```

La proportion de valeurs aberrantes:

```{r}
sum(!is_in) / length(is_in)
```

Mais en cas de dissym√©trie des donn√©es, ces tests (MVE, MCD) ne seraient pas applicables ([Planchon, 2005](https://www.researchgate.net/publication/26406403_Traitement_des_valeurs_aberrantes_concepts_actuels_et_tendances_generales)).

#### Les m√©thodes par projection

Ces m√©thodes de d√©tection des observations aberrantes trouvent des projections appropri√©es des donn√©es dans lesquelles les observations aberrantes sont facilement apparentes. Ces observations sont ensuite pond√©r√©s pour produire un estimateur robuste pouvant √™tre utilis√© pour identifier les observations aberrantes ([Filzmoser et al., 2008](https://www.researchgate.net/publication/222423330_Outlier_identification_in_high_dimension)). Ces m√©thodes n'assument pas une distribution particuli√®re des donn√©es mais cherchent des projections utiles. Elles ne sont donc pas affect√©es par la non-normalit√© et s'appliquent sur divers types de distributions ([Filzmoser et al., 2008](https://www.researchgate.net/publication/222423330_Outlier_identification_in_high_dimension); [Hadi et al., 2009](https://onlinelibrary.wiley.com/doi/pdf/10.1002/wics.6)). Le but de cette projection exploratoire est d'utiliser les donn√©es pour trouver des projections minimales (√† une, deux ou trois dimensions) qui fournissent les vues les plus r√©v√©latrices des donn√©es compl√®tes ([Friedman, 1987](https://amstat.tandfonline.com/doi/abs/10.1080/01621459.1987.10478427)). La m√©thode attribue un indice num√©rique √† chaque projection en fonction de la densit√© des donn√©es projet√©e pour capturer le degr√© de structure non lin√©aire pr√©sent dans la distribution projet√©e ([Friedman, 1987](https://amstat.tandfonline.com/doi/abs/10.1080/01621459.1987.10478427); [Hadi et al., 2009](https://onlinelibrary.wiley.com/doi/pdf/10.1002/wics.6)).

En R, nous revenons au module mvoutlier, mais cette fois-ci avec la fonction `sign2`.

```{r}
is_out <- sign2(ilr_elements, qcrit = 0.975)$wfinal01
plot(ilr_elements, col = is_out + 2)
```

La proportion de valeurs aberrantes:

```{r}
sum(is_out == 0) / length(is_out)
```

```{r, include=FALSE}
rm(list = ls())
```

<!--chapter:end:09_outliers-imputation.Rmd-->

# Les s√©ries temporelles {#chapitre-temps}

***
Ô∏è\ **Objectifs sp√©cifiques**:

√Ä la fin de ce chapitre, vous

- saurez comment importer et manipuler des donn√©es temporelles (utiliser le format de date, filtrer, effectuer des sommaires, agr√©ger des donn√©es, etc.)
- effectuer une r√©gression sur une s√©rie temporelle

***

Les s√©ries temporelles (ou chronologiques) sont des donn√©es associ√©es √† des indices temporels de tout ordre de grandeur: seconde, minute, heure, jour, mois, ann√©e, etc. En analyse de s√©rie temporelle, le temps est une variable explicative (ou d√©pendante) incontournable. L'√©mergence de cycles est une particularit√© des s√©ries temporelles. Ceux-ci peuvent √™tre analys√©s en vue d'en d√©terminer la tendance. Les s√©ries temporelles peuvent √©galement √™tre mod√©lis√©s en vue d'effectuer des pr√©visions.

![](https://media.giphy.com/media/495yjUBKN7TE6fr5vq/giphy.gif)

Source: Sc√®ne de Back to the future, Robert Zemeckis et and Bob Gale, 1985

Nous allons couvrir les concepts de base en analyse et mod√©lisation de s√©ries temporelles. Mais avant cela, voyons comment les donn√©es temporelles sont manipul√©es en R.

Cette section est bas√©e sur le livre *Forecasting: Principles and Practice*, de Rob J. Hyndman et George Athanasopoulos, qui peut √™tre enti√®rement [consult√© gratuitement en ligne](https://otexts.com/fpp2/), ainsi que le cours associ√© sur la plateforme d'apprentissage [DataCamp](https://campus.datacamp.com/courses/forecasting-using-r).

```{r fpp2, out.width="30%", fig.align="center", fig.cap="[*Forecasting: Principles and Practice*, de Rob J. Hyndman et George Athanasopoulos](https://otexts.com/fpp2/).", echo = FALSE}
knitr::include_graphics("images/09_fpp2_cover.jpg")
```

## Op√©rations sur les donn√©es temporelles

Le d√©bit de la [rivi√®re Chaudi√®re](https://fr.wikipedia.org/wiki/Rivi%C3%A8re_Chaudi%C3%A8re), dont l'exutoire se situe pr√®s de Qu√©bec, sur la rive Sud du fleuve Saint-Laurent, est mesur√© depuis 1915.

```{r, load-data}
library("tidyverse")
hydro <- read_csv("data/023402_Q.csv")
```

La fonction `read_csv()` d√©tecte automatiquement que la colonne `Date` est une date.

```{r, inspect-data}
glimpse(hydro)
```

Le d√©bit de la rivi√®re Chaudi√®re peut √™tre explor√© graphiquement.

```{r, plot-hydro}
hydro %>%
  ggplot(aes(x = Date, y = `D√©bit`)) +
  geom_line()
```

On observe des donn√©es sont manquantes de la fin des ann√©es 1920 √† la fin des ann√©es 1930. Autrement, il est difficile de visualiser la structure du d√©bit en fonction du temps, notamment si le d√©bit suit des cycles r√©guliers. On pourra isoler les donn√©es depuis 2014.

```{r, plot-hydro-2014}
hydro %>%
  filter(Date >= as.Date("2014-01-01")) %>%
  ggplot(aes(x = Date, y = `D√©bit`)) +
  geom_line()
```

R comprend la fonction `as.Date()`, o√π l'argument `format` d√©crit la mani√®re avec laquelle la date est exprim√©e.

```{r as-date}
as.Date(x = "1999/03/29", format = "%Y/%m/%d")
```

L'argument `x` peut aussi bien √™tre une cha√Æne de caract√®res qu'un vecteur o√π l'on retrouve plusieurs cha√Ænes de caract√®res exprimant un format de date commun. La fonction `as.Date()` permet ainsi de transformer des caract√®res en date si `read_csv()` ne le d√©tecte pas automatiquement. Ce format peut prendre la forme d√©sir√©e, dont les param√®tres sont list√©s sur la [page d'aide de la fonction `strptime()`](https://rdrr.io/r/base/strptime.html). Toutefois, le plus petit incr√©ment de temps accept√© par `as.Date()` est le jour: `as.Date()` exclut les heures, minutes et secondes. Le module [**`lubridate`**](https://lubridate.tidyverse.org/), issu du *tidyverse*, permet quant √† lui de manipuler avec plus de gr√¢ce les formats de date standards, incluant les dates et les heures: **`lubridate`** sera pr√©f√©r√© dans ce chapitre.

```{r echo = FALSE}
library("lubridate")
ymd_hms("2011-02-19 09:14:00")
```

Plusieurs autres formats standards sont pr√©sent√©s sur un [aide-m√©noire de **`lubridate`**](https://rawgit.com/rstudio/cheatsheets/master/lubridate.pdf). Si vos donn√©es comprennent des formats de date non standard, vous pourrez utiliser la fonction `as.POSIXlt()`, mais il pourrait √™tre pr√©f√©rable de standardiser les dates *a priori*.

```{r lubridate-cheatsheet, out.width="100%", fig.align="center", fig.cap="Aide-m√©moire du module [lubridate](https://rawgit.com/rstudio/cheatsheets/master/lubridate.pdf).", echo = FALSE}
knitr::include_graphics("images/09_lubridate-cheatsheet-thumbs.png")
```

Le module **`lubridate`** rend possible l'extraction de la date (`date()`), l'ann√©e (`year()`), le mois (`month()`), le jour de la semaine (`wday()`), le jour julien (`yday()`), etc. pour plus d'options, voir [l'aide-m√©moire de **`lubridate`**])(https://rawgit.com/rstudio/cheatsheets/master/lubridate.pdf).

```{r}
date_1 <- ymd_hms("2019-03-14 09:14:00")
date_1 %>% date()
date_1 %>% month()
date_1 %>% yday()
date_1 %>% wday()
date_1 %>% seconds()
```

Ces extractions peuvent √™tre utilis√©es dans des suites d'op√©ration (*pipelines*). Par exemple, si nous d√©sirons obtenir le d√©bit mensuel moyen de la rivi√®re Chaudi√®re depuis 1990, nous pouvons cr√©er une nouvelle colonne `Year` et une autre `Month` avec la fonction `mutate()`, effectuer un filtre sur l'ann√©e, regrouper par mois pour obtenir le sommaire en terme de moyenne, puis lancer le graphique.

```{r}
hydro_month <- hydro %>%
  mutate(Year = Date %>% year(),
         Month = Date %>% month()) %>%
  filter(Year >= 1990) %>%
  group_by(Month) %>%
  dplyr::summarise(MeanFlow = mean(`D√©bit`, na.rm = TRUE))

hydro_month %>%
  ggplot(aes(x=Month, y=MeanFlow)) +
  geom_line() +
  scale_x_continuous(breaks = 1:12) +
  expand_limits(y = 0)
```

On pourra aussi agr√©ger par moyenne mensuelle en gardant l'ann√©e respective en cr√©ant une nouvelle colonne de date `YearMonth` qui permettra le regroupement avec `group_by()`, puis cr√©er plusieurs facettes.

```{r fig.width=10, fig.height=3}
hydro %>%
  mutate(Year = Date %>% year(),
         Month = Date %>% month(),
         YearMonth = ymd(paste0(Year, "-", Month, "-01"))) %>%
  filter(Year >= 2010 & Year < 2018) %>%
  group_by(Year, YearMonth) %>%
  dplyr::summarise(`D√©bit` = mean(`D√©bit`, na.rm = TRUE)) %>%
  ggplot(aes(x=YearMonth, y=`D√©bit`)) +
  facet_wrap(~Year, scales = "free_x", ncol = 4) +
  geom_line() +
  expand_limits(y = 0)
```

Il est possible d'effectuer des op√©rations math√©matiques sur des donn√©es temporelles. Par exemple, ajouter 10 jours √† chaque date.

```{r}
hydro %>%
  head(5) %>%
  mutate(DateOffset = Date + days(10))
```

Pour effectuer des op√©rations sur des incr√©ments inf√©rieurs aux jours, il faut s'assurer que le type des donn√©es temporelles soit bien `POSIXct`, et non pas `Date`.

```{r}
hydro %>% pull(Date) %>% class()
hydro <- hydro %>%
  mutate(Date = as_datetime(Date))
hydro %>% pull(Date) %>% class()
hydro %>%
  head(5) %>%
  mutate(DateOffset = Date + seconds(10))
```

## Analyse de s√©ries temporelles

Tout comme c'est le cas de nombreux sujet couverts lors de ce cours, l'analyse et mod√©lisation de s√©ries temporelles est un domaine d'√©tude en soi. Nous allons nous restreindre ici aux s√©ries temporelles consign√©es √† fr√©quence r√©guli√®re. Les exemples d'analyses et mod√©lisation de s√©ries temporelles sont typiquement des donn√©es √©conomiques, bien que les principes qui les guident sont les m√™mes qu'en d'autres domaines. Cette section est vou√©e √† l'analyse, alors que la prochaine est vou√©e √† la mod√©lisation.

Par exemple, voici une s√©rie temporelle √©conomique typique, qui exprime les d√©penses mensuelles en restauration en Australie.

```{r}
library("forecast")
library("fpp2")
data("auscafe")
autoplot(auscafe)
```

On y d√©tecte une tendance g√©n√©rale, probablement propuls√©e par la croissance de la d√©mographie et des revenus, ainsi que des tendances cycliques. On verra plus loin comment pr√©dire des occurrences futures, ainsi que l'incertitude de ces pr√©dictions, √† partir des donn√©es consign√©es.

Jusqu'√† pr√©sent, nous avons travaill√© avec des tableaux de donn√©es incluant une colonne en format date. Nous allons maintenant travailler avec des s√©ries temporelles telles que repr√©sent√©es en R.

### Cr√©er et visualiser des s√©ries temporelles

L'information consign√©e dans une s√©rie temporelle inclut n√©cessairement un indice temporel associ√© √† au moins une variable. En R, cette information est consign√©e dans un objet de type `ts`, pour *time series*. Prenons une mesure quelconque prise √† chaque trimestre de l'ann√©e 2018.

```{r}
set.seed(96683)
date <- ymd(c("2018-01-01", "2018-04-01", "2018-07-01", "2018-10-01"))
mesure <- runif(length(date), 1, 10)
mesure_ts <- ts(mesure, start = date[1], frequency = 4)
mesure_ts
```

L'argument `start` est la date de la premi√®re observation et `frequency` est le nombre d'observations par unit√© temporelle, ici l'ann√©e.

J'ai auparavant recueilli des donn√©es m√©t√©o avec **`weathercan`** (dispobibles seulement depuis 1998) et fusionn√© avec le tableau `hydro`. Pour acc√©l√©rer la proc√©dure, j'ai enregistr√© les donn√©es dans un fichier RData. De facto, ne gardons que les donn√©es disponibles entre 1998 et 2008, ainsi que les colonnes d√©signant la date, le d√©bit, les pr√©cipitations totales et la temp√©rature.

```{r echo = FALSE}
#source("lib/hydrometeo.R")
load("data/09_hydrometeo.RData")
hydrometeo <- hydrometeo %>%
  filter(Date >= ymd("1998-01-01"), Date <= ymd("2008-01-01")) %>%
  dplyr::select(Date, `D√©bit`, total_precip, mean_temp)
hydrometeo %>% glimpse()
```

Pour cr√©er une s√©rie temporelle de type `ts`, j'enl√®ve la date, je d√©marre au premier √©v√©nement de 1998, et chaque incr√©ment a une fr√©quence de 1/365.25 unit√©s depuis 1998 (il y a en moyenne 365.25 jours par an).

```{r}
hydrometeo_ts <- ts(hydrometeo %>% dplyr::select(-Date),
                    start = c(hydrometeo$Date[1] %>% year(), 1),
                    frequency = 365.25)
```

Le module **`ggplot2`** comprend la fonction `autoplot()`, pratique pour visualiser les s√©ries temporelles.

```{r}
autoplot(hydrometeo_ts, facets = TRUE) +
  scale_x_continuous(breaks = 1998:2008)
```

Il est possible de filtrer des s√©ries temporelles en mode tidyverse. Toutefois, il est plus simple d'utiliser la fonction de base `windows()`. Disons, les 10 premiers jours de l'an 2000.

```{r, echo = FALSE}
window(hydrometeo_ts, start = c(2000, 1), end = c(2000, 11)) # 11, parce qu'un jour est un peu plus long pour tenir compte des ann√©es bissextiles
```

Voyons l'√©volution des d√©bits mensuelles.

```{r}
hydrometeo_monthly <- hydrometeo %>%
  mutate(Year = Date %>% year(),
         Month = Date %>% month(),
         YearMonth = ymd(paste0(Year, "-", Month, "-01"))) %>%
  group_by(Year, YearMonth) %>%
  dplyr::summarise(`D√©bit` = mean(`D√©bit`, na.rm = TRUE),
                   total_precip = sum(total_precip, na.rm = TRUE), # somme
                   mean_temp = mean(mean_temp, na.rm = TRUE)) # moyenne
hydrometeo_monthly_ts <- ts(hydrometeo_monthly %>% ungroup() %>% dplyr::select(`D√©bit`, total_precip, mean_temp), start = c(1998, 1), frequency = 12)
```

Contraignons la p√©riode gr√¢ce √† `window()`, puis visualisons les tendances cycliques avec `forecast::ggseasonplot()` et `forecast::ggsubseriesplot()`. Notez que j'utilise la fonction `cowplot::plot_grid()` pour arranger diff√©rents graphiques ggplot2 en une grille.

```{r, fig.width=15, fig.height=4}
library("cowplot")
theme_set(theme_grey()) # cowplot change le theme
ggA <- ggseasonplot(window(hydrometeo_monthly_ts[, 1], 1998, 2004-1/365.25)) + ggtitle("")
ggB <- ggseasonplot(window(hydrometeo_monthly_ts[, 1], 1998, 2004-1/365.25), polar = TRUE) + ggtitle("")
ggC <- ggsubseriesplot(window(hydrometeo_monthly_ts[, 1], 1998, 2004-1/365.25), polar = TRUE) + ggtitle("") + labs(y="Flow")
plot_grid(ggA, ggB, ggC, ncol = 3, labels = c("A", "B", "C"))
```

### Structures dans les s√©ries temporelles

Les s√©ries temporelles sont susceptibles d'√™tre caract√©ris√©es par des structures commun√©ment observ√©es.

- La **tendance** est une structure d√©crivant la hausse ou la baisse √† *long terme* d'une variable num√©rique.
- La **fluctuation saisonni√®re** est une structure p√©riodique, qui oscille autour de la tendance g√©n√©rale de mani√®re r√©guli√®re selon le calendrier.
- La **fluctuation cyclique** est aussi une structure p√©riodique, mais irr√©guli√®re (par exemple, les oscillations peuvent durer parfois 2 ans, parfois 3). Les fluctuations cycliques sont souvent de plus longue fr√©quence que les fluctuations saisonni√®res, et leur irr√©gularit√© rend les pr√©dictions plus difficiles.

**Note**. Une tendance d√©tect√©e sur une p√©riode de temps trop courte peut s'av√©rer √™tre une fluctuation.

La figure \@ref(fig:trend) montre diff√©rentes structures. La figure \@ref(fig:trend)A montre une tendance croissante des d√©penses mensuelles en restauration en Australie, ainsi que des fluctuations saisonni√®res. La figure \@ref(fig:trend)B montre des fluctuations saisonni√®res des temp√©ratures quotidiennes moyennes √† l'Universit√© Laval, sans pr√©senter de tendance claire. La figure \@ref(fig:trend)C montre des fluctuations cycliques du nombre de lynx trapp√©s par ann√©e au Canada de 1821 √† 1934, sans non plus pr√©senter de tendance claire. Les cycles sont cons√©quents des m√©canismes de dynamique des populations (plus de proie entra√Æne plus de pr√©dateur, plus de pr√©dateur entra√Æne moins de proie, moins de proie entra√Æne moins de pr√©dateur, moins de pr√©dateur entra√Æne plus de proie, etc.), que nous couvrirons au chapitre \@ref(chapitre-ode).

```{r trend, fig.width=12, fig.height=4, fig.cap="Identification des tendances et fluctuations dans des s√©ries temporelles"}
data("lynx")
plot_grid(autoplot(auscafe),
          autoplot(hydrometeo_ts[, 3]) + labs(y="Mean temperature"),
          autoplot(lynx),
          ncol = 3,
          labels = c("A", "B", "C"))
```

Il est possible que l'on retrouve une hi√©rarchie dans les fluctuations, c'est-√†-dire que de grandes fluctuations (saisonni√®res ou cycliques) peuvent contenir des fluctuations sur des incr√©ments de temps plus petits.

### L'autocorr√©lation

Lorsque les donn√©es pr√©sentes des fluctuations (saisonni√®res ou cycliques), le graphique d'autocorr√©lation montrera un sommet aux √©tapes des cycles ou des saisons. Le graphique d'autocorr√©lation de donn√©es al√©atoires (aussi appel√©es *bruit blanc*) montera des sommets sans signification.

Un graphique de retardement (*lag plot*) met successivement en relation $y_t$ avec $y_{t-p}$. Un graphique d'autocorr√©lation est la corr√©lation entre $y_t$, $y_{t-1}$, $y_{t-2}$, etc. Une graphique de retardement donne un aper√ßu de la d√©pendance d'une variable selon ses valeurs pass√©es. Les graphiques de retardement de donn√©es ayant une forte tendance pr√©senteront des points pr√®s de la diagonale, tandis que ceux montrant des donn√©es fluctuantes de type sinuso√Ødal pr√©senteront des points dispos√©s de mani√®re circulaire. Des donn√©es al√©atoires, quant √† elles, ne pr√©senteront pas de structure de retardement facilement identifiable.

```{r, fig.width=15, fig.height=10}
set.seed(64301)
bruit_blanc <- ts(runif(114, 0, 6000), start = c(1821, 1), frequency = 1)

plot_grid(autoplot(lynx) + ggtitle("Lynx: S√©rie temporelle"),
          ggAcf(lynx) + ggtitle("Lynx: Autocorr√©lation"),
          gglagplot(lynx) + ggtitle("Lynx: Lag plot"),
          autoplot(bruit_blanc) + ggtitle("Bruit blanc: S√©rie temporelle"),
          ggAcf(bruit_blanc) + ggtitle("Bruit blanc: Autocorr√©lation"),
          gglagplot(bruit_blanc) + ggtitle("Bruit blanc: Lag plot"),
          ncol = 3)

```

**Exercice**. Cr√©ez, puis interpr√©tez des graphiques `autoplot()`, `ggAcf()` et `gglagplot()` pour les donn√©es `auscafe`.

**Exercice**. Trouvez le graphique d'autocorr√©lation et le graphique de retardement correspondant √† chaque s√©rie temporelle.

```{r exercice-viz, out.width="100%", fig.align="center", fig.cap="Exercice: Trouvez le graphique d'autocorr√©lation et le graphique de retardement correspondant √† chaque s√©rie temporelle.", echo = FALSE}
#source("lib/09_exercice-hydrometeo.R")
knitr::include_graphics("images/09_exercice-plot.png")
```

R√©ponse, voir `source("lib/09_exercice-hydrometeo.R")`:
- `D√©bit`: A-B-C
- `total_precip`: B-A-A
- `mean_temp`: C-C-B

### Signification statistique d'une s√©rie temporelle

J'ai pr√©c√©demment introduit la notion de bruit blanc, qui est un signal ne contenant pas de structure, comme le gr√©sillement d'une radio mal syntonis√©e. Nous avons vu au chapitre \@ref(chapitre-biostats) que les tests d'hypoth√®se en statistiques fr√©quentielles visent entre autre √† d√©tecter la probabilit√© que les donn√©es soient g√©n√©r√©es par une distribution dont la tendance centrale est nulle. De m√™me, pour les s√©ries temporelles, il est possible de calculer la probabilit√© qu'un signal soit un bruit blanc. Deux outils peuvent nous aider √† effectuer ce test: l'un visuel, l'autre sous forme de calcul.

Le graphique d'autocorr√©lation est √† m√™me d'inclure des seuils pour lesquels la corr√©lation est significative (lignes pointill√©es bleues).

```{r}
ggAcf(lynx, ci = 0.95) + ggtitle("Lynx: Autocorr√©lation")
```

L'analyse des seuils de signification de l'autocorr√©lation indique sur la possibilit√© de conduire la s√©rie temporelle vers un processus de mod√©lisation pr√©dictive. Dans l'exemple ci-dessus, on remarque qu'il existe des corr√©lations significatives pour un d√©calage de 4 √† 6 donn√©es, mais que les donn√©es situ√©es pr√®s les unes des autres pourraient √™tre plus difficiles √† mod√©liser.

Le test de Ljung-Box permet quant √† lui de tester si la s√©rie temporelle enti√®re peut √™tre diff√©renci√©e d'un bruit blanc.

```{r}
Box.test(lynx, lag = 20, type = "Ljung-Box")
```

La probabilit√© que la s√©rie soit un bruit blanc est presque nulle.

Notons que les tests statistiques sont aussi valides sur les d√©riv√©es des s√©ries temporelles. En outre, une d√©riv√©e premi√®re de la s√©rie temporelle sur les d√©penses devient une s√©rie temporelle de la variation des d√©penses en restauration.

```{r fig.width=15, fig.height=5}
plot_grid(autoplot(diff(auscafe)) + ggtitle("Restauration: S√©rie temporelle"),
          ggAcf(diff(auscafe)) + ggtitle("Restauration: Autocorr√©lation"),
          gglagplot(diff(auscafe)) + ggtitle("Restauration: Lag plot"),
          ncol = 3)
Box.test(diff(auscafe), lag = 16, type = "Ljung-Box")
```

Jusqu'√† pr√©sent, nous nous sommes content√©s d'observer des s√©ries temporelles. Lan√ßons-nous maintenant dans un domaine plus excitant.

![](https://media.giphy.com/media/U7r30sy3u73Og/giphy.gif)

Source: Sc√®ne de Back to the future, Robert Zemeckis et and Bob Gale, 1985

## Mod√©lisation de s√©ries temporelles

L'objectif g√©n√©ral de la mod√©lisation de s√©rie temporelle est la pr√©vision (*forecast*). La majorit√© des mod√®les se base sur des simulations de futurs possibles, desquels on pourra d√©duire une tendance centrale (*point forecast*) ainsi que des intervalles pr√©visionnels. Il est important d'insister sur le fait que la tendance centrale ne signifie pas que les donn√©es futures suivront cette tendance, mais que, selon les donn√©es et le mod√®le, la moiti√© des donn√©es devrait se retrouver sous la ligne, et l'autre moiti√© au-dessus. De plus, la r√©gion de confiance d√©finie par les intervalles pr√©visionnels signifient que par exemple 95% des points devraient se situer dans cette r√©gion.

Une mani√®re d'√©valuer la performance d'une pr√©vision est de pr√©voir des donn√©es auparavant observ√©es √† partir des donn√©es qui les pr√©c√®dent. Ces valeurs sont dites *liss√©es*. Tout comme c'est le cas en r√©gression statistique, il est possible de d√©duire les r√©sidus du mod√®le. Pour les r√©gressions couvertes au chapitre \@ref(chapitre-biostats), nous v√©rifions la validit√© du mod√®le en v√©rifiant si les r√©sidus √©taient distribu√©es normalement. Pour une s√©rie temporelle, on tend plut√¥t √† v√©rifier si les r√©sidus forment un bruit blanc, c'est-√†-dire qu'ils ne sont pas corr√©l√©s. De plus, pour √©viter d'√™tre biais√©es, leur moyenne doit √™tre de 0. De mani√®re compl√©mentaire pour la validit√© des intervalles pr√©visionnels, mais non essentielle √† la validit√© du mod√®le, les r√©sidus devraient √™tre distribu√©s normalement et leur variance devrait √™tre constante ([Hyndman et Athanasopoulos, 2018](https://otexts.com/fpp2/)).

Il est possible qu'un mod√®le remplisse toutes ces conditions, mais que sa pr√©vision soit m√©diocre. Comme nous le verrons √©galement au chapitre \@ref(chapitre-ml), une pr√©diction ou une pr√©vision issue d'un mod√®le ne peut pas √™tre √©valu√©e sur des donn√©es qui ont servies √† lisser le mod√®le. **Pour v√©rifier une pr√©vision temporelle, il faut s√©parer les donn√©es en deux s√©ries: une s√©rie d'entra√Ænement et une s√©rie de test** (figure \@ref(fig:train-test)).

```{r train-test, out.width="100%", fig.align="center", fig.cap="Les points bleus d√©signe la s√©rie d'entra√Ænement et les points rouges, la s√©rie de test. Source de l'image: [Hyndman et Athanasopoulos, 1998](https://otexts.com/fpp2/).", echo = FALSE}
knitr::include_graphics("https://otexts.com/fpp2/fpp_files/figure-html/traintest-1.png")
```

La s√©paration dans le temps entre la s√©rie d'entra√Ænement et la s√©rie de test se fait √† votre convenance, selon la disponibilit√© des donn√©es. Vous aurez toutefois avantage √† conserver davantage de donn√©es en entra√Ænement (typiquement, 70%), et √† tout le moins, s√©parer au moins une fluctuation saisonni√®re ou cyclique. La s√©rie d'entra√Ænement servira √† lisser le mod√®le pour en d√©couvrir les possibles structures. La s√©rie de test servira √† √©valuer sa performance sur des donn√©es obtenues, mais inconnues du mod√®le pour v√©rifier les structures d√©couvertes par le mod√®le. L'erreur pr√©visionnelle est la diff√©rence entre une donn√©e observ√©e en test et sa pr√©vision (l'√©quivalent des r√©sidus, mais appliqu√©s sur des donn√©es ind√©pendantes du mod√®le). La performance d'une pr√©vision peut √™tre √©valu√©e de diff√©rentes mani√®res, mais l'erreur moyenne absolue √©chelonn√©e (*mean absolute scaled error*, MASE) est conseill√©e puisqu'elle ne d√©pend pas de la m√©trique de la quantit√© produite: plus la MASE se rapproche de z√©ro, meilleure est la pr√©vision.

Plusieurs m√©thodes de pr√©vision sont possibles. Nous en couvrirons 3 dans ce chapitre: la m√©thode na√Øve, la m√©thode SES et la m√©thode ARIMA. Nous allons couvrir les diff√©rents aspects de la mod√©lisation des s√©ries temporelles √† travers l'utilisation de ces m√©thodes.

### M√©thode na√Øve

La m√©thode na√Øve d√©finit la valeur suivante selon la valeur pr√©c√©dente (fonction `forecast::naive()`), ou la valeur de la saison pr√©c√©dente (fonction `forecast::snaive()`). Ces fonctions du module **`forecast`** incluent un composante al√©atoire pour simuler des occurrences futures selon des marches al√©atoires (*random walks*), o√π chaque valeur suivante est simul√©e al√©atoirement, consid√©rant la valeur pr√©c√©dente.

Nous tenterons de pr√©voir les d√©bits de la rivi√®re Chaudi√®re. Ceux-ci √©tant caract√©ris√© par des fluctuations saisonni√®res, mieux vaut utiliser `snaive()`. Mais auparavant, s√©parons la s√©rie en s√©rie d'entra√Ænement et s√©rie de test.

```{r}
flow_ts <- hydrometeo_monthly_ts[, 1]
flow_ts_train <- window(flow_ts, start = 1998, end = 2005.999)
flow_ts_test <- window(flow_ts, start = 2006)
```

Lan√ßons la mod√©lisation sur les donn√©es d'entra√Ænement.

```{r}
hm_naive <- snaive(flow_ts_train, h = 24)
autoplot(hm_naive) +
  autolayer(fitted(hm_naive)) +
  autolayer(flow_ts_test, color = rgb(0, 0, 0, 0.6)) +
  labs(x = "Ann√©e", y = "D√©bit")
```

Le graphique pr√©c√©dent montre que la pr√©vision na√Øve (en rose) prend bien la valeur observ√©e au cycle pr√©c√©dent (en noir). Les donn√©es de test sont en gris transparent. Notons que la pr√©sence de d√©bit n√©gatifs pourrait √™tre √©vit√©e en utilisant une transformation logarithmique du d√©bit pr√©alablement √† la mod√©lisation.

Voyons maintenant l'analyse des r√©sidus avec la fonction `forecast::checkresiduals()`.

```{r}
checkresiduals(hm_naive)
```

La `p-value` √©tant de 0.01546, il est peu probable que les r√©sidus forment un bruit blanc. Les r√©sidus contiennent de l'autocorr√©lation, ce qui devrait √™tre √©vit√©. Ceci est toutefois d√ª √† un seul point allant au-del√† du seuil de 0.05, que l'on peut observer sur le graphique d'autocorr√©lation. Le graphique de la distribution des r√©sidus montre des valeurs aberrantes, ainsi qu'une distribution plut√¥t pointue, qui donnerait un test de Kurtosis probablement √©lev√©.

```{r}
shapiro.test(residuals(hm_naive)) # non-normal si p-value < seuil (0.05)
library("e1071")
kurtosis(residuals(hm_naive), na.rm = TRUE) # le r√©sultat d'un test de kurtosis sur une distribution normale devrait √™tre de 0.
```

Pas de panique, les pr√©dictions peuvent n√©anmoins √™tre valides: seulement, les intervalles pr√©visionnels pourraient √™tre trop vagues ou trop restreintes: √† prendre avec des pincettes.

L'√©valuation du mod√®le peut √™tre effectu√©e avec la fonction `forecast::accuracy()`, qui d√©tecte automatiquement la s√©rie d'entra√Ænement et la s√©rie de test si on lui fournit la s√©rie enti√®re (ici l'objet `flow_ts`).

```{r}
accuracy(hm_naive, flow_ts)
```

La m√©thode na√Øve est rarement utilis√©e en pratique autrement que comme standard par rapport auquel la performance d'autres mod√®les est √©valu√©e.

### M√©thode SES

Alors que la m√©thode na√Øve donne une cr√©dibilit√© compl√®te √† la valeur pr√©c√©dente (ou au cycle pr√©c√©dent), la m√©thode SES (*simple exponential smoothing*) donne aux valeurs pr√©c√©dentes des poids exponentiellement d√©croissants selon leur anciennet√©. La pr√©vision par SES sera une moyenne pond√©r√©e des derni√®res observations, en donnant plus de poids sur les observations plus rapproch√©es.

Math√©matiquement, la m√©thode SES est d√©crite ainsi.

$$\hat{y}_{t + h|t} = \alpha y_t + \alpha\left( 1-\alpha \right) y_{t-1} + \alpha\left( 1-\alpha \right)^2 y_{t-2} + ...$$

o√π $\hat{y}_{t + h|t}$ est la pr√©vision de $y$ au temps $t + h|t$, qui est le d√©calage de $h$ √† partir de la derni√®re mesure au temps $t$. Le param√®tre $\alpha$ prend une valeur de 0 √† 1, et d√©crit la distribution des poids. Une valeur de $\alpha$ √©lev√© donnera davantage de poids aux √©v√©nements r√©cents. La somme de tous poids $\alpha$ tend vers 1 lorsque les pas de temps pr√©c√©dents tendent vers l'$\infty$.

```{r echo = FALSE}
ses_weights <- function(a, n) {
  weights <- vector(mode = "numeric", length = n)
  for (i in 1:n) {
    weights[i] <- a*(1-a)^(i-1)
  }
  return(weights)
}
plot(1:10, ses_weights(a = 0.8, n = 10),
     type = 'l',
     xlab = "t + h|t",
     ylab = "weight", lwd = 2)
text(1.2, 0.6, "alpha = 0.8", pos = 4)
lines(1:10, ses_weights(a = 0.4, n = 10), type = 'l', lwd = 2, col = "red")
text(4, 0.1, "alpha = 0.4", pos = 4, col = "red")
```

Une autre mani√®re d'exprimer l'√©quation est de la segmenter en deux: une pour la pr√©vision en fonction du niveau (*level*, le mod√®le), une autre pour d√©crire comment le niveau change au fil du temps.

| Description | √âquation |
|---|---|
| Pr√©vision | $\hat{y}_{t + h|t} = l_t$ |
| Niveau | $l_t = \alpha y_t + \alpha\left( 1-\alpha \right) l_{t-1}$ |

Exprim√©e ainsi, la pr√©vision n'exprimera aucune tendance ni fluctuation. Il s'agira d'une projection jusqu'√† l'infini de la moyenne des observations pr√©c√©dentes pond√©r√©e par leur d√©calage.

#### SES de base

Prenons les [donn√©es de la NASA](https://data.giss.nasa.gov/gistemp/graphs/graph_data/Global_Mean_Estimates_based_on_Land_and_Ocean_Data/graph.txt) sur l'indice de temp√©rature terre-oc√©an, qui d√©crit un d√©calage par rapport √† la moyenne des temp√©ratures globales observ√©es entre de 1951 √† 1980. La m√©thode SES est appel√©e par la fonction `forecast::ses()`, de la m√™me mani√®re qu'on l'a fait pr√©c√©demment avec la m√©thode na√Øve.

```{r message = FALSE}
loti_ts <- read_csv("data/09_nasa.csv") %>% pull(LOTI) %>% ts(., start = 1880, frequency = 1)
#loti_ts <- window(loti_ts, start = 1950)
loti_ts_tr <- window(loti_ts, end = 2004)
loti_ses <- ses(loti_ts_tr, h = 20, alpha = 0.5)
autoplot(loti_ses) + autolayer(fitted(loti_ses))
```

> **Note**. Les pr√©visions climatiques sont effectu√©es par des mod√®les bien plus complexes que ce que nous voyons ici. Les pr√©visions du GIEC agr√®gent des tendances localis√©es et incluent une batterie de covariables, dont la plus √©vidente est la concentration en CO2 dans l'atmosph√®re. Il s'agit seulement d'un exemple d'application.

#### SES avec tendance

La pr√©vision a peu d'int√©r√™t, √©tant donn√©e qu'elle n'inclut pas de tendance. Or, nous pouvons en ajouter une √† l'√©quation. Ainsi exprim√©e, la tendance changera aussi au fil du temps.

| Description | √âquation |
|---|---|
| Pr√©vision | $\hat{y}_{t + h|t} = l_t + \left( \phi + \phi^2 + ... + \phi^h \right) \times b_t$ |
| Niveau | $l_t = \alpha y_t + \alpha\left( 1-\alpha \right) \left( l_{t-1} + \phi b_{t-1} \right) $ |
| Tendance | $b_t = \beta^* \left( l_t - l_{t-1} \right) + (1-\beta^*) \phi b_{t-1}$ |

Le param√®tre $\beta^*$ d√©crit la vitesse √† laquelle la tendance peut changer, de 0 o√π la pente ne change pas √† 1 o√π la pente change rapidement. Le param√®tre $\phi$ adouci la pente en s'√©loignant de la derni√®re mesure. Un \phi tendant vers 0 g√©n√©rera un fort adoucissement, alors qu'un \phi tendant vers 1 ne g√©n√©rera pas d'adoucissement. Il peut √™tre difficile de d√©terminer les param√®tres de lissage $\alpha$, $\beta^*$ et $\phi$, ainsi que les param√®tres d'√©tat $l_0$ et $b_0$. La fonction de `forecast::holt()` permet de les estimer automatiquement.

```{r, fig.width=10, fig.height=4}
loti_holt_dF <- holt(loti_ts_tr, damped = FALSE, h = 100)
loti_holt_dT <- holt(loti_ts_tr, damped = TRUE, h = 100)
plot_grid(autoplot(loti_holt_dF), autoplot(loti_holt_dT))
loti_holt_dF$model$par
loti_holt_dT$model$par
```

Dans ce cas, l'optimisation de $\phi$ lui donne une valeur de 0.8, une valeur suffisamment faible pour que l'adoucissement soit fort. Vous obtiendrez une valeur de $\phi$ plus √©lev√©e en ne consid√©rant que les donn√©es obtenues depuis 1950 (en d√©commentant `loti_ts <- window(loti_ts, start = 1950)`, plus haut).

#### SES avec fluctuation saisonni√®re

D'autres param√®tres peuvent √™tre ajout√©s pour de tenir compte des fluctuations saisonni√®res (les fluctuations cycliques sont plus difficiles √† mod√©liser) de mani√®re additive ou multiplicative. Voici la modification apport√©e pour la mod√©lisation additive, en laissant tomber l'adoucissement.

| Description | √âquation |
|---|---|
| Pr√©vision | $\hat{y}_{t + h|t} = l_t + h \times b_t + s_{t-m+h_m^+}$ |
| Niveau | $l_t = \alpha \left(y_t - s_{t-m} \right) + \alpha\left( 1-\alpha \right) \left( l_{t-1} + b_{t-1} \right) $ |
| Tendance | $b_t = \beta^* \left( l_t - l_{t-1} \right) + (1-\beta^*) b_{t-1}$ |
| Saison | $s_t = \gamma \left( y_t - l_{t-1} - b_{t-1} \right) + (1-\gamma) s_{t-m}$ |

o√π $m$ est la p√©riodicit√© des fluctuations saisonni√®re, par exemple 4 pour quatre saisons annuelles et $\gamma$ est un param√®tre de la portion saisonni√®re, qui, tout comme un effet al√©atoire en biostatistiques, fluctue autour de z√©ro. La variante multiplicative multiplie la pr√©vision par un facteur plut√¥t que d'imposer un d√©calage. La math√©matique n'est pas pr√©sent√©e ici pour plus de simplicit√© (consulter [Hyndman et Athanasopoulos (2018), chapitre 7.3](https://otexts.com/fpp2/holt-winters.html) pour plus de d√©tails). Dans le cas multiplicatif, l'effet saisonnier fluctue autour de 1. Si l'amplitude de la fluctuation s'accro√Æt au fil de la s√©rie temporelle, la m√©thode multiplicative donnera probablement de meilleurs r√©sultats.

La fonction que nous utiliserons pour les SES-saisonniers est `forecast::hw()`. Les donn√©es de la NASA ne sont pas saisonni√®res (`frequency(loti_ts)` donne 1).

```{r}
flow_hw <- hw(flow_ts_train, damped = TRUE, h = 12*3, seasonal = "additive")
autoplot(flow_hw) + autolayer(fitted(flow_hw))
```

#### Automatiser la pr√©vision avec les SES

L'erreur du mod√®le peut aussi √™tre calcul√©e de sorte qu'elle soit constante ou augmente selon le niveau (ou d√©calage). Nous avons donc plusieurs types de mod√®les de la famille SES.

- Tendance: [sans tendance, tendance additive, tendance adoucie]
- Saison: [sans saison, saison additive, saison multiplicative]
- Erreur: [erreur additive, erreur multiplicative]

Lequel choisir? Encore une fois, on peut laisser R optimiser notre choix avec un mod√®le ETS (*error, tend and seasonnal*). L'optimisation est lanc√©e avec la fonction `forecast::ets()`.

```{r}
flow_model <- ets(flow_ts_train)
flow_model
```

Le mod√®le retenu est un `ETS(M,N,M)`, d√©finissant dans l'ordre le type d'erreur, de tendance et de saison selon `A` pour additif, `M` pour multiplicatif et `N` pour l'absence. Nous avons une erreur de type `M` (multiplicative), une tendance de type `N` (sans tendance) et une saison de type `M` (multiplicative). L'absence de valeur pour `phi` indique que l'adoucissement n'est probablement pas n√©cessaire.

Nous pouvons visualiser l'√©volution des diff√©rentes composantes.

```{r}
autoplot(flow_model)
```

Dans un mod√®le sans tendance, avec saisonnalit√© multiplicative, les donn√©es *levels* sont multipli√©es par les donn√©es *season* pour obtenir la pr√©vision. Malgr√© l'absence de tendance dans le mod√®le, il semble que le d√©bit a diminu√© de 2000 √† 2003 entre deux √©tats stables de 1998 √† 2000 et de 2003 √† 2006.

La fonction `forecast::ets()` g√©n√®re un mod√®le, mais pas de pr√©diction. Pour obtenir une pr√©diction, nous devons utiliser la fonction `forecast::forecast()`, que j'utiliserai ici en mode *tidyverse*.

```{r}
flow_ets <- flow_ts_train %>% ets()
flow_fc <- flow_ets %>% forecast()
flow_fc %>% autoplot()
```

L'analyse d'exactitude et celle des r√©sidus sont toutes aussi pertinentes. La premi√®re est effectu√©e sur la pr√©vision, et la seconde sur le mod√®le.

```{r}
accuracy(flow_fc, flow_ts)
checkresiduals(flow_ets)
```

Il est peu probable que les r√©sidus aient √©t√© g√©n√©r√©s par un bruit blanc, indiquant qu'il existe une structure dans les donn√©es qui n'a pas √©t√© captur√©e par le mod√®le.

**Exercice**. Mod√©liser la s√©rie temporelle `lynx` avec `forecast::ets()`. Que se passe-t-il?

#### Pr√©traitement des donn√©es

J'ai sp√©cifi√© plus haut que les donn√©es de d√©bit pourraient avantageusement √™tre transform√©es avec un logarithme pour √©viter les pr√©dictions de d√©bits n√©gatifs. D'autres types de transformation peuvent √™tre utilis√©es, comme la racine carr√©e ou cubique, l'oppos√©e de l'inverse ($-1/x$) ou les transformations compositionnelles (chapitre \@ref(chapitre-explorer)). La transformation Box-Cox est aussi largement utilis√©e pour sa polyvalence.

$$
w =
\begin{cases}
ln(y_t) &\text{if } \lambda = 0 \\
\frac{y_t - 1}{\lambda} &\text{if } \lambda \neq 0
\end{cases}
$$

$\lambda = 1$: pas de transformation
$\lambda = 1/2$: ressemble √† $\sqrt{y_t}$
$\lambda = 1/3$: ressemble √† $\sqrt[3]{y_t}$
$\lambda = 0$: log naturel
$\lambda = -1$: ressemble √† $1/y_t$

La fonction `forecast::BoxCox.lambda()` estime la valeur optimale de $\lambda$.

```{r}
BoxCox.lambda(flow_ts_train)
```

Cette valeur peut √™tre imput√©e √† l'argument `lambda` de la fonction `forecast::ets()`. Dans notre cas, nous d√©sirions plut√¥t une transformation logarithmique. Cons√©quemment, nous utilisons `lambda = 0`.

```{r}
flow_ts_train %>%
  ets(lambda = 0) %>%
  forecast() %>%
  autoplot()
```

Les erreurs ne franchissent pas le 0, mais sont vraisemblablement surestim√©es lors des sommets. Notez que R s'occupe de la transformation retour.

La **diff√©renciation** est aussi une forme de pr√©traitement. La diff√©renciation (fonction `base::diff()`) consiste en la soustraction de la valeur pr√©c√©dente √† la valeur suivante. La valeur pr√©c√©dente peut √™tre d√©cal√©e √† la valeur de la p√©riode de l'unit√© temporelle pr√©c√©dente, par exemple le mois de mars de l'ann√©e pr√©c√©dente. Un objectif de la diff√©renciation est de rendre la s√©rie temporelle stationnaire en termes de tendance et de fluctuation saisonni√®re, de sorte que la s√©rie diff√©renci√©e se comporte comme un bruit blanc.

```{r}
plot_grid(flow_ts_train %>% autoplot() + ggtitle("D√©bit"),
          loti_ts_tr %>% autoplot() + ggtitle("LOTI"),
          flow_ts_train %>% diff(., lag = 12) %>% autoplot() + ggtitle("D√©bit avec diff√©renciation saisonni√®re"),
          loti_ts_tr %>% diff(., lag = 1) %>% autoplot() + ggtitle("LOTI avec diff√©renciation d'ordre 1"))
```

### La m√©thode ARIMA

Un mod√®le ARIMA, l'acronyme de l'anglais *auto-regressive integrated moving average*, est une combinaison de trois parties: AR-I-MA. L'**autor√©gression** consiste en une r√©gression lin√©aire dont la variable r√©ponse $y_t$ est la variable √† l'instant $t$ et les variables explicatives sont les variables aux instants pr√©c√©dents. Pour un nombre $p$ de p√©riodes pr√©c√©dentes, nous obtenons une r√©gression lin√©aire typique.

$$y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + ... + \phi_p y_{t-p} + \epsilon_t$$
o√π $\epsilon_t$ est l'erreur sur la pr√©diction.

La partie concernant la **moyenne mobile** est une r√©gression non pas sur les observations, mais sur les erreurs. Consid√©rant les $q$ erreurs pr√©c√©dentes, nous obtenons

$$y_t = c + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + ... + \theta_q \epsilon_{t-q} + \epsilon_t$$

La somme de l'autor√©gression et de la moyenne mobile donne un mod√®le ARMA. Le I de ARIMA, mis pour *integrated*, est le contraire de la diff√©renciation, que j'ai pr√©sent√© √† la fin de la section sur les SES. Puisque la s√©rie temporelle doit √™tre stationnaire pour effectuer l'ARMA, nous devons diff√©rencier la s√©rie un nom $d$ de fois avant de proc√©der √† l'autor√©gression et au calcul de la moyenne mobile.

Nous obtenons ainsi une ARIMA d'ordres $p$, $d$ et $q$, not√©e $ARIMA(p,d,q)$. Nous devons aussi statuer si $c$ (l'intercept ou le *drift*) doit √™tre ou non consid√©r√© come nul. Ces ordres peuvent √™tre sp√©cifi√©s dans la fonction telle que `forecast::Arima(order = c(0, 1, 1), include.constant = TRUE)`. Toutefois, il est possible de les optimiser gr√¢ce √† la fonction `forecast::auto.arima()`. Tout comme les sorties de `forecast::ets()`, `forecast::auto.arima()` fourni le mod√®le, mais pas les pr√©dictions: la fonction `forecast::forecast()` doit √™tre lanc√©e pour obteir la pr√©diction.

```{r}
loti_arima <- loti_ts_tr %>% auto.arima()
loti_arima %>% forecast(h = 30) %>% autoplot()
summary(loti_arima)
```

Le sommaire du mod√®le sp√©cifie une $ARIMA(1,1,3)$ en utilisant l'intercept $c$ (*with drift*).

Lorsque l'on compte pr√©dire des s√©ries saisonni√®res, nous devons ajouter un nouveau jeu d'ordres $(P,D,Q)m$, o√π $P$, $D$ et $Q$ sont √©quivalents √† leurs minuscules, mais portent sur des d√©calages saisonniers et non pas des d√©calages d'unit√©s de temps. L'ordre $m$ est le nombre de p√©riodes √† consid√©rer par unit√© temporelle, par exemple 12 mois par an. Bonne nouvelle: `forecast::auto.arima()` automatise le tout. Nous pouvons utiliser `lambda = 0` pour effectuer une transformation logarithmique.

```{r}
flow_arima <- flow_ts_train %>% auto.arima(lambda = 0)
flow_arima %>% forecast(h = 36) %>% autoplot()
summary(flow_arima)
```

Le sommaire du mod√®le, `ARIMA(1,0,0)(2,1,0)[12]` sous le format $ARIMA(p,d,q)(P,D,Q)m$, retourne automatiquement une p√©riode de 12 mois avec une diff√©renciation saisonni√®re mais sans diff√©renciation ordinaire, excluant la moyenne mobile dans les deux cas.

**Exercice**. Il est toujours pertinent d'effectuer l'analyse des r√©sidus...

### Les mod√®les dynamiques

J'ai not√© pr√©c√©demment que l'√©volution du climat tient compte d'une s√©rie de covariables explicatives. De m√™me, le d√©bit dans la rivi√®re Chaudi√®re n'est pas un effet de la saison, mais de son environnement (climat, changements dans la morphologie du paysage, utilisation de l'eau, etc.). La pr√©vision du d√©bit aura avantage √† consid√©rer ces covariables. L'ARIMA peut accueillir des covariables en mod√©lisant le terme d'erreur, $\epsilon_t$ en fonction de s√©ries temporelles conjointes.

Le d√©bit mensuel de la rivi√®re Chaudi√®re peut √™tre mod√©lis√© en fonction de la temp√©rature moyenne mensuelle et des pr√©cipitations totales mensuelles en ajoutant l'argument `xreg` √† la fonction `forecast::auto.arima()`. L'argument consiste en la matrice temporelle des variables explicatives. Notez que `forecast::auto.arima()` ne fonctionne pas (encore?) avec l'interface-formule de R, mais que l'on peut se d√©brouiller en transformant en s√©rie temporelle la sortie de la fonction `base::model.matrix()`, qui elle peut accueillir une formule.

```{r}
hm_tr <- window(hydrometeo_monthly_ts, end = c(2004, 12))
hm_te <- window(hydrometeo_monthly_ts, start = c(2005, 1))
flow_darima <- auto.arima(y = hm_tr[, "D√©bit"],
                          xreg = hm_tr[, c("total_precip", "mean_temp")],
                          lambda = 0)
summary(flow_darima)
```

> **Note**. Pour obtenir des r√©sultats plus pr√©cis, mais dont les r√©sultats seront plus longs √† venir, sp√©cifiez l'argument `stepwise = FALSE` dans la fonction `forecast::auto.arima()`.

Les coefficients sur les covariables sont interpr√©tables *dans l'√©chelle de la pr√©vision transform√©e*. Ainsi, 1 mm de pr√©cipitation par mois augmentera le logarithme naturel du d√©bit augmente de 0.0028. De m√™me, 1 ¬∞C de temp√©rature moyenne diminuera le logarithme naturel du d√©bit augmente de 0.0215: notez que l'erreur standard sur ce coefficient √©tant tr√®s √©lev√©e, le coefficient n'est √† premi√®re vue pas diff√©rent de 0. La temp√©rature moyenne aurait avantage √† √™tre remplac√©e par un meilleur indicateur incluant les p√©riodes d'accumulation de neige et de leur fonte. Pas mal doc?

![](https://media.giphy.com/media/kk1zu1tVwuecM/giphy.gif)

Source: Sc√®ne de Back to the future, Robert Zemeckis et and Bob Gale, 1985

La pr√©vision d'un mod√®le dynamique demandera les s√©ries temporelles des covariables, qui peuvent elles-m√™mes √™tre mod√©lis√©es ou √™tre issues de simulations. Dans notre cas, nous pouvons utiliser la s√©rie de test.

```{r}
flow_darima %>%
  forecast(xreg = hm_te[, c("total_precip", "mean_temp")]) %>%
  autoplot()
checkresiduals(flow_darima)
```

### Les mod√®les TBATS

Les mod√®les TBATS ([Hyndman et Athanasopoulos, 2018](https://otexts.com/fpp2/complexseasonality.html)) combinent tout ce que l'on a vu jusqu'√† pr√©sent, *√† l'exception notable des covariables*, dans une interface automatis√©e. L'automatisation a l'avantage d'une utilisation rapide, mais donne parfois des pr√©dictions erron√©es.

```{r}
lynx_tbats <- lynx %>% tbats()
lynx_tbats_f <- lynx_tbats %>% forecast()
lynx_tbats_f %>% autoplot()
summary(lynx_tbats_f)
```

Le sommaire du mod√®le le type de mod√®le s√©lectionn√© ainsi que ses param√®tres. Le titre du graphique en donne aussi un aper√ßu: `BATS(0.159, {5,1}, 0.833, -)`:

- Le coefficient lambda utilis√© est 0.159.
- Le {5, 1} signifie que p = 5 et et q = 1.
- L'adoucissement est de 0.833
- Aucune p√©riode n'est incluse

L'approche TBATS performe bien pour les donn√©es √† fluctuations cycliques. Toutefois, les intervalles pr√©visionnels son souvent trop larges et l'optimisation peut √™tre longue.

## Pour terminer...

Nous avons vu comment manipuler des s√©ries temporelles avec **`dplyr`** et le format de base `ts`. Le nouveau format de s√©rie temporelle du module [**`tsibble`**](https://tsibble.tidyverts.org/) permet des manipulations dans un flux de travail plus conforme au *tidyverse*. De m√™me, la nouvelle mouture du module **`forecast`** nomm√©e [**`fable`**](https://fable.tidyverts.org/), r√©√©crite vers l'approche *tidyverse*, offrira des fonctions permettant notamment une hi√©rarchisation dans les fluctuations saisonni√®res, par exemple des cycles journaliers ench√¢ss√©s dans des cycles hebdomadaires, ench√¢ss√©s dans des cycles trimestriels. Le module [**`prophet`**](https://facebook.github.io/prophet/), distribu√© par Facebook en mode open source, gagne en popularit√©. Bien qu'il soit r√©put√© pour offrir des pr√©visions fiables, ses bases math√©matiques me semblent insuffisamment document√©es. Fait int√©ressant: **`prophet`** est en mesure d'effectuer des pr√©visions en mode dynamique. Quoi qu'il en soit, j'ai favoris√© des modules plus matures et mieux document√©s, qui pourront vous servir de tremplin vers les nouveaux modules.

Maintenant, le futur vous appartient.

![](https://media.giphy.com/media/3o7aCRBQC8u5GaW092/giphy.gif)

```{r, include=FALSE}
rm(list = ls())
```

<!--chapter:end:10_series-temporelles.Rmd-->

# Introduction √† l'autoapprentissage {#chapitre-ml}

***
Ô∏è\ **Objectifs sp√©cifiques**:

√Ä la fin de ce chapitre, vous

- saurez √©tablir un plan de mod√©lisation par autoapprentissage
- saurez d√©finir le sous-apprentissage et le surapprentissage
- serez en mesure d'effectuer un autoapprentissage avec les techniques des *k*-proches voisins, les arbres de d√©cision, les for√™ts al√©atoires, les r√©seaux neuronnaux et les processus gaussiens

***

Plusieurs cas d'esp√®ces en sciences et g√©nies peuvent √™tre approch√©s en liant un variable avec une ou plusieurs autres √† l'aide de r√©gressions lin√©aires, polynomiales, sinuso√Ødales, exponentielle, sigmo√Ødales, [etc](https://dl.sciencesocieties.org/publications/aj/pdfs/107/2/786). Encore faut-il s'assurer que ces formes pr√©√©tablies repr√©sentent le ph√©nom√®ne de mani√®re fiable.

Lorsque la forme de la r√©ponse est difficile √† envisager, en particulier dans des cas non-lin√©aires ou impliquant plusieurs variables, on pourra faire appel √† des mod√®les dont la structure n'est pas contr√¥l√©e par une √©quation rigide gouvern√©e par des param√®tres (comme la pente ou l'intercept).

L'**autoapprentissage**, apprentissage automatique, ou *machine learning*, vise √† d√©tecter des structures complexes √©mergeant d'ensembles de donn√©es √† l'aide des math√©matiques et de processus automatis√©s afin de pr√©dire l'√©mergence de futures occurrences. Comme ensemble de techniques empiriques, l'autoapprentissage est un cas particulier de l'**intelligence artificielle**, qui elle inclut aussi les m√©canismes d√©terministes et des ensembles d'op√©rations logiques. Par exemple, les premiers ordinateurs √† comp√©titionner aux √©checs se basaient sur des r√®gles de logique (si la reine noire est positionn√©e en c3 et qu'un le fou blanc est en position f6 et que ... alors bouge la tour en g5 - j'√©cris n'importe quoi). Un jeu simple d'intelligence artificielle consiste √† lancer une marche al√©atoire, par exemple bouger √† chaque pas d'une distance au hasard en x et y, puis de recalculer le pas s'il arrive dans une bo√Æte d√©finie (figure \@ref(fig:random-walk)). Dans les deux cas, il s'agit d'intelligence artificielle, mais pas d'autoapprentissage. 

```{r random-walk, out.width='100%', fig.align="center", fig.cap="Petite tortue, n'entre pas dans la bo√Æte!", echo = FALSE}
set.seed(68538)
n_step <- 100
y <- x <- vector(length = n_step)
y[1] <- x[1] <- 0

box <- c(xg = 0, xd = 4, yb = 0, yh = 2)

for (i in 2:n_step) {
  x[i] <- x[i-1] + runif(1, -1, 1)
  y[i] <- y[i-1] + runif(1, -1, 1)
  while (x[i] > box[1] & x[i] < box[2] & y[i] > box[3] & y[i] < box[4]) {
    x[i] <- x[i-1] + runif(1, -1, 1)
    y[i] <- y[i-1] + runif(1, -1, 1)
  }
}

plot(x, y, type = 'l', col = rgb(0, 0, 1, 0.5), asp = 1)
for (i in 1:(n_step-1)) {
  arrows(x0 = x[i], y0 = y[i],
         x1 = x[i+1], y1 = y[i+1], 
         col = rgb(i/n_step, 0, 1-i/n_step),
         length = 0.08)
}
polygon(x = c(box[1], box[2], box[2], box[1]),
        y = c(box[3], box[3], box[4], box[4]),
        col = rgb(1, 0, 0, 0.25))
points(x, y,  pch = "üê¢")

```

L'autoapprentissage passera davantage par la simulation de nombreuses parties et d√©gagera la structure optimale pour l'emporter consid√©rant les positions des pi√®ces sur l'√©chiquier.

## Lexique

L'autoapprentissage poss√®de son jargon particulier. Puisque certains termes peuvent porter √† confusion, voici quelques d√©finitions de termes que j'utiliserai dans ce chapitre.

- **R√©ponse**. La variable que l'on cherche √† obtenir. Il peut s'agir d'une variable continue comme d'une variable cat√©gorielle. On la nomme aussi la *cible*.
- **Pr√©dicteur**. Une variable utilis√©e pour pr√©dire une r√©ponse. Les pr√©dicteurs sont des variables continues. Les pr√©dicteurs de type cat√©goriel doivent pr√©alablement √™tre dummifi√©s (voir chapitre 5). On nomme les pr√©dicteurs les *entr√©es*.
- **Apprentissage supervis√©** et **non-supervis√©**. Si vous avez suivi le cours jusqu'ici, vous avez d√©j√† utilis√© des outils entrant dans la grande famille de l'apprentissage automatique. La r√©gression lin√©aire, par exemple, vise √† minimiser l'erreur sur la r√©ponse en optimisant les coefficients de pente et l'intercept. Un apprentissage supervis√© a une cible, comme c'est le cas de la r√©gression lin√©aire. En revanche, un apprentissage non supervis√© n'en a pas: on laisse l'algorithme le soin de d√©tecter des structures int√©ressantes. Nous avons d√©j√† utilis√© cette approche. Pensez-y un peu... l'analyse en composante principale ou en coordonn√©es principales, ainsi que le partitionnement hi√©rarchique ou non, couverts au chapitre \@ref(chapitre-ordination), sont des exemples d'apprentissage non supervis√©. En revanche, l'analyse de redondance a une r√©ponse. L'analyse discriminante aussi, bien que sa r√©ponse soit cat√©gorielle. L'apprentissage non supervis√© ayant d√©j√† √©t√© couvert (sans le nommer) au chapitre \@ref(chapitre-ordination), ce chapitre ne s'int√©resse qu'√† l'apprentissage supervis√©.
- **R√©gression** et **Classification**. Alors que la r√©gression est un type d'apprentissage automatique pour les r√©ponses continues, la classification vise √† pr√©dire une r√©ponse cat√©gorielle. Il existe des algorithmes uniquement application √† la r√©gression, uniquement applicables √† la classification, et plusieurs autres adaptable aux deux situations.
- **Donn√©es d'entra√Ænement** et **donn√©es de test**. Lorsque l'on g√©n√®re un mod√®le, on d√©sire qu'il sache comment r√©agir √† ses pr√©dicteurs. Cela se fait avec des donn√©es d'entra√Ænement, sur lesquelles on **calibre** et **valide** le mod√®le. Les donn√©es de test servent √† v√©rifier si le mod√®le est en mesure de pr√©dire des r√©ponses sur lesquelles il n'a pas √©t√© entra√Æn√©.
- **Fonction de perte**. Une fonction qui mesure l'erreur d'un mod√®le.

## D√©marche

La premi√®re t√¢che est d'explorer les donn√©es, ce que nous avons couvert au chapitres \@ref(chapitre-tableaux) et \@ref(chapitre-visualisation).

### Pr√©traitement

Pour la plupart des techniques d'autoapprentissage, le choix de l'√©chelle de mesure est d√©terminant sur la mod√©lisation subs√©quente. Par exemple, un algorithme bas√© sur la distance comme les *k* plus proches voisins ne mesurera pas les m√™mes distances entre deux observations si l'on change l'unit√© de mesure d'une variable du m√®tre au kilom√®tre. Il est donc important d'effectuer, ou d'envisager la possibilit√© d'effectuer un pr√©traitement sur les donn√©es. Je vous r√©f√®re au chapitre \@ref(chapitre-explorer) pour plus de d√©tails sur le pr√©traitement.

### Entra√Ænement et test

Vous connaissez peut-√™tre l'expression sportive "avoir l'avantage du terrain". Il s'agit d'un principe pr√©tendant que les athl√®tes performent mieux en terrain connu. Idem pour les mod√®les ph√©nom√©nologiques. Il est possible qu'un mod√®le fonctionne tr√®s bien sur les donn√©es avec lesquelles il a √©t√© entra√Æn√©, mais tr√®s mal sur des donn√©es externes. De mauvaises pr√©dictions effectu√©es √† partir d'un mod√®le qui semblait bien se comporter peut mener √† des d√©cisions qui, pourtant prises de mani√®re confiante, se r√©v√®lent fallacieuses au point d'aboutir √† de graves cons√©quences. C'est pourquoi, **en mode pr√©dictif, on doit √©valuer la pr√©cision et la justesse d'un mod√®le sur des donn√©es qui n'ont pas √©t√© utilis√©s dans son entra√Ænement**.

En pratique, il convient de s√©parer un tableau de donn√©es en deux: un tableau d‚Äôentra√Ænement et un tableau de test. Il n'existe pas de standards sur la proportion √† utiliser dans l'un et l'autre. Cela d√©pend de la prudence de l'analyse et de l'ampleur de son tableau de donn√©es. Dans certains cas, nous pr√©f√©rerons couper le tableau √† 50%. Dand d'autres, nous pr√©f√©rerons r√©server le deux-tiers des donn√©es pour l'entra√Ænement, ou 70%, 75%. Rarement, toutefois, r√©servera-t-on moins plus de 50% et moins de 20% √† la phase de test.

Si les donn√©es sont peu √©quilibr√©es (par exemple, on retrouve peu de donn√©es de l'esp√®ce $A$, que l'on retrouve peu de donn√©es √† un pH inf√©rieur √† 5 ou que l'on a peu de donn√©es crois√©es de l'esp√®ce $A$ √† ph inf√©rieur √† 5), il y a un danger qu'une trop grande part, voire toute les donn√©es, se retrouvent dans le tableau d'entra√Ænement (certaines situations ne seront ainsi pas test√©es) ou dans le tableau de test (certaines situations ne seront pas couvertes par le mod√®le). L'analyste doit s'assurer de s√©parer le tableau au hasard, mais de mani√®re consciencieuse.

### Sousapprentissage et surapprentissage

Une difficult√© en mod√©lisation ph√©nom√©nologique est ce qui tient de la structure et ce qui tient du bruit. Lorsque l'on consid√®re une structure comme du bruit, on est dans un cas de sousapprentissage. Lorsque, au contraire, on interpr√®te du bruit comme une structure, on est en cas de surapprentissage. Les graphiques de la figure \@ref(fig:mesapprentissage) pr√©sentent ces deux cas, avec au centre un cas d'apprentissage conforme.

```{r, mesapprentissage, fig.align="center", fig.cap="Cas de figure de m√©sapprentissage. √Ä gauche, sous-apprentissage. Au centre, apprentissage valide. √Ä droite, surapprentissage.", echo = FALSE}
set.seed(35473)
n <- 50
x <- seq(0, 20, length = n) 
y <- 500 + 0.4 * (x-10)^3 + rnorm(n, mean=10, sd=80) # le bruit est g√©n√©r√© par rnorm()

par(mfrow = c(1, 3))
plot(x, y, main = "Sousapprentissage", col = "#46c19a", pch=16)
lines(x, predict(lm(y~x)), col = "#b94a73")

plot(x, y, main = "Apprentissage conforme", col = "#46c19a", pch=16)
lines(x, 
      predict(lm(y~x + I(x^2) + I(x^3))),
      col = "#b94a73")

plot(x, y, main = "Surapprentissage", col = "#46c19a", pch=16)
lines(x, 
      predict(lm(y~x + I(x^2) + I(x^3) + I(x^4) + 
                   I(x^5) + I(x^6) + I(x^7) + I(x^8) +
                   I(x^9) + I(x^10) + I(x^11) + I(x^12) +
                   I(x^13) + I(x^14) + I(x^15) + I(x^16))),
      col = "#b94a73")

```

Il est n√©anmoins difficile d'inspecter un mod√®le comprenant plusieurs entr√©es. On d√©tectera le m√©sapprentissage lorsque la pr√©cision d'un mod√®le est lourdement alt√©r√©e en phase de test. Une mani√®re de limiter le *m√©sapprentissage* est d'avoir recours √† la validation crois√©e.

### Validation crois√©e

Souvent confondue avec le fait de s√©parer le tableau en phases d'entra√Ænement et de test, la validation crois√©e est un principe incluant plusieurs algorithmes qui consiste √† entra√Æner le mod√®le sur un √©chantillonnage al√©atoire des donn√©es d'entra√Ænement. La technique la plus utilis√©e est le *k-fold*, o√π l'on s√©pare al√©atoirement le tableau d'entra√Ænement en un nombre *k* de tableaux. √Ä chaque √©tape de la validation crois√©e, on calibre le mod√®le sur tous les tableaux sauf un, puis on valide le mod√®le sur le tableau exclu. La performance du mod√®le en entra√Ænement est jug√©e sur les validations.

### Choix de l'algorithme d'apprentissage

Face aux [centaines d‚Äôalgorithmes d'apprentissages qui vous sont offertes](https://topepo.github.io/caret/available-models.html), choisir l'algorithme (ou les algorithmes) ad√©quats pour votre probl√®me n'est pas une t√¢che facile. Ce choix sera motiv√© par les tenants et aboutissants des algorithmes, votre exp√©rience, l'exp√©rience de la litt√©rature, l'exp√©rience de vos coll√®gues, etc. √Ä moins d'√™tre particuli√®rement surdou√©.e, il vous sera pratiquement impossible de ma√Ætriser la math√©matique de chacun d'eux. Une approche raisonnable est de tester plusieurs mod√®les, de retenir les mod√®les qui semblent les plus pertinents, et d'approfondir si ce n'est d√©j√† fait la math√©matique des options retenues. Ajoutons qu'il existe des algorithmes g√©n√©tiques, qui ne sont pas couverts ici, qui permettent de s√©lectionner des mod√®les d'autoapprentissages optimaux. Un de ces algorithmes est offert par le module Python [`tpot`](https://epistasislab.github.io/tpot/).

### D√©ploiement

Nous ne couvrirons pas la phase de d√©ploiement d'un mod√®le. Notons seulement qu'il est possible, en R, d'exporter un mod√®le dans un fichier `.Rdata`, qui pourra √™tre charg√© dans un autre environnement R. Cet environnement peut √™tre une feuille de calcul comme une interface visuelle mont√©e, par exemple, avec [Shiny](https://shiny.rstudio.com/) (chapitre \@ref(chapitre-explorer)).

----

En r√©sum√©,

1. Explorer les donn√©es
1. S√©lectionner des algorithmes
1. Effectuer un pr√©traitement
1. Cr√©er un ensemble d'entra√Ænement et un ensemble de test
1. Lisser les donn√©es sur les donn√©es d'entra√Ænement avec validation crois√©e
1. Tester le mod√®le
1. D√©ployer le mod√®le

## L'autoapprentissage en R

Plusieurs options sont disponibles.

1. Les modules que l'on retrouve en R pour l'autoapprentissage sont nombreux, et parfois sp√©cialis√©s. Il est possible de les utiliser individuellement.
1. Chacun de ces modules fonctionne √† sa fa√ßon. Le module **`caret`** de R a √©t√© con√ßu pour donner acc√®s √† des centaines de fonctions d'autoapprentissage via une interface commune.
1. Le module **`mlr`** occupe sensiblement le m√™me cr√©neau que **`caret`**, mais utilise plut√¥t une approche par objets connect√©s. Au moment d'√©crire ces lignes, **`mlr`** est peu document√©, donc *a priori* plus complexe √† prendre en main.
1. En Python, le module **`scikit-learn`** offre un interface unique pour l'utilisation de nombreuses techniques d'autoapprentissage. Il est possible d'appeler des fonctions de Python √† partir de R gr√¢ce au module **`reticulate`**.

Dans ce chapitre, nous verrons comment fonctionnent certains algorithmes s√©lectionn√©s, puis nous les appliquerons avec le module respectif qui m'a sembl√© le plus appropri√©.

```{r}
library("tidyverse") # √©videmment
library("caret")
```

## Algorithmes

Il existe des centaines d'algorithmes d'apprentissage. Je n'en couvrirai que quatre, qui me semblent √™tre appropri√©s pour la mod√©lisation ph√©nom√©nologique en agro√©cologie, et utilisables autant pour la r√©gression et la classification.

- Les *k* plus proches voisins 
- Les arbres de d√©cision
- Les r√©seaux neuronaux
- Les processus gaussiens

### Les *k* plus proches voisins

```{r les-voisons, out.width="100%", fig.align="center", fig.cap="<< Le... l'id√©e en arri√®re pour √™tre... euh... simpliste, l√† c'est que c'est un peu de... euhmm... de la vitamine de vinyle.>> - Georges ([Les voisins](https://youtu.be/-RpYi_Vuviw?t=6m40s), une pi√®ce de Claude Meunier).", echo = FALSE}
knitr::include_graphics("images/11_les-voisins.jpg")
```

Pour dire comme Georges, le... l'id√©e en arri√®re des KNN pour √™tre... euh... *simpliste*, c'est qu'un objet va ressembler √† ce qui se trouve dans son voisinage. Les KNN se basent en effet sur une m√©trique de distance pour rechercher un nombre *k* de points situ√©s √† proximit√© de la mesure. Les *k* points les plus proches sont retenus, *k* √©tant un entier non nul √† optimiser. Un autre param√®tre parfois utilis√© est la distance maximale des voisins √† consid√©rer: un voisin trop √©loign√© pourra √™tre discart√©. La r√©ponse attribu√©e √† la mesure est calcul√©e √† partir de la r√©ponse des *k* voisins retenus. Dans le cas d'une r√©gression, on utiliser g√©n√©ralement la moyenne. Dans le cas de la classification, la mesure prendra la cat√©gorie qui sera la plus pr√©sente chez les *k* plus proches voisins.

L'algorithme des *k* plus proches voisins est relativement simple √† comprendre. Certains pi√®ges sont, de m√™me, peuvent √™tre contourn√©s facilement. Imaginez que vous rechercher les points les plus rapproch√©s dans un syst√®me de coordonn√©es g√©ographiques o√π les coordonn√©es $x$ sont exprim√©es en m√®tres et les coordonn√©es $y$, en centim√®tres. Vous y projetez trois points (figure \@ref(fig:knn1)).

```{r, knn1, fig.align="center", fig.cap="Distances entre les points pour utilisation avec les KNN", echo = FALSE}
data <- data.frame(X = c(0, 1, 0),
                   Y = c(0, 0, 1),
                   row.names = c('A', 'B', 'C'))
par(pty="s")
plot(data, cex=3,
     xlab = 'Position X (m)', ylab = 'Position Y (cm)')
text(data, labels = rownames(data))
```

Techniquement la distance A-B est 100 plus √©lev√©e que la distance A-C, mais l'algorithme ne se soucie pas de la m√©trique que vous utilisez (figure \@ref(fig:knn1)). Il est primordial dans ce cas d'utiliser la m√™me m√©trique. Cette strat√©gie est √©vidente lorsque les variables sont comparables. C'est rarement le cas, que ce soit lorsque l'on compare des dimensions physionomiques (la longueur d'une phalange ou celle d'un f√©mur) mais lorsque les variables incluent des m√©langes de longueurs, des pH, des d√©comptes, etc., il est important de bien identifier la m√©trique et le type de distance qu'il convient le mieux d'utiliser. En outre, la standardisation des donn√©es √† une moyenne de z√©ro et √† un √©cart-type de 1 est une approche courrament utilis√©e.

#### Exemple d'application

Pour ce premier exemple, je pr√©senterai un cheminement d'autoapprentissage, du pr√©traitement au test. Nous allons essayer de classer les esp√®ces de dragon selon leurs dimensions.

```{r dragons, out.width="50%", fig.align="center", fig.cap="Dimensions mesur√©s sur les dragons captur√©s.", echo = FALSE}
knitr::include_graphics("images/11_dragon.png")
```

```{r}
dragons <- read_csv("data/11_dragons.csv")
```

Assurons-nous que les donn√©es sont toutes √† l'√©chelle. Nous pourrions utiliser la fonction `scale()`. Toutefois, si je capture un nouveau dragon, je n'aurai pas l'information pour convertir mes nouvelles dimensions dans la m√™me m√©trique que celle utilis√©e pour lisser mon mod√®le. Prenez donc soin de conserver la moyenne et l'√©cart-type pour subs√©quemment calculer des mises √† l'√©chelle.

```{r}
dim_means <- dragons %>% 
  dplyr::select(starts_with("V")) %>% 
  summarise_all(mean, na.rm = TRUE)

dim_sds <- dragons %>% 
  dplyr::select(starts_with("V")) %>% 
  summarise_all(sd, na.rm = TRUE)

dragons_sc <- dragons %>% 
  dplyr::select(starts_with("V")) %>%
  scale(.) %>% 
  as_tibble() %>% 
  mutate(Species = dragons$Species)
```

S√©parons les donn√©es en entra√Ænement (`_tr`) et en test (`_te`) avec une proportion 70/30 (`p = 0.7`). Il est essentiel d'utiliser `set.seed()` pour s'assurer que la partition soit la m√™me √† chaque session de code (pour la reproductibilit√©) - j'ai l'habitude de taper n'importe quel num√©ro √† environ 6 chiffres, mais lors de publications, je vais sur [random.org](https://www.random.org/) et je g√©n√®re un num√©ro au hasard, sans biais.

```{r}
set.seed(68017)
id_tr <- createDataPartition(dragons_sc$Species, p = 0.7, list = FALSE)
dragons_tr <- dragons_sc[id_tr, ]
dragons_te <- dragons_sc[-id_tr, ]
```

Avant de lancer nos calculs, allons vois sur la [page de caret](https://topepo.github.io/caret/available-models.html) les modules qui effectuent des KNN pour la classification. Nous trouvons **`knn`** et **`kknn`**. Si les modules n√©cessaires aux calculs ne sont pas install√©s sur votre ordinateur, **`caret`** vous demandera de les installer. Prenons le module **`kknn`**, qui demande le param√®tre `kmax`, soit le nombre de voisins √† consid√©rer, ainsi qu'un param√®tre de `distance` (sp√©cifiez 1 pour la distance de Mahattan et 2 pour la distance euclidienne), et un `kernel`, qui est une fonction pour mesurer la distance. Comment choisir les bons param√®tres? Une mani√®re de proc√©der est de cr√©er une grille de param√®tres.

```{r}
kknn_grid <- expand.grid(kmax = 3:6,
                         distance = 1:2,
                         kernel = c("rectangular", "gaussian", "optimal"))
```

Les noms des colonnes de la grille doivent correspondre aux noms des param√®tres du mod√®le. Nous allons mod√©liser avec une validation crois√©e √† 5 plis.

```{r}
ctrl <- trainControl(method="repeatedcv", repeats = 5)
```

Pour finalement lisser le mod√®le.

```{r}
set.seed(8961704)
clf <- train(Species ~ .,
             data = dragons_tr,
             method = "kknn",
             tuneGrid = kknn_grid,
             trainControl = ctrl)
clf
```

Nous obtenons les param√®tres du mod√®le optimal. Pr√©disons l'esp√®ce de dragons selon ses dimensions pour chacun des tableaux.

```{r}
pred_tr <- predict(clf)
pred_te <- predict(clf, newdata = dragons_te)
```

Une mani√®re d'√©valuer la pr√©diction est d'afficher un tableau de contingence.

```{r}
table(dragons_tr$Species, pred_tr)
```

```{r}
table(dragons_te$Species, pred_te)
```

Les esp√®ces de dragon sont toutes bien class√©es tant entra√Ænement qu'en test (c'est rarement le cas dans les situations r√©elles).

### Les arbres d√©cisionnels

```{r ents, out.width="100%", fig.align="center", fig.cap="Les Ents, tir√© du film le Seigneur des anneaux, qui prennent trop de temps avant de se d√©cider - paradoxalement, les abrbres de d√©cisions sont dot√©s d'algorithmes rapides.", echo = FALSE}
knitr::include_graphics("images/11_Entmoot.jpg")
```

Un arbre d√©cisionnel est une collection hi√©rarchis√©e de d√©cisions, le plus souvent binaires. Chaque embranchement est un test √† vrai ou faux sur une variable. La r√©ponse, que ce soit une cat√©gorie ou une valeur num√©rique, se trouve au bout de la derni√®re branche. Les suites de d√©cisions sont organis√©es de mani√®re √† ce que la pr√©cision de la r√©ponse soit optimis√©e. Ils ont l'avantage de pouvoir √™tre exprim√©s en un sch√©ma simple et imprimable.

```{r jj-dt, out.width="50%", fig.align="center", fig.cap="Exemple d'arbre de d√©cision, tir√© du [blogue de Jeremy Jordon](https://www.jeremyjordan.me/decision-trees/).", echo = FALSE}
knitr::include_graphics("https://www.jeremyjordan.me/content/images/2017/03/Screen-Shot-2017-03-11-at-10.15.37-PM.png")
```

Les arbres sont notamment param√©tr√©s par le nombre maximum d'embranchements, qui s'il est trop √©lev√© peut mener √† du surapprentissage. [Il existe de nombreux algorithmes d'arbres de d√©cision](https://topepo.github.io/caret/available-models.html).

Une collection d'arbres devient une for√™t. Les for√™ts al√©atoires (*random forest*) sont une cat√©gorie d'algorithmes compos√©s de plusieurs arbres de d√©cision optimis√©s sur des donn√©es r√©pliqu√©es al√©atoirement par *bagging*. Allons-y par √©tape. √Ä partir des donn√©es existantes compos√©es de *n* observations (donc *n* lignes) s√©lectionn√©es pour l'entra√Ænement, √©chantillonnons au hasard *avec remplacement* un nombre *n* de nouvelles observations. Le remplacement implique qu'on retrouvera fort probablement dans notre nouveau tableau des lignes identiques. Lissons un arbre sur notre tableau al√©atoire. Effectuons un nouveau tirage, puis un autre arbre. Puis encore, et encore, disons 10 fois. Nous obtiendrons une for√™t de 10 arbres. Pour une nouvelle observation √† pr√©dire, nous obtenons donc 10 pr√©dictions, sur lesquelles nous pouvons effectuer un moyenne s'il s'agit d'une variable num√©rique, ou bien prenons la cat√©gorie la plus souvent pr√©dite dans le cas d'une classification. Les for√™ts al√©atoires peuvent √™tre constitu√©s de 10, 100, 1000 arbres: autant qu'il en est n√©cessaire.

#### Exemple d'application

Utilisons toujours nos donn√©es de dimensions de dragons. Bien qu'il en existe plusieurs, le module conventionnel pour effectuer un arbre de d√©cision est **`rpart2`**. [Sur la page de **`caret`**](https://topepo.github.io/caret/available-models.html), nous trouvons **`rpart2`**, apte pour les classifications et les r√©gressions, [qui n'a besoin que du param√®tre `maxdepth`](https://topepo.github.io/caret/train-models-by-tag.html#random-forest).

```{r}
rpart2_grid <- expand.grid(maxdepth = 3:10) # expand_grid n'est pas n√©cessaire ici
```

Prenons 5 plis encore une fois.

```{r}
ctrl <- trainControl(method="repeatedcv", repeats = 5)
```

Pour finalement lisser le mod√®le.

```{r}
set.seed(3468973)
clf <- train(Species ~ .,
             data = dragons_tr,
             method = "rpart2",
             tuneGrid = rpart2_grid)
clf
```

Nous obtenons les param√®tres du mod√®le optimal: `maxdepth = 3` - puisque c'est √† la limite inf√©rieure de la grille, mieux vaudrait √©tendre la grille, mais passons pour l'exemple. Comme je l'ai mentionn√©, un arbre de d√©cision est un outil convivial √† visualiser.

```{r}
plot(clf$finalModel)
text(clf$finalModel)
```

Ou en plus beau, je vous laisse essayer.

```{r eval = FALSE}
library("rattle")
fancyRpartPlot(clf$finalModel)
```

Tout comme pour les KNN, pr√©disons l'esp√®ce de dragons selon ses dimensions pour chacun des tableaux.

```{r}
pred_tr <- predict(clf)
pred_te <- predict(clf, newdata = dragons_te)
```

En ce qui a trait aux tableaux de contigence...

```{r}
table(dragons_tr$Species, pred_tr)
```

```{r}
table(dragons_te$Species, pred_te)
```

Les esp√®ces de dragon sont toutes bien class√©es en entra√Ænement et en test... sauf pour les dragons de caverne, qui (l'avez-vous remarquez?) n'apparaissent pas dans l'arbre de d√©cision!

Le module **`caret`** vient avec la fonction `varImp()` qui offre une appr√©ciation de l'importance des variables dans le mod√®le final. La notion d'importance varie d'un mod√®le √† l'autre, et reste √† ce jour mal document√©. Mieux vaut en examiner les tenants et aboutissants avant d'interpr√©ter exessivement la sortie de cette fonction.

```{r}
varImp(clf) %>% plot(.)
```

On pourra effectuer de la m√™me mani√®re une for√™t al√©atoire, mais cette fois-ci avec le module **`rf`**.

```{r}
set.seed(3468973)
ctrl <- trainControl(method="repeatedcv", repeats = 5)
clf <- train(Species ~ .,
             data = dragons_tr,
             method = "rf")
clf
```

Et les r√©sultats.

```{r}
pred_tr <- predict(clf)
pred_te <- predict(clf, newdata = dragons_te)
table(dragons_te$Species, pred_te)
table(dragons_tr$Species, pred_tr)
```

Notez que les for√™ts al√©atoires ne g√©n√®re par de visuel.

### Les r√©seaux neuronaux

Apr√®s les KNN et les *random forests*, nous passons au domaine plus complexe des r√©seaux neuronaux. Le terme *r√©seau neuronal* est une m√©taphore li√©e √† une perception que l'on avait du fonctionnement du cerveau humain lorsque la technique des r√©seaux neuronaux a √©t√© d√©velopp√©e dans les ann√©es 1950. Un r√©seau neuronal comprend une s√©rie de bo√Ætes d'entr√©es li√©e √† des fonctions qui transforment et acheminent successivement l'information jusqu'√† la sortie d'une ou plusieurs r√©ponse. Il existe plusieurs formes de r√©seaux neuronnaux, dont la plus simple manifestation est le *perceptron multicouche*. Dans l'exemple de la figure \@ref(fig:nn1), on retrouve 4 variables d'entr√©e et trois variables de sortie entre lesquelles on retrouve 5 couches dont le nombre de neurones varient entre 3 et 6.

```{r nn1, out.width="50%", fig.align="center", fig.cap="R√©seau neuronal sch√©matis√©, Source: [Neural designer](https://www.neuraldesigner.com/).", echo = FALSE}
knitr::include_graphics("images/11_deep_neural_network.png")
```

Entre la premi√®re couche de neurones (les variables pr√©dictives) et la derni√®re couche (les variables r√©ponse), on retrouve des *couches cach√©es*. Chaque neurone est reli√© √† tous les neurones de la couche suivante.

Les liens sont des poids, qui peuvent prendre des valeurs dans l'ensemble des nombres r√©els. √Ä chaque neurone suivant la premi√®re couche, on fait la somme des poids multipli√©s par la sortie du neurone. Le nombre obtenu entre dans chaque neurone de la couche. Le neurone est une fonction, souvent tr√®s simple, qui transforme le nombre. La fonction plus utilis√©e est probablement la fonction ReLU, pour *rectified linear unit*, qui expulse le m√™me nombre aux neurones de la prochaine couche s'il est positif: sinon, il expulse un z√©ro.

**Exercice**. Si tous les neurones sont des fonctions ReLU, calculez la sortie de ce petit r√©seau neuronal.

<img src="images/11_nn_ex1_Q.jpg" width="600px">

Vous trouverez la r√©ponse sur l'image `images/11_nn_ex1_R.jpg`.

Il est aussi possible d'ajouter un *biais* √† chaque neurone, qui est un nombre r√©el additionn√© √† la somme des neurones pond√©r√©e par les poids.

L'optimisation les poids pour chaque lien et les biais pour chaque neurone (gr√¢ce √† des algorithmes dont le fonctionnement sort du cadre de ce cours) constitue le processus d'apprentissage. Avec l'aide de logiciels et de modules sp√©cialis√©s, la construction de r√©seaux de centaines de neurones organis√©s en centaines de couches vous permettra de capter des patrons complexes dans des ensembles de donn√©es.

Vous avez peut-√™tre d√©j√† entendu parler d'apprentissage profond (ou *deep learning*). Il s'agit simplement d'une appellation des r√©seaux neuronaux modernis√© pour insister sur la pr√©sence de nombreuses couches de neurones. C'est un terme √† la mode.

#### Les r√©seaux neuronaux sur R avec **`neuralnet`**

Plusieurs modules sont disponibles sur R pour l'apprentissage profond. Certains utilisent le module [H2O.ia](https://github.com/h2oai/h2o-3), propuls√© en Java, d'autres utilisent plut√¥t [Keras](https://keras.rstudio.com/), propuls√© en Python par l'interm√©diaire de [Tensorflow](https://www.tensorflow.org/). J'ai une pr√©f√©rence pour Keras, puisqu'il supporte les r√©seaux neuronaux classiques (perceptrons multicouche) autant que convolutifs ou r√©currents. Keras pourrait n√©anmoins √™tre difficile √† installer sur Windows, o√π Python ne vient pas par d√©faut. Sur Windows, Keras ne fonctionne qu'avec Anaconda: vous devez donc installez [Anaconda ou Miniconda](https://www.anaconda.com/download/#windows) (Miniconda offre une installation minimaliste).

Donc, pour ce cours, nous utiliserons le module **`neuralnet`**. Il est possible de l'utilser gr√¢ce √† l'interface de **`caret`**, mais son utilisation directe permet davantage de flexibilit√©. Chargeons les donn√©es d'iris.

```{r}
library("neuralnet")
data("iris")
```

Prenons soin de segmenter nos donn√©es en entra√Ænement et en test.

```{r}
set.seed(8453668)
iris_tr_index <- createDataPartition(y=iris$Species, p = 0.75, list = FALSE)
```

Nous pouvons ainsi cr√©er nos tableaux d'entra√Ænement et de test pour les variables pr√©dictives.

Les r√©seaux neuronnaux sont aptes √† g√©n√©rer des sorties multiples. Nous d√©sirons pr√©dire une cat√©gorie, et **`neuralnet`** ne s'occupe pas de les transformer de facto. Lors de la pr√©diction d'une cat√©gorie, nous devons g√©n√©r√©e des sorties multiples qui permettront de d√©cider de l'appartenance exclusive √† une cat√©gorie ou une autre. Nous avons abord√© l'encodage cat√©goriel aux chapitres \@ref(chapitre-biostats) et \@ref(chapitre-explorer). C'est ce que nous ferons ici.

```{r}
species_oh <- model.matrix(~ 0 + Species, iris)
colnames(species_oh) <- levels(iris$Species)
iris_oh <- iris %>% 
  cbind(species_oh)
iris_tr <- iris_oh[iris_tr_index, ]
iris_te <- iris_oh[-iris_tr_index, ]
```

Lan√ßons le r√©seau neuronnal avec l'interface-formule de R (neuralnet n'accepte pas le `.` pour indiquer *prend toutes les variables √† l'exeption de celles utilis√©es en y*): nous allons les inclure √† la main.  L'argument `hidden` est un vecteur qui indique le nombre de neuronnes pour chaque couche. L'argument `linear.input` indique si l'on d√©sire travailler en r√©gression (`linear.output = TRUE`) ou en classification (`linear.output = FALSE`). Lorsque les donn√©es sont nombreuses, patience, le calcul prend pas mal de temps. Dans ce cas-ci, nous avons un tout petit tableau.

```{r}
nn <- neuralnet(setosa + versicolor + virginica ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,
                data = iris_tr, 
                hidden = c(5, 5),
                act.fct = "tanh",
                linear.output = FALSE)
```

Un r√©seau neuronnal peu complexe peut √™tre lisible.

```{r}
plot(nn)
```

Il n'existe pas de r√®gle stricte sur le nombre de couche et le nombre de noeud par couche. Il est n√©anmoins conseill√© de g√©n√©rer d'abord un mod√®le simple, puis au besoin de le complexifier graduellement en terme de nombre de noeuds, puis de nombre de couches. Si vous d√©sirez aller plus loin et utiliser keras, le module [`autokeras`](https://autokeras.com/), disponible seulement en Python, est con√ßu pour optimiser un mod√®le Keras.

La sortie du r√©seau neuronal est une valeur pr√®s de 1 ou une valeur pr√®s de 0. Voici une mani√®re de g√©n√©rer un vecteur cat√©goriel.

```{r}
compute_te <- compute(nn, iris_te)
pred_te <- compute_te$net.result %>%
  as_tibble() %>% 
  apply(., 1, which.max) %>% 
  levels(iris$Species)[.] %>%
  as.factor()
```

La fonction `caret::confusionMatrix()` permet de g√©n√©rer les statistiques du mod√®le.

```{r}
confusionMatrix(iris_te$Species, pred_te)
```

Encore une fois, c'est rarement le cas mais nous obtenons une classification parfaite.

#### Pour aller plus loin

En une heure divis√©e en [4 vid√©os](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi), Grant Sanderson explique les r√©seaux neuronaux de mani√®re intuitive. En ce qui a trait √† Keras, je recommande le livre [Deep learning with R, de Fran√ßois Allaire](https://www.safaribooksonline.com/library/view/deep-learning-with/9781617295546/?ar), auquel vous avez acc√®s avec un IDUL de l'Universit√© Laval. Si vous vous sentez √† l'aise √† utiliser Keras avec le langage Python, je vous recommande le cours gratuit en ligne [*Applications of deep neural networks*, de Jeff Heaton](https://www.youtube.com/watch?v=sRy26qWejOI&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN).

Des types de r√©seaux neuronaux sp√©cialis√©s ont √©t√© d√©velopp√©s. Je les pr√©sente sans aller dans les d√©tails.

- **R√©seaux neuronaux convolutif**. Ce type de r√©seau neuronal est surtout utilis√© en reconnaissance d'image. Les couches de neurones convolutifs poss√®dent, en plus des fonctions des perceptrons classiques, des filtres permettant d'int√©grer les variables descriptives connexes √† l'observation: dans le cas d'une image, il s'agit de scanner les pixels au pourtour du pixel trait√©. [Une br√®ve introduction sur Youtube](https://www.youtube.com/watch?v=YRhxdVk_sIs).
- **R√©seaux neuronaux r√©currents**. Pr√©dire des occurrences futures √† partir de s√©ries temporelles implique que la r√©ponse au temps t d√©pend non seulement de conditions externes, mais aussi le la r√©ponse au temps t-1. Les r√©seaux neuronaux r√©currents. Vous devrez ajouter des neurones particuliers pour cette t√¢che, qui pourra √™tre pris en charge par Keras gr√¢ce aux couches de type [*Long Short-Term Memory network*, ou LSTM](https://www.youtube.com/watch?v=UnclHXZszpw).
- **R√©seaux neuronaux probabilistes**. Les r√©seaux neuronaux non-probabilistes offre une estimation de la variable r√©ponse. Mais quelle est la cr√©dibilit√© de la r√©ponse selon les variables descriptives? Question qui pourrait se r√©v√©ler cruciale en m√©decine ou en ing√©nierie, √† la laquelle on pourra r√©pondre en mode probabiliste. Pour ce faire, on pose des distributions *a priori* sur les poids du r√©seau neuronal. Le module [`edward`](http://edwardlib.org/), programm√© et distribu√© en Python, offre cette possibilit√©. Vous pourrez acc√©der √† `edward` gr√¢ce au module `reticulate`, mais √† ce stade mieux vaudra basculer en Python. Pour en savoir davantage, consid√©rez [cette conf√©rence de Andrew Rowan](https://www.youtube.com/watch?v=I09QVNrUS3Q).

### Les processus gaussiens

Les sorties des techniques que sont les KNN, les arbres ou les for√™ts ainsi que les r√©seaux neuronaux sont (classiquement) des nombres r√©els ou des cat√©gories. Dans les cas o√π la cr√©dibilit√© de la r√©ponse est importante, il devient pertinent que la sortie soit probabiliste: les pr√©dictions seront alors pr√©sent√©es sous forme de distributions de probabilit√©. Dans le cas d‚Äôune classification, la sortie du mod√®le sera un vecteur de probabilit√© qu‚Äôune observation appartienne √† une classe ou √† une autre. Dans celui d‚Äôune r√©gression, on obtiendra une distribution continue.

Les **processus gaussiens** tirent profit des statistiques bay√©siennes pour effectuer des pr√©dictions probabilistes. D‚Äôautres techniques peuvent √™tre utilis√©es pour effectuer des pr√©dictions probabilistes, comme les [r√©seaux neuronaux probabilistes](http://edwardlib.org/iclr2017), que j'ai introduits pr√©c√©demment.

Bien que les processus gaussiens peuvent √™tre utilis√©s pour la classification, son fonctionnement s'explique favorablement, de mani√®re intuitive, pas la r√©gression.

#### Un approche intuitive

Ayant acquis de l'exp√©rience en enseignement des processus gaussiens, [John Cunningham](http://stat.columbia.edu/~cunningham/) a d√©velopp√© une approche intuitive permettant de saisir les m√©canismes des processus gaussiens. lors de conf√©rences disponible sur YouTube ([1](https://youtu.be/BS4Wd5rwNwE), [2](https://www.youtube.com/watch?v=Jv25sg-IYHU)), il aborde le sujet par la n√©cessit√© d'effectuer une r√©gression non-lin√©aire.

G√©n√©rons d'abord une variable pr√©dictive `x`, l'heure, et une variable r√©ponse `y`, le rythme cardiaque d'un individu en battements par minute (bpm).

```{r}
x <- c(7, 8, 10, 14, 17)
y <- c(61, 74, 69, 67, 78)

plot(x, y, xlab="Heure", ylab="Rythme cardiaque (bpm)")
abline(v=12, lty=3, col='gray50');text(12, 67, '?', cex=2)
abline(v=16, lty=3, col='gray50');text(16, 72, '?', cex=2)
```

Poser un probl√®me par un processus gaussien, c'est se demander les valeurs cr√©dibles qui pourraient √™tre obtenues hors du domaine d'observations (par exemple, dans la figure ci-dessus, √† `x=12` et `x=16`)? Ou bien, de mani√®re plus g√©n√©rale, *quelles fonctions ont pu g√©n√©rer les variables r√©ponse √† partir d'une structure dans les variables pr√©dictives?*

Les distributions normales, que nous appellerons *gaussiennes* dans cette section par concordance avec le terme *processus gaussien*, sont particuli√®rement utiles pour r√©pondre √† cette question.

Nous avons vu pr√©c√©demment ce que sont les distributions de probabilit√©: des outils math√©matiques permettant d'appr√©hender la structure des processus al√©atoires. Une distribution gaussienne repr√©sente une situation o√π l'on tire au hasard des valeurs continues. Une distribution gaussienne de la variable al√©atoire $X$ de moyenne $0$ et de variance de $1$ est not√©e ainsi:

$$ X \sim \mathcal{N} \left( 0, 1\right)$$

Par exemple, une courbe de distribution gaussienne du rythme cardiaque √† 7:00 pourrait prendre la forme suivante.

$$ bpm \sim \mathcal{N} \left( 65, 5\right)$$

En `R`:

```{r}
x_sequence <- seq(50, 80, length=100)
plot(x_sequence,
     dnorm(x_sequence, mean=65, sd=5),
     type="l",
     xlab="Rythme cardiaque (bpm)",
     ylab="Densit√©")
```

Une distribution **bi**normale, un cas particulier de la distribution **multi**normale, comprendra deux vecteurs, $x_1$ et $x_2$. Elle aura donc deux moyennes. Puisqu'il s'agit d'une distribution binormale, et non pas deux distributions normales, les deux variables ne sont pas ind√©pendantes et l'on utilisera une matrice de covariance au lieu de deux variances ind√©pendantes.

$$
\binom{x_1}{x_2} \sim \mathcal{N}
\Bigg( 
\binom{\mu_1}{\mu_2},
\left[ {\begin{array}{cc}
\Sigma_{x_1} & \Sigma_{x_1,x_2} \\
\Sigma_{x_1,x_2}^T & \Sigma_{x_2} \\
\end{array} } \right]
\Bigg)
$$

La matrice $\Sigma$, dite de *variance-covariance*, indique sur sa diagonale les variances des variables ($\Sigma_{x_1}$ et $\Sigma_{x_2}$). Les covariances $\Sigma_{x_1,x_2}$ et $\Sigma_{x_1,x_2}^T$ sont sym√©triques et indiquent le lien entre les variables.

On pourrait supposer que le rythme cardiaque √† 8:00 soit corr√©l√© avec celui √† 7:00. Mises ensembles, les distributions gaussiennes √† 7:00 et √† 8:00 formeraient une distribution gaussienne binormale.

$$
\binom{bpm_7}{bpm_8} \sim \mathcal{N}
\Bigg( 
\binom{65}{75},
\left[ {\begin{array}{cc}
10 & 6 \\
6 & 15 \\
\end{array} } \right]
\Bigg)
$$

En `R`:

```{r}
library("ellipse")
means_vec <- c(65, 75)
covariance_mat <- matrix(c(10, 6, 6, 15), ncol=2)
par(pty='s')
plot(ellipse(x=covariance_mat, centre=means_vec, levels=0.95), 
     type='l',
     xlab="Rythme cardiaque √† 7:00 (bpm)",
     ylab="Rythme cardiaque √† 8:00 (bpm)")
#lines(ellipse(x=covariance_mat, centre=means_vec, level=0.8))
```

On peut se poser la question: √©tant donn√©e que $x_1 = 68$, quelle serait la distribution de $x_2$? Dans ce cas bivari√©e, la distribution marginale serait univari√©e, mais dans le cas multivari√© en $D$ dimensions, la distribution marginale o√π l'on sp√©cifie $m$ variables serait de $D-m$. de  Une propri√©t√© fondamentale d'une distribution gaussienne est que peu importe l'endroit o√π l'angle selon lequel on la tranche, la distribution marginale sera aussi gaussienne. Lorsque l'on retranche une ou plusieurs variables en sp√©cifiant la valeur qu'elles prennent, on applique un *conditionnement* √† la distribution.

```{r echo = FALSE}
library("condMVNorm")

condition_x1 <- 61 # changer ce chiffre pour visualiser l'effet

cond_parameters <- condMVN(mean=means_vec, sigma=covariance_mat,
                           dependent=2, given=1, X.given=condition_x1)
cond_mean <- cond_parameters$condMean
cond_sd <- sqrt(cond_parameters$condVar)
x2_sequence <- seq(50, 90, length=100)
x2_dens <- dnorm(x2_sequence, mean=cond_mean, sd=cond_sd)

par(pty='s')
plot(ellipse(x=covariance_mat, centre=means_vec, levels=0.95), type='l',
     xlab="Rythme cardiaque √† 7:00 (bpm)",
     ylab="Rythme cardiaque √† 8:00 (bpm)")
abline(v=condition_x1, col='#f8ad00', lwd=2, lty=2)
lines(x=condition_x1 + x2_dens*40, y=x2_sequence, col="#f8ad00", lwd=2)
lines(x = c(condition_x1, condition_x1),
      y = c(cond_mean-cond_sd, cond_mean+cond_sd),
      lwd=3, col='#46c19a')
points(condition_x1, cond_mean, 
       col='#46c19a', pch=16, cex=2)

n_sample <- 20
points(x = rep(condition_x1, n_sample),
       y = rnorm(n_sample, cond_mean, cond_sd),
       pch=4, col = rgb(0, 0, 0, 0.5))
```

Les points sur l'axe (symbole x) conditionn√©s sont des √©chantillons tir√©s au hasard dans la distribution conditionn√©e.

Une autre mani√®re de visualiser la distribution gaussienne binormale est de placer $x_1$ et $x_2$ c√¥te √† c√¥te en abscisse, avec leur valeur en ordonn√©e. Le bloc de code suivant peut sembler lourd au premier coup d‚Äô≈ìil: pas de panique, il s'agit surtout d'instructions graphiques. Vous pouvez vous amuser √† changer les param√®tres de la distribution binormale (section 1) ainsi que la valeur de $x_1$ √† laquelle est conditionn√©e la distribution de $x_2$ (section 2).

```{r echo = FALSE}
source("lib/plot_matrix.R")

# 1. Distribution
means_vec <- c(65, 65)
covariance_mat <- matrix(c(10, 6, 6, 15), ncol=2)

# 2. Condition
condition_x1 <- 61 # changer ce chiffre pour visualiser l'effet

# 3. Densit√© conditionn√©e
cond_parameters <- condMVN(mean=means_vec, sigma=covariance_mat,
                           dependent=2, given=1, X.given=condition_x1)
cond_mean <- cond_parameters$condMean
cond_sd <- sqrt(cond_parameters$condVar)
x2_sequence <- seq(50, 90, length=100)
x2_dens <- dnorm(x2_sequence, mean=cond_mean, sd=cond_sd)
x2_draw <- rnorm(1, cond_mean, cond_sd)

# 4. Graphiques
options(repr.plot.width = 8, repr.plot.height = 5)
layout(matrix(c(1,2,3,3), nrow=2), widths=c(1,2))
par(mar=c(4, 4, 1, 1), pty='s')

## 4.1 Ellipse
plot(ellipse(x=covariance_mat, centre=means_vec, levels=0.95), 
     type='l', xlab="BPM √† 7:00", ylab="BPM √† 8:00")
abline(v=condition_x1, col='#f8ad00', lwd=1)
lines(x=condition_x1 + x2_dens*40, y=x2_sequence, col="#f8ad00", lwd=1)
lines(x = c(condition_x1, condition_x1),
      y = c(cond_mean-cond_sd, cond_mean+cond_sd),
      lwd=2, col='#46c19a')
points(condition_x1, cond_mean, 
       col='#46c19a', pch=16, cex=1)
points(condition_x1, x2_draw, pch=16, col="#b94a73")

## 4.2 Covariance
plot_matrix(covariance_mat)

## 4.3 S√©rie
plot(c(1, 2), c(condition_x1, x2_draw), xlim=c(0, 6), ylim=c(55, 75), type='l',
     xlab="Indice de la variable", ylab="Rythme cardiaque (bpm)")
points(1, condition_x1, pch=16, col='#46c19a', cex=3)
points(2, x2_draw, pch=16, col='#b94a73', cex=3)
```

Les valeurs que peuvent prendre le rythme cardiaque en $x_2$ sont tir√©es al√©atoirement d'une distribution conditionn√©e. Sautons maintenant au cas multinormal, incluant 6 variables (*hexanormal*!). Afin d'√©viter de composer une matrice de covariance √† la mitaine, je me permets de la g√©n√©rer avec une fonction. Cette fonction particuli√®re est nomm√©e *fonction de base radiale* ou *exponentiel de la racine*.

$$K_{RBF} \left( x_i, x_j \right) = \sigma^2 exp \left( -\frac{\left( x_i - x_j \right)^2}{2 l^2}  \right) $$

```{r}
RBF_kernel <- function(x, sigma, l) {
  n <- length(x)
  k <- matrix(ncol = n, nrow = n)
  for (i in 1:n) {
    for (j in 1:n) {
      k[i, j] = sigma^2 * exp(-1/(2*l^2) * (x[i] - x[j])^2)
    }
  }
  colnames(k) <- paste0('x', 1:n)
  rownames(k) <- colnames(k)
  return(k)
}
```

Dans la fonction `RBF_kernel`, `x` d√©signe les dimensions, `sigma` d√©signe un √©cart-type commun √† chacune des dimensions et `l` est la longueur d√©signant l'amplification de la covariance entre des dimensions √©loign√©es (dans le sens que la premi√®re dimension est √©loign√©e de la derni√®re). Pour 6 dimensions, avec un √©cart-type de 4 et une longueur de 2.

```{r}
covariance_6 <- RBF_kernel(1:6, sigma=4, l=2)
round(covariance_6, 2)
```

Changez la valeur de `l` permet de bien saisir son influence sur la matrice de covariance. Avec un `l` de 1, la covariance entre $x_1$ et $x_6$ est pratiquement nulle: elle est un peut plus √©lev√©e avec `l=2`. Pour reprendre l'exemple du rythme cardiaque, on devrait en effet s'attendre √† retrouver une plus grande corr√©lation entre celles mesur√©es aux temps 4 et 5 qu'entre les temps 1 et 6.

De m√™me que dans la situation o√π nous avions une distribution binormale, nous pouvons conditionner une distribution multinormale. Dans l'exemple suivant, je conditionne la distribution multinormale de 6 dimensions en sp√©cifiant les valeurs prises par les deux premi√®res dimensions. Le r√©sultat du conditionnement est une distribution en 4 dimensions. Puisqu'il est difficile de pr√©senter une distribution en 6D, le graphique en haut √† gauche ne comprend que les dimensions 1 et 6. Remarquez que la corr√©lation entre les dimensions 1 et 6 est faible, en concordance avec la matrice de covariance g√©n√©r√©e par la fonction `RBF_kernel`. Lancez plusieurs fois le code et voyez ce qui advient des √©chantillonnages dans les dimensions 3 √† 6 selon le conditionnement en 1 et 2.

```{r echo = FALSE}
library("MASS")

# 1. Distribution
means_vec <- rep(65, 6)
covariance_mat <- covariance_6

# 2. Condition
conditions_x <- c(61, 74) # changer ces chiffres pour visualiser l'effet

# 3. Densit√© conditionn√©e
cond_parameters <- condMVN(mean=means_vec, sigma=covariance_mat, 
                           dependent.ind = 3:6, given.ind=1:2,
                           X.given=conditions_x)
cond_mean <- cond_parameters$condMean
cond_sd <- sqrt(cond_parameters$condVar)
x6_sequence <- seq(50, 90, length=100)
x6_dens <- dnorm(x2_sequence, mean=cond_mean[4], sd=cond_sd[4, 4])

x_3.6_draw <- mvrnorm(n = 1, mu = cond_mean, Sigma = cond_sd^2)

# 4. Graphiques
layout(matrix(c(1,2,3,3), nrow=2), widths=c(1,2))
par(mar=c(4, 4, 1, 1))

## 4.1 Ellipse
plot(ellipse(x=covariance_mat[c(1, 6), c(1, 6)], centre=means_vec[c(1, 6)], levels=0.95), 
     type='l', xlab="BPM √† 7:00", ylab="BPM √† 8:00")
abline(v=conditions_x[1], col='#f8ad00', lwd=1)
lines(x=condition_x1 + x6_dens*40, y=x2_sequence, col="#f8ad00", lwd=1)
lines(x = c(conditions_x[1], conditions_x[1]),
      y = c(cond_mean[4]-cond_sd[4, 4], cond_mean[4]+cond_sd[4, 4]),
      lwd=2, col='#46c19a')
points(conditions_x[1], cond_mean[4],
       col='#46c19a', pch=16, cex=1)
points(conditions_x[1], x_3.6_draw[4], pch=16, col="#b94a73")

## 4.2 Covariance
plot_matrix(covariance_mat, cex=0.8)

## 4.3 S√©rie
plot(1:6, c(conditions_x, x_3.6_draw), xlim=c(0, 6), ylim=c(60, 85), type='l',
     xlab="Indice de la variable", ylab="Rythme cardiaque (bpm)")
points(c(1, 2), conditions_x, pch=16, col='#46c19a', cex=3)
points(3:6, x_3.6_draw, pch=16, col='#b94a73', cex=3)
```

La structure de la covariance assure que les dimensions proches prennent des valeurs similaires, assurant une courbe lisse et non en dents de scie. Pourquoi s'arr√™ter √† 6 dimensions? Prenons-en plusieurs, puis g√©n√©rons plus d'un √©chantillon. Ensuite, utilisons ces simulations pour de calculer la moyenne et l'√©cart-type de chacune des dimensions.

```{r echo=FALSE}
# 1. Distribution
n <- 20
means_vec <- rep(65, n)
covariance_mat <- RBF_kernel(x = 1:n, sigma = 10, l = 2)

# 2. Condition
conditions_x <- c(61, 74) # changer ces chiffres pour visualiser l'effet

# 3. Densit√© conditionn√©e
cond_parameters <- condMVN(mean=means_vec, sigma=covariance_mat, 
                           dependent.ind = 3:n, given.ind=1:2,
                           X.given=conditions_x)
cond_mean <- cond_parameters$condMean
cond_sd <- cond_parameters$condVar

# 4. Graphiques
par(mar=c(4, 4, 1, 1))

## 4.3 S√©rie
plot(0, 0, xlim=c(0, n), ylim=c(40, 95), type='l',
     xlab="Indice de la variable", ylab="Rythme cardiaque (bpm)")

samples <- 50
x_3.n_draw <- mvrnorm(n = samples, mu = cond_mean, Sigma = cond_sd)
for (i in 1:samples) {
  lines(1:n, c(conditions_x, x_3.n_draw[i, ]), col = rgb(0, 0, 0, 0.15))
}
x_3.n_draw_mean <- apply(x_3.n_draw, 2, mean)
x_3.n_draw_sd <- apply(x_3.n_draw, 2, stats::sd)

lines(1:n, c(conditions_x, x_3.n_draw_mean), lwd = 2)
lines(1:n, c(conditions_x, x_3.n_draw_mean + x_3.n_draw_sd), col = "#b94a73", lwd = 2)
lines(1:n, c(conditions_x, x_3.n_draw_mean - x_3.n_draw_sd), col = "#b94a73", lwd = 2)
points(c(1, 2), conditions_x, pch=16, col='#46c19a', cex=2)
```

Revenons au rythme cardiaque. On pourra utiliser le conditionnement aux temps observ√©s, soit 7:00, 8:00, 10:00, 14:00 et 17:00 pour estimer la distribution √† 12:00 et 16:00, o√π √† des dimensions artificielles quelconques ici fix√©es aux demi-heures.

```{r echo = FALSE}
# 1. Distribution
n <- 21
means_vec <- rep(65, n)
covariance_mat <- RBF_kernel(x = 1:n, sigma = 5, l = 2)

# 2. Condition
conditions_x <- c(61, 74, 69, 67, 78)
conditions_indices <- c(1, 3, 7, 15, 21)
dependent_indices <- (1:20)[! 1:20 %in% conditions_indices]

# 3. Densit√© conditionn√©e
cond_parameters <- condMVN(mean=means_vec, sigma=covariance_mat, 
                           dependent.ind = dependent_indices,
                           given.ind=conditions_indices,
                           X.given=conditions_x)
cond_mean <- cond_parameters$condMean
cond_sd <- cond_parameters$condVar
samples <- 100
x_draw <- mvrnorm(n = samples, mu = cond_mean, Sigma = cond_sd)
means_draw <- apply(x_draw, 2, mean)
sd_draw <- apply(x_draw, 2, stats::sd)

# 4. Graphiques
par(mar=c(4, 4, 1, 1))

## 4.1 Combiner les pr√©dictions
bpm <- rep(NA, n)
bpm[conditions_indices] <- conditions_x
bpm[dependent_indices] <- means_draw

bpm_sd <- rep(NA, n)
bpm_sd[conditions_indices] <- 0
bpm_sd[dependent_indices] <- sd_draw


## 4.2 Combiner les tirages et les donn√©es
x_draw_all <- matrix(ncol = n, nrow = samples)
for (i in 1:length(conditions_x)) x_draw_all[, conditions_indices[i]] <- conditions_x[i]
x_draw_all[, dependent_indices] <- x_draw


## 4.3 S√©rie
plot(1:n, bpm, xlim=c(0, n), ylim=c(40, 90), type='l', lwd = 2,
     xlab="Indice de la variable", ylab="Rythme cardiaque (bpm)")
for (i in 1:samples) {
  lines(1:n, x_draw_all[i, ], col = rgb(0, 0, 0, 0.1))
}
lines(1:n, bpm+bpm_sd, col = "#b94a73", lwd = 2)
lines(1:n, bpm-bpm_sd, col = "#b94a73", lwd = 2)
points(conditions_indices, bpm[conditions_indices], pch=16, col='#46c19a', cex=2)

```

Comme on devrait s'y attendre, la r√©gression r√©sultant de la mise en indices de la distribution est pr√©cise aux mesures, et impr√©cise aux indices peu garnis en mesures. Nous avions utilis√© 21 dimensions. **Lorsque l'on g√©n√©ralise la proc√©dure √† une quantit√© infinie de dimensions, on obtient un *processus gaussien*.** 

![](https://media.giphy.com/media/12R2bKfxceemNq/giphy.gif)

L'indice de la variable devient ainsi une valeur r√©elle. Un processus gaussien, $\mathcal{GP}$, est d√©fini par une fonction de la moyenne, $m \left( x \right)$, et une autre de la covariance que l'on nomme *noyau* (ou *kernel*), $K \left( x, x' \right)$. Un processus gaussien est not√© de la mani√®re suivante:

$$\mathcal{GP} \sim \left( m \left( x \right), K \left( x, x' \right) \right)$$

La fonction d√©finissant la moyenne peut √™tre facilement √©cart√©e en s'assurant de centrer la variable r√©ponse √† z√©ro ($y_{centr√©} = y - \hat{y}$). Ainsi, par convention, on sp√©cifie une fonction de moyenne comme retournant toujours un z√©ro. Quant au noyau, il peut prendre diff√©rentes fonctions de covariance ou combinaisons de fonctions de covariance. R√®gle g√©n√©rale, on utilisera un noyau permettant de d√©finir deux param√®tres: la hauteur ($\sigma$) et la longueur de l'ondulation ($l$) (figure \@ref(fig:gp-hyperp)).

```{r gp-hyperp, out.width="100%", fig.align="center", fig.cap="Hyperparam√®tres d'un noyau RBF.", echo = FALSE}

hyperparameters <- expand.grid(l=c(1, 3, 9), sigma=1:3)

# Graphique
n <- 100

samples_list <- list()
for (i in 1:nrow(hyperparameters)) {
  sample <- mvrnorm(n = 1, mu = rep(0, n), 
                    Sigma = RBF_kernel(x=1:n,
                                       sigma = hyperparameters$sigma[i],
                                       l = hyperparameters$l[i]))
  samples_list[[i]] <- data.frame(sigma = paste("sigma =", hyperparameters$sigma[i]),
                                  l = paste("l =", hyperparameters$l[i]),
                                  x = 1:n,
                                  sample = sample)
  
}
samples_df <- do.call(rbind.data.frame, samples_list)
samples_df %>%
  ggplot(mapping = aes(x = x, y = sample)) +
  geom_line() +
  facet_grid(l ~ sigma)
```

On pourra ajouter √† ce noyau un bruit blanc, c'est-√†-dire une variation purement al√©atoire, sans covariance (noyau g√©n√©rant une matrice diagonale).

Le noyau devient ainsi un *a priori*, et le processus gaussien conditionn√© aux donn√©es devient un *a posteriori* probabiliste.

Finalement, les processus gaussiens peuvent √™tre extrapol√©s √† plusieurs variables descriptives.

### Les processus gaussiens en `R`

Pas de souci, vous n'aurez pas √† programmer vos propres fonctions pour lancer des processus gaussiens. Vous pourrez [passer par `caret`](https://topepo.github.io/caret/train-models-by-tag.html#gaussian-process). Vous pourriez, comme c'est le cas avec les r√©seaux neuronnaux, obtenir davantage de contr√¥le sur l'autoapprentissage en utilisant directement la fonction `gausspr()` du package **`kernlab`**.

```{r}
library("kernlab")
x <- c(7, 8, 10, 14, 17)
y <- c(61, 74, 69, 67, 78)
y_sc <- (y - mean(y)) / sd(y)

m <- gausspr(x, y_sc, 
             kernel = 'rbfdot', # le noyau: diff√©rents types disponibles (?gausspr)
             kpar = list(sigma = 4), # hyperparam√®tre du noyau (l est optimis√©)
             variance.model = TRUE, # pour pouvoir g√©n√©rer les √©carts-type
             scaled = TRUE, # mettre √† l'√©chelle des variables
             var = 0.01, # bruit blanc
             cross = 2) # nombre de plis de la validation crois√©e

xtest <- seq(6, 18, by = 0.1)
y_sc_pred_mean <- predict(m, xtest, type="response")
y_pred_mean <- y_sc_pred_mean * sd(y) + mean(y)
y_sc_pred_sd <- predict(m, xtest, type="sdeviation") # "sdeviation" en r√©gression et "probabilities" pour la classification
y_pred_sd <- y_sc_pred_sd * sd(y)

plot(x, y, xlim = c(6, 18), ylim = c(45, 90))
lines(xtest, y_pred_mean)
lines(xtest, y_pred_mean + y_pred_sd, col="red")
lines(xtest, y_pred_mean - y_pred_sd, col="red")
abline(v=12, lty=3, col='gray50');text(12, 67, '?', cex=2)
abline(v=16, lty=3, col='gray50');text(16, 72, '?', cex=2)
```

#### Application pratique

Les processus gaussiens sont utiles pour effectuer des pr√©dictions sur des ph√©nom√®ne sur lesquels on d√©sire √©viter de se commettre sur la structure. Les s√©ries temporelles ou les signaux spectraux en sont des exemples. Aussi, j'ai utilis√© les processus gaussiens pour mod√©liser des courbes de r√©ponse aux fertilisants. Prenons ces donn√©es g√©n√©r√©es au hasard, comprenant l'identifiant de la mesure, le bloc du test, la dose de fertilisant, trois variables environnementales ainsi que la performance de la culture en terme de rendement.

```{r, message=FALSE}
fert <- read_csv("data/11_response_fert.csv")
```

```{r}
fert %>% 
  ggplot(aes(x = Dose, y = Yield)) +
  geom_line(aes(group = Block), colour = rgb(0, 0, 0, 0.5))
```

Les blocs 1 √† 30 serviront d'entra√Ænement, les autres de test. Le rendement est mis √† l'√©chelle pour la mod√©lisation.

```{r}
environment <- fert %>% 
  dplyr::select(Dose, var1, var2, var3)
yield_sc <- (fert$Yield - mean(fert$Yield)) / sd(fert$Yield)

environment_tr <- environment[fert$Block <= 30, ]
environment_te <- environment[fert$Block > 30, ]
yield_tr <- yield_sc[fert$Block <= 30]
yield_te <- yield_sc[fert$Block > 30]
```

Je pourrais optimiser les hyperparam√®tres en cr√©ant une grille puis en lan√ßant plusieurs processus gaussiens en boucle. Mais pour l'exemple j'utilise des hyperparam√®tres quelconque.

```{r}
yield_gp <- gausspr(environment_tr, yield_tr, kernel = 'rbfdot',
                    kpar = list(sigma = 0.1),
                    variance.model = TRUE,
                    scaled = TRUE,
                    var = 0.1,
                    cross = 10)

# rendements pr√©dits dans l'√©chelle originale
gp_pred_tr <- predict(yield_gp, environment_tr, type="response") * sd(fert$Yield) + mean(fert$Yield)
gp_pred_te <- predict(yield_gp, environment_te, type="response") * sd(fert$Yield) + mean(fert$Yield)

# rendements r√©els dans l'√©chelle originale
yield_tr_os <- yield_tr * sd(fert$Yield) + mean(fert$Yield)
yield_te_os <- yield_te * sd(fert$Yield) + mean(fert$Yield)

par(mfrow = c(1, 2))
plot(yield_tr_os, gp_pred_tr, main = "train")
abline(0, 1, col = "red")
plot(yield_te_os, gp_pred_te, main = "test")
abline(0, 1, col = "red")
```

La pr√©diction semble bien fonctionner en entra√Ænement comme en test. Pour une application √† un cas d'√©tude, disons que pour mon site j'ai des variables environnementales de valeurs du bloc 50, et que je cherche la dose optmale.

```{r}
fert %>% 
  dplyr::filter(Block == 50) %>% 
  dplyr::select(var1, var2, var3) %>% 
  dplyr::slice(1)
  
```


Je peux cr√©er un tableau comprenant des environnements √©gaux pour chaque ligne, mais comprenant des incr√©ments de dose, puis pr√©dire la courbe de r√©ponse ainsi que son incertitude. Et puisque c'est un cas document√©, je peux afficher les r√©sultats de l'essai pour v√©rifier si le mod√®le est cr√©dible.

```{r}
environment_appl <- data.frame(Dose = seq(0, 200, 5), var1 = 1.57, var2 = 101.5, var3 = -10.7)
yield_appl_sc <- predict(yield_gp, environment_appl, type="response")
y_sc_pred_sd_sc <- predict(yield_gp, environment_appl, type="sdeviation")

yield_appl <- yield_appl_sc * sd(fert$Yield) + mean(fert$Yield)
yield_appl_sd <- y_sc_pred_sd_sc * sd(fert$Yield)

plot(environment_appl$Dose, yield_appl, type = "l", ylim = c(0, 35))
points(x = fert[fert$Block == 50, ]$Dose, y = fert[fert$Block == 50, ]$Yield)
lines(environment_appl$Dose, yield_appl + yield_appl_sd, col = "red")
lines(environment_appl$Dose, yield_appl - yield_appl_sd, col = "red")
```

<our chaque incr√©ment de dose de la courbe de rponse, il est possible de calculer un rendement √©conomique et/ou √©cologique en fonction du prix de la dose pond√©r√© par un co√ªt environnemental, puis de soutirer une performance optimale en terme de fertilisation.

**Exercice**. Changez les valeurs des variables environnementales pour g√©n√©rer le tableau `environment_appl` avec des valeurs qui sortent du lot (voir figure \@ref(fig:variables-env)). Qu'observez-vous? Pourquoi?

```{r variables-env, out.width="100%", fig.align="center", fig.cap="Vairables environnementales du tableau fictif `fert`.", echo = FALSE}
fert %>% 
  gather(key = "variable", value = "value", var1, var2, var3) %>% 
  ggplot(aes(x = value)) +
  facet_wrap(~variable, scales = "free_x") +
  geom_histogram(bins = 10)
```

**Exercice**. Effectuer la pr√©diction du rendement avec d'autres techniques, comme des r√©seaux neuronaux. Comment les mod√®les se comportent-ils?

```{r, echo = FALSE, eval = FALSE}
# r√©seau neuronal
yield_nn <- neuralnet(Yield ~ Dose + var1 + var2 + var3,
                      data = bind_cols(Yield = yield_tr, environment_tr), 
                      hidden = c(20, 100, 100, 20),
                      threshold = 0.05,
                      stepmax = 100000,
                      linear.output = TRUE)

# calcul des r√©ponses
yield_tr_nn_compute <- compute(yield_nn, environment_tr)
yield_tr_nn <- yield_tr_nn_compute$net.result * sd(fert$Yield) + mean(fert$Yield)

yield_te_nn_compute <- compute(yield_nn, environment_te)
yield_te_nn <- yield_te_nn_compute$net.result * sd(fert$Yield) + mean(fert$Yield)

par(mfrow = c(1, 2))
plot(yield_tr_os, yield_tr_nn, main = "train")
abline(0, 1, col = "red")
plot(yield_te_os, yield_te_nn, main = "test")
abline(0, 1, col = "red")

environment_appl <- data.frame(Dose = seq(0, 200, 5), var1 = 1.57, var2 = 101.5, var3 = -10.7)
yield_appl_sc <- compute(yield_nn, environment_appl)$net.result
yield_appl <- yield_appl_sc * sd(fert$Yield) + mean(fert$Yield)

plot(environment_appl$Dose, yield_appl, type = "l", ylim = c(0, 50))
points(x = fert[fert$Block == 50, ]$Dose, y = fert[fert$Block == 50, ]$Yield)
```

```{r, include=FALSE}
rm(list = ls())
```

<!--chapter:end:11_autoapprentissage.Rmd-->

# Les donn√©es g√©ospatiales {#chapitre-geo}

***
Ô∏è\ **Objectifs sp√©cifiques**:

√Ä la fin de ce chapitre, vous

- saurez cartographier des donn√©es g√©or√©f√©renc√©es avec ggplot
- serez en mesure d'effectuer un autoapprentissage spatial
- saurez utiliser R comme outil d'analyse spatiale (donn√©e associ√©es √† des points, lignes, polygones et rasters)

***

## Les donn√©es spatiales

Des donn√©es associ√©es √† un endroit sont spatiales. Puisque ce cours ne traite pas d'√©cologie exoplan√©taire, nous traiterons en particulier de donn√©es *g√©o*spatiales, mot que l'on utilise pour d√©signer des donn√©es avec r√©f√©rence spatiale sur la plan√®te Terre.

Lorsque nous avons abord√© les s√©ries temporelles, j'ai pris pour acquis que nous utilisions le calendrier gr√©gorien comme r√©f√©rence temporelle. Les donn√©es g√©ospatiales, quant √† elles, sont souvent exprim√©es en termes d'angles donnant une position √† la surface d'une r√©f√©rence dont la forme est un ellipsoide de r√©volution (le syst√®me g√©od√©sique): la longitude d√©crivant l'angle de part et d'autre (entre 0¬∞ et 180¬∞ Ouest ou Est) du m√©ridien de r√©f√©rence (le *premier m√©ridien*, pr√®s de Greenwich) et la latitude d√©crivant l'angle entre l'√©quateur et l'un des p√¥les (entre 0¬∞ et 90¬∞ Nord ou Sud). Les angles sont parfois exprim√©es sous forme `degr√©¬∞ minute' seconde'' Sens cardinal`, par exemple `46¬∞ 53' 21.659'' S`. Toutefois, il est plus commun (et plus pratique) d'exprimer les angles de mani√®re d√©cimale, accompagn√©e d'un signe pour indiquer le sens cardinal (par convention positif au Nord et √† l'Est). Par exemple, on exprimerait `46¬∞ 53' 21.659'' S` sous forme d√©cimale par les op√©rations suivantes.

$$
secondes = \frac{21.659''}{3600'' \cdot ¬∞^{-1}} = 0.00602¬∞
$$

$$
minutes = \frac{53'}{60' \cdot ¬∞^{-1}} = 0.883¬∞
$$

$$
- \left( 46¬∞ + 0.883¬∞ + 0.00602¬∞ \right) = -46.889¬∞
$$

La r√©f√©rence de l'altitude est g√©n√©ralement donn√©e par rapport √† un g√©o√Øde, qui est une √©l√©vation th√©orique du niveau de la mer ainsi qu'une direction de la gravit√© √©valu√©e sur toute la surface du globe.

Toutefois, les angles ne sont pas pratiques pour √©valuer des distances, ce que l'on fera bien mieux sur une carte. Pour pr√©senter la Terre sous forme de carte, on cr√© des repr√©sentations applaties du globe sous forme de carte avec l'aide d'√©quations de projection.

Or, il existe diff√©rents syst√®mes g√©od√©siques, diff√©rents g√©oides et de nombreuses mani√®res de calculer les projections. Ainsi, il est important de sp√©cifier les r√©f√©rences utilis√©es lorsque l'on donne dans la pr√©cision. Pour cette s√©ance, je prendrai pour acquis que vous poss√©dez certaines bases en positionnement, qui sont par ailleurs essentielles pour pratiquer ad√©quatement un m√©tier scientifique.

```{r, message=FALSE, warning=FALSE}
library("tidyverse")
```

Dans ce chapitre, j'utiliserai notamment comme exemple d'application des donn√©es m√©t√©orologiques soutir√©es d'Environnement Canada gr√¢ce au module **`weathercan`**, obtenues entre les longitudes -60¬∞ et -80¬∞ et entre les latitudes 45¬∞ et 50¬∞ en mai 2018. J'ai effectu√© quelques op√©rations pour obtenir des indicateurs m√©t√©o: degr√©s-jour (somme des degr√©s de temp√©rature moyenne > 5 ¬∞C, `degree_days`), pr√©cipitations totales (`cumul_precip`) et indice de diversit√© des pr√©cipitations (plus l'indice `sdi` s'approche de 1, plus les temp√©ratures sont uniform√©ment distribu√©es pendant la p√©riode). Les calculs sont consign√©s dans le fichier `lib/12_weather-fetch.R`, mais √©tant donn√© que le t√©l√©chargement prend pas mal de temps, j'ai cr√©√© un csv. Les coordonn√©es se trouvent dans les colonnes de latitude (`lat`) et longitude (`lon`).

```
source("lib/12_weather-fetch.R")
```

```{r message=FALSE}
weather <- read_csv("data/12_weather.csv")
weather %>% head()
```

Dans le tableau `weather`, chaque observation est li√©e √† un point dans l'espace. Dans ce cas, nous avons tous les outils n√©cessaires pour afficher nos points dans l'espace (figure \@ref(fig:weather-ggplot1)).

```{r weather-ggplot1, out.width="100%", fig.align="center", fig.cap="Position des stations m√©t√©o du tableau `weather`"}
weather %>%
  ggplot(mapping = aes(x = lon, y = lat)) +
  geom_point()
```

Si vous avez l'oeil averti, vous avez peut-√™tre rep√©r√© le Qu√©bec, le Nouveau-Brunswick et la fronti√®re avec les √âtats-Unis. L'absence de rep√®re rend n√©anmoins difficle l'interpr√©tation de cette carte.

## Cartographier avec le module **`ggmap`**

Le module **`ggmap`** ajoute des couches d'images t√©l√©charg√©es depuis des services de cartorgaphie en ligne. Dans cette section, nous allons utiliser le service de carte [Stamen](http://maps.stamen.com), qui ne demande pas de frais d'utilisation ou d'enregistrement particulier. La fonction `get_stamenmap()` demande une bo√Æte de coordonn√©es d√©limitant la carte √† produire, un param√®tre de zoom (plus le zoom est √©lev√©, plus la carte incluera de d√©tails: un zoom de 2 est suffisant pour une carte du monde, mais pour l'Est du Canada, on prendra plut√¥t un zoom de 6 - un bon zoom est obtenu par t√¢tonnement) et accessoirement un type de carte. 

```{r message = FALSE}
library("ggmap")
east_canada <- get_stamenmap(bbox = c(left=-81, right = -59, bottom = 44, top = 51),
                             zoom = 6,
                             maptype = "terrain")
```

Pour afficher la carte, nous ench√¢ssons notre objet dans une fonction `ggmap()`, √† laquelle nous pouvons ajouter une couche.

```
ggmap(east_canada) + 
  geom_point(data = weather, mapping = aes(x = lon, y = lat))
```

Une approche plus g√©n√©raliste consiste √† sp√©cifier dans la fonction `ggmap()` l'agument de base utilis√© pour lancer un graphique ggplot, comme √† la figure figure \@ref(fig:weather-ggmap1). En outre, l'utiliation de l'argument `base_layer` permet d'effectuer des fecettes et d'√©viter de sp√©cifier la source des donn√©es dans toutes les couches subs√©quentes.

```{r weather-ggmap1, out.width="100%", fig.align="center", fig.cap="Position des stations m√©t√©o du tableau `weather` superpos√© √† une carte import√©e par **`ggmap`**"}
ggmap(east_canada,
      base_layer = ggplot(weather, aes(x = lon, y = lat))) + 
  geom_point()
```

La carte que nous avons cr√©√©e est de type `terrain`, un type d'affichage efficace mais peu appropri√© pour une publication visant √† √™tre imprim√©e. Le type `toner-lite` est davantage vou√© √† l'impression, alors que le type `watercolor` est plus joli pour le web. Les types offerts sont list√©s dans la ficher d'aide `?get_stamenmap`.

```
maptype = c("terrain", "terrain-background", "terrain-labels",
            "terrain-lines", "toner", "toner-2010", "toner-2011",
            "toner-background", "toner-hybrid",
            "toner-labels", "toner-lines", "toner-lite", "watercolor")
```

## Types g√©n√©riques de donn√©es spatiales

Nous avons jusqu'√† pr√©sent utilis√© des donn√©es spatiales attach√©es √† un point. Ce ne sont pas les seuls.

1. **Donn√©es ponctuelles**: associ√©es √† un point. Exemple: mesure √† un endroit pr√©cis.
1. **Donn√©es lin√©aires**: associ√©es √† une s√©rie de point. Exemple: mesure associ√©e √† une route ou une rivi√®re.
1. **Donn√©es de polygone**: associ√©es √† une aire d√©limit√©e par des points. Exemples: Donn√©es associ√©es √† un champ, une unit√© administrative, un bassin versant, etc.
1. **Donn√©es raster**: associ√©es √† une grille. Exemple: une image satellite o√π chaque pixel est associ√© √† un recouvrement foliaire.

L'enregistrement des donn√©es ponctuelles ne posent pas de d√©fi particulier. Les donn√©es associ√©es √† un ligne, toutefois posent un probl√®me d'organisation, puisqu'une ligne elle-m√™me contient des informations sur les *coordonn√©es* de ses points ainsi que l'*ordre* dans lequel les points sont connect√©s. On pourra soit cr√©er un tableau de donn√©es ayant une colonne o√π l'identifiant de la ligne est consign√©, renvoyant √† un autre tableau o√π chaque ligne d√©crit un point en terme d'identifiant de ligne √† laquelle il appartient, ses coordonn√©es, ainsi que sont ordre de rattachement dans la ligne. Les informations de la ligne pourraient aussi √™tre ench√¢ss√©es dans une cellule de tableau de donn√©es, en tant que sous-tableau. Ou bien, on pourrait cr√©er un tableau sous forme de jointure entre le tableau des donn√©es et le tableau des lignes. Un d√©fi similaire pourrait subvenir avec des polygones, qui demandent davantage d'information √©tant donn√©e qu'ils peuvent √™tre trou√©s (par exemple un lac) ou s√©par√©s en diff√©rents morceaux (un archipel, par exemple). Enfin, il existe des formats de donn√©es spatiales g√©n√©riques (shapefiles et geojson) ou sp√©cialement con√ßus pour R (module **`sf`**), que nous couvrirons plus loin dans ce chapitre.

## Les choropl√®the

Les cartes de type *choropl√®the* se pr√©sentent sous forme de **polygones** d√©crits par un *groupe* et un *ordre*, dont un la couleur de remplissage d√©pend d'une variable. Les pays, par exemple, forment des polygones.

```{r message=FALSE, warning=FALSE}
world <- map_data(map = "world") # jeu de donne√©s de ggplot2
head(world)
```

La fonction `map_data()` de **`ggplot2`** permet de soutirer des polygone de [certaines cartes](https://www.rdocumentation.org/packages/ggplot2/versions/3.1.0/topics/map_data). Pour les zones g√©ographiques pr√©d√©finies, il est pr√©f√©rable de soutirer les polygones d√©sir√©es de donn√©es existantes plut√¥t que les cr√©er soi-m√™me. Souvent, ces polygones ne sont pas directement disponibles en R. Dans ce cas, il faudra trouver des fichiers de carte aupr√®s de [Statistique Canada](https://www.statcan.gc.ca/fra/debut), [Donn√©es Qu√©bec](https://www.donneesquebec.ca), etc., ce que nous verrons plus loin.

```{r message=FALSE, warning=FALSE}
especes_menacees <- read_csv('data/WILD_LIFE_14012020030114795.csv')
iucn_oecd <- especes_menacees %>%
  dplyr::filter(IUCN == 'THREATENED') %>%
  dplyr::select(Country, Value) %>%
  dplyr::group_by(Country) %>% 
  dplyr::summarise(n_threatened_species = sum(Value)) %>%
  dplyr::arrange(desc(n_threatened_species))
```

Les noms des pays doivent correspondre exactement, et la colonne des pays doit porter le m√™me nom (j'ai inspect√© les vecteurs `iucn_oecd30$Country` et `unique(world$region)`).

```{r}
iucn_oecd <- iucn_oecd %>% 
  replace(.=="United States", "USA") %>% 
  replace(.=="Slovak Republic", "Slovakia") %>% 
  replace(.=="United Kingdom", "UK") %>% 
  dplyr::rename("region" = "Country")
```

Les esp√®ces sont jointes au tableau contenant les polygones.

```{r}
world_iucn <- world %>% 
  left_join(iucn_oecd, by = "region")
```

Pour le graphique de la figure \@ref(fig:iucn1), La strat√©gie est de cr√©er des polygones group√©s par groupes de polygones (`group = group`), dont la couleur de remplissage correspond √† au nombre d'esp√®ce. J'ajoute `coord_map()` en sp√©cifiant une projection de type Mercator (essayez `projection = "ortho"`). Le reste est de la d√©coration.

```{r iucn1, out.width="100%", fig.align="center", fig.cap="Nombre d'esp√®ces en danger dans les pays de l'OCDE"}
ggplot(world_iucn, aes(long, lat)) +
  geom_polygon(aes(group = group, fill = n_threatened_species),
               colour = "grey50", lwd = 0.1) +
  coord_map(projection = "mercator", xlim = c(-180, 180), ylim = c(-90, 90)) +
  scale_fill_gradient(low = "#8CBFE6", high = "#FF0099", na.value = "grey80") +
  labs(title = "Number of threatened species in OECD countries",
       subtitle = "Source: OCDE, 2019") +
  theme(panel.background = element_rect(fill = "grey97"),
        plot.background = element_rect(fill = "white"),
        axis.text = element_blank(),
        axis.ticks = element_blank(),
        axis.title = element_blank(),
        panel.grid = element_blank()) +
  guides(fill = guide_legend(title = "Number of\nthreatened\nspecies"))
```

Comme c'est le cas des points de la figure \@ref(fig:weather-ggmap1), on peut superposer des choropl√®thes √† des cartes t√©l√©charg√©es (figure \@ref(fig:choropleth-ggpmap1)).

```{r choropleth-ggpmap1, message=FALSE, out.width="100%", fig.align="center", fig.cap="Nombre d'esp√®ces en danger dans les pays de l'OCDE superpos√© √† une carte import√©e par **`ggmap`**"}
worldmap <- get_stamenmap(bbox = c(left=-170, right = 170, bottom = -80, top = 80),
                          zoom = 2,
                          maptype = "watercolor")
world_iucn_oecd <- world_iucn %>% 
  filter(!is.na(n_threatened_species))
ggmap(worldmap,
      base_layer = ggplot(world_iucn_oecd, aes(long, lat))) +
  geom_polygon(aes(group = group, fill = n_threatened_species),
               colour = "black", lwd = 0.2) +
  coord_map(projection = "mercator", xlim = c(-180, 180), ylim = c(-90, 90)) +
  scale_fill_gradient(low = "blue", high = "red", na.value = "grey80") +
  labs(title = "Number of threatened species in OECD countries",
       subtitle = "Source: OCDE, 2019") +
  theme(axis.text = element_blank(),
        axis.ticks = element_blank(),
        axis.title = element_blank(),
        panel.grid = element_blank()) +
  guides(fill = guide_legend(title = "Number of\nthreatened\nspecies"))
```

Si vous savez cr√©er des polygones, vous saurez cr√©er des lignes de mani√®re similaire avec la couche graphique `geom_path()`. Pour les rasters, c'est moins √©vident.

## Les rasters

Les *rasters* sont des donn√©es associ√©es √† un grille. Nous avons introduit la fonction `expand.grid()` au chapitre \@ref(chapitre-ml) lorsque nous d√©sirions cr√©er un tableau o√π chaque ligne d√©signe une des combinaisons possibles d'hyperparam√®tres pour ajuster un mod√®le d'autoapprentissage. De m√™me, nous pouvons cr√©er une grille comprenant les combinaisons de longitudes et de latitudes, puis cr√©er une variable spatialis√©e $z = 10\times sin \left( xy \right) - 0.01x^2 + 0.05y^2$.

```{r}
grid <- expand.grid(lon = seq(from = -80, to = -60, by = 0.25),
                    lat = seq(from = 45, to = 50, by = 0.25))
grid <- grid %>% 
  mutate(z = 10*sin(lon*lat) - 0.01*lon^2 + 0.05*lat^2) # cr√©er une variable spatialis√©e

grid %>% head()
```

Pour visualiser une grille avec **`ggplot2`**, on peut avoir recourt √† la couche graphique `geom_tile()`, dont la couleur remplissage est associ√©e √† la colonne `z` du tableau. Ce type de graphique est appel√©e *heatmap* (figure \@ref(fig:geom-tile)).

```{r geom-tile, out.width="100%", fig.align="center", fig.cap=" "}
ggplot(grid, aes(lon, lat)) +
  geom_tile(aes(fill = z))
```

Remarquez que j'ai cr√©√© une fonction pour g√©n√©rer une variable spatialis√©e. *Une telle fonction n'a pas besoin d'√™tre invent√©e*: on peut en cr√©er une en utilisant les outils que nous avons appris jusqu'√† pr√©sent, en particulier avec l'autoapprentissage.

## Autoapprentissage spatial

La g√©ostatistique est l'√©tude statistique des variables spatiales, un sujet complexe qui sort du cadre de ce cours - que vous pourrez creuser dans le livre [*Applied Spatial Data Analysis with R*](https://asdar-book.org/). Ici, nous allons projeter des variables spatialis√©es √† l'aide de l'autoapprentissage, o√π la position (coordonn√©es en longitude et latitude, par exemple) peut servir de variable pr√©dictive (ainsi que, √©ventuellement, des variables spatialis√©es concernant l'altitude, l'hydrologie, la g√©omorphologie, l'√©cologie, la sociologie, la gestion du territoire, etc.). Pour ce faire, vous pourrez utiliser algorithme qui convient √† vos donn√©es et √† votre domaine d'√©tude. Nous allons utiliser les processus gaussiens, qui sont particuli√®rement utiles pour √©valuer l'*incertitude* des pr√©dictions. Par exemple, nous allons pr√©dire sur une grille les donn√©es de degr√©s-jour du tableau `weather` avec un processus gaussien. Pour √©valuer la *performance* d'une pr√©diction, n'oublions de s√©parer nos donn√©es en jeux d'entra√Ænement et de test (avec la fonction `caret::createDataPartition()`)!

```{r message=FALSE}
library("caret")
weather_dd <- weather %>% 
  dplyr::select(lon, lat, degree_days) %>% 
  drop_na()
weather_dd_sc <- weather_dd %>% 
  mutate(degree_days = (degree_days - mean(degree_days))/sd(degree_days))
train_id <- createDataPartition(y = weather_dd_sc$degree_days, p = 0.7, list = FALSE)
```

Utilisons la fonction `kernlab::gausspr()`, vue au chapitre \@ref(chapitre-ml).

```{r message=FALSE}
library("kernlab")
dd_gp <- gausspr(x = weather_dd_sc[train_id, c("lon", "lat")],
                 y = weather_dd_sc[train_id, "degree_days"],
                 kernel = "rbfdot",
                 #kpar = list(sigma = 01), # laisser optimiser
                 variance.model = TRUE,
                 scale = TRUE,
                 var = 0.1,
                 cross = 5)
```

√âvaluons visuellement al performance de la pr√©diction (figure \@ref(fig:gp-perf)).

```{r gp-perf, out.width="100%", fig.align="center", fig.cap="Performance du processus gaussien en entra√Ænement et en test."}
pred_dd_tr <- predict(dd_gp)
pred_dd_te <- predict(dd_gp, newdata =  weather_dd_sc[-train_id, c("lon", "lat")])

par(mfrow = c(1, 2))
plot(weather_dd_sc$degree_days[train_id], pred_dd_tr, main = "Train prediction", xlab = "mesur√©", ylab = "pr√©dit")
abline(0, 1, col="red")
plot(weather_dd_sc$degree_days[-train_id], pred_dd_te, main = "Test prediction", xlab = "mesur√©", ylab = "pr√©dit")
abline(0, 1, col="red")
```

La pr√©diction n'est pas extraordinaire, mais gardons-la pour l'exemple (j'ai essay√© avec des r√©seaux neuronaux sans plus de succ√®s). La prochaine √©tape est de cr√©er une gille o√π chaque point [longitude, latitude] servira de variable explicative pour calculer les degr√©s-jour.

```{r}
grid <- expand.grid(lon = seq(from = -80, to = -60, by = 0.25),
                    lat = seq(from = 45, to = 50, by = 0.25))
grid <- grid %>% 
  mutate(pred_dd_mean = predict(dd_gp, newdata = ., type = "response") * sd(weather_dd$degree_days) + mean(weather_dd$degree_days),
         pred_dd_sd = predict(dd_gp, newdata = ., type = "sdeviation") * sd(weather_dd$degree_days))
head(grid)
```

Utilisons les polygones de la carte du monde zoom√©e √† l'endroit qui nous int√©resse, et ajoutons-y notre pr√©diction superpos√©e par les localisations des stations m√©t√©o. J'ajoute des contours ainsi que des √©tiquettes de contours (ce qui n√©cessite le module **`metR`**). Les processus gaussiens permettent de juxtaposer une carte des √©cart-type des pr√©dictions, donnant une appr√©ciation de la pr√©cision du mod√®le (figure \@ref(fig:pred-dd)). Cette juxtaposition est effectu√©e avec la fonction `plot_grid()` le module **`cowplot`**.

```{r pred-dd, warning=FALSE, out.width="100%", fig.align="center", fig.cap="Pr√©diction des degr√©s-jour dans l'espace avec les processus gaussiens", fig.width=12}
library("metR")
gg_mean <- ggplot(grid, aes(x = lon, y = lat)) +
  xlim(c(-80, -60)) +  ylim(c(45, 50)) +  coord_equal() +
  geom_tile(aes(fill = pred_dd_mean)) +
  geom_contour(data = grid, mapping = aes(x = lon, y = lat, z = pred_dd_mean), binwidth = 50, colour = "black", lwd = 0.2) +
  geom_label_contour(aes(z = pred_dd_mean)) +
  geom_path(data = world, aes(x = long, y = lat, group = group)) +
  geom_point(data = weather, mapping = aes(x = lon, y = lat), size = 0.1) +
  scale_fill_gradient(low = "#8CBFE6", high = "#FF0099", na.value = "grey80")

gg_sd <- ggplot(grid, aes(x = lon, y = lat)) +
  xlim(c(-80, -60)) +  ylim(c(45, 50)) +  coord_equal() +
  geom_tile(aes(fill = pred_dd_sd)) +
  geom_contour(data = grid, mapping = aes(x = lon, y = lat, z = pred_dd_sd), binwidth = 50, colour = "black", lwd = 0.2) +
  geom_label_contour(aes(z = pred_dd_sd)) +
  geom_path(data = world, aes(x = long, y = lat, group = group)) +
  geom_point(data = weather, mapping = aes(x = lon, y = lat), size = 0.1) +
  scale_fill_gradient(low = "#8CBFE6", high = "#FF0099", na.value = "grey80")

cowplot::plot_grid(gg_mean, gg_sd, labels = c("A", "B"), nrow = 2)

```

## Les objets spatialis√©s en R

Au chapitre \@ref(chapitre-temps), nous avons couvert le type d'objet `ts`, sp√©cialis√© pour les s√©ries temporelles. De m√™me, le type d'objet `sf` est sp√©cialis√© pour les objets geor√©f√©renc√©s. Les formats de donn√©es spatiales conventionnellement utilis√©s en R depuis 2003 sont offerts par le module **`sp`**. Ce format h√©ritait de difficult√©s, r√©cemment surmont√©es par le module [**`sf`**](https://r-spatial.github.io/sf/), plus convivial et mieux adapt√© au *tidyverse*. Bien que **`sp`** soit plus largement document√©, **`sf`** est suffisamment mature pour une utilisation professionnelle. √âvidemment, [un aide-m√©moire a √©t√© cr√©√©](https://github.com/rstudio/cheatsheets/raw/master/sf.pdf) (figure \@ref(fig:sf-cheatsheet)).

```{r sf-cheatsheet, out.width="100%", fig.align="center", fig.cap="[Aide-m√©moire du module **`sf`**](https://github.com/rstudio/cheatsheets/raw/master/sf.pdf), cr√©√© par RStudio", echo = FALSE}
knitr::include_graphics("images/12_sf-cheatsheet_canvas.png")
```

Nous avons couvert quatre types de donn√©es spatiales. Nous allons maintenant les traiter en deux cat√©gories:

1. les donn√©es vectorielle, comprenant les points, lignes et polygones et
1. les donn√©es raster, comprenant les grilles de donn√©es.

### Donn√©es vectorielles (points, lignes et polygones)

Un cas typique consiste √† importer un tableau de donn√©es localis√©es en un point, que l'on d√©sire localiser en format `sf` avec la fonction `st_as_sf()`. Le tableau weather, par exemple, comporte une latitude (colonne `lat`) et une longitude (colonne `lon`), sp√©cifi√©es dans l'argument `coord`. Puisqu'il s'agit de donn√©es canadiennes, je suppose que les coordonn√©es sont projet√©es en format NAD83, tel qu'utilis√© par Statistique Canada et Ressources naturelles Canada (√† d√©faut de trouver la bonne info en ce moment). Le code PROJ4, sp√©cifi√© sous l'argument `crs`, d√©crit l'ellipso√Øde utilis√© pour calculer les longitudes et latitudes ainsi que, s'il y a lieu, la projection (d√©tails plus loin dans la section \@ref(section-systeme-coord)).

```{r}
library("sf")
weather_geo <- weather %>% 
  st_as_sf(coords = c("lon", "lat"), crs = "+proj=longlat +datum=NAD83")
weather_geo
```

Notre objet `sf` comprend des m√©tadonn√©es sur le type de g√©om√©trie (`geometry type: POINT`), les limites des objets (`bbox: ...`), le syst√®me de r√©f√©rence (`epsg` ou `proj4string: ...`) ainsi que le tableau descriptif. En format `sf`, la colonne `geometry` (qui elle est de type `sfc`) comprend dans chacune des cellules, exprim√©e sous forme de liste, toute l'information n√©cessaire √† la construction de la g√©om√©trie, que ce soit un point, une ligne ou un polygone.

Pour ce qui est des polygones et des lignes, il est plus commun de les importer depuis des sources institutionnelles. On pourra t√©l√©charger des donn√©es g√©ographiques, puis d√©zipper les fichier manuellement. Mais on peut aussi copier un lien, le coller dans R et d√©zipper automatiquement. Le block de code suivant t√©l√©charge un dossier de shapefiles d√©crivant les polygones des r√©gions administratives du Qu√©bec.

```{r, eval=FALSE}
download.file("ftp://ftp.mrnf.gouv.qc.ca/public/dgig/produits/bdga5m/vectoriel/region_admin_SHP.zip",
              destfile="data/12_quebec/12_region_admin_SHP.zip")
unzip("data/12_quebec/12_region_admin_SHP.zip", exdir = "data/12_quebec/")
```

Pour charger dans R des shapefiles en format `sf`, nous utilisons la fonction `st_read()` pointant vers le fichier `.shp`.

```{r}
quebec <- st_read("data/12_quebec/region_admin_polygone.shp")
head(quebec)
```

Nous avons vu au chapitre \@ref(chapitre-tableaux) qu'il est pr√©f√©rable d'√©viter la r√©p√©tition de l'information. Dans le format `tibble` que nous avons utilis√© pour d√©crire les polygones, l'information attach√©e √† un polygone est r√©p√©t√©e pour chaque point qui le compose: forcer une information hi√©rarchis√©e √† se conformer √† une structure rectangulaire multiplie la quantit√© d'information. Le format `sf` √©vite cette mutiplication d'information en hi√©rachisant les polygones dans la colonne `geometry`.

En guise d'exploration rapide, la fonction `plot()` affichera les choropl√®thes.

```{r quebec-plot, out.width="100%", fig.align="center", fig.cap=" "}
plot(quebec)
```

Si nous ne d√©sirons que la g√©om√©trie,

```{r quebec-geometry-plot, out.width="100%", fig.align="center", fig.cap=" "}
quebec %>%
  st_geometry() %>%
  plot() # ou bien plot(st_geometry(quebec)), ou bien plot(quebec %>% select(geometry))
```

**Exercice**. Explorer l'objet `quebec`, en particulier la colonne `geometry`, notamment en utilisant la fonction `str()`.

Vous pourrez soutirer les informations du syst√®me de coordonn√©es avec la fonction `st_crs()`. Il est possible de calculer des attributs des g√©om√©tries √† l'aide des fonctions `st_area()` pour les polygones, ou `st_length()` pour les lignes et les polygones et la fonction `st_centroid()` pour trouver le centro√Øde d'un polygone  - √† cette √©tape, il se pourrait que R vous demande d'installer le module **`lwgeom`**: suivez ses consignes!

```{r quebec-centroid-plot, out.width="100%", fig.align="center", fig.cap=" "}
quebec_point <- quebec %>% 
  mutate(st_area = st_area(quebec),
         st_length = st_length(quebec)) %>% 
  st_centroid()
plot(quebec_point)
```

Les aires et les p√©rim√®tres calcul√©s ne correspondent pas tout √† fait √† ceux des variables `AREA` et `PERIMETER`, probablement calcul√©s sur une autre base. La fonction `st_centroid()` cr√©e un nouveau tableau dont la g√©om√©trie est le `POINT`, elle doit donc √™tre pass√©e apr√®s les op√©rations sur les polygones.

La fonction `st_simplify()` permet de simplifier les polygones en un nombre r√©duit de points, ce qui peut √™tre utile pour acc√©l√©rer les calculs. La fonction `st_buffer()` permet de cr√©er un rayon d'une longueur donn√©e autour d'un point, proc√©dure souvent utilis√©e pour visualiser un rayon d'influence. Mais pour calculer des distances, les donn√©es doivent projet√©es. Nous pouvons les projeter avec `st_transform()` avec [le code EPSG d√©sir√©](https://epsg.io/3348).

```{r quebec-radius-plot, out.width="100%", fig.align="center", fig.cap=" "}
quebec_point %>% 
  st_transform(3348) %>% 
  st_buffer(50000) %>% # 50 km du centre de la r√©gion
  plot()
```

D'autres op√©rations sur les vecteurs sont offertes et document√©es sous la fiche d'aire [`sf::geos_unary()`](https://r-spatial.github.io/sf/reference/geos_unary.html). 

Enfin, pour exporter un tableau `sf` en format csv *incluant la g√©om√©trie*, utilisez `st_write(obj = tableau,dsn = "tableau.csv", layer_options = "GEOMETRY=AS_XY")`.  Toutefois, si la g√©om√©trie n'est pas consitu√©e de points, il faudra pr√©alablement transformer les polygones en points avec `st_cast()`.

```{r, eval = FALSE}
st_write(obj = quebec %>% 
           dplyr::filter(AREA < 1) %>% # ne retenir que quelques r√©gions pour cr√©er un fichier moins volumineux
           st_cast("POINT"),
         dsn = "data/12_quebec_export.csv",
         layer_options = "GEOMETRY=AS_XY")
```

### Donn√©es raster

Les donn√©es rasters sont des grilles, souvent ench√¢ss√©es dans des images `tif` g√©or√©f√©renc√©es. Ces images peuvent comprendre plusieurs variables, que l'on nomme des *bandes*, en r√©f√©rence aux bandes spectrales des images sat√©litaires (rouge, vert et bleu). Les donn√©es raster peuvent √™tre import√©es dans votre session gr√¢ce √† deux fonctions du module **`raster`** : `raster()` importera des donn√©es raster √† une bande et `brick()`, des donn√©es raster √† plusieurs bandes.

```{r message=FALSE, warning=FALSE}
library("raster")
canopy <- raster("data/12_nytrees/canopy.tif") # source: https://assets.datacamp.com/production/repositories/738/datasets/79cb56df0fa27272e16b366a697aba8ac1d3e923/canopy.zip
canopy
manhattan <- brick("data/12_nytrees/manhattan/manhattan.tif") # source: https://assets.datacamp.com/production/repositories/738/datasets/30830f8ba4a60aa1711f41e9a842b22cba3204f3/manhattan.zip
manhattan
```

Les informations des objets `RasterLayer` et `RasterBrick` peuvent √™tre extraites par les fonctions `extent()`, `ncell()`, `nlayers()` et `crs()`. La fonction `plot()` permet d'explorer les donn√©es en cr√©ant un graphique par bande.

```{r manhattan-plot, out.width="100%", fig.align="center", fig.cap=" "}
plot(manhattan)
```

Les fichiers raster, en format qui viennent souvent en format `tif`, sont typiquement tr√®s volumineux. Si une plus faible r√©solution convient √† une analyse spatiale, on pourra simplifier un raster avec la fonction `raster::aggregate()` (j'utilise la notation `module::fonction()` pour √©viter la confusion avec la fonction `dplyr::aggregate()`). L'argument `fact` est le facteur de conversion et l'argument `fun` est la fonction d'aggr√©gation (typiquement `mean` ou `median`).

```{r manhattan-lowres-plot, out.width="100%", fig.align="center", fig.cap=" "}
manhattan_lowres <- raster::aggregate(manhattan, fact = 20, fun = median)
manhattan_lowres
plot(manhattan_lowres)
```

Avec un facteur de conversion de 20, nous sommes pass√©s d'une grille de 773 $\times$ 801 √† 39 $\times$ 41. L'exemple utilis√© est volontairement exag√©r√© pour montrer l'effet de la perte de r√©solution, et g√©n√©ralement le facteur de conversion utilis√© est plus faible que 20.

La fonction `reclassify()` est l'√©quivalent de `cut()` pour les rasters. L'argument demand√©, en plus de l'objet raster, est une matrice de classification √† trois colonnes. Les deux premi√®res colonnes sp√©cifient la plage de valeur √† classifier et la troisi√®me colonne sp√©cifie la valeur de remplacement (qui peut √™tre `NA`). La classification s'applique √† toutes les couches s'il s'agit d'un `RasterBrick`.

```{r manhattan-rcl-plot, out.width="100%", fig.align="center", fig.cap=" "}
manhattan_rcl <- reclassify(manhattan_lowres, rcl = matrix(c(0, 50, 1,
                                                             50, 100, 2,
                                                             100, 1000, NA),
                                                           ncol = 3, byrow = TRUE))
plot(manhattan_rcl)
```

## Les syst√®mes de coordonn√©es {#section-systeme-coord}

Les longitudes et latitudes sont des angles sur un ellipso√Øde de r√©volution. Diff√©rentes institutions utilisent diff√©rentes formes d'ellipso√Øde portant leur nom particulier: NAD83, WGS84, ETRS89, etc. Les projections servent √† aplanir des coordonn√©es g√©od√©siques obtenues selon un ellipso√Øde donn√© en vue de cr√©er des repr√©sentations 2D, comme la projection Mercator universelle ou de [nombreuses autres](https://bl.ocks.org/mbostock/3711652). Le syst√®me de coordonn√©es peut √™tre projet√© ou non.

1. **Syst√®me de coordonn√©es *non*-projet√©es**: caract√©ris√© par des angles de *longitude* et de *latitudes* sur un syst√®me g√©od√©sique 3D repr√©sent√© par un ellipso√Øde de r√©volution.
1. **Syst√®me de coordonn√©es projet√©es**: caract√©ris√© par des distances X et Y sur une syst√®me g√©od√©sique repr√©sent√© en 2D.

Lorsque vous utilisez des shapefiles, les informations du syst√®me de coordonn√©es seront incluses dans le fichier ayant une extension `prj`. Examinons le syst√®me de coordonn√©es du tableau `quebec` avec la fonction `st_crs()`.

```{r}
st_crs(quebec)
```

D'embl√©e, la mention `+proj=longlat` retrouv√©e dans `proj4string` (une repr√©sentation de [PROJ4](https://proj4.org/)) indique que le syst√®me n'est pas projet√©, et que le syst√®me g√©od√©sique est le [GRS80](https://en.wikipedia.org/wiki/Geodetic_Reference_System_1980), [pratiquement identique au WGS84](https://mern.gouv.qc.ca/territoire/outils/outils-faq.jsp). Le code `EPSG` contient la m√™me information que le `proj4string`, traduite de mani√®re succincte par un code √† 4 chiffres. Dans certains cas, les informations du syst√®me de coordonn√©es ne sont pas disponibles: il vous faudra creuser si elles sont essentielles √† vos travaux. Pour assigner un syst√®me de coordonn√©es, vous pourrez soit assigner l'EPSG par `st_crs(objet_sg) <- 4269` ou `objet_sg <- objet_sg %>% st_set_crs()`, ou bien le PROJ4 par `st_crs(objet_sg) <- "+proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs"`. La proc√©dure est la m√™me pour les rasters, mais avec la fonction `crs()` au lieu de `st_crs()`.

Pour passer d'un syst√®me de coordonn√©es √† un autre, utilisez `st_transform()` pour les vecteurs et `projectRasters()` pour les rasters. Pour les rasters, si vos donn√©es sont cat√©gorielles, et non pas num√©rique, utilisez `method = "ngb"` plut√¥t que la valeur par d√©faut, `method = "bilinear"`, con√ßue pour les variables num√©riques.

## Manipuler des tableaux `sf`

Vous avez peut-√™tre remarqu√© que j'ai pr√©c√©demment effectu√© une op√©ration `mutate()` en mode pipeline (`%>%`) sur un tableau `sf`. Eh oui, les `sf` sont compatibles avec le mode *tidyverse*. Vous pourrez filtrer avec `filter()`, s√©lectionner avec `select()` et manipuler des colonnes avec `mutate()`. Reprenons les donn√©es des esp√®ces en danger, mais cette fois-ci nous allons travailler avec des donn√©es spatialis√©es avec `sf`. D'abord, allons chercher une carte du monde: le format geojson peut √™tre import√© de la m√™me mani√®re que des shape files. Puisqu'un geojson consigne l'information g√©ographique en un seul fichier (les shapefiles en contiennent plusieurs), on peut l'importer directement d'Internet.

```{r message=FALSE}
world_gj <- st_read("https://raw.githubusercontent.com/johan/world.geo.json/master/countries.geo.json")
world_gj
```

Des multipolygones sont form√©s lorsque plusieurs polygones forment une seule entit√©, par exemple un pays constitu√© de plusieurs √Æles. Nous allons effectu√© la jointure entre le tableau `world_gj` et les donn√©es de l'IUCN. De m√™me que pr√©c√©demment, les noms des pays doivent correspondre exactement.

```{r}
iucn_oecd <- iucn_oecd %>% 
  replace(.=="USA", "United States of America") %>% 
  replace(.=="UK", "United Kingdom") %>% 
  dplyr::rename("name" = "region")
```

Pour cette jointure, je d√©sire ne conserver que les donn√©es g√©ographiques des pays de l'OCDE. Je peux effectuer une jointure √† gauche sur le tableau `iucn_oecd` ou une joiture √† droite sur le tableau `world_gj`.

```{r}
oecd_gj <- world_gj %>% 
  right_join(iucn_oecd, by = "name")
```

Contrairement aux tableaux `tibble`, le format `sf` conserve la g√©om√©trie.

```{r}
oecd_gj %>% 
  dplyr::select(name, n_threatened_species)
```

Pour une raison ou une autre, si vous d√©sirex retirer la g√©om√©trie, 

```{r}
oecd_gj %>% 
  dplyr::select(name, n_threatened_species) %>% 
  st_set_geometry(NULL) %>% 
  top_n(4)
```

On pourra explorer notre tableau avec la fonction `plot()`.

```{r oecd_gj-plot, out.width="100%", fig.align="center", fig.cap=" "}
plot(oecd_gj)
```

Tout comme on effectue des jointures entre des tableaux, on peut effectuerd es jointures spatiales sur des `sf`. On pourra trouver des intersections entre polygones, effectuer des unions, des diff√©rences, etc. Par exemple, en mod√©lisation, il est commun d'extrapoler des r√©sultats sur une grille. Ici, nous cr√©ons une grille couvrant tout le qu√©bec (figure \@ref(fig:quebec-grid)).

1. Nous cr√©ons une grille constitu√©e des centro√Ødes, `%>%` # (par d√©faut, il s'agit d'une grille de polygones rectangulaires)
1. nous transformons le r√©sultat en format `sf` (au lieu de `sfc`), `%>%`
1. nous effectuons une jointure sous forme d'intersection et `%>%`
1. nous retirons les occurences hors de la jointure.

```{r quebec-grid, out.width="100%", fig.align="center", fig.cap=" "}
quebec_grid <- quebec %>% 
  st_make_grid(n = 80, what = "centers") %>% 
  st_sf() %>%  # transformer en objet sf
  st_join(quebec, join = st_intersects) %>% 
  drop_na()

par(mfrow = c(1, 2))
plot(quebec_grid %>% st_geometry(), pch = 16, cex = 0.2, main = "Grille Qu√©bec")
plot(quebec_grid %>% filter(RES_NM_REG == "Mont√©r√©gie") %>% st_geometry(), main = "Grille Mont√©r√©gie")
```

Pour soutirer la grille en vue de mod√©liser,

```{r}
quebec_grid %>%
  st_coordinates() %>% 
  as_tibble() %>% 
  dplyr::rename("lon" = "X", "lat" = "Y")
```


## Manipuler des objets raster

Comme les objets vectoriels, les objets raster peuvent subir diff√©rents types d'op√©rations. Nous en couvrirons trois.

- Masque (`mask()`): l'intersection entre un polygone et un raster.
- D√©couper (`crop()`): d√©coupe rectangulaire selon les limites de l'objet.
- Extraction (`extract()`): extrait, et accessoirement effectue un sommaire, des rasters dans un polygone donn√©

Cr√©ons d'abord un polygone ayant le m√™me syst√®me de coordonn√©es que le raster `canopy`.

```{r}
poly <- st_sfc(st_polygon(list(cbind(c(1800000, 1830000, 1820000, 1800000),
                                     c(2160000, 2200000, 2150000, 2160000))))) %>%
  st_set_crs(as.character(crs(canopy))) %>%
  st_cast("POLYGON")
```

En ce moment, le module **`raster`** n'est pas adapt√© au format `sf`, qu'il faudrait pr√©alablement convertir vers l'ancien format `sp`.

```{r}
poly_sp <- as(poly, "Spatial")
```

Appliquons un masque, puis un crop, puis les deux.

```{r raster-operations, fig.height=3, fig.width=12, out.width="100%", fig.align="center", fig.cap=" "}
canopy_mask <- mask(canopy, mask = poly_sp)
canopy_crop <- crop(canopy, y = poly_sp)
canopy_mc <- crop(canopy_mask, y = poly_sp)

par(mfrow = c(1, 4))
plot(canopy, main = "Original")
plot(poly_sp, add = TRUE)
plot(canopy_mask, main = "mask()")
plot(poly_sp, add = TRUE)
plot(canopy_crop, main = "crop()")
plot(poly_sp, add = TRUE)
plot(canopy_mc, main = "mask() & crop()")
plot(poly_sp, add = TRUE)
```

Pour effectuer un calcul sur l'int√©rieur du polygone avec `extract()`... on sp√©cifie le raster, le polygone et la fonction!

```{r}
extract(canopy, poly_sp, fun = mean)
```

## Graphiques d'objets spatialis√©s

Pour afficher les objets `sf` et raster, nous avons utilis√© les fonctions de base √† titre exploratoire. Mais lorsque vient le temps de publier une carte, la trousse de **`ggplot2`** est toute indiqu√©e, en y ajoutant l'outil `geom_sf()`.

```{r quebec-sf-gg, out.width="100%", fig.align="center", fig.cap=" "}
ggplot(quebec) +
  geom_sf(aes(fill = RES_NM_REG)) +
  geom_sf(data = quebec_point)
```

Les coordonn√©es peuvent √™tre manipul√©es avec `coord_sf()`.

```{r iucn-north-america, out.width="100%", fig.align="center", fig.cap=" "}
world_gj_iucn <- world_gj %>% 
  full_join(iucn_oecd, by = "name")

ggplot(world_gj_iucn) +
  geom_sf(aes(fill = n_threatened_species), colour = "gray50") +
  coord_sf(xlim = c(-170, -40), ylim = c(10, 80)) +
  scale_fill_gradient(low = "#8CBFE6", high = "#FF0099", na.value = "grey80") +
  labs(title = "Number of threatened species in OECD countries",
       subtitle = "Source: OCDE, 2019") +
  guides(fill = guide_legend(title = "Number of\nthreatened\nspecies"))
```

Les cartes th√©matiques de [`tmap`](https://www.jstatsoft.org/article/view/v084i06) (*thematic maps*) sont construites sensiblement de la m√™me mani√®re que **`ggplot2`**. Diff√©rents types de projections sont [disponibles](https://www.rdocumentation.org/packages/tmaptools/versions/2.0-1/topics/get_proj4) avec diff√©rentes palettes de couleurs (`palette_explorer()`).

```{r iucn-tmap, out.width="100%", fig.align="center", fig.cap=" "}
library("tmap")
library("tmaptools")
tm_shape(set_projection(world_gj_iucn, "wintri")) +
  tm_polygons("n_threatened_species", palette = "viridis")
```

Si vous d√©sirez cr√©er des cartes int√©ractives, passez en mode [*leaflet*](https://rstudio.github.io/leaflet/) en sp√©cifiant `tmap_mode("view")` (pour revenir en mode statique, `tmap_mode("plot")`)

```{r iucn-tmap-view, out.width="100%", fig.align="center", fig.cap=" "}
tmap_mode("view")
tm_shape(world_gj_iucn) +
  tm_polygons("n_threatened_species", palette = "viridis")
```

Petit exemple avec des facettes synchronis√©es.

```{r quebec-tmap-facet, out.width="100%", fig.align="center", fig.cap=" "}
quebec_tmap <- tm_shape(quebec) +
  tm_polygons(c("AREA", "PERIMETER")) +
  tm_facets(sync = TRUE, ncol = 2)
quebec_tmap
```

Les cartes `tmap` peuvent √™tre export√©es sous forme d'image.

```{r}
tmap_save(tm = quebec_tmap, filename = "images/12_quebec_tmap.png", height=7)
```
 
Toutefois, au moment d'√©crire ces lignes, l'exportation en format dynamique [ne fonctionne pas pour les facettes](https://github.com/r-spatial/mapview/issues/35). De plus, il semble y avoir [un bogue avec les chemins relatifs](https://stackoverflow.com/a/48691210). Il faudra donc coller (`paste0`) le r√©pertoire de travail (`getwd()`) au chemin relatif (`"/images/12_quebec_tmap_widget/12_quebec_tmap.html"`). Enfin, les fichiers html font souvent r√©f√©rence √† des fichiers externes: mieux vaut les enregistrer dans des dossiers ind√©pendants (dans ce cas dans `"12_quebec_tmap_widget"`, que vous pourrez zipper avant de partager, ou placer sur un site web) plut√¥t que dans un dossier comprenant plusieurs images. Pas si compliqu√©... quand on le sait.
 
```{r}
quebec_tmap_area <- tm_shape(quebec) +
  tm_polygons("AREA")
tmap_save(tm = quebec_tmap_area, filename = paste0(getwd(),"/images/12_quebec_tmap_widget/12_quebec_tmap.html"))
```

## Ressources compl√©mentaires

- [Geocomputation with R](https://geocompr.robinlovelace.net/), de Robin Lovelace, Jakub Nowosad et Jannes Muenchow (2019).
- [Spatial Data Science](https://www.rspatial.org/), de Robert J. Hijmans, du Geospatial and Farming Systems Research Consortium (GFC), University of California (2019) 
- [Simple Features for R](https://r-spatial.github.io/sf/index.html), de Edzer Pebesma (2019)

```{r, include=FALSE}
rm(list = ls())
```

<!--chapter:end:12_donnees-spatiales.Rmd-->

# Mod√©lisation de m√©canismes √©cologiques {#chapitre-ode}

***
Ô∏è\ **Objectifs sp√©cifiques**:

√Ä la fin de ce chapitre, vous

- saurez d√©finir une √©quation diff√©rentielle ordinaire et une √©quation diff√©rentielle partielle
- saurez aptes √† d√©tecter un probl√®me impliquant le besoin d'utiliser des √©quations diff√©rentielles
- serez en mesure d'effectuer une mod√©lisation impliquant un syst√®me d'√âDO en contexte √©cologique

***

On se r√©f√®re √† la mod√©lisation m√©canistique lorsque des principes th√©oriques guident une mod√©lisation, √† l'inverse de la mod√©lisation ph√©nom√©nologique, qui est guid√©e par les donn√©es. Il existe de nombreuses techniques de mod√©lisation m√©canistique, mais la plupart sont guid√©es par les √©quations diff√©rentielles.

## √âquations diff√©rentielles
Les √©quations diff√©rentielles permettent la r√©solution de probl√®mes impliquant des gradients dans le temps et dans l'espace. On les utilise pour mod√©liser la dynamique des populations, la thermodynamique, l'√©coulement de l'eau dans les sols, le transport des solut√©s, etc. On en distingue deux grandes cat√©gories: les √©quations diff√©rentielles ordinaires et partielles.

**√âquations diff√©rentielles ordinaires (√âDO)**. Les √©quations diff√©rentielles ordinaires s'appliquent sur des fonctions s'appliquant √† une seule variables, qui est souvent le temps. On pourra suivre, par exemple, l'√©volution de la temp√©rature en un point, en fonction du temps √† partir d'une condition initiale. Parfois, plusieurs √âDO sont utilis√©es conjointement pour cr√©er un syst√®me d'√âDO que l'on pourra nomm√© un *syst√®me dynamique*. Les solutions analytiques des √âDO sont parfois relativement faciles √† r√©soudre, mais les ordinateurs permettent des r√©solutions num√©riques en quelques lignes de code.

**√âquations diff√©rentielles partielles (√âDP)**. Dans ce cas, ce sont plusieurs variables qui sont diff√©renci√©es dans la m√™me fonction. Il peut s'agir des coordonn√©es dans l'espace $[x, y, z]$ (r√©gime permanent), qui peuvent aussi √™tre appliqu√©s √† diff√©rents pas de temps (r√©gime transitoire). Le probl√®me sera d√©limit√© non pas seulement par des conditions initiales, mais aussi par des conditions aux fronti√®res du mod√®le. Puisque que les solutions analytiques des EDP peuvent rarement √™tre d√©velopp√©es, on utilisera pratiquement toujours des approches num√©riques que sont principalement les m√©thodes de r√©solution par diff√©rences finies ou par √©l√©ments finis. La pr√©sente mouture de ce manuel ne comprend pas la r√©solution d'√âDP.

## Les √©quations diff√©rentielles ordinaires en mod√©lisation √©cologique

L'√©volution des populations dans le temps peut √™tre abord√©e √† l'aide de syst√®mes d'√©quations diff√©rentielles. Une simple √©quation d√©crivant la croissance d'une population peut √™tre coupl√©e √† des sch√©mas d'exploitation de cette population, que ce soit une exploitation foresti√®re, une terre fourrag√®re ou un territoire de chasse. On pourra aussi faire interagir des populations dans des sch√©mas de relations biologiques. Ces processus peuvent √™tre impl√©ment√©s avec des processus al√©atoires pour g√©n√©rer des sch√©mas probabilistes. De plus, les biostatistiques et l'autoapprentissage peuvent √™tre mis √† contribution afin de calibrer les mod√®les.

### √âvolution d'une seule population en fonction du temps

La croissance d'une population (ou de sa densit√©) isol√©e en fonction du temps d√©pend des conditions qui lui offre son environnement. Dans le cas de la biomasse d'une culture √† croissance constante, le taux de croissance est toujours le m√™me.

$$ \frac{d üåø }{dt} = c $$

$$ \int_0^t c dt = \int_{üåø_0}^{üåø(t)} ~düåø $$

$$ ct = üåø(t) - üåø_0$$

$$ üåø(t) = üåø_0 + ct $$

```{r croissance-const, message=FALSE}
library("tidyverse")
y0 <- 2
c <- 2 # exprim√© en individu / pas de temps
time <- seq(0, 6, 0.1)
y <- y0 + c * time
tibble(time, y) %>%
  ggplot(aes(x = time, y = y)) +
  geom_line() +
  geom_label(x = max(time), y = max(y), label = round(max(y))) +
  expand_limits(y = 0)
```

Dans le cas d'une population qui se reproduit, une formulation simple mod√©lise une √©volution lin√©aire associ√©e √† un taux de natalit√© $n$ et un taux de mortalit√© $m$, o√π $r = n-m$ est le taux de croissance de la population d'une population de lapins üê∞ en fonction du temps $t$.

$$ \frac{düê∞}{dt} = nüê∞ - müê∞ = rüê∞ $$

$$ \int_0^t dt = \int_{üê∞_0}^{üê∞(t)} \frac{1}{rüê∞} ~düê∞ $$

$$ t = \frac{1}{r} ln(üê∞) \bigg\rvert_{üê∞_0}^{üê∞(t)} $$

$$ rt = ln \left( \frac{üê∞(t)}{üê∞_0} \right) $$

$$ üê∞(t) = üê∞_0 exp(rt) $$

La vitesse de croissance est constante pour une population constante, mais la croissance de la population est exponentielle √©tant donn√©e que chaque nouvel individu se reproduit.

```{r croissance-exp}
y0 <- 10
r <- 0.2 # exprim√© en individu / pas de temps
time <- seq(0, 10, 0.1)
y <- y0 * exp(r*time)
tibble(time, y) %>%
  ggplot(aes(x = time, y = y)) +
  geom_line() +
  geom_label(x = max(time), y = max(y), label = round(max(y))) +
  expand_limits(y = 0)
```

De 10 lapins au d√©part, nous en avons un peu plus de 75 apr√®s 10 ans... et pr√®s de 5 milliards apr√®s 100 ans! En fait, la capacit√© de support d'une population √©tant g√©n√©ralement limit√©e, on peut supposer que le taux de natalit√© d√©croit et que le taux de mortalit√© croit lin√©airement avec l'effectif.

$$ n(üê∞) = \alpha - \beta üê∞ $$
$$ m(üê∞) = \gamma + \delta üê∞ $$

On aura donc

$$ \frac{düê∞}{dt} = üê∞ \left( \alpha - \beta üê∞ \right) - üê∞ \left( \gamma + \delta üê∞ \right) = rüê∞ \left( 1 - \frac{üê∞}{K} \right) $$

o√π $r = \alpha - \gamma$ est l'ordonn√©e √† l'origine du taux de croissance (th√©orique, lorsque la population est nulle) et $K = \frac{\alpha-\gamma}{\beta + \delta}$ est la capacit√© limite du milieu de subsistance. On pourra s'aider d'un logiciel de calcul symbolique comme `sympy` ou [`maxima`](https://andrejv.github.io/wxmaxima/) pour en tirer une solution analytique. Mais √† ce point, nous utiliserons une approximation num√©rique. Nous utiliserons le module `deSolve`.

```{r}
library("deSolve")
```

`deSolve` demande de d√©finir les param√®tres de l'√âDO ou du syst√®me d'√âDO. Nous devons d'abord sp√©cifier √† quels pas de temps notre √âDO doit √™tre approxim√©e. J'√©tends la plage de temps √† 30 ans pour bien visualiser la courbe de croissance.

```{r}
time <- seq(0, 30, by = 0.5)
```

Les conditions initiales du syst√®me d'√âDO sont aussi d√©finies dans un vecteur. La seule condition initiale de notre √âDO est le nombre initial de lapin.

```{r}
y0 <- c(lapin = 10)
```

On d√©finira les param√®tres dans un vecteur `p`. Dans notre cas, nous avons $r$, le taux de croissance √† l'origine et $K$, la capacit√© de support de l'√©cosyst√®me. Il est pr√©f√©rable de nommer les param√®tres du vecteur pour √©viter les erreurs.

```{r}
p <- c(r = 0.2, K = 40)
```

Enfin, une fonction d√©finit l'√âDO avec, comme entr√©es, les pas de temps, les conditions initiales et les param√®tres. La sortie de la fonction est un vecteur des d√©riv√©es embo√Æt√©s dans une liste (lisez le fichier d'aide de la fonction `ode` pour les d√©tails en lan√ßant `?ode`).

```{r}
model_logistic <- function(t, y, p) {
  lapin <- y[1]
  dlapin_dt <- p[1] * lapin * (1 - lapin/p[2])
  return(list(c(dlapin_dt)))
}
```

Une fois que les pas de temps, les conditions initiales, les param√®tres et le mod√®le sont d√©finis, on les sp√©cifie comme arguments dans la fonction `ode`. La sortie de la fonction `ode` est une matrice dont la premi√®re colonne comprend les pas de temps impos√©s, et les autres colonnes sont les d√©riv√©es sp√©cifi√©es √† la sortie de la fonction `ode`.

```{r}
lapin_t <- ode(y = y0, times = time, model_logistic, p)
head(lapin_t)
```

```{r croissance-logistique, message=FALSE}
lapin_t %>%
  as_tibble() %>%
  ggplot(aes(x = time, y = lapin)) +
  geom_line() +
  expand_limits(y = 0)
```

**Exercice**. Que ce passerait-il si le taux de croissance √©tait n√©gatif? Profitez-en pour changer les param√®tres `r` et `K`.

```{r solution-croissance-neg, message=FALSE, eval=FALSE, echo=FALSE}
p <- c(r = -0.1, K = 100)
lapin_t <- ode(y = y0, times = time, model_logistic, p)
lapin_t %>%
  as_tibble() %>%
  ggplot(aes(x = time, y = lapin)) +
  geom_line() +
  expand_limits(y = 0)
```

**Exercice**. D'autres formulations existent pour exprimer des taux de croissance (Gompertz, Allee, etc.). En outre la formulation de Gompertz s'√©crit comme suit.

$$ \frac{düê∞}{dt} = rüê∞ \left( ln \frac{K}{üê∞} \right) $$

Entrer cet √âDO dans `R` avec `deSolve`.

```{r solution-croissance-gompertz, message=FALSE, eval=FALSE, echo=FALSE}
time <- seq(0, 30, by = 0.5)
y0 <- c(lapin = 10)
p <- c(r = 0.2, K = 40)
model_gompertz <- function(t, y, p) {
  lapin <- y[1]
  dlapin_dt <- p[1] * lapin * (log(p[2]/lapin))
  return(list(c(dlapin_dt)))
}
lapin_t <- ode(y = y0, times = time, model_gompertz, p)
lapin_t %>%
  as_tibble() %>%
  ggplot(aes(x = time, y = lapin)) +
  geom_line() +
  expand_limits(y = 0)
```

### Population exploit√©e

L'exploitation d'une population peut √™tre effectu√©e de diff√©rentes mani√®res. D'abord, le pr√©l√®vement peut √™tre effectu√© de mani√®re constante, par exemple dans un √©levage ou par la chasse ou la cueillette. Ajoutons un pr√©l√®vement constant dans une courbe de croissance logistique.

$$ \frac{düê∞}{dt} = rüê∞ \left( 1 - \frac{üê∞}{K} \right) - Q $$

o√π $Q$ est le quota, ou le pr√©l√®vement constant.

On pourra aussi effectuer un pr√©l√®vement proportionnel √† la population.

$$ \frac{düê∞}{dt} = rüê∞ \left( 1 - \frac{üê∞}{K} \right) - Eüê∞ $$

o√π $E$ est l'effort d'exploitation.

Ou bien effectuer une s√©rie de pr√©l√®vement ponctuels, comme la r√©colte de plantes fourrag√®res.

$$ \frac{düåø}{dt} = c - \left[ üåø - \gamma \right] \bigg\rvert_{t=a, b, c, d, e, ...} $$

o√π $\gamma$ est le reste de la biomasse apr√®s la r√©colte et $t=a, b, c, d, e, ...$ sont les pas de temps o√π le bloc entre les crochets est actif, c'est-√†-dire la p√©riode de r√©colte. La solution analytique d'une culture √† croissance constante est plut√¥t facile √† d√©duire.

Les fonctions de pr√©l√®vement peuvent √™tre modul√©es √† votre guise.

Prenons pour l'exemple un pr√©l√®vement constant et une croissance logistique.

```{r prel-const, message=FALSE}
p <- c(r = 0.2, K = 40, Q = 1)

model_logistic_expl <- function(t, y, p) {
  lapin <- y[1]
  dlapin_dt <- p[1] * lapin * (1 - lapin/p[2]) - p[3]
  return(list(c(dlapin_dt)))
}

lapin_t <- ode(y = y0, times = time, model_logistic_expl, p)
lapin_t %>%
  as_tibble() %>%
  ggplot(aes(x = time, y = lapin)) +
  geom_line() +
  expand_limits(y = 0)
```

**Exercice**. Mod√©liser avec un pr√©l√®vement proportionnel. Qu'arrive-t-il lorsque le pr√©l√®vement est trop √©lev√©?

```{r solution-prel-prop, eval=FALSE, echo=FALSE, message=FALSE}
p <- c(r = 0.2, K = 40, E = 0.25)

model_logistic_expl <- function(t, y, p) {
  lapin <- y[1]
  dlapin_dt <- p[1] * lapin * (1 - lapin/p[2]) - lapin*p[3]
  return(list(c(dlapin_dt)))
}

lapin_t <- ode(y = y0, times = time, model_logistic_expl, p)
lapin_t %>%
  as_tibble() %>%
  ggplot(aes(x = time, y = lapin)) +
  geom_line() +
  expand_limits(y = 0)
```

L'**exploitation ponctuelle**, comme la r√©colte ou l'administration d'une s√©rie de traitements, implique l'utilisation d'approches intermittentes. Bien que `deSolve` ignore les changements dans les variables d'√©tat (`y`) tels que d√©finis dans les d√©riv√©s, nous pouvons avoir recours √† des *√©v√®nements* dans le jargon de `deSolve`. Ces √©v√®nements doivent √™tre sp√©cifi√©s dans un `data.frame` ou une liste. Il est difficile de trouver un exemple g√©n√©rique pour mod√©liser des √©v√®nements. Pour en savoir davantage, je vous invite donc √† consulter la fiche d'aide `?events`.

Dans notre cas, nous allons mod√©liser une r√©colte de plantes fourrag√®res. La r√©colte est d√©clench√©e lorsque le rendement atteint 2 t/ha, et laisser 0.3 t/ha au sol pour assurer le renouvellement pour les coupes subs√©quentes. D√©finissons d'abord les entr√©es du mod√®les.

```{r}
time <- seq(0, 120, 0.1)
p <- c(r = 0.1, K = 2.5)
y0 <- c(champ = 0.1)
```

Nous devons d√©finir une fonction *root* (racine), comprenant tous les arguments de la fonction d'√âDO, dont la sortie est une valeur qui d√©clenchera un √©v√®nement lorsque la valeur sera nulle. Dans notre cas, la valeur correspond simplement au rendement moins 2, la quantit√© au champ y[1]. Notez que d'autres strat√©gies peuvent √™tre utilis√©es pour d√©clencher une r√©colte, par exemple le pourcentage de floraison qui demanderait des simulations plus pouss√©es.

```{r}
recolte_root <- function(t, y, p) y[1]-2
```

Puis, lorsque la fonction root est d√©clench√©e, l‚Äô√©v√®nement ram√®ne la quantit√© au champs √† 0.3 t/ha, une quantit√© qui permet de relancer la croissance.

```{r}
recolte_event <- function(t, y, p) {
  y[1] <- 0.3
  return(y)
}
```

La fonction du mod√®le est telle qu'utilis√©e auparavant: une fonction logistique.

```{r}
recolte <- function(t, y, p) {
  champ <- y[1]
  dchamp_dt <- p[1] * champ * (1 - champ/p[2])
  return(list(c(dchamp_dt)))
}
```

La fonction `ode` est lanc√©e en entrant les fonction `root` et `events`.

```{r recolte}
out <- ode(times = time, y = y0, func = recolte, parms = p,
           rootfun = recolte_root,
           events = list(func = recolte_event, root = TRUE),
           method="impAdams")
plot(out)
```

Nous pourrons organiser deux r√©coltes de 1.7 t/ha et une de 2 t/ha pour terminer la saison.

**Exercice**. Qu'adviendrait-il si vous laissiez 0.15 t/ha au champ au lieu de 0.3? Ou si vous laissiez 1 t/ha? Ou si vous d√©clenchiez une r√©colte √† 2.3 t/ha?

**D√©fi**. Pouvez-vous mod√©liser l'ensilage?

### Interactions biologiques

Les interactions biologiques entre deux esp√®ces √† un stade de croissance d√©fini peuvent prendre diff√©rentes formes, du mutualisme (les deux esp√®ces b√©n√©ficient de la relation) √† la comp√©tition (les deux esp√®ces se nuisent) en passant par la pr√©dation ou le parasitisme (une esp√®ce b√©n√©ficie de l'autre en lui nuisant) ou le neutralisme (aucun effet). Ces effets sont d√©crits dans [Pringle (2016)](https://doi.org/10.1371/journal.pbio.2000891) en un tableau synth√®se.


```{r interactions-biologiques, out.width="60%", fig.align="center", fig.cap="Interactions biologiques, Pringle, E.G. 2016. Orienting the Interaction Compass: Resource Availability as a Major Driver of Context Dependence. Plos Biology. https://doi.org/10.1371/journal.pbio.2000891.", echo = FALSE}
knitr::include_graphics("images/13_journal.pbio.2000891.g001.png")
```

Ces interactions peuvent √™tre d√©crite math√©matiquement dans des syst√®mes d'√âDO, ou √âDO coupl√©es. Le cas d'√©tude le plus courant reprend le syst√®me d'√©quation pr√©dateur-proie de **Lotka-Volterra**, deux auteurs ayant d√©velopp√© de mani√®re ind√©pendante des √©quations similaires respectivement en 1925 et 1926.

Les √©quations de Lotka-Volterra supposent une croissance illimit√©e des deux esp√®ces: les proies üê∞ se reproduisent par elles-m√™mes ($\alpha üê∞$), tandis que les pr√©dateurs ü¶ä croissent selon la disponibilit√© des proies ($\delta üê∞ü¶ä$). √Ä l'inverse, la mortalit√© des proies d√©pend du nombre de pr√©dateurs ($- \beta üê∞ü¶ä$), mais la mortalit√© des pr√©dateurs est ind√©pendante des proies ($- \gamma ü¶ä$). On obtient ainsi un syst√®me d'√©quation.

$$\frac{düê∞}{dt} = \alpha üê∞ - \beta üê∞ü¶ä = üê∞ \left( \alpha - \beta ü¶ä \right)$$

$$\frac{dü¶ä}{dt} = \delta üê∞ü¶ä - \gamma ü¶ä = ü¶ä \left( \delta üê∞ - \gamma \right) $$

√Ä l'√©quilibre de üê∞, c'est-√†-dire o√π $\frac{düê∞}{dt} = 0$, on retrouve $üê∞=0$ ou $ü¶ä = \frac{\alpha}{\beta}$. De m√™me, √† l'√©quilibre de ü¶ä, on retrouve $ü¶ä=0$ ou $üê∞ = \frac{\gamma}{\delta}$. En termes math√©matiques, ces √©quilibre sont des isoclines, des points d'inflexion dans le syst√®me d'√âDO.

Nous allons r√©soudre les √©quations de Lotka-Volterra avec `deSolve`. Rappelons-nous que nous devons d√©finir des pas de temps o√π approximer les populations (`times`), des conditions initiales (`y0`) et des param√®tres (`p`).

```{r}
time <- seq(0, 30, by = 0.1)
y0 <- c(lapin = 3, renard = 1)
p <- c(alpha = 2, # taux de croissance des lapins (naissance - mortalit√©, 1/an)
       beta = 0.8, # taux de pr√©dation des lapins (renard / an)
       delta = 0.1, # taux de conversion lors de la pr√©dation (lapin / renard)
       gamma = 0.2) # mortalit√© naturelle des renards (1/an)
```

On peut calculer d'embl√©e les isoclines.

```{r}
lapin_iso <- p[4]/p[3]
renard_iso <- p[1]/p[2]
```

Nous devons ensuite cr√©er notre mod√®le.

```{r}
modele_LV <- function(t, y, p) {
  lapin = y[1]
  renard = y[2]
  dlapin_dt = p[1] * lapin - p[2] * lapin * renard
  drenard_dt = p[3] * lapin * renard - p[4] * renard
  return(list(c(dlapin_dt, drenard_dt)))
}
```

Lan√ßons l'approximation.

```{r}
effectifs_t = ode(y = y0, times = time, modele_LV, p)
head(effectifs_t)
```

```{r lv-time, message=FALSE}
effectifs_t %>%
  as_tibble() %>%
  gather(key="espece", value = "value", -time) %>%
  ggplot(aes(x=time, y=value)) +
  geom_line(aes(colour=espece)) +
  expand_limits(y = 0)
```

Lorsque la population de lapins croit, celle des renards croit √† retardement jusqu'√† ce que la population de lapin diminue jusqu'√† √™tre presque √©teinte. Dans ces conditions, la population de renard ne peut plus √™tre soutenue, et d√©croit, ce qui en retour donne l'opportunit√© de la population de lapins de resurgir.

```{r lv-cycle, message=FALSE}
effectifs_t %>%
  as_tibble() %>%
  ggplot(aes(x = lapin, y = renard)) +
  geom_path() +
  geom_hline(yintercept = lapin_iso, linetype = 2) +
  geom_vline(xintercept = renard_iso, linetype = 2)
```

Les conditions initiales sont responsables de l'amplitude des cycles.

**Excercice**. V√©rifier l'effet des param√®tres sur les cycles.

Qu‚Äôadviendrait-il des populations si l'on prenait plut√¥t un profil de croissance logistique chez les lapins?

$$\frac{düê∞}{dt} = \alphaüê∞ \left( 1-\frac{üê∞}{K} \right) - \beta üê∞ü¶ä $$

$$\frac{dü¶ä}{dt} = \delta üê∞ü¶ä - \gamma ü¶ä $$

Pour les isoclines, √† l'√©quilibre o√π $\frac{düê∞}{dt} = 0$, on retrouve $ü¶ä=\frac{\alpha}{\beta} \left( 1-\frac{üê∞}{K} \right)$ ou $üê∞=0$. De m√™me que pr√©c√©demment, √† l'√©quilibre de ü¶ä, on retrouve $ü¶ä=0$ ou $üê∞ = \frac{\gamma}{\delta}$.

Reprenons nos param√®tres, mais en ajoutant la capacit√© de support des lapins, √† $K = 40$.

```{r message = FALSE}
time <- seq(0, 60, by = 0.1)
y0 <- c(lapin = 3, renard = 1)
p <- c(alpha = 2, # taux de croissance des lapins (naissance - mortalit√©, 1/an)
       beta = 0.8, # taux de pr√©dation des lapins (renard / an)
       delta = 0.1, # taux de conversion lors de la pr√©dation (lapin / renard)
       gamma = 0.2, # mortalit√© naturelle des renards (1/an)
       K = 40) # capacit√© de support de l'√©cosyst√®me
```

Calculons les isoclines, en tenant compte que, cette fois-ci, l'isocline des renards est une fonction du nombre de lapins.

```{r}
lapin_iso <- p[4] / p[3]
renard_iso <- tibble(lapin = seq(from = 0, to = 40, by = 1)) %>%  # acec une s√©quence de lapins ...
  mutate(renard = p[1] / p[2] * (1 - lapin/p[5])) # ... calculer les renards
```

Le mod√®le logistique diff√®re peu du mod√®le classique de Lotka-Volterra.

```{r}
modele_LV_logist <- function(t, y, p) {
  lapin = y[1]
  renard = y[2]
  dlapin_dt = p[1] * lapin * (1-y[1]/p[5]) - p[2] * lapin * renard
  drenard_dt = p[3] * lapin * renard - p[4] * renard
  return(list(c(dlapin_dt, drenard_dt)))
}
```

Lan√ßons la mod√©lisation, puis affichons les r√©sultats.

```{r lv-logistique, message=FALSE, warning=FALSE}
effectifs_t <- ode(y = y0, times = time, modele_LV_logist, p)

gg_time <- effectifs_t %>%
  as_tibble() %>%
  gather(key="espece", value = "value", -time) %>%
  ggplot(aes(x=time, y=value)) +
  geom_line(aes(colour=espece)) +
  expand_limits(y = 0)

gg_cycle <- effectifs_t %>%
  as_tibble() %>%
  ggplot(aes(x = lapin, y = renard)) +
  geom_path() +
  geom_vline(xintercept = lapin_iso, linetype = 2) +
  geom_line(data = renard_iso, linetype = 2) +
  xlim(c(0, 10))

cowplot::plot_grid(gg_time, gg_cycle)
```

Ainsi con√ßu, le syst√®me tant vers des effectifs constants aux isoclines.

Dans les cycles √©tudi√©s jusqu'ici, les effectifs atteignent syst√©matiquement un √©tat critique, mais se recouvrent sans cesse. Il serait toutefois √©tonnant que les param√®tres des √©quations (reproduction, mortalit√©, pr√©dation, support des √©cosyst√®mes) soient constants. On peut admettre que les param√®tres peuvent varier en fonction de d'autres param√®tres, ou simplement au hasard. Justement, il est possible d'ajouter de la stochastique (processus al√©atoire) dans nos fonctions. En outre, plusieurs simulations pourront nous indiquer un risque d'effondrement d'un √©cosyst√®me.

Mais adviendra la possibilit√© que les effectifs des populations prennent des valeurs n√©gatives, ce qui n'est pas admissible. Une solution est de reformuler nos √©quations pour faire en sorte de mod√©liser le logarithme des effectifs, qui pourront √™tre recalcul√©es par l'exponentielle dans la base du log. Un log n√©gatif retransform√© par l'exponentiel devient une fraction de 1 (si $log_{10}(x) = -1$, $x = 0.1$). Une autre approche est d'utiliser un √©v√©nement ramenant l'effectif n√©gatif √† z√©ro, d√©clanch√© lorsqu'un des effectifs est infrieur ou √©gal √† z√©ro. C'est ce que nous allons faire, avec les m√™mes `time`, `y0` et `p` que pr√©c√©demment.

La fonction *root* est un moyen de d√©clencher l'√©v√©nement. Elle prend la valeur de z√©ro si l'un des deux effectifs est nul.

```{r message=FALSE}
zero_root <- function(t, y, p) {
  x1 <- y[1] >= 0
  x2 <- y[2] >= 0
  xnum <- as.numeric(x1 & x2)
  return(xnum)
}

zero_event <- function(t, y, p) {
  if (y[1] <= 0) y[1] <- 0
  if (y[2] <= 0) y[2] <- 0
  return(y)
}
```

Reprenons la fonction logistique, mais en ajoutant un effet al√©atoire √† chacun des param√®tres.

```{r}
modele_LV_alea <- function(t, y, p) {
  lapin = y[1]
  renard = y[2]
  alpha <- rnorm(1, p[1], 0.0005)
  beta <- rnorm(1, p[2], 0.0005)
  delta <- rnorm(1, p[3], 0.001)
  gamma <- rnorm(1, p[4], 0.001)
  K <- rnorm(1, p[5], 1)
  dlapin_dt <- alpha * lapin * (1-lapin/K) - beta * lapin * renard
  drenard_dt <- delta * lapin * renard - gamma * renard
  return(list(c(dlapin_dt, drenard_dt)))
}
```

La mod√©lisation prend en compte l'√©v√©nement.

```{r}
set.seed(14389)
effectifs_t = ode(y = y0,
                  times = time,
                  func = modele_LV_alea,
                  parms = p,
                  rootfun = zero_root,
                  events = list(func = zero_event, root = TRUE),
                  method="impAdams")
effectifs_tibble <- effectifs_t %>% unclass() %>% as_tibble()
```

On lance ensuite les m√™mes fonctions de visualisation que pr√©c√©demment.

```{r lv-alea, message=FALSE}
gg_time <- effectifs_tibble %>%
  gather(key="espece", value = "value", -time) %>%
  ggplot(aes(x=time, y=value)) +
  geom_line(aes(colour=espece)) +
  expand_limits(y = 0)

gg_cycle <- effectifs_tibble %>%
  ggplot(aes(x = lapin, y = renard)) +
  geom_path(aes(colour = time)) +
  geom_vline(xintercept = lapin_iso, linetype = 2) +
  geom_line(data = renard_iso, linetype = 2) +
  expand_limits(x = 0, y = 0)

cowplot::plot_grid(gg_time, gg_cycle)
```

Une tr√®s faible variance sur les param√®tres peu grandement perturber le syst√®me. Il est possible, en effectuant plusieur simulations en boucle, d'√©valuer le risque d'effondrement des effectifs d'une esp√®ce, ce qui arrive pour le cas simul√© pour les lapins, puis pour les renards.

Nous avons mod√©lis√© une relation biologique de pr√©dation. Il existe dans la litt√©rature une panoplie de mod√®les d'√âDO pour d√©crire les relations biologiques, qui peuvent √™tre mod√©lis√©s entre plusieurs esp√®ces pour cr√©er des r√©seaux trophiques complexes. Toutefois, la difficult√© de collecter des donn√©es en quantit√© et en qualit√© suffisante rendent ces mod√®les difficiles √† appr√©hender.

**Exercice**. Mod√©liser une comp√©tition intersp√©cifique o√π chaque population croit de mani√®re logistique.

$$\frac{düêÅ}{dt} = r_1 üêÅ \left( 1-\frac{üêÅ}{K_1} -\alpha \frac{üêÄ}{K_2} \right) $$

$$\frac{düêÄ}{dt} = r_2 üêÄ \left( 1-\frac{üêÄ}{K_2} -\beta \frac{üêÅ}{K_1} \right) $$

o√π $r_1$ et $r_2$ sont les taux de croissances respectifs des üêÅ et des üêÄ, ainsi que $K_1$ et que $K_2$ sont les capacit√©s de support des üêÅ et des üêÄ. Le coefficient $\alpha$ d√©crit l'ampleur de la comp√©tition de üêÄ sur üêÅ et le coefficient $\beta$ d√©crit l'ampleur de la comp√©tition de üêÅ sur üêÄ ($\alpha$ et $\beta$ sont >= 0).

```{r solution-competition-is, eval = FALSE, echo = FALSE}
time <- seq(0, 200, by = 1)
y0 <- c(blanc = 3, gris = 3)
p <- c(alpha = 0.9, # taux de comp√©tition des gris sur les blancs
       beta = 0.2, # taux de comp√©tition des blancs sur les gris
       r1 = 0.2, # taux de croissance des blancs
       r2 = 0.15, # taux de croissance des gris
       K1 = 40,  # capacit√© de support de l'√©cosyst√®me des blancs
       K2 = 45) # capacit√© de support de l'√©cosyst√®me des gris

modele_IS <- function(t, y, p) {
  dblanc_dt = p[3] * y[1] * (1 - y[1]/p[5] - p[1]*y[2]/p[6])
  dgris_dt = p[4] * y[2] * (1 - y[2]/p[6] - p[2]*y[1]/p[5])
  return(list(c(dblanc_dt, dgris_dt)))
}

effectifs_t <- ode(y = y0, times = time, modele_IS, p)

gg_time <- effectifs_t %>%
  as_tibble() %>%
  gather(key="espece", value = "value", -time) %>%
  ggplot(aes(x=time, y=value)) +
  geom_line(aes(colour=espece)) +
  expand_limits(y = 0)

gg_cycle <- effectifs_t %>%
  as_tibble() %>%
  ggplot(aes(x = blanc, y = gris)) +
  geom_path()

cowplot::plot_grid(gg_time, gg_cycle)
```

**Exercice**. Les interactions biologiques forment une bonne introduction aux syst√®mes d'√©quations diff√©rentielles ordinaires. On fait n√©anmoins souvent r√©f√©rence aux √©quations de [Lorenz (1963)](https://journals.ametsoc.org/doi/abs/10.1175/1520-0469%281963%29020%3C0130:DNF%3E2.0.CO;2), qui a d√©velopp√© un syst√®me d'√âDO chaotique depuis trois √©quations,

$$ X' = aX + YZ, $$
$$ Y' = b \left(Y-Z\right), $$
$$ Z' =  -XY + cY - Z, $$

o√π $X$ est la temp√©rature horizontale, $Y$ est la temp√©rature verticale, $Z$ est le flux de chaleur convectif, et o√π l'on retrouve les param√®tres $a = -8/3$, $b=-10$ et $c=28$.

R√©soudre les √©quations de Lorents avec `deSolve`. Porter graphiquement les relations entre X, Y et Z.

```{r solution-lorenz, echo=FALSE, eval=FALSE}
time <- seq(0, 20, by = 0.01)
y0 <- c(X = 10, Y = 10, Z = 10)
p <- c(a = -8/3, b = -10, c = 28)

modele_lorenz <- function(t, y, p) {
  dxdt <- p[1]*y[1] + y[2]*y[3]
  dydt <- p[2]*(y[2] - y[3])
  dzdt <- -y[1]*y[2] + p[3] * y[2] - y[3]
  return(list(c(dxdt, dydt, dzdt)))
}

lorenz_t <- ode(y = y0, times = time, modele_lorenz, p)

gg_time <- lorenz_t %>%
  as_tibble() %>%
  gather(key="variable", value = "value", -time) %>%
  ggplot(aes(x=time, y=value)) +
  geom_line(aes(colour=variable)) +
  expand_limits(y = 0)

gg_XY <- lorenz_t %>%
  as_tibble() %>%
  ggplot(aes(x=X, y=Y)) +
  geom_path(alpha = 0.6)

gg_XZ <- lorenz_t %>%
  as_tibble() %>%
  ggplot(aes(x=X, y=Z)) +
  geom_path(alpha = 0.6)

gg_YZ <- lorenz_t %>%
  as_tibble() %>%
  ggplot(aes(x=Y, y=Z)) +
  geom_path(alpha = 0.6)

cowplot::plot_grid(gg_XY, gg_XZ, gg_YZ, gg_time, ncol=2)
```

```{r, include=FALSE}
rm(list = ls())
```

<!--chapter:end:13_modelisation-deterministe.Rmd-->

