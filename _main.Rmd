--- 
title: "Analyse et modélisation d'agroécosystèmes"
author: "Serge-Étienne Parent"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
delete_merged_file: true
description: "Ce cours a pour objectif de former les étudiants gradués en génie agroenvironnemental, génie civil, génie écologique, agronomie, biologie, foresterie et écologie en analyse et modélisation de systèmes vivants. Les sujets traités sont l’introduction au langage de programmation R, l’analyse statistique descriptive, la visualisation, la modélisation inférentielle, prédictive et déterministe."
#bibliography: [book.bib, packages.bib]
#biblio-style: apalike
#link-citations: yes # bookdown::render_book("index.Rmd", "bookdown::gitbook")
---

# Introduction {#chapitre-intro-cours}

En développant son jeu de la vie (*game of life*) en 1970, John Horton Connway a présenté un exemple percutant que des règles simples peuvent mener à des résultats inattendus. Le jeu consiste à placer des jetons sur les cases d'un plateau de jeu consistant en une simple grille orthogonale. Le jeu évolue en fonction du nombre de jetons présents parmi les huit cases du voisinage des jetons ou des cases vides.

1. Les jetons ayant 0 ou 1 voisin sont retirés.
2. Les jetons ayant 2 ou 3 voisins restent intacts
3. Les jetons ayant plus de 3 voisins sont retirés
4. Un jeton est posé sur les cases ayant exactement 3 voisins 

C'est tout. Selon la manière dont les jetons sont placés au départ, il se peut que la grille se vide de ses jetons, ou que les jetons y prennent beaucoup de place. Il arrive aussi que des cycles réguliers se dégagent ou que l'on se retrouve avec des formes régulières. Vous aurez peut-être compris à ce stade pourquoi le jeu est appelé "jeu de la vie". La première règle est une situation localisée de sous-population, condition dans laquelle la reproduction est difficile. La deuxième règle est une situation localisée stable. La troisième est une situation de surpopulation, où des individus meurent par un environnement rendu inadéquat par insuffisance de ressource ou surplus de toxicité. Enfin, la quatrième indique une situation favorable à la reproduction.

Une grille vidée correspond à une extinction et une grille remplie correspond à une explosion de population. Une oscillation est un "climax", un état stable en écologie. Un léger changement initial dans la disposition initiale des jetons peut mener à des solutions différentes.

Le jeu, qui en fait est une application de la technique des *automates cellulaires*, se complexifie à mesure que le nombre de jetons grandit. Un humain passera des heures à calculer une itération à 50 jetons, commettra probablement quelques erreurs et demandera quelques cafés. Un processeur pourra gérer des centaines d'itérations sur des grilles de centaines de jetons en quelques secondes.

En établissant des règles correspondant aux mécanismes de l'objet étudié, il devient possible de modéliser l'évolution des systèmes vivants, comme l'émergence d'espèces invasives.

<img src="https://media.giphy.com/media/dkaEYYiCDclAA/giphy.gif?response_id=5925bb98f431280237d48493" width=300>

Simulation avec automates cellulaires. Source: Anonyme, publié sur [Giphy](https://giphy.com/gifs/misc-dkaEYYiCDclAA).


## Définitions

Les mathématiques confèrent aux humains une capacité d'abstraction suffisamment complexe pour leur permettre de toucher les étoiles et les atomes, d'assembler la pensée pour mieux apprécier l'histoire et de prédire le futur, de toucher l'infini et de goûter à l'éternité. À partir des maths, on a pu créer des outils de calcul permettent de projeter des images de l'univers, bien au-delà de la Voie lactée. Mais appréhender le vivant demeure néanmoins une tâche complexe.

<img src="images/01_disciplines.png" width=280 style="padding:5px;">

Carte des domaines de l'écologie mathématique

L'écologie mathématique couvre un large spectre de domaines, mais peut être divisée en deux branches: l'**écologie théorique** et l'**écologie quantitative** (Legendre et Legendre, 2012). Alors que l'écologie théorique s'intéresse à l'expression mathématique des mécanismes écologiques, l'écologie quantitative, plus empirique, en étudie principalement les phénomènes. La **modélisation écologique** vise à prévoir une situation selon des conditions données. Faisant partie à la fois de l'écologie théorique et de l'écologie quantitative, elle superpose souvent des mécanismes de l'écologie théorique et des phénomènes empiriques de l'écologie quantitative. L'**écologie numérique** comprend la branche descriptive de l'écologie quantitative, c'est-à-dire qu'elle s'intéresse à évaluer des effets à partir de données empiriques. L'exploration des données dans le but d'y découvrir des structures passe souvent par des techniques multivariées comme la classification hiérarchique ou la réduction d'axe (par exemple, l'analyse en composantes principales), qui sont davantage heuristiques (dans notre cas, **bioheuristique**) que statistiques. Les tests d'hypothèses et l'analyse des probabilités, quant à eux, relèvent de la **biostatistique**.

Le **génie écologique**, une discipline intimement liée à l'écologie mathématique, est voué à l'analyse, la modélisation, la conception et la construction de systèmes vivants dans le but de résoudre de manière efficace des problèmes liés à l'écologie et à une panoplie de domaines qui lui sont raccordés. L'agriculture est l'un de ces domaines. C'est d'emblée la discipline qui sera prisée dans ce manuel. Néanmoins, les principes qui seront discutés sont transférables à l'écologie générale.

## À qui s'adresse ce manuel?

Le cours vise à introduire des étudiants gradués en agronomie, biologie, écologie, sols, génie agroenvironnemental, génie civil et génie écologique à l'analyse et la modélisation dans leur domaine, tant pour les appuyer pour leurs travaux de recherche que pour leur fournir une trousse d'outil émancipatrice pour leur cheminement professionnel. Plus spécifiquement, vous serez accompagné à découvrir différents outils numériques qui vous permettront d'appréhender vos données, d'en faire émerger l'information et de construire des modèles. En ce sens, **c'est un cours de pilotage, pas un cours de mécanique**.

Bien que des connaissances en programmation et en statistiques aideront grandement les étudiant.e.s à appréhender ce document, une littératie informatique n'est pas requise. Dans tous les cas, quiconque voudra tirer profit de ce manuel devra faire preuve d'autonomie. Vous serez guidés vers des ressources et des références, mais je vous suggère vivement de développer votre propre bibliothèque adaptée à vos besoins et à votre manière de comprendre.

## Les logiciels libres

Tous les outils numériques qui sont proposés dans ce cours sont des logiciels libres:

> « Logiciel libre » [free software] désigne des logiciels qui respectent la liberté des utilisateurs. En gros, cela veut dire que les utilisateurs ont la liberté d'exécuter, copier, distribuer, étudier, modifier et améliorer ces logiciels. Ainsi, « logiciel libre » fait référence à la liberté, pas au prix1 (pour comprendre ce concept, vous devez penser à « liberté d'expression », pas à « entrée libre »). - [Projet GNU](https://www.gnu.org/philosophy/free-sw.fr.html)

Donc: codes sources ouverts, développement souvent communautaire, gratuité. Plusieurs [raisons éthiques](https://www.youtube.com/watch?v=Ag1AKIl_2GM), principalement liées au contrôle de l'environnement virtuel par les utilisateurs et les communautés, peuvent justifier l'utilisation de logiciels libres. Plusieurs raisons pratiques justifient aussi cette orientation. Les logiciels libres vous permettent de transporter vos outils avec vous, d'une entreprise à l'autre, au bureau, ou à la maison, et ce, sans vous soucier d'acheter de coûteuses licences.

On soulève avec justesse les risques liés aux possibles erreurs dans les codes des logiciels communautaires. Pour les scientifiques, une erreur peut mener à une étude retirée de la littérature et même, potentiellement, des politiques publiques mal avisées. Pour les ingénieurs, les conséquences pourraient être dramatiques. Mais retenez qu'en toute circonstance, **comme professionnel.le, vous êtes responsable des outils que vous utilisez: vous devez vous assurer de la bonne qualité d'un logiciel, qu'il soit propriétaire ou communautaire**.

Alors que la qualité des logiciels propriétaires est généralement suivie par audits, celle des logiciels libres est plutôt soumise à la vigilance communautaire. Chaque approche a ses avantages et inconvénients, mais elles ne sont pas exclusives. Ainsi les logiciels libres peuvent être audités à l'externe par quiconque décide de le faire. Différentes entreprises, souvent concurrentes, participent tant à cette vigilance qu'au développement des logiciels libres: elles en sont même souvent les instigatrices (comme [RStudio](https://www.rstudio.com/), [Anaconda](https://www.anaconda.com/) et [Enthought](https://www.enthought.com/)).

Par ailleurs, ce manuel est distribué librement (licence [MIT](LIEN)). 

## Langage de programmation

### R
Ce cours est basé sur le langage [R](https://www.r-project.org/). En plus d'être libre, R est un langage de programmation dynamique largement utilisé dans le monde universitaire, et dont l'utilisation s'étend de manière soutenue hors des tours d'ivoire.

> R is also the name of a popular programming language used by a growing number of data analysts inside corporations and academia.  It is becoming their lingua franca partly because data mining has entered a golden age, whether being used to set ad prices, find new drugs more quickly or fine-tune financial models. [New York Times, janvier 2019](https://www.nytimes.com/2009/01/07/technology/business-computing/07program.html)

Son développement est supporté par la [R Foundation for Statistical Computing](https://www.r-project.org/foundation/), basée à l'Université de Vienne. Également, l'équipe de [RStudio](https://www.rstudio.com/) contribue largement au [développement de modules génériques](https://www.rstudio.com/products/rpackages/). R est principalement utilisé pour le calcul statistique, mais les récents développements le rendent un outil de choix pour tout ce qui entoure la science des données, de l'interaction avec les bases de données au déploiement d'outils d'intelligence artificielle en passant par la visualisation. Une fois implémenté avec des modules de calcul scientifique spécialisés en biologie, en écologie et en agronomie (que nous couvrirons au long du cours), R devient un outil de calcul convivial, rapide et fiable pour le calcul écologique.

### Pourquoi pas Python?
La première mouture de ce cours se fondait sur le langage Python. Tout comme R, Python est un langage de programmation dynamique prisé pour le calcul scientifique. Python est un langage générique apprécié pour sa polyvalence et sa simplicité. Python est utilisé autant pour créer des logiciels ou des sites web que pour le calcul scientifique. Ainsi, Python peut être utilisé en interopérabilité avec une panoplie de logiciels libres, comme [QGIS](http://www.qgis.org) pour la cartographie et [FreeCAD](https://github.com/FreeCAD/FreeCAD) pour le dessin technique. Il est particulièrement apprécié en ingénierie pour ses modules de calcul par éléments finis ([FeNICS](https://fenicsproject.org/), [SfePy](http://sfepy.org/doc-devel/index.html)) et en bioinformatique pour ses outils liés au séquençage ([scikit-bio](http://scikit-bio.org/)), mais ses lacunes en analyse statistique, en particulier en statistiques multivariées m'ont amené à favoriser R.

Bien que leurs possibilités se superposent largement, ce serait une erreur d'aborder R et Python comme des langages rivaux. Les deux langages s'expriment de manière similaire et s'inspirent mutuellement: apprendre à travailler avec l'un revient à apprendre l'autre. Les spécialistes en calcul scientifique tendent à apprendre à travailler avec plus d'un langage de programmation. Par ailleurs, l’entreprise [Ursa labs](https://ursalabs.org/) travaille en ce moment à l'élaboration d'une infrastructure de données permettant de partager des objets R et Python, en vue d'intégrer différents langages de programmation dans un même flux de travail.

### Pourquoi pas Matlab?
Parce qu'on est en 2019.

### Et... SAS?
Parce qu'on est à l'université.

### Mais pourquoi pas ______ ?
D'autres langages, comme [Julia](http://julialang.org), [Scala](http://www.scala-lang.org), [Javascript](https://dtabio.gitbooks.io/data-science-with-javascript/content/) et même [Ruby](http://sciruby.com) sont utilisés en calcul scientifique. Ils sont néanmoins moins garnis et moins documentés que R. Des langages de plus bas niveau, comme Fortran et C++, viennent souvent appuyer les fonctions des autres langages: ces langages sont plus ardus à utiliser au jour le jour, mais leur rapidité de calcul est imbattable.

## Contenu du manuel

Le pire angle avec lequel je pourrais aborder le sujet, c'est avec du code et des formules mathématiques. À travers chacun des chapitres, je tenterai de vous amener à résoudre des problèmes de la manière la plus intuitive possible. Nous aborderons l’analyse et la modélisation inférentielle, prédictive et déterministe appliquée aux agroécosystèmes.

**Chapitre \@ref(chapitre-intro-a-R) - Introduction au langage de programmation R**. Qu'est-ce que R? Comment l'aborder? Quelles sont les fonctionnalités de base et comment tirer profit de tout l'écosystème de programmation?

**Chapitre \@ref(chapitre-tableaux) - Organisation des données et opérations sur des tableaux**. Les tableaux permettent d'enchâsser l'information dans un format prêt-à-porter pour R. Comment les importer, les exporter, les filtrer, et en faire des sommaires?

**Chapitre \@ref(chapitre-visualisation) - Visualisation**. Comment présenter l'information contenue dans un long tableau en un seul coup d'oeil?

**Chapitre \@ref(chapitre-biostats) - Biostatistiques**. Il est audacieux de ne consacrer qu'un seul chapitre sur ce vaste sujet. Nous irons à l'essentiel... pour vous donner les outils qui permettront d'approfondir le sujet.

**Chapitre \@ref(chapitre-biostats-bayes) - Biostatistiques bayésiennes**. Une très brève introduction pour qui est intéressé à l'analyse bayésienne.

**Chapitre \@ref(chapitre-explorer) - Explorer R**. La science des données évolue rapidement. Vous gagnerez à vous tenir au courrant de son évolution, et immanquablement vous vous buterez sur des opérations qui vous sembleront insolubles. Ce chapitre vous accompagnera à rester à jour sur le développement de R, à poser de bonnes questions et proposera des modules intéressants en écologie mathématique.

**Chapitre \@ref(chapitre-ordination) - Association, partitionnement et ordination**. Les écosystèmes diffèrent, mais en quoi sont-ils semblables, et en quoi dffèrent-ils? Ces questions importantes peuvent être abordés par l'écologie numérique, domaine d'étude au sein duquel l'association, le partitionnement et l'ordination sont des outils prédominants.

**Chapitre \@ref(chapitre-outliers) - Détection de valeurs aberrantes et imputation**. Une donnée aberrante sortira du lot, pour une raison ou pour une autre. Comment les détecter de manière systématique? D'autre part, que faire lorsqu'une donnée est manquante? Peut-on l'imputer? Comment?

**Chapitre \@ref(chapitre-temps) - Les séries temporelles**. Les capteurs modernes permettent de générer des données en fonction du temps. Que ce soit des données météorologiques enregistrées quotidiennement ou des données de teneur en eau enregistrées au 5 secondes, les données en fonction du temps forment un signal. Comment analyser ces signaux?

**Chapitre \@ref(chapitre-git) - Le travail collaboratif, le suivi de version et la science ouverte**. Ce chapitre offre une introduction à l'utilisation des outils de calcul collaboratif, ainsi qu'un aperçu du système de suivi de version *git* et de son utilisation sur [GitHub](https://github.com/).

**Chapitre \@ref(chapitre-ml) - L'autoapprentissage**. Les applications de l'intelligence artificielle ne sont limitées que par votre imagination. Encore faut-il l'utiliser intelligemment.

**Chapitre \@ref(chapitre-geo) - Les données spatiales**. Non, nous n'aborderons pas les géostatistiques. Ce chapitre porte plutôt sur l'utilisation de R comme système d'information géographique de base. Nous utiliserons aussi l'autoapprentissage comme outil d'interpolation spatial.

**Chapitre \@ref(chapitre-ode) - La modélisation déterministe**. Les modèles sont des maquettes simplifiées. Comment utiliser les équations différentielles ordinaires pour créer ces maquettes?

Si les chapitres 3 à 5 peuvent être considérés comme fondamentaux pour bien maîtriser R, les autres peuvent être feuilletés à la pièce, bien qu'ils forment une suite logique.

Chaque chapitre de ce manuel est rédigé en format  *R notebook*, dans un environnement RStudio. Pour exécuter les commandes, les vous pourrez soit copier-coller les commandes dans R (ou RStudio), soit [télécharger les fichiers-sources](https://github.com/essicolo/ecologie-mathematique-R) et exécuter les blocs de code.

## Objectifs généraux

À la fin du cours, l'étudiant.e sera en mesure:

- de programmer en langage R
- d'importer, de manipuler (sélection des colonnes, filtres, sommaires statistiques) et d'exporter des tableaux
- de générer des graphiques d'utilisation commune
- d'appréhender des données écologiques et agronomiques à l'aide de tests statistiques fréquentiels
- d'explorer par lui.elle-même les possibilités offertes par la communauté de développement de modules R
- d'explorer les données à l'aide des outils de l'écologie numérique (association, partitionnement et ordination)
- d'imputer des données manquantes dans un tableau et de détecter des valeurs aberrantes
- d'effectuer une analyse de série temporelle
- de s'assurer que ses calculs soit auditables et reproductibles dans une perspective de science ouverte
- de créer un modèle d'autoapprentissage
- d'intrapoler des données spatiales
- de modéliser des équations différentielles ordinaires

## Lectures complémentaires
### Écologie mathématique

- [How to be a quantitative ecologist](). Jason Mathipoulos vous prend par la main pour découvrir les notions de mathématiques fondamentales en écologie, appliquées avec le langage R.  
- [Numerical ecology](https://www.elsevier.com/books/numerical-ecology/legendre/978-0-444-53868-0). L'ouvrage hautement détaillé des frères Legendre est non seulement fondamental, mais aussi fondateur d'une science qui évolue encore aujourd'hui: l'analyse des données écologiques.
- [A practical guide to ecological modelling](http://www.springer.com/us/book/9781402086236). Soetaert et Herman portent une attention particulière à la présentation des principes de modélisation dans un langage accessible - ce qui est rarement le cas dans le domaine de la modélisation. Les modèles présentés concernent principalement les bilans de masse, en termes de systèmes de réactions chimiques et de relations biologiques.
- [Modélisation mathématique en écologie](http://www.documentation.ird.fr/hor/fdi:010050350). Rare livre en modélisation écologique publié en français, la première partie s'attarde aux concepts mathématiques, alors que la deuxième planche à les appliquer. Si le haut niveau d'abstraction de la première partie vous rebute, n'hésitez pas débuter par la seconde partie et de vous référer à la première au besoin.
- [A new ecology: systems perspective](https://www.elsevier.com/books/a-new-ecology/jorgensen/978-0-444-53160-5). Principalement grâce au soleil, la Terre forme un ensemble de gradients d'énergie qui se déclinent en des systèmes d'une étonnante complexité. C'est ainsi que le regretté Sven Erik Jørgensen (1934-2016) et ses collaborateurs décrivent les écosystèmes dans cet ouvrage qui fait suite aux travaux fondateurs de Howard Thomas Odum.

![](images/01_sven-jorgensen.png)

[Sven Erik Jørgensen](http://scitechconnect.elsevier.com/in-memoriam-of-dr-sven-erik-jorgensen/)

- Ecological engineering. Principle and Practice.
- Ecological processes handbook.
- Modeling complex ecological dynamics

### Programmation
- [R for data science](http://r4ds.had.co.nz/). L'analyse de données est une branche importante de l'écologie mathématique. Ce manuel traite des matrices et la manipulation de données chapitre 3), de la visualisation (chapitre 4) ainsi que de l'apprentissage automatique (chapitre 11). *R for data science* repasse ces sujets plus en profondeur. En particulier, l'ouvrage de [Garrett Grolemund](https://twitter.com/StatGarrett) et [Hadley Wickham](https://twitter.com/hadleywickham) offre une introduction au module graphique `ggplot2`.
- [Numerical ecology with R](http://www.springer.com/la/book/9781441979759). Daniel Borcard enseigne l'écologie numérique à l'Université de Montréal. Son cours est condensé dans ce livre recettes voué à l'application des principes lourdement décrits dans [Numerical ecology](https://www.elsevier.com/books/numerical-ecology/legendre/978-0-444-53868-0).

### Divers
- [The truthful art](http://www.thefunctionalart.com/p/the-truthful-art-book.html). Dans cet ouvrage, Alberto Cairo s'intéresse à l'utilisation des données et de leurs présentations pour fournir une information adéquate à différents publics.

## Besoin d'aide?

Les ouvrages de référence reconnus vous offrent des bases solides sur lesquelles vous pouvez vous appuyer dans vos travaux. Mais au-delà des principes, au jour le jour, vous vous buterez immanquablement à toutes sortes de petits problèmes. Quel module utiliser pour cette tâche précise? Que veut dire ce message d'erreur? Comment interpréter ce résultat? Pour tous les petits accrocs du quotidien en calcul scientifique, internet offre de nombreuses ressources qui sont très hétérogènes en qualité. Vous apprendrez à reconnaître les ressources fiables à celles qui sont douteuses. Les plateformes basées sur Stack Exchange, comme [Stack Overflow](https://stackoverflow.com) et [Cross Validated](https://stats.stackexchange.com), m'ont souvent été d'une aide précieuse. Vous aurez avantage à vous construire une petite banque d'information ([Turtl](https://turtlapp.com/), [Notion](https://www.notion.so/), [Evernote](https://evernote.com/), Google Keep, One Note, etc.) en collectant des liens, en prenant en notes certaines recettes et en suivant des sites d'intérêt avec des flux RSS.

## À propos de l'auteur
Je m'appelle Serge-Étienne Parent. Je suis ingénieur écologue et professeur adjoint au Département des sols et de génie agroalimentaire de l'Université Laval, Québec, Canada. Je crois que la science est le meilleur moyen d'appréhender le monde pour prendre des décisions avisées.

## Un cours complémentaire à d'autres cours

Ce cours a été développé pour ouvrir des perspectives mathématiques en écologie et en agronomie à la FSAA de l'Université Laval. Il est complémentaire à certains cours offerts dans d'autres institutions académiques au Québec, dont ceux-ci.

- [BIO2041. Biostatistiques 1](https://admission.umontreal.ca/cours-et-horaires/cours/bio-2041/), Université de Montréal
- [BIO2042. Biostatistiques 2](https://admission.umontreal.ca/cours-et-horaires/cours/BIO-2042/), Université de Montréal
- [BIO109. Introduction à la programmation scientifique](https://github.com/EcoNumUdS/BIO109), Université de Sherbrooke
- [BIO500. Méthodes en écologie computationnelle](https://github.com/EcoNumUdS/BIO500), Université de Sherbrooke.

## Contribuer au manuel

Je suis ouvert aux commentaires et suggestions. Pour contribuer directement, dirigez-vous sur le dépôt du manuel sur [GitHub](https://github.com), puis ouvrez une *Issue* pour en discuter. Créez une nouvelle branche (*fork*), effectuez les modifications, puis lancer une requête de fusion (*pull resquest*).

<!--chapter:end:index.Rmd-->

---
title: "La science des données avec R"
author: "Serge-Étienne Parent"
date: "`r format(Sys.Date())`"
output: github_document
---

# La science des données avec R {#chapitre-intro-a-R}

 ***
️\ **Objectifs spécifiques**:

À la fin de ce chapitre, vous

- saurez contextualiser la science des données par rapport aux statistiques,
- serez en mesure de vous lancer dans un environnement de programmation R,
- serez en mesure d'effectuer des opérations de base en R,
- saurez différencier les grands types d'objets de R et
- saurez installer et charger des modules complémentaire.

 ***

Un projet en science des données comprend trois grandes étapes. D'abord, vous devez **collecter des données** et vous les compilez adéquatement. Cela peut consister à télécharger des données existantes, exécuter un dispositif expérimental ou effectuer une recensement (étude observationnelle). Compiler les données dans un format qui puisse être importé est une tâche souvent longue et fastidieuse. Puis, vous **investiguez les données** collectées, c'est-à-dire vous les visualisez, vous appliquez des modèles et testez des hypothèses. Enfin, la **communication des résultats** consiste à présenter les connaissances qui émergent de votre analyse sous forme visuelle et narrative, *avec un langage adapté à la personne qui vous écoute*, qu'elle soit experte ou novice, réviseure de revue savante ou administratrice. [Grolemund et Wickham (2018)](http://r4ds.had.co.nz/introduction.html) propose la structure d'analyse suivante, avec de légères modifications de ma part.

![](images/02_science-des-donnees-flow_.png)

Le grand cadre spécifie **Programmer**. Oui, vous aurez besoin d'écrire du code. Mais comme je l'ai indiqué dans le premier chapitre, ceci n'est pas un cours de programmation et je préférerai les approches intuitives.

## Statistiques ou Science des données?

Selon [Whitlock et Schluter (2015)](http://whitlockschluter.zoology.ubc.ca/), la statistique est l'*étude des méthodes pour décrire et mesurer des aspects de la nature à partir d'échantillon*. Pour [Grolemund et Wickham (2018)](http://r4ds.had.co.nz/introduction.html), la science des données est *une discipline excitante permettant de transformer des données brutes en compréhension, perspectives et connaissances*. Oui, *excitante*! La différence entre les deux champs d'expertise est subtile, et certaines personnes n'y voient qu'une différence de ton.

<blockquote class="twitter-tweet" data-lang="fr"><p lang="en" dir="ltr">Data Science is statistics on a Mac.</p>&mdash; Big Data Borat (@BigDataBorat) <a href="https://twitter.com/BigDataBorat/status/372350993255518208?ref_src=twsrc%5Etfw">27 août 2013</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

Confinées à ses applications traditionnelles, les statistiques sont davantage vouées à la définition de dispositifs expérimentaux et à l'exécution de tests d'hypothèses, alors que la science des données est moins linéaire, en particulier dans sa phase d'analyse, où de nouvelles questions (donc de nouvelles hypothèses) peuvent être posées au fur et à mesure de l'analyse. Cela arrive généralement davantage lorsque l'on fait face à de nombreuses observations sur lesquelles ne nombreux paramètres sont mesurés.

La quantité de données et de mesures auxquelles nous avons aujourd'hui accès grâce aux technologies de mesure et de stockage relativement peu dispendieux rend la science des données une discipline particulièrement attrayante, pour ne pas dire [sexy](https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century).

## Organiser son environnement de travail en R

[R](https://www.r-project.org) est un langage de programmation dérivé du langage S, qui fut initialement lancé en 1976.

<img src="images/02_R_logo.svg.png">

R figure parmi [les langages de programmation les plus utilisés au monde](https://www.tiobe.com/tiobe-index/). Bien qu'il soit basé sur les langages statiques C et Fortran, R est un langage dynamique, c'est-à-dire que le code peut être exécuté ligne par ligne ou bloc par bloc: un avantage majeur pour des activités qui nécessitent des interactions fréquentes. Bien que R soit surtout utilisé pour le calcul statistique, il s'impose de plus en plus comme outil privilégié en sciences des données en raison des récents développements de modules d'analyse, de modélisation et de visualisation, dont plusieurs seront utilisés dans ce manuel.

## Préparer son flux de travail

Il existe de nombreuses manières d'utiliser R. Parmi celles-ci, j'en couvrirai 2:

- Installation classique
- Installation avec Anaconda

Pour l'instant, j'écarte l'option infonuagique, qui n'est pas tout à fait au point. Les services de [Azure Notebooks](https://notebooks.azure.com) et de [CoCalc](https://cocalc.com/) peuvent néanmoins s'avérer utiles... lorsqu'ils fonctionnent convenablement.

### Installation classique

Sur Windows ou Mac, dirigez-vous [ici](https://cloud.r-project.org/), téléchargez et installez. Sur Linux, ouvrez votre gestionnaire d'application, chercher `r-base` (Ubuntu, Debian), `R-base` (openSuse) ou `R-core` (Fedora) et installez-le (assurez-vous que les librairies suivantes sont aussi installées: `gcc`, `gcc-fortran`, `gcc-c++` et `make`), vous aurez peut-être besoin d'installer des librairies supplémentaires pour faire fonctionner certains modules.

> **Note**. Les modules présentés dans ce cours devraient être disponibles sur Linux, Windows et Mac. Ce n'est pas le cas pour tous les modules R. La plupart fonctionnent néanmoins sur Linux, dont les systèmes d'opération (je recommande [Ubuntu](https://www.ubuntu.com/download/desktop) ou l'une de ses dérivées) sont de bonnes options pour le calcul scientifique.

À cette étape, R devrait fonctionner dans un interpréteur de commande . Si vous lancez R dans un terminal (chercher `cmd` dans le menu si vous êtes sur Windows), vous obtiendrez quelque chose comme ceci.

<center><img src="images/02_terminal-prompt.png" width=800></center>

Le symbole `>` indique que R attend que vos instructions. Vous voilà dans un état méditatif devant l'indéchiffrable vide du terminal. Afin de travailler dans un environnement de travail plus convivial, je recommande l'installation de l'interface [RStudio](https://www.rstudio.com/products/rstudio/download/), gratuite et open source: téléchargez l'installateur et suivez les instructions. RStudio ressemble à ceci.

<center><img src="images/02_rstudio.png" width=800></center>

En haut à droite se trouve un menu *Project (None)*. Il s'agit d'un menu de vos projets. Je recommande d'utiliser ces projets avec RStudio, qui vous permettront de mieux gérer vos environnements de travail, en particulier en lien avec les chemins vers de vos données, graphiques, etc., que vous pouvez gérer relativement à l'emplacement de votre dossier de projet plutôt qu'à l'emplacement des fichiers sur votre machine.

- En haut à gauche, vous avez vos feuilles de calcul, qui apparaîtront en tant qu'onglets. Je recommande de prendre en main les *R notebooks*, dans lesquels vous pouvez écrire du texte en format [*Markdown*](https://github.com/adam-p/markdown-here/wiki/Markdown-Here-Cheatsheet) (dont il sera question plus loin) entre des blocs de code. Ceci vous permet de détailler votre flux de travail.
- En bas à gauche apparaît la Console, où vous voyez les commandes envoyées à R ainsi que ses sorties. Si vous travaillez en format *notebook*, vous n'en aurez probablement pas besoin.
- En haut à droite, les différents onglets indiquent où vous en êtes dans vos calculs. En particulier, la liste sous *Environment* indique les objets qui ont été générés jusqu'alors.
- En bas à droite, on retrouve des onglets de nature variés. *Files* contient les sous-dossiers et fichiers du dossier de projets. *Plots* est l'endroit où apparaîtront vos graphiques. *Packages* contient la liste des modules déjà installés, ainsi qu'un outil de gestion des modules pour leur installation, leur désinstallation et leur mise à jour. *Help* affiche les fiches d'aide des fonctions (pour obtenir de l'aide sur une fonction dans RStudio, surlignez la fonction dans votre feuille de calcul, puis appuyez sur `F1`). Enfin, l'onglet *Viewer* affichera les sorties HTML, en particulier les graphiques interactifs que vous générerez par exemple avec le module `plotly`.

### R notebooks

Les *R notebooks* offrent une approche de programmation littéraire, c'est-à-dire que vous écrivez votre code comme vous écrivez un article, une thèse ou une histoire. Cette approche permet de partager plus facilement vos codes, que ce soit avec une équipe de travail ou à la communauté scientifique pour accompagner un article scientifique en tant que matériel supplémentaire. Lorsque vous créez un notebook (*File > New file > R notebook*), les instructions de base apparaissent. Ajoutons que pour lancer du code ligne par ligne, vous pouvez surligner le code en question ou placez le curseur sur la ligne à exécuter, puis taper `Ctrl + Enter`. La sortie de R apparaîtra sous le bloc de code. Dans votre texte, vous pouvez ajouter des équations mathématiques en format *Mathjax* inspiré du format Latex, par exemple `$a = \sum_{i=1}^n x_i^2$` sera affiché comme $a = \sum_{i=1}^n x_i^2$ (pour aider dans l'édition d'équation, vous pouvez utiliser un [éditeur dans les nuages](http://www.sciweavers.org/free-online-latex-equation-editor)). Pour les titres, les caractères gras, l'insertion d'image, les hyperliens, les tableaux, etc., référez-vous à la documentation de [*Markdown*](https://github.com/adam-p/markdown-here/wiki/Markdown-Here-Cheatsheet).

Si votre environnement de travail était un avion, R serait le moteur et RStudio serait le cockpit!

![](https://media.giphy.com/media/GmaV9oet9MAmI/giphy.gif)

### Installation avec Anaconda

Si vous cherchez une trousse complète d'analyse de données, comprenant R et Python, vous pourrez préférer [Anaconda](https://www.anaconda.com/download/#linux). Une fois installée, vous pourrez isoler un environnement de travail sur R, ou même isoler des environnements de travail particuliers pour vos projets. Une manière conviviale de créer des environnements de travail est de passer par l'interface *Anaconda navigator*, que vous lancerez soit dans le menu Windows, soit en ligne de commande `anaconda-navigator` sous Mac et Linux, puis d'installer `r-essentials`, `rstudio` et `jupyterlab` dans l'onglet *Environment*. Vous pourrez aussi installer `RStudio` et `Jupyter lab` via l'onglet *Home* de `Anaconda navigator`. Dans l'environnement de base, installez le package `nb_conda_kernels` pour vous assurer que tous les noyaux (R, Python, etc.) installés dans les environnements de travail soient automatiquement accessibles dans Jupyter.

<center><img src="images/02_anaconda-navigator.png" width=800></center>

*Jupyter lab* est une interface notebook semblable à *R notebook*. À vrai dire, l'utilisation de R en Anaconda n'est pas tout à fait au point, et pourrait poser problème pour l'installation de certains modules. Si vous optez pour cette option, préparez-vous à avoir à bidouiller un peu.

## Premiers pas avec R

R ne fonctionne pas avec des menus, en faisant danser une souris sous une musique de clics. Vous devrez donc entrer des commandes avec votre clavier, que vous apprendrez par cœur au fur et à mesure, ou que vous retrouverez en lançant des recherches sur internet. Par expérience personnelle, lorsque je travaille avec R, j'ai toujours un navigateur ouvert prêt à recevoir une question.

Les étapes qui suivent sont des premiers pas. Elles ne feront pas de vous des *ceintures noires* delà programmation. La plupart des utilisateurs de R ont appris R en se pratiquant sur leurs données, en frappant des murs, en apprenant comment les escalader ou les contourner...

Pour l'instant, ouvrez seulement un interpréteur de commande, et lancez R. Voyons si R est aussi libre qu'on le prétend.

> "La liberté, c’est la liberté de dire que deux et deux font quatre. Si cela est accordé, tout le reste suit." - George Orwell, 1984

```{r}
2 + 2
```

Et voilà.

<img width="200" src="images/02_braveheart224.png">

Les opérations mathématiques sont effectuées telles que l'on devrait s'attendre.

```{r}
67.1 - 43.3

2 * 4

1 / 2
```

L'exposant peut être noté `^`, comme c'est le cas dans *Excel*, ou `**` comme c'est le cas en Python.

```{r}
2^4
```

```{r}
2**4
```

```{r}
1 / 2 # utilisez des espaces de part et d'autre des opérateurs (sauf pour l'exposant) pour éclaircir le code
```

R ne lit pas ce qui suit le caractère `#`. Cela vous laisse l'opportunité de commenter un code comprenant une séquence de plusieurs lignes. Remarquez également que la dernière opération comporte des espaces entre les nombres et l'opérateur `/`. Dans ce cas (ce n'est pas toujours le cas), les espaces ne signifient rien: ils aident seulement à éclaircir le code. Il existe des guides pour l'écriture de code en R. Je recommande le guide de style de [Hadley Wickahm](http://adv-r.had.co.nz/Style.html).

Assigner des objets à des variables est fondamental en programmation. En R, on assigne traditionnellement avec la flèche `<-`, mais vous verrez parfois le `=`, qui est davantage utilisé comme standard dans d'autres langages de programmation. Par exemple.

```{r}
a <- 3
```

Techniquement, `a` pointe vers le nombre entier 3. Conséquemment, on peut effectuer des opérations sur `a`.

```{r}
a * 6
```

```{r}
#A + 2
```

Le message d'erreur nous dit que `A` n'est pas défini. Sa version minuscule, `a`, l'est pourtant. La raison est que R considère la *case* dans la définition des objets. Utiliser la mauvaise case mène donc à des erreurs.

**Note**. Les messages d'erreur ne sont pas toujours clairs, mais vous apprendrez à les comprendre. Dans tous les cas, ils sont fait pour vous aider. Lisez-les attentivement!

En général, le nom d'une variable doit toujours commencer par une lettre, et ne doit pas contenir de caractères réservés (espaces, `+`, `*`). Dans la définition des variables, plusieurs utilisent des symboles `.` pour délimiter les mots, mais la barre de soulignement `_` est à préférer. En effet, dans d'autres langages de programmation comme Python, le `.` a une autre signification: son utilisation est à éviter autant que possible.

**Note**. À ce stade, vous serez probablement plus à l'aise de copier-coller ces commandes dans votre terminal.

```{r}
rendement_arbre <- 50 # pomme/arbre
nombre_arbre <- 300 # arbre
nombre_pomme <- rendement_arbre * nombre_arbre
nombre_pomme
```

Comme chez la plupart des langages de programmation, R respecte les conventions des [priorités des opérations mathéatiques](https://fr.wikipedia.org/wiki/Ordre_des_op%C3%A9rations).

```{r}
10 - 9^0.5 * 2
```

### Types de données

Jusqu'à maintenant, nous n'avons utilisé que des **nombres entiers** (*integer* ou `int`) et des **nombres réels** (*numeric* ou `float64`). R inclut d'autres types. La **chaîne de caractère** (*string* ou *character*) contient un ou plusieurs symboles. Elle est définie entre des doubles guillemets `" "` ou des apostrophes `' '`. Il n'existe pas de standard sur l'utilisation de l'un ou de l'autre, mais en règle générale, on utilise les apostrophes pour les expressions courtes, contenant un simple mot ou séquence de lettres, et les guillemets pour les phrases. Une raison pour cela: les guillemets sont utiles pour insérer des apostrophes dans une chaîne de caractère.

```{r}
a <- "L'ours"
b <- "polaire"
paste(a, b)
```

On *colle* `a` et `b` avec la fonction `paste`. Notez que l'objet `a` a été défini précédemment. Il est possible en R de réassigner une variable, mais cela peut porter à confusion, jusqu'à générer des erreurs de calcul si une variable n'est pas assignée à l'objet auquel on voulait référer.

Combien de caractères contient la chaîne `"L'ours polaire"`? R sait compter. Demandons-lui.

```{r}
c <- paste(a, b)
nchar(c)
```

Quatorze, c'est bien cela (comptez "L'ours polaire", en incluant l'espace). Comme `paste`, `nchar` est une fonction incluse par défaut dans l'environnement de travail de R: plus précisément, ces fonctions sont incluses dans le module `base`, inclut par défaut lorsque R est lancé. La fonction est appelée en écrivant `nchar()`. Mais une fonction de quoi? Des *arguments*, qui se trouvent entre les parenthèses. Dans ce cas, il y a un seul argument: `c`.

En calcul scientifique, il est courant de lancer des requêtes sur si un résultat est vrai ou faux.

```{r}
a <- 17
a < 10
a > 10
a == 10
a != 10
a == 17
!(a == 17)
```

Je viens d'introduire un nouveau type de donnée: les données booléennes (*boolean*, ou `logical`), qui ne peuvent prendre que deux états - `TRUE` ou `FALSE`. En même temps, j'ai utilisé la fonction `print` parce que dans mon carnet, seule la dernière opération permet d'afficher le résultat. Si l'on veut forcer une sortie, on utilise `print`. Puis, on a vu plus haut que le symbole `=` est réservé pour assigner des objets: pour les tests d'égalité, on utilise le double égal, `==`, ou `!=` pour la non-égalité. Enfin, pour inverser une donnée de type booléenne, on utilise le point d'exclamation `!`.

### Les collections de données

Les exercices précédents ont permis de présenter les types de données offerts par défaut sur R qui sont les plus importants pour le calcul scientifique: `int` (*integer*, ou nombre entier), `numeric` (nombre réel), `character` (*string*, ou chaîne de caractère) et `logical` (booléen). D'autres s'ajouteront tout au long du cours, comme les catégories (`factor`) et les unités de temps (date-heure).

Lorsque l'on procède à des opérations de calcul en science, nous utilisons rarement des valeurs uniques. Nous préférons les organiser et les traiter en collections. Par défaut, R offre quatre types importants de collections: les **vecteurs**, les **matrices**, les **listes** et les **tableaux**.

#### Vecteurs

D'abord, les **vecteurs** sont une série de variables de même type. Un vecteur est délimité par la fonction `c( )` (`c` pour **c**oncaténation). Les éléments de la liste sont séparés par des virgules.

```{r}
espece <- c('Petromyzon marinus', 'Lepisosteus osseus', 'Amia calva', 'Hiodon tergisus')
espece
```

Pour accéder aux éléments d'une liste, appelle la liste suivie de la position de l'objet désiré entre crochets.

```{r}
espece[1]
espece[2]
espece[1:3]
espece[c(1, 3)]
```

On peut noter que le premier élément de la liste est noté `1`, et non `0` comme c'est le cas de la plupart de langages. Le raccourcis `1:3` crée une liste de nombres entiers de `1` à `3` inclusivement, c'est-à-dire l'équivalent de `c(1, 2, 3)`. En effet, on crée une liste d'indices pour soutirer des éléments d'une liste. On peut utiliser le symbole de soustraction pour retirer un ou plusieurs éléments d'un vecteur.

```{r}
espece[-c(1, 3)]
```

Pour ajouter un élément à notre liste, on peut utiliser la fonction `c( )`.

```{r}
espece <- c(espece, "Cyprinus carpio")
espece
```

Notez que l'on efface l'objet `espece` par une concaténation de l'objet `espece`, précédemment définie, et d'un autre élément.

En lançant `espece[3] <- "Lepomis gibbosus"`, il est possible de changer un élément de la liste.

```{r}
espece[3] <- "Lepomis gibbosus"
espece
```

#### Matrices

Une **matrice** est un vecteur de dimension plus élevée que 1. En écologie, on dépasse rarement la deuxième dimension, quoi que les matrices en `N` dimensions soient courantes en modélisation mathématique. Je ne considérerai pour le moment que des matrices `2D`. Comme c'est la cas des vecteurs, les matrices contiennent des valeurs de même type. En R, on peut attribuer aux matrices `2D` des noms de ligne et de colonne.

```{r}
mat <- matrix(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12), ncol=3)
mat
```

```{r}
colnames(mat) <- c('A', 'B', 'C')
rownames(mat) <- c('site_1', 'site_2', 'site_3', 'site_4')
mat
```

On peut soutirer les noms de colonne et les noms de ligne. Le résultat est un vecteur.

```{r}
colnames(mat)
rownames(mat)
```

#### Listes

Les **listes** sont des collections hétérogènes dans lesquelles on peut placer les objets désirés, sans distinction: elles peuvent même inclure d'autres listes. Chacun des éléments de la liste peut être identifié par une clé.

```{r}
ma_liste <- list(especes = c('Petromyzon marinus', 'Lepisosteus osseus',
                             'Amia calva', 'Hiodon tergisus'),
                 site = 'A101',
                stations_meteos = c('746583', '783786', '856363'))
ma_liste
```

Les éléments de la liste peuvent être soutirés par le nom de la clé ou par l'indice, de cette manière.

```{r}
ma_liste$especes
ma_liste[[1]]
```

**Exercice**. Accéder au deuxième élément du vecteur d'espèces dans la liste ma_liste.

#### Tableaux

Enfin, le type de collection de données le plus important est le tableau, ou `data.frame`. Techniquement, il s'agit d'une liste composée de vecteurs de même longueur. Chaque colonne peut ainsi prendre un type de donnée indépendamment des autres colonnes.

```{r}
tableau <- data.frame(espece = c('Petromyzon marinus', 'Lepisosteus osseus',
                                'Amia calva', 'Hiodon tergisus'),
                     poids = c(10, 13, 21, 4),
                     longueur = c(35, 44, 50, 8))
tableau
```

En programmation classique en R (nous verrons plus loin la méthode `tidyverse`), les éléments d'un tableau se manipulent comme ceux d'une matrice et les colonnes peuvent être appelés comme les éléments d'une liste.

```{r}
tableau[, 2:3]
tableau$poids
```

Vous verrez aussi, quoi que rarement, ce format, qui à la différence du format `$` génère un tableau.

```{r}
tableau['poids']
```

Le tableau est le format de collection à privilégier pour manipuler des données. Récemment, le format de tableau `tibble` a été créé par l'équipe de RStudio pour offrir un format plus moderne.

### Les fonctions

Lorsque vous écrivez une commande suivit de parenthèses, comme `data.frame(especes = ...)`, vous demandez à R de passer à l'action en appelant une fonction. De manière très générale, une fonction transforme quelque chose en quelque chose d'autre.

![](images/02_fonctions_io_.png)

Par exemple, la fonction `mean()` prend une collection de nombre comme entrée, puis en sort vous devinez quoi.

```{r}
mean(tableau$poids)
```

Les entrées sont appelés les **arguments** de la fonction. Leur définition est toujours disponible dans la documentation.

**Exercice**. Familiarisez-vous avec la documentation de R en lançant `?mean`. Truc: si vous avez pris de l'avance et que vous travaillez déjà en RStudio, mettez le terme en surbrillance, puis appuyez sur F1.

Vous verrez dans la documentation que la fonction `mean()` demande trois arguments, `x`, `trim` et `na.rm`. Or nous avons seulement placé un vecteur, sans spécifier d'argument!

En effet. En l'absence d'une définition des arguments, R supposera que les arguments dans la parenthèse, séparés par une virgule, sont présentés dans le même ordre que celui spécifié dans la définition de la fonction (celle qui est présentée dans le fichier d'aide). Dans le cas qui nous intéresse, `mean(tableau$poids)` est équivalent à `mean(x = tableau$poids)`.

Maintenant, selon la fiche d'aire l'argument `na.rm` est un valeur logique spécifiant si oui (`TRUE`) ou non (`FALSE`) les valeurs manquantes doivent être considérées (une moyenne d'un vecteur comprenant au moins un `NA` sera de `NA`). En ne spécifiant rien, R prend la valeur par défaut, telle que spécifiée dans la documentation. Il en va de même que l'argument `trim`, qui permet d'élaguer des valeurs extrêmes. Dans la fiche d'aide,  `mean(x, trim = 0, na.rm = FALSE, ...)` signifie que par défaut, l'argument `x` est vide (il doit donc être spécifié), l'argument `trim` est de 0 et l'argument `na.rm` est `FALSE`.

```{r}
mean(c(6, 1, 7, 4, 9, NA, 1))
mean(c(6, 1, 7, 4, 9, NA, 1), na.rm = TRUE)
```

Vous n'êtes pas emprisonné par les fonctions offertes par R. Vous pouvez installer des modules qui complètent les fonctions de base de R: on le verra un peu plus loin dans ce chapitre. Mais pour l'instant, voyons comment vous pouvez créer vos propres fonctions. Disons que vous voulez créer une fonction qui calcule la sortie de $x^3-2y+a$. Pour obtenir la réponse on a besoin des arguments `x`, `y` et `a`. La sortie de la fonction est ici triviale: la réponse de l'équation. L'opération `function` permet de prendre ça en charge.

```{r}
operation_f <- function(x, y, a = 10) {
    return(x^3-2*y+a)
}
```

Notez que `a` a une valeur par défaut. La sortie de la fonction est ce qui se trouve entre les parenthèses de `return`. Vous pouvez maintenant utiliser la fonction operation_nl au besoin.

```{r}
operation_f(x = 2, y = 3, a = 1)
```

Une telle fonction est peu utile. Mais l'utilisation de fonctions personnalisées vous permettra d'éviter de répéter la même opération plusieurs fois dans un flux de travail, en évitant de générer trop de code, donc aussi de potentielles erreurs. Personnellement, j'utilise les fonctions surtout pour générer des graphiques personnalisés.

**Exercice**. Afin d'acquérir de l'autonomie, vous devrez être en mesure de trouver le nom des commandes dont vous avez besoin pour effectuer la tâche que vous désirer effectuer. Cela peut causer des frustrations, mais vous vous sentirez toujours plus à l'aise avec R jour après jour. L'exercice ici est de trouver par vous-même la commande qui vous permettra mesurer la longueur d'un vecteur.

### Les boucles

Les boucles permettent d'effectuer une même suite d'opérations sur plusieurs objets. Pour faire suite à notre exemple, nous désirons obtenir le résultat de l'opération *f* pour des paramètres que nous enregistrons dans ce tableau.

```{r}
params <- data.frame(x = c(2, 4, 1, 5, 6),
                     y = c(3, 4, 8, 1, 0),
                     a = c(6, 1, 8, 2, 5))
params
```

Nous créons un vecteur vide, puis nous itérons ligne par ligne en remplissant le vecteur.

```{r}
operation_res <- c()
for (i in 1:nrow(params)) {
    operation_res[i] <- operation_f(x = params[i, 1], y = params[i, 2], a = params[i, 3])
}
operation_res
```

En faisant varier `i` sur des valeurs du vecteur donné par la séquence de nombre entiers de 1 au nombre de ligne du tableau de paramètres, nous demandons à R d'effectuer la suite d'opération entre les accolades `{}`. À chaque boucle, `i` prend une valeur de la séquence. `i` est utilisé ici comme indice de la ligne à soutirer du tableau `params`, qui correspond à l'indice dans le vecteur operation_res.

Ainsi, chaque résultat est calculé dans l'ordre des lignes du tableau de paramètres et l'on pourra très bien y coller nos résultats:

```{r}
params$resultats <- operation_res
params
```

Notez que puisque la colonne `resultat` n'existe pas dans le tableau `params`, R crée automatiquement une nouvelle colonne.

Les boucles `for` vous permettront par exemple de générer en peu de temps 10, 100, 1000 graphiques (autant que vous voulez), chacun issu de simulations obtenues à partir de conditions initiales différentes, et de les enregistrer dans un répertoire sur votre ordinateur. Un travail qui pourrait prendre des semaines sur Excel peut être effectué en R en quelques secondes.

Un second outil est disponible pour les itérations: les boucles **`while`**. Elles effectue une opération tant qu'un critère n'est pas atteint. Elles sont utiles pour les opérations où l'on cherche une convergence. Je les couvre rapidement puisqu’elles sont rarement utilisées dans les flux de travail courants. En voici un petit exemple.

```{r}
x <- 100
while (x > 1.1) {
    x <- sqrt(x)
    print(x)
}
```

Nous avons initié x à une valeur de 100. Puis, tant que (`while`) le test `x > 1.1` est vrai, attribuer à `x` la nouvelle valeur calculée en extrayant la racine de la valeur précédente de `x`. Enfin, indiquer la valeur avec `print`.

### Conditions: if, else if, else

> Si la condition 1 est remplie, effectuer une suite d'instruction 1. Si la condition 1 n'est pas remplie, et si la condition 2 est remplie, effectuer la suite d'instruction 2. Sinon, effectuer la suite d'instruction 3.

Voilà comment on exprime une suite de conditions. Prenons l'exemple simple d'une discrétisation d'une valeur continue. Si $x<10$, il est classé comme faible. Si $10 \leq x <20$, il est classé comme moyen. Si $x \geq 20$, il est classé comme élevé. Plaçons cette classification dans une fonction.

```{r}
classification <- function(x, lim1=10, lim2=20)  {
    if (x < lim1) {
        categorie <- "faible"
    } else if (x < lim2) {
        categorie <- "moyen"
    } else {
        categorie <- "élevé"
    }
    return(categorie)
}
classification(-10)
classification(15.4)
classification(1000)
```

Une condition est définie avec le `if`, suivi du test à vrai ou faux entre parenthèses. Si le test retourne un *vrai* (`TRUE`), l'instruction entre accolades est exécutée. Si elle est fausse, on passe au suivant.

**Exercice**. Explorer les commandes `ifelse` et `cut` et réfléchissez à la manière qu'elles pourraient être utilisées pour effectuer une discrétisation plus efficacement qu'avec les `if` et les `else`.

### Installer et charger un module

La plupart des opérations d'ordre général (comme les racines carrées, les tests statistiques, la gestion de matrices et de tableau, les graphiques, etc.) sont accessibles grâce aux modules de base de R, qui sont installés et chargés par défaut lors du démarrage de R. Des équipes de travail ont néanmoins développé plusieurs modules pour répondre à leurs besoins spécialisés, et les ont laissées disponibles au grand public dans des modules que vous pouvez installer d'un dépôt CRAN (le AppStore de R), d'un dépôt Anaconda (le AppStore de Anaconda, si vous utilisez cette plate-forme), d'un dépôt Github (dépôts décentralisés), etc.

RStudio possède un pratique bouton *Install* qui vous permet d'y inscrire une liste de modules. Le navigateur anaconda offre aussi une interface d'installation. La commande R pour installer un module est `install.packages("ggplot2")`, si par exemple vous désirez installer `ggplot2`, le module graphique par excellence en R. C'est la commande que RStudio lancera tout seul si vous lui demandez d'installer `ggplot2`.

Les modules sont l'équivalent des applications spécialisées que vous installez sur un téléphone mobile. Pour les utiliser, il faut les ouvrir.

Généralement, j'ouvre toutes les applications nécessaires à mon flux de travail au tout début de ma feuille de calcul (la prochaine cellule retournera un message d'erreur si les packages ne sont pas installés).

```{r}
library("tidyverse") # méta-package qui charge entre autres dplyr et ggplot2
library("vegan")
library("nlme")
```

Les modules sont installés sur votre ordinateur à un endroit que vous pourrez retrouver avec la commande `.libPaths()`

**Exercice**. À partir d'ici jusqu'à la fin du cours, nous utiliserons RStudio. Ouvrez-le et familiarisez-vous avec l'interface! Quelques petits trucs:

- pour lancer une ligne, placez votre curseur sur la ligne, puis appuyez sur Ctrl+Enter
- pour lancer une partie de code précise, mettez le en surbrillance, puis Ctrl+Enter
- utilisez toujours le gestionnaire de projets, en haut à droite!
- installez le module `tidyverse`
- lancez data(iris) pour obtenir un tableau d'exercice, puis cliquez sur l'objet dans la fenêtre environnement
- essayer R notebook

## Enfin...

Comme une langue, on n'apprend à s'exprimer en un langage informatique qu'en se mettant à l'épreuve, ce que vous ferez tout au long de ce cours. Pour vous encourager, voici quelques trucs pour apprendre à coder en R.

- **R n'aime pas l’ambiguïté**. Une simple virgule mal placée et il ne sait plus quoi faire. Cela peut être frustrant au début, mais cette rigidité est nécessaire pour effectuer du calcul scientifique.
- **Le copier-coller est votre ami**. En gardant à l'esprit que vous être responsable de votre code et que vous respectez les droits d'auteur, n'ayez pas peur de copier-coller des lignes de code et de personnaliser par la suite.
- **L'erreur que vous obtenez: d'autres l'ont obtenue avant vous**. Le site de question-réponse [stackoverflow](https://stackoverflow.com/questions/tagged/r) est une ressource inestimable où des gens ayant posté des questions ont reçu des réponses d'experts (les meilleures réponses et les meilleures questions apparaissent en premier). Apprenez à chercher intelligemment des réponses en formulant précisément vos questions!
- **Étudiez et pratiquez**. Les messages d'erreur en R sont courants, même chez les personnes expérimentées. La meilleure manière d'apprendre une langue est de la parler, d'étudier ses susceptibilités, de les tester dans une conversation, etc.

<!--chapter:end:02_R.Rmd-->

---
title: "Organisation des données et opérations sur des tableaux"
author: "Serge-Étienne Parent"
date: "`r format(Sys.Date())`"
output: github_document
---

# Organisation des données et opérations sur des tableaux {#chapitre-tableaux}

 ***
️\ **Objectifs spécifiques**:

À la fin de ce chapitre, vous

- comprendrez les règles guidant la création et la gestion des tableaux,
- saurez importer et exporter des données et
- saurez effectuer des opérations en cascade avec le module tidyverse, dont
  - des filtres sur les lignes,
  - des sélections de colonnes,
  - des sommaires statistiques et
  - des jointures entre tableaux.
  
 ***

Les données sont utilisées à chaque étape dans les flux de travail en sciences. Elles alimentent l'analyse et la modélisation. Les résultats qui en découlent sont aussi des données qui peuvent alimenter les travaux subséquents. Une bonne organisation des données facilitera le flux de travail.

> **Dicton**. Proportions de temps voué aux calcul scientifique: 80% de nettoyage de données mal organisées, 20% de calcul.

Qu'est-ce qu'une donnée? De manière abstraite, il s'agit d'une valeur associée à une variable. Une variable peut être une dimension, une date, une couleur, le résultat d'un test statistique, à laquelle on attribue la valeur quantitative ou qualitative d'un chiffre, d'une chaîne de caractère, d'un symbole conventionné, etc. Par exemple, lorsque vous commandez un café *latte* végane, *au latte* est la valeur que vous attribuez à la variable *type de café*, et *végane* est la valeur de la variable *type de lait*.

L'exemple est peut être horrible. J'ai besoin d'un café...

![](https://media.giphy.com/media/3nbxypT20Ulmo/giphy.gif)

Ce chapitre traite de l'importation, l'utilisation et l'exportation de données structurées, en R, sous forme de vecteurs, matrices, tableaux et ensemble de tableaux (bases de données).

Bien qu'il soit toujours préférable d'organiser les structures qui accueilleront les données d'une expérience avant-même de procéder à la collecte de données, l'analyste doit s'attendre à réorganiser ses données en cours de route. Or, des données bien organisées au départ faciliteront aussi leur réorganisation.

Ce chapitre débute avec quelques définitions: les données, les matrices, les tableaux et les bases de données, ainsi que leur signification en R. Puis nous verrons comment organiser un tableau selon quelques règles simples, mais importantes pour éviter les erreurs et les opérations fastidieuses pour reconstruire un tableau mal conçu. Ensuite, nous traiterons des formats de tableau courant, pour enfin passer à l'utilisation de [`dplyr`](http://pandas.pydata.org), le module tidyverse pour effectuer des opérations sur les tableaux.

## Les collections de données

Dans le chapitre \@ref(chapitre-intro-a-R), nous avons survolé différents types d'objets: réels, entiers, chaînes de caractères et booléens. Les données peuvent appartenir à d'autres types: dates, catégories ordinales (ordonnées: faible, moyen, élevé) et nominales (non ordonnées: espèces, cultivars, couleurs, unité pédologique, etc.). Comme mentionné en début de chapitre, une donnée est une valeur associée à une variable. Les données peuvent être organisées en collections.

Nous avons aussi vu au chapitre \@ref(chapitre-intro-a-R) que la manière privilégiée d'organiser des données était sous forme de **tableau**. De manière générale, un tableau de données est une organisation de données en deux dimensions, comportant des *lignes* et des *colonnes*. Il est préférable de respecter la convention selon laquelle **les lignes sont des observations et les colonnes sont des variables**. Ainsi, un tableau est une collection de vecteurs de même longueur, chaque vecteur représentant une variable. Chaque variable est libre de prendre le type de données approprié. La position d'une donnée dans le vecteur correspond à une observation.

Imaginez que vous consignez des données de différents sites (A, B et C), et que chaque site possède ses propres caractéristiques. Il est redondant de décrire le site pour chaque observation. Vous préférerez créer deux tableaux: un pour décrire vos observations, et un autre pour décrire les sites. De cette manière, vous créez une collection de tableaux intereliés: une **base de données**. R peut soutirer des données des bases de données grâce au module DBI, qui n'est pas couvert à ce stade de développement du cours.

Dans R, les données structurées en tableaux, ainsi que les opérations sur les tableaux, peuvent être gérés grâce aux modules readr, dplyr et tidyr, tous des modules faisant partie du méta-module *tidyverse*, devenu incontoutnable. Mais avant de se lancer dans l'utilisation de ces modules, voyons quelques règles à suivre pour bien structurer ses données en format *tidy*, un jargon du *tidyverse* qui signifie *proprement organisé*.


## Organiser un tableau de données

Afin de repérer chaque cellule d'un tableau, on attribue à chaque lignes et à chaque colonne colonnes un identifiant *unique*, que l'on nomme *indice* pour les lignes et *entête* pour les colonnes.

> **Règle no 1.** Une variable par colonne, une observation par ligne, une valeur par cellule.

Les unités expérimentales sont décrits par une ou plusieurs variables par des chiffres ou des lettres. Chaque variable devrait être présente en une seule colonne, et chaque ligne devrait correspondre à une unité expérimentale où ces variables ont été mesurées. La règle parait simple, mais elle est rarement respectée. Prenez par exemple le tableau suivant.

| Site | Traitement A | Traitement B | Traitement C |
| --- | --- | --- | --- |
| Sainte-Souris | 4.1 | 8.2 | 6.8 |
| Sainte-Fourmi | 5.8 | 5.9 | NA |
| Saint-Ours | 2.9 | 3.4 | 4.6 |

*Tableau 1. Rendements obtenus sur les sites expérimentaux selon les traitements.*

Qu'est-ce qui cloche avec ce tableau? Chaque ligne est une observation, mais contient plusieurs observations d'une même variable, le rendement, qui devient étalé sur plusieurs colonnes. *À bien y penser*, le type de traitement est une variable et le rendement en est une autre:

| Site | Traitement | Rendement |
| --- | --- | --- |
| Sainte-Souris | A | 4.1 |
| Sainte-Souris | B | 8.2 |
| Sainte-Souris | C | 6.8 |
| Sainte-Fourmi | A | 5.8 |
| Sainte-Fourmi | B | 5.9 |
| Sainte-Fourmi | C | NA |
| Saint-Ours | A | 2.9 |
| Saint-Ours | B | 3.4 |
| Saint-Ours | C | 4.6 |

*Tableau 2. Rendements obtenus sur les sites expérimentaux selon les traitements.*

Plus précisément, l'expression *à bien y penser* suggère une réflexion sur la signification des données. Certaines variables peuvent parfois être intégrées dans une même colonne, parfois pas. Par exemple, les concentrations en cuivre, zinc et plomb dans un sol contaminé peuvent être placés dans la même colonne "Concentration" ou déclinées en plusieurs colonnes Cu, Zn et Pb. La première version trouvera son utilité pour des créer des graphiques (chapitre 3), alors que la deuxième favorise le traitement statistique (chapitre 5). Il est possible de passer d'un format à l'autre grâce à la fonction `gather()` et `spread()` du module tidyr.

> **Règle no 2.** Un tableau par unité observationnelle: ne pas répéter les informations.

Reprenons la même expérience. Supposons que vous mesurez la précipitation à l'échelle du site.

| Site | Traitement | Rendement | Précipitations |
| --- | --- | --- | --- |
| Sainte-Souris | A | 4.1 | 813 |
| Sainte-Souris | B | 8.2 | 813 |
| Sainte-Souris | C | 6.8 | 813 |
| Sainte-Fourmi | A | 5.8 | 642 |
| Sainte-Fourmi | B | 5.9 | 642 |
| Sainte-Fourmi | C | NA | 642 |
| Saint-Ours | A | 2.9 | 1028 |
| Saint-Ours | B | 3.4 | 1028 |
| Saint-Ours | C | 4.6 | 1028 |

*Tableau 3. Rendements obtenus sur les sites expérimentaux selon les traitements.*

Segmenter l'information en deux tableaux serait préférable.

| Site | Précipitations |
| --- | --- |
| Sainte-Souris | 813 |
| Sainte-Fourmi | 642 |
| Saint-Ours | 1028 |

*Tableau 4. Précipitations sur les sites expérimentaux.*

Les tableaux 2 et 4, ensemble, forment une base de données (collection organisée de tableaux). Les opérations de fusion entre les tableaux peuvent être effectuées grâce aux fonctions de jointure (`left_join()`, par exemple) du module tidyr.

> **Règle no 3.** Ne pas bousiller les données.

Par exemple.

- *Ajouter des commentaires dans des cellules*. Si une cellule mérite d'être commentée, il est préférable de placer les commentaires soit dans un fichier décrivant le tableau de données, soit dans une colonne de commentaire juxtaposée à la colonne de la variable à commenter. Par exemple, si vous n'avez pas mesure le pH pour une observation, n'écrivez pas "échantillon contaminé" dans la cellule, mais annoter dans un fichier d'explication que l'échantillon no X a été contaminé. Si les commentaires sont systématique, il peut être pratique de les inscrire dans une colonne `commentaire_pH`.
- *Inscription non systématiques*. Il arrive souvent que des catégories d'une variable ou que des valeurs manquantes soient annotées différemment. Il arrive même que le séparateur décimal soit non systématique, parfois noté par un point, parfois par une virgule. Par exemple, une fois importés dans votre session, les catégories `St-Ours` et `Saint-Ours` seront traitées comme deux catégories distinctes. De même, les cellules correspondant à des valeurs manquantes ne devraient pas être inscrite parfois avec une cellule vide, parfois avec un point, parfois avec un tiret ou avec la mention `NA`. Le plus simple est de laisser systématiquement ces cellules vides.
- *Inclure des notes dans un tableau*. La règle "une colonne, une variable" n'est pas respectée si on ajoute des notes un peu n'importe où sous ou à côté du tableau.
- *Ajouter des sommaires*. Si vous ajoutez une ligne sous un tableau comprenant la moyenne de chaque colonne, qu'est-ce qui arrivera lorsque vous importerez votre tableau dans votre session de travail? La ligne sera considérée comme une observation supplémentaire.
- *Inclure une hiérarchie dans le entêtes*. Afin de consigner des données de texture du sol, comprenant la proportion de sable, de limon et d'argile, vous organisez votre entête en plusieurs lignes. Une ligne pour la catégorie de donnée, *Texture*, fusionnée sur trois colonnes, puis trois colonnes intitulées *Sable*, *Limon* et *Argile*. Votre tableau est joli, mais il ne pourra pas être importé conformément dans un votre session de calcul: on recherche *une entête unique par colonne*. Votre tableau de données devrait plutôt porter les entêtes *Texture sable*, *Texture limon* et *Texture argile*. Un conseil: réserver le travail esthétique à la toute fin d'un flux de travail.

## Formats de tableau

Plusieurs outils sont à votre disposition pour créer des tableaux. Je vous présente ici les plus communs.

### *xls* ou *xlsx*
Microsoft Excel est un logiciel de type *tableur*, ou chiffrier électronique. L'ancien format *xls* a été remplacé par le format *xlsx* avec l'arrivée de Microsoft Office 2010. Il s'agit d'un format propriétaire, dont l'alternative libre la plus connue est le format *ods*, popularisé par la suite bureautique LibreOffice. Les formats *xls*, *xlsx* ou *ods* sont davantage utilisés comme outils de calcul que d'entreposage de données. Ils contiennent des formules, des graphiques, du formatage de cellule, etc. *Je ne les recommande pas pour stocker des données*.

### *csv*
Le format *csv*, pour *comma separated values*, est un fichier texte, que vous pouvez ouvrir avec n'importe quel éditeur de texte brut (Bloc note, [Atom](https://atom.io), [Notepad++](https://notepad-plus-plus.org), etc.). Chaque colonne doit être délimitée par un caractère cohérent (conventionnellement une virgule, mais en français un point-virgule ou une tabulation pour éviter la confusion avec le séparateur décimal) et chaque ligne du tableau est un retour de ligne. Il est possible d'ouvrir et d'éditer les fichiers csv dans un éditeur texte, mais il est plus pratique de les ouvrir avec des tableurs (LibreOffice Calc, Microsoft Excel, Google Sheets, etc.).

**Encodage des fichiers texte**. Puisque le format *csv* est un fichier texte, un souci particulier doit être porté sur la manière dont le texte est encodé. Les caractères accentués pourrait être importer incorrectement si vous importez votre tableau en spécifiant le mauvais encodage. Pour les fichiers en langues occidentales, l'encodage UTF-8 devrait être utilisé. Toutefois, par défaut, Excel utilise un encodage de Microsoft. Si le *csv* a été généré par Excel, il est préférable de l'ouvrir avec votre éditeur texte et de l'enregistrer dans l'encodage UTF-8.

### *json*
Comme le format *csv*, le format *json* indique un fichier en texte clair. Il est utilisé davantage pour le partage de données des applications web. En analyse et modélisation, ce format est surtout utilisé pour les données géoréférencées. L'encodage est géré de la même manière qu'un fichier *csv*.

### SQLite
SQLite est une application pour les bases de données relationnelles de type SQL qui n'a pas besoin de serveur pour fonctionner. Les bases de données SQLite sont encodés dans des fichiers portant l'extension *db*, qui peuvent être facilement partagés.

### Suggestion
En *csv* pour les petits tableaux, en *sqlite* pour les bases de données plus complexes. Ce cours se concentre toutefois sur les données de type *csv*.

## Entreposer ses données

La manière la plus sécurisée pour entreposer ses données est de les confiner dans une base de données sécurisée sur un serveur sécurisé dans un environnement sécurisé et d'encrypter les communications. C'est aussi la manière la moins accessible. Des espaces de stockage nuagiques, comme Dropbox ou d'autres [options similaires](https://alternativeto.net/software/dropbox/), peuvent être pratiques pour les backups et le partage des données avec une équipe de travail (qui risque en retour de bousiller vos données). Le suivi de version est possible chez certains fournisseurs d'espace de stockage. Mais pour un suivi de version plus rigoureux, les espaces de développement (comme GitHub et GitLab) sont plus appropriés (couverts au chapitre \@ref(chapitre-git)). Dans tous les cas, il est important de garder (1) des copies anciennes pour y revenir en cas d'erreurs et (2) un petit fichier décrivant les changements effectués sur les données.

## Manipuler des données en mode tidyverse

Le méta-module tidyverse regroupe une collection de précieux modules  pour l'analyse de données en R. Il permet d'importer des données dans votre session de travail avec readr, de les explorer avec le module de visualisation ggplot2, de les transformer avec tidyr et dplyr et de les exporter avec readr. Les tableaux de classe *data.frame*, comme ceux de la plus moderne classe *tibble*, peuvent être manipulés à travers le flux de travail pour l'analyse et la modélisation (chapitres suivants). Comme c'était le cas pour le chapitre sur la visualisation, ce chapitre est loin de couvrir les nombreuses fonctionnalités qui sont offertes dans le tidyverse.

### Importer vos données dans voter session de travail

Supposons que vous avec bien organisé vos données en mode *tidy*. Pour les importer dans votre session et commencer à les inspecter, vous lancerez une des commandes du module readr, décrites dans la documentation dédiée.

- ` read_csv()` si le séparateur de colonne est une virgule
- ` read_csv2()` si le séparateur de colonne est un point-virgule et que le séparateur décimal est une virgule
- ` read_tsv()` si le séparateur de colonne est une tabulation
- ` read_table()` si le séparateur de colonne est un espace blanc
- ` read_delim()` si le séparateur de colonne est un autre caractère (comme le point-virgule) que vous spécifierez dans l'argument `delim = ";"`

Les principaux arguments sont les suivants.

- `file`: le chemin vers le fichier. Ce chemin peut aussi bien être une adresse locale (data/...) qu'une adresse internet (https://...).
- `delim`: le symbole délimitant les colonnes dans le cas de `read_delim`.
- `col_names`: si TRUE, la première ligne est l'entête du tableau, sinon FALSE. Si vous spécifiez un vecteur numérique, ce sont les numéros des lignes utilisées pour le nom de l'entête. Si vous utilisez un vecteur de caractères, ce sont les noms des colonnes que vous désirez donner à votre tableau.
- `na`: le symbole spécifiant une valeur manquante. L'argument `na=''` signifie que les cellules vides sont des données manquantes. Si les valeurs manquantes ne sont pas uniformes, vous pouvez les indiquer dans un vecteur, par exemple `na = c("", "NA", "NaN", ".", "-")`.
- `local`: cet argument prend une fonction `local()` qui peut inclure des arguments de format de temps, mais aussi d'encodage ([voir documentation](https://readr.tidyverse.org/reference/locale.html))

D'autres arguments peuvent être spécifiés au besoin, et les répéter ici dupliquerait l'information de la documentation de [la fonction `read_csv` de readr](https://readr.tidyverse.org/reference/read_delim.html).

Je déconseille d'importer des données en format xls ou xlsx. Si toutefois cela vous convient, je vous réfère au module [readxl](https://readxl.tidyverse.org/).

L'[aide-mémoire de readr](https://github.com/rstudio/cheatsheets/raw/master/data-import.pdf) est à afficher près de soi.

[![](https://www.rstudio.com/wp-content/uploads/2015/01/data-import-cheatsheet-1-600x464.png)](https://github.com/rstudio/cheatsheets/raw/master/data-import.pdf)
Aide-mémoire de readr, source: https://www.rstudio.com/resources/cheatsheets/

Nous allons charger des données de culture de la chicouté (*Rubus chamaemorus*), un petit fruit nordique, tiré de Parent et al. (2013). Ouvrons d'abord le fichier pour vérifier les séparateurs de colonne et de décimale.

![](images/04_chicoute-csv-atom.png)

Le séparateur de colonne est un point-virgule et le décimal est une virgule.

Avec [Atom](https://atom.io/), mon éditeur texte préféré, je vais dans Edit > Select Encoding et j'obtiens bien le UTF-8.

![](images/04_chicoute-csv-encoding.png)

Nous allons donc utiliser `read_csv2()` avec ses arguments par défaut.

```{r}
library("tidyverse")
chicoute <- read_csv2('data/chicoute.csv')
```

Quelques commandes utiles inspecter le tableau:

- `head()` présente l'entête du tableau, soit ses 6 premières lignes
- `str()` et `glimpse()` présentent les variables du tableau et leur type - `glimpse()`est la fonction tidyverse et `str()` est la fonction classique (je préfère `str()`)
- `summary()` présente des statistiques de base du tableau
- `names()` ou `colnames()` sort les noms des colonnes sous forme d'un vecteur
- `dim()` donne les dimensions du tableau, `ncol()` son nombre de colonnes et `nrow()` son nombre de lignes
- `skim` est une fonction du module skimr montrant un portrait graphique et numérique du tableau

**Extra 1**. Plusieurs modules ne se trouvent pas dans les dépôt CRAN, mais sont disponibles sur GitHub. Pour les installer, installez d'abord le module devtools disponible sur CRAN. Vous pourrez alors installer les packages de GitHub comme on le fait avec le package skimr.

**Extra 2**. Lorsque je désire utiliser une fonction, mais sans charger le module dans la session, j'utilise la notation `module::fonction`. Comme dans ce cas, pour skimr.

```{r}
# devtools::install_github("ropenscilabs/skimr")
skimr::skim(chicoute)
```

**Exercice**. Inspectez le tableau.

### Comment sélectionner et filtrer des données?

On utiliser le terme *sélectionner* lorsque l'on désire choisir une ou plusieurs lignes et colonnes d'un tableau (la plupart du temps des colonnes). L'action de *filtrer* signifie de sélectionner des lignes selon certains critères.

#### Sélectionner

Voici trois manières de sélectionner une colonne en R.

- Une méthode rapide mais peu expressive consiste à indiquer les valeurs numériques de l'indice de la colonne entre des crochets. Il s'agit d'appeler le tableau suivit de crochets. L'intérieur des crochets comprend deux éléments séparés par une virgule. Le premier élément sert à filtrer selon l'indice, le deuxième sert à sélectionner selon l'indice. Ainsi:
- `chicoute[, 1]`: sélectionner la première colonne
- `chicoute[, 1:10]`: sélectionner les 10 premières colonnes
- `chicoute[, c(2, 4, 5)]`: sélectionner les colonnes 2, 4 et 5
- `chicoute[c(10, 13, 20), c(2, 4, 5)]`: sélectionner les colonnes 2, 4 et 5 et les lignes 10, 13 et 20.

- Une autre méthode rapide, mais plus expressive, consiste à appeler le tableau, suivi du symbole `$`, puis le nom de la colonne.

> **Truc**. La plupart des IDE, comme RStudio, peuvent vous proposer des colonnes dans une liste. Après avoir entrer le `$`, taper sur la touche de tabulation: vous pourrez sélectionner la colonne dans une liste défilante.

![](images/04_auto-complete-cols.png)

- Une autre option est d'inscrire le nom de la colonne, ou du vecteur des colonnes, entre des crochets suivant le nom du tableau, c'est-à-dire `chicoute[c("Site", "Latitude_m", "Longitude_m")]`.

- Enfin, dans une séquence d'opérations en mode pipeline (chaque opération est mise à la suite de la précédente en plaçant le *pipe* `%>%` entre chacune), il peut être préférable de sélectionner des colonnes avec la fonction `select()`, i.e.

```
chicoute %>%
  select(Site, Latitude_m, Longitude_m)
```

La fonction `select()` permet aussi de travailler en exclusion. Ainsi pour enlever des colonnes, on placera un `-` (signe de soustraction) devant le nom de la colonne.

D'autre arguments de `select()` permettent une sélection rapide. Par exemple, pour obtenir les colonnes contenant des pourcentages:

```{r}
chicoute %>%
  select(ends_with("pourc")) %>%
  head(3)
```

#### Filtrer

Comme c'est le cas de la sélection, on pourra filtrer un tableau de plusieurs manières. J'ai déjà présenté comment filtrer selon les indices des lignes. Les autres manières reposent néanmoins sur une opération logique `==`, `<`, `>` ou `%in%` (le %in% signifie *se trouve parmi* et peut être suivi d'un vecteur de valeur que l'on désire accepter).

Les conditions booléennes peuvent être combinées avec les opérateurs *et*,  `&`, et *ou*, `|`. Pour rappel,


| Opération | Résultat |
| --------- | -------- |
| Vrai **et** Vrai | Vrai |
| Vrai **et** Faux | Faux |
| Faux **et** Faux | Faux |
| Vrai **ou** Vrai | Vrai |
| Vrai **ou** Faux | Vrai |
| Faux **ou** Faux | Faux |

- La méthode classique consiste à appliquer une opération logique entre les crochets, par exemple `chicoute[chicoute$CodeTourbiere == "BEAU", ]`
- La méthode tidyverse, plus pratique en mode pipeline, passe par la fonction `filter()`, i.e.

```
chicoute %>%
  filter(CodeTourbiere == "BEAU")
```

Combiner le tout.

```{r}
chicoute %>%
  filter(Ca_pourc < 0.4 & CodeTourbiere %in% c("BEAU", "MB", "WTP")) %>%
  select(contains("pourc"))
```

### Le format long et le format large

Dans le tableau `chicoute`, chaque élément possède sa propre colonne. Si l'on voulait mettre en graphique les boxplot des facettes de concentrations d'azote, de phosphore et de potassium dans les différentes tourbières, il faudrait obtenir une seule colonne de concentrations.

Pour ce faire, nous utiliserons la fonction `gather()`. Le premier argument est le nom de la colonne des variables, le deuxième est le nom de la nouvelle colonne des valeurs. La suite consiste à décrire les colonnes à inclure ou à exclure. Dans le cas qui suit, j'exclue CodeTourbiere de la refonte j'utilise `sample_n()` pour présenter un échantillon du résultat.

```{r}
chicoute_long <- chicoute %>%
  select(CodeTourbiere, N_pourc, P_pourc, K_pourc) %>%
  gather(key = element, value = concentration, -CodeTourbiere)
chicoute_long %>% sample_n(10)
```

L'opération inverse est `spread()`. Pour que cette opération fonctionne, `spread()` a besoin d'une colonne ayant un identifiant unique.

```{r}
chicoute_long$ID <- 1:nrow(chicoute_long)
```

Nous pouvons enlever cet identifiant une fois l'opération effectuée.

```{r}
chicoute_large <- chicoute_long %>%
  spread(key=element, value=concentration, fill=0) %>%
  select(-ID)
chicoute_large %>% sample_n(10)
```


### Combiner des tableaux

Nous avons introduit plus haut la notion de base de données. Nous voudrions peut-être utiliser le code des tourbières pour inclure leur nom, le type d'essai mené à ces tourbières, etc. Importons d'abord le tableau des noms liés aux codes.

```{r}
tourbieres <- read_csv2("data/chicoute_tourbieres.csv")
tourbieres
```

Notre information est organisée en deux tableaux, liés par la colonne `CodeTourbiere`. Comment fusionner l'information pour qu'elle puisse être utilisée dans son ensemble?  La fonction `left_join` effectue cette opération typique avec les bases de données.

```{r}
chicoute_merge <- left_join(x = chicoute, y = tourbieres, by = "CodeTourbiere")
# ou bien chicoute %>% left_join(y = tourbieres, by = "CodeTourbiere")
chicoute_merge %>% sample_n(4)
```

D'autres types de jointures sont possibles, et décrites en détails dans la [documentation](https://dplyr.tidyverse.org/reference/join.html).

[Garrick Aden-Buie](https://www.garrickadenbuie.com/) a préparé de [jolies animations](https://gist.github.com/gadenbuie/077bcd2700ac1241c65c324581a9f619) pour décrire les différents types de jointures.

`left_join(x, y)` colle y à x seulement ce qui dans y correspond à ce que l'on trouve dans x.

![](images/04_animated-left-join.gif)

`right_join(x, y)` colle y à x seulement ce qui dans x correspond à ce que l'on trouve dans y.

![](images/04_animated-right-join.gif) 

`inner_join(x, y)` colle x et y en excluant les lignes où au moins une variable de joint est absente dans x et y.

![](images/04_animated-inner-join.gif) 

`full_join(x, y)`garde toutes les lignes et les colonnes de x et y.

![](images/04_animated-full-join.gif) 

### Opérations sur les tableaux

Les tableaux peuvent être segmentés en éléments sur lesquels on calculera ce qui nous chante.

On pourrait vouloir obtenir:

- la somme avec la function `sum()`
- la moyenne avec la function `mean()` ou la médiane avec la fonction `median()`
- l'écart-type avec la function `sd()`
- les maximum et minimum avec les fonctions `min()` et `max()`
- un décompte d’occurrence avec la fonction `n()` ou `count()`

Par exemple,

```{r}
mean(chicoute$Rendement_g_5m2, na.rm = TRUE)
```

**En mode classique**, pour effectuer des opérations sur des tableaux, on utilisera la fonction `apply()`. Cette fonction prend, comme arguments, le tableau, l'axe (opération par ligne = 1, opération par colonne = 2), puis la fonction à appliquer.

```{r}
apply(chicoute %>% select(contains("pourc")), 2, mean)
```

Les opération peuvent aussi être effectuées par ligne, par exemple une somme (je garde seulement les 10 premiers résultats).

```{r}
apply(chicoute %>% select(contains("pourc")), 1, sum)[1:10]
```

La fonction à appliquer peut être personnalisée, par exemple:

```{r}
apply(chicoute %>% select(contains("pourc")), 2,
      function(x) (prod(x))^(1/length(x)))
```

Vous reconnaissez cette fonction? C'était la moyenne géométrique (la fonction `prod()` étant le produit d'un vecteur).

**En mode tidyverse**, on aura besoin principalement des fonction suivantes:

* `group_by()` pour effectuer des opérations par groupe, l’opération `group_by()` sépare le tableau en plusieurs petits tableaux, en attendant de les recombiner. C'est un peu l'équivalent des facettes avec le module de visualisation ggplot2, que nous explorons au chapitre \@ref(chapitre-visualisation).
* `summarise()` pour réduire plusieurs valeurs en une seule, il applique un calcul sur le tableau ou s'il y a lieu sur chaque petit tableau segmenté. Il en existe quelques variantes.
    + `summarise_all()` applique la fonction à toutes les colonnes
    + `summarise_at()` applique la fonction aux colonnes spécifiées
    + `summarise_if()` applique la fonction aux colonnes qui ressortent comme `TRUE` selon une opération booléenne
* `mutate()` pour ajouter une nouvelle colonne
    + Si l'on désire ajouter une colonne à un tableau, par exemple le sommaire calculé avec `summarise()`. À l'inverse, la fonction `transmute()` retournera seulement le résultat, sans le tableau à partir duquel il a été calculé. De même que `summarise()`, `mutate()` et `transmute()` possèdent leurs équivalents `_all()`, `_at()` et `_if()`.
* `arrange()` pour réordonner le tableau
    + On a déjà couvert `arrange()` dans le chapitre 3. Rappelons que cette fonction n'est pas une opération sur un tableau, mais plutôt un changement d'affichage en changeant l'ordre d'apparition des données.

Ces opérations sont décrites dans l'aide-mémoire [*Data Transformation Cheat Sheet*](https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf).

[![](https://www.rstudio.com/wp-content/uploads/2015/01/data-transformation-cheatsheet-600x464.png)](https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf)

Aide-mémoire de dplyr, source: https://www.rstudio.com/resources/cheatsheets/

Pour effectuer des statistiques par colonne, on utilisera `summarise_all()` étant donnée que l'on désire un sommaire sur toutes les variables sélectionnées. Pour spécifier que l'on désire la moyenne et l'écart-type on inscrit les noms des fonctions dans `funs()`.

```{r}
chicoute %>%
  select(contains("pourc")) %>%
  summarise_all(funs(mean, sd))
```

On utilisera `group_by()` pour segmenter le tableau, et ainsi obtenir des statistiques pour chaque groupe.

```{r}
chicoute %>%
  group_by(CodeTourbiere) %>%
  select(contains("pourc")) %>%
  summarise_all(funs(mean, sd))
```

Pour obtenir des statistiques à chaque ligne, mieux vaut utiliser `apply()`, tel que vu précédemment. Le point, `.`, représente le tableau dans la fonction.

```{r}
chicoute %>%
  select(contains("pourc")) %>%
  apply(., 1, sum)
```

Revenons à notre tableau des espèces menacées.

```{r warning=FALSE}
especes_menacees <- read_csv('data/WILD_LIFE_09012019174644084.csv')
```

Nous avions exécuté le pipeline suivant.

```{r}
especes_menacees %>%
  filter(IUCN == 'CRITICAL') %>%
  select(Country, Value) %>%
  group_by(Country)  %>%
  summarise(n_critical_species = sum(Value)) %>%
  arrange(desc(n_critical_species)) %>%
  top_n(10)
```

Ce pipeline consistait à:

```
prendre le tableau especes_menacees, puis
  filtrer pour n'obtenir que les espèces critiques, puis
  sélectionner les colonnes des pays et des valeurs (nombre d'espèces), puis
  segmenter le tableaux en plusieurs tableaux selon le pays, puis
  appliquer la fonction sum pour chacun de ces petits tableaux (puis de recombiner ces sommaires), puis
  trier les pays en nombre décroissant de décompte d'espèces, puis
  afficher le top 10
```

### Exemple (difficile)

Pour revenir à notre tableau `chicoute`, imaginez que vous aviez une station météo (station_A) située aux coordonnées (490640, 5702453) et que vous désiriez calculer la distance entre l'observation et la station. Prenez du temps pour réfléchir à la manière dont vous procéderez... 

On pourra créer une fonction qui mesure la distance entre un point x, y et les coordonnées de la station A...

```{r}
dist_station_A <- function (x, y) {
  return(sqrt((x - 490640)^2 + (y - 5702453)^2))
}
```

... puis ajouter une colonne avec mutate grâce à une fonction prenant les arguments x et y spécifiés.

```{r}
chicoute %>%
  mutate(dist = dist_station_A(x = Longitude_m, y= Latitude_m)) %>%
  select(ID, CodeTourbiere, Longitude_m, Latitude_m, dist) %>%
  top_n(10)
```


Nous pourrions procéder de la même manière pour fusionner des données climatiques. Le tableau `chicoute` ne possède pas d'indicateurs climatiques, mais il est possible de les soutirer de stations météo placées près des site. Ces données ne sont pas disponibles pour le tableau de la chicouté, alors j'utiliserai des données fictives pour l'exemple.

Voici ce qui pourrait être fait.

1. Créer un tableau des stations météo ainsi que des indices météo associés à ces stations.
2. Lier chaque site à une station (à la main où selon la plus petite distance entre le site et la station).
3. Fusionner les indices climatiques aux sites, puis les sites aux mesures de rendement.

Ces opérations demandent habituellement du tâtonnement. Il serait surprenant que même une personne expérimentée soit en mesure de compiler ces opérations sans obtenir de message d'erreur, et retravailler jusqu'à obtenir le résultat souhaité. L'objectif de cette section est de vous présenté un flux de travail que vous pourriez être amenés à effectuer et de fournir quelques éléments nouveau pour mener à bien une opération. Il peut être frustrant de ne pas saisir toutes les opérations: passez à travers cette section sans jugement. Si vous devez vous frotter à problème semblable, vous saurez que vous trouverez dans ce manuel une recette intéressante.

```{r}
stations <- data.frame(Station = c('A', 'B', 'C'),
                       Longitude_m = c(490640, 484870, 485929),
                       Latitude_m = c(5702453, 5701870, 5696421), 
                       t_moy_C = c(13.8, 18.2, 16.30),
                       prec_tot_mm = c(687, 714, 732))
stations
```

La fonction suivante calcule la distance entre des coordonnées x et y et chaque station d'un tableau de stations, puis retourne le nom de la station dont la distance est la moindre.

```{r}
dist_station <- function (x, y, stations_df) {
    # stations est le tableau des stations à trois colonnes
    # 1iere: nom de la station
    # 2ieme: longitude
    # 3ieme: latitude
    distance <- c()
    for (i in 1:nrow(stations)) {
        distance[i] <- sqrt((x - stations[i, 2])^2 + (y - stations[i, 3])^2)
    }
    nom_station <- as.character(stations$Station[which.min(distance)])
    return(nom_station)
}
```

Testons la fonction avec des coordonnées.

```{r}
dist_station(x = 459875, y = 5701988, stations_df = stations)
```

Nous appliquons cette fonction à toutes les lignes du tableau, puis en retournons un échantillon.

```{r}
chicoute %>%
    rowwise() %>%
    mutate(Station = dist_station(x = Longitude_m, y = Latitude_m, stations_df = stations)) %>%
    select(ID, CodeTourbiere, Longitude_m, Latitude_m, Station) %>%
    sample_n(10)
```

Cela semble fonctionner. On peut y ajouter un `left_join()` pour joindre les données météo au tableau principal.

```{r}
chicoute_weather <- chicoute %>%
    rowwise() %>%
    mutate(Station = dist_station(x = Longitude_m, y = Latitude_m, stations_df = stations)) %>%
    left_join(y = stations, by = "Station")
chicoute_weather %>% sample_n(10)
```

### Exporter un tableau

Simplement avec `write_csv()`.

```{r}
write_csv(chicoute_weather, "data/chicoute_weather.csv")
```

### Aller plus loin dans le tidyverse

Le livre [R for Data Science](http://r4ds.had.co.nz), de Garrett Grolemund et Hadley Wickham, est un incontournable.

[<img src="http://r4ds.had.co.nz/cover.png" width=200>](http://r4ds.had.co.nz)


## Références

Parent L.E., Parent, S.É., Herbert-Gentile, V., Naess, K. et  Lapointe, L. 2013. Mineral Balance Plasticity of Cloudberry (Rubus chamaemorus) in Quebec-Labrador Bogs. American Journal of Plant Sciences, 4, 1508-1520. DOI: 10.4236/ajps.2013.47183

<!--chapter:end:03_tableaux.Rmd-->

---
title: "Visualisation"
author: "Serge-Étienne Parent"
date: "`r format(Sys.Date())`"
output: github_document
---

# Visualisation {#chapitre-visualisation}

 ***
️\ **Objectifs spécifiques**:

À la fin de ce chapitre, vous

- comprendrez l'importance de l'exploration des données
- comprendrez les guides généraux pour créer un graphique approprié
- comprendrez la différence entre les modes impératifs et déclaratifs pour la création de graphique
- serez en mesure de créer des nuages de points, lignes, histogrammes, diagrammes en barres et boxplots en R
- saurez exporter un graphique en vue d'une publication

 ***

Lorsque j'aborde un document scientifique, la première chose que je fais après avoir lu le résumé est de regarder les graphiques. Un graphique bien conçu est dense en information, de sorte qu'il met en lumière une information qui pourrait passer inaperçue dans un tableau.

Reconnaissez-vous cette image?

<img src="images/03_mann-hockey-stick-2001.png" width="600">
<center>Source: [GIEC, Bilan 2001 des changements climatiques : Les éléments scientifiques](https://www.ipcc.ch/pdf/climate-changes-2001/scientific-basis/scientific-spm-ts-fr.pdf)</center>

Elle a été conçue par Michael E. Mann, Raymond S. Bradley et Malcolm K. Hughes. Le graphique montre l'évolution des températures en °C normalisées selon la température moyenne entre 1961 et 1990 sur l'axe des Y en fonction du temps, sur l'axe des X. On le connait aujourd'hui comme le *bâton de hockey*, et on reconnait son rôle clé pour sensibiliser la civilisation entière face au réchauffement global.

On aura recours à la visualisation des données pour plusieurs raison: en particulier, lorsque l'information d'un tableau devient difficile à interpréter. Ainsi, créer des graphiques est une tâche courante dans un flux de travail en science, que ce soit pour explorer les données ou les communiquer... ce à quoi cette section est vouée.

## Pourquoi explorer graphiquement?

La plupart des graphiques que vous générerez ne seront pas destinés à être publiés. Ils viseront probablement d'abord à explorer des données. Cela vous permettra de mettre en évidence de nouvelles perspectives.

Prenons par exemple deux variables, $X$ et $Y$. Vous calculez leur moyenne, écart-type et la corrélation entre les deux variables (nous verrons les statistiques en plus de détails dans un prochain chapitre). Pour démontrer que ces statistiques ne vous apprendront pas grand chose sur la structure des données, Matejka et Fitzmaurice (2017) ont généré 12 jeux de données $X$ et $Y$, ayant chacun pratiquement les mêmes statistiques. Mais avec des structures bien différentes.

![](https://d2f99xq7vri1nk.cloudfront.net/DinoSequentialSmaller.gif)
<center>Animation montrant la progression du jeu de données *Datasaurus* pour toutes les formes visées. Source: [Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing](https://www.autodeskresearch.com/publications/samestats).</center>


## Publier un graphique

Vous voilà sensibilisé à l'importance d'explorer les données graphiquement. Mais ce qui ultimement émanera d'un projet sera le rapport que vous déposerez, l'article scientifique que vous ferez publier ou le billet de blogue que vous posterez. Les graphiques inclus dans vos publications méritent une attention particulière pour que votre audience puisse comprendre les découvertes et perspectives offertes par vos travaux. Pour ce faire, un graphique doit évidemment répondre honnêtement à la question posée, sans artifices inutiles, mais tout de même attrayante.

### Cinq qualités d'un bon graphique

Alberto Cairo, chercheur spécialisé en visualisation de données, a fait paraître en 2016 le livre *The Truthful art*, note cinq qualités d'une visualisation bien conçue (les citations de cette section proviennent de ma traduction de Alberto Cairo, *The Truthful Art* (2016), p. 45.).

> 1- **Elle est véritable**, puisqu'elle est basée sur une recherche exhaustive et honnête.

Cela vaut autant pour les graphiques que pour l'analyse de données. Il s'agit froidement de **présenter les données selon l'interprétation la plus exacte**. Les pièges à éviter sont le *picorage de cerises* et la *surinterprétation des données*. Le *picorage*, c'est lorsqu'on réduit les perspectives afin de soutenir un argumentaire. Par exemple, retirer des données d'une région ou d'une décennie qui rendraient factice une conclusion fixée *a priori*. Ceci vaut autant pour les graphiques que pour les statistiques (nous parlerons du p-hacking au prochain chapitre). La *surinterprétation*, c'est lorsque l'on saute rapidement aux conclusions: par exemple, que l'on génère des corrélations, voire même des relations de causalités à partir de ce qui n'est que du bruit de fond. À ce titre, lors [d'une conférence](https://youtu.be/uw1Tag08dK4), Heather Krause insiste sur l'importance de faire en sorte que les représentations graphiques répondent correctement aux questions posées dans une étude (à voir!).

[![](images/03_Heather-Krause_youtube_.png)](https://youtu.be/uw1Tag08dK4)

> 2- **Elle est fonctionnelle**, puisqu'elle constitue une représentation précise des données, et qu'elle est construite de manière à laisser les observateurs.trices prendre des initiatives conséquentes.

"La seule chose qui est pire qu'un diagramme en pointe de tarte, c'est d'en présenter plusieurs" (Edward Tufte, designer, cité par Alberto Cairo, 2016, p. 50). Choisir le bon graphique pour représenter vos données est beaucoup moins une question de bon goût qu'**une question de démarche rationnelle sur l'objectif visé par la présentation d'un graphique**. Je présenterai des lignes guides pour sélectionner le type de graphique qui présentera vos données de manière fonctionnelle en fonction de l'objectif d'un graphique (d'ailleurs, avez-vous vraiment besoin d'un graphique?).

> 3- **Elle est attrayante** et intrigante, et même esthétiquement plaisante pour l'audience visée - les scientifiques d'abord, mais aussi le public en général.

En sciences naturelles, la pensée rationnelle, la capacité à organiser la connaissance et créer de nouvelles avenues sont des qualités qui sont privilégiées au talent artistique. **Que vous ayez où non des aptitudes en art visuel, présentez de l'information, pas des décorations**. Excel vous permet d'ajouter une perspective 3D à un diagramme en barres. La profondeur contient-elle de l'information? Non. Cette décoration ne fait qu'ajouter de la confusion. Minimalisez, fournissez le plus d'information possible avec le moins d'éléments possibles. C'est ce que vous proposent les guides graphiques que j'introduirai plus loin.

> 4- **Elle est pertinente**, puisqu'elle révèle des évidences scientifiques autrement difficilement accessibles.

Il s'agit de susciter un *eurêka*, dans le sens qu'**elle génère une idée, et parfois une initiative, en un coup d’œil**. Le graphique en bâton de hockey est un exemple où l'on a spontanément une idée de la situation. Cette situation peut être la présence d'un phénomène comme l'augmentation de la température globale, mais aussi l'absence de phénomènes pourtant attendus.

> 5- **Elle est instructive**, parce que si l'on saisit et accepte les évidences scientifiques qu'elle décrit, cela changera notre perception pour le mieux.

En présentant cette qualité, Alberto Cairo voulait insister ses lecteurs.trices à choisir des sujets de discussion visuelle de manière à participer à un monde meilleur. En ce qui nous concerne, il s'agit de bien **sélectionner l'information que l'on désire transmettre**. Imaginez que vous avez travaillé quelques jours pour créer un graphique, sont vous êtes fier, mais vous (ou un collègue hiérarchiquement favorisé) vous rendez compte que le graphique soutient peu ou pas le propos ou l'objectif de votre thèse/mémoire/rapport/article. Si c'est bien le cas, vous feriez mieux de laisser tomber votre oeuvre et considérer votre démarche comme une occasion d'apprentissage.

Alberto Cairo résume son livre *The Truthful Art* dans [une entrevue avec le National Geographic](http://news.nationalgeographic.com/2015/10/151016-data-points-alberto-cairo-interview/).

## Choisir le type de graphique le plus approprié

Vous connaissez sans doute les nuages de point, les lignes, les histogrammes, les diagrammes en barre et en pointe de tarte. De nombreuses manières de présenter les données ont été développées. Les principaux types de graphique seront couverts dans ce chapitre. D'autres types spécialisés seront couverts dans les chapitres appropriés (graphiques davantage orientés vers les statistiques, les biplots, les dendrogrammes, les diagrammes ternaires, les cartes, etc.).

La visualisation de données est aujourd’hui devenue une expertise en soi. Plusieurs personnes ayant acquis une expertise dans le domaine partage leurs expériences. À ce titre, le site [*from data to viz*](https://www.data-to-viz.com/) est à conserver dans vos marques-page. Il comprend des arbres décisionnels qui vous guident vers les options appropriées pour présenter vos données, puis fournissent des exemples en R que vous pourrez copier-coller-adapter dans vos feuilles de calcul. Également, je suggère le [site internet de Ann K. Emery](https://annkemery.com/essentials/), qui présente des lignes guide pour présenté le graphique adéquat selon les données en main. De nombreuses recettes sont également proposées sur [r-graph-gallery.com](https://www.r-graph-gallery.com/). En ce qui a trait aux couleurs, le choix n'est pas anodin. Si vous avez le souci des détails sur les éléments esthétiques de vos graphiques, je recommande la lecture de ce billet de blog de [Lisa Charlotte Rost](https://blog.datawrapper.de/colors/).

Cairo (2016) propose de procéder avec ces étapes:

0. Réfléchissez au message que vous désirez transmettre: comparer les catégories $A$ et $B$, visualiser une transition ou un changement de $A$ vers $B$, présenter une relation entre $A$ et $B$ ou la distribution de $A$ et $B$ sur une carte.
0. Essayez différentes représentations: si le message que vous désirez transmettre a plusieurs volets, il se pourrait que vous ayez besoin de plus d'un graphique.
0. Mettez de l'ordre dans vos données. Par ailleurs le prochain chapitre vise à vous sensibilisé à l'importance d'avoir des données bien organisées, qui seront par la suite plus facile à visualiser.
0. Testez le résultat. "Hé, qu'est-ce que tu comprends de cela?" Si la personne hausse les épaules, il va falloir réévaluer votre stratégie.

## Choisir son outils de visualisation

Les modules et logiciels de visualisation sont basés sur des approches que l'on pourrait placer sur un spectre allant de l'impératif au déclaratif.

### Approche impérative

Selon cette approche, vous indiquez comment placer l'information dans un espace graphique. Vous indiquer les symboles, les couleurs, les types de ligne, etc. Peu de choses sont automatisées, ce qui laisse une grande flexibilité, mais demande de vouer beaucoup d'énergie à la manière de coder pour obtenir le graphique désiré. Le module graphique de Excel, ainsi que le module graphique de base de `R`, utilisent des approches impératives.

### Approche déclarative

Les stratégies d'automatisation graphique se sont grandement améliorées au cours des dernières années. Plutôt que de vouer vos énergies à créer un graphique, il est maintenant possible de spécifier ce que l'on veut présenter.

> La visualisation déclarative vous permet de penser aux données et à leurs relations, plutôt que des détails accessoires.
>
> [*Jake Vanderplas, Declarative Statistical Visualization in Python with Altair*](https://www.youtube.com/watch?v=FytuB8nFHPQ) (ma traduction)

L'approche déclarative passe souvent par une *grammaire graphique*, c'est-à-dire un langage qui explique ce que l'on veut présenter - en mode impératif, on spécifie plutôt comment on veut présenter les données. Le module `ggplot2` est le module déclaratif par excellence en R.

## Visualisation en R

En R, votre trousse d'outils de visualisation mériterait de comprendre les modules suivants.

- **base**. Le module de base de R contient des fonctions graphique très polyvalentes. Les axes sont générées automatiquement, on peut y ajouter des titres et des légendes, on peut créer plusieurs graphiques sur une même figure, on peut y ajouter différentes géométries (points, lignes et polygones), avec différents types de points ou de trait, et différentes couleurs, etc. Les modules spécialisés viennent souvent avec leurs graphiques spécialisés, construit à partir du module de base. En tant que module graphique impératif, on peut tout faire ou presque (pas d’interactivité), mais l'écriture du code est peut expressive.
- **ggplot2**. C'est le module graphique par excellence en R (et j'ose dire: en calcul scientifique). ggplot2 se base sur une grammaire graphique. À partir d'un tableau de données, une colonne peut définir l'axe des x, une autre l'axe des y, une autre la couleur couleur des points ou leur dimension. Une autre colonne définissant des catégories peut segmenter la visualisation en plusieurs graphiques alignés horizontalement ou verticalement. Des extensions de ggplot2 permettent de générer des cartes (ggmap), des diagrammes ternaires (ggtern), des animations (gganimate), etc.
- **plotly**. plotly offre une fonction toute simple pour rendre interactif un graphique ggplot2. plotly est aussi un module graphique en soit, particulièrement utile pour les graphiques interactifs.

Nous survolerons rapidement le module de base, irons plus en profondeur avec ggplot2, puis je présenterai brièvement les graphiques interactifs avec plotly.

## Module de base pour les graphiques

Nous allons d'abord survoler le module de base, en mode impératif. La fonction de base pour les graphiques en R est `plot()`. Pour nous exercer avec cette fonction, chargeons d'abord le tableau de données d'exercice [`iris`](https://en.wikipedia.org/wiki/Iris_flower_data_set), publié en 1936 par le célèbre biostatisticien Ronald Fisher.

```{r}
Sys.setlocale('LC_ALL','C')
data(iris)
head(iris)
```

Le tableau `iris` contient 5 colonnes, les 4 premières décrivant les longueurs et largeurs des pétales et sépales de différentes espèces d'iris dont le nom apparaît à la 5ième colonne. La manière la plus rapide d’extraire une colonne d'un tableau est d'appeler le tableau, suivit du `$`, puis du nom de la colonne, par exemple `iris$Species`. Pour générer un graphique avec la fonction `plot()`:

```{r}
plot(iris$Sepal.Length, iris$Petal.Length)
```

Par défaut, le premier argument est le vecteur définissant l'axe des x et le deuxième est celui définissant l'axe des y. Le graphique précédent peut être amplement personnalisé en utilisant différents arguments.

<img src="images/03_plot-iris.png" width=400>

**Exercice**. Utilisez ces arguments dans la cellule de code de la figure `plot(iris$Sepal.Length, iris$Petal.Length)`.

Remarquez que la fonction a décidé toute seule de créer un nuage de point. La fonction plot() est conçue pour créer le graphique approprié selon le type des données spécifiées: lignes, boxplot, etc. Si l'on spécifiait les espèces comme argument `x`...

```{r}
plot(iris$Species, iris$Petal.Length)
```

De même, la fonction `plot()` appliquée à un tableau de données générera une représentation bivariée.

```{r}
plot(iris)
```

Il est possible d'encoder des attributs grâce à des vecteurs de facteurs (catégories).

```{r}
plot(iris, col = iris$Species)
```

L'argument `type = ""` permet de personnaliser l'apparence:

- `type = "p"`: ligne
- `type = "l"`: ligne
- `type = "o"` et `type = "b"`: ligne et points
- `type = "n"`: ne rien afficher

Créons un jeu de données.

```{r}
time <- seq(0, 100, 10)
height <- abs(time * 0.1 + rnorm(length(time), 0, 2)) # abs pour forcer les valeurs positives
plot(time, height, type = 'b')
```

Le type de ligne est spécifié par l'argument `lty` et la largeur du trait, par l'argument `lwd`.

La fonction `hist()` permet quant à elle de créer des histogrammes. Parmi ses arguments, `breaks` est particulièrement utile, car il permet d'ajuster la segmentation des incréments.

```{r}
hist(iris$Petal.Length, breaks = 60)
```

**Exercice**. Ajustez le titre de l'axe des x, ainsi que les limites de l'axe des x. Êtes-vous en mesure de colorer l'intérieur des barres en bleu?

La fonction `plot()` peut être suivie de plusieurs autres couches comme des lignes (`lines()` ou `abline()`), des points (`points()`), du texte (`text()`), des polygones (`polygon()`, des légendes (`legend()`)), etc. On peut aussi personnaliser les couleurs, les types de points, les types de lignes, etc. L'exemple suivant ajoute une ligne au graphique. Ne prêtez pas trop attention aux fonctions `predict()` et `lm()` pour l'instant: nous les verrons au chapitre 5.

```{r}
plot(time, height)
lines(time, predict(lm(height ~ time)))
```

Pour exporter un graphique, vous pouvez passer par le menu Export de RStudio. Mais pour des graphiques destinés à être publiés, je vous suggère d'exporter vos graphiques avec une haute résolution à la suite de la commande `png()` (ou `jpg()` ou `svg()`).

```{r}
png(filename = 'images/mon-graphique.png', width = 3000, height=2000, res=300)
plot(x = iris$Petal.Length,
     y = iris$Sepal.Length,
     col = iris$Species,
     cex=3, # dimension des points
     pch = 16) # type de points
dev.off()
```

Ce format crée une version vectorielle du graphique, c'est-à-dire que l'image exportée est un fichier contenant les formes, non pas les pixels. Cela vous permet d'éditer votre graphique dans un logiciel de dessin vectoriel (comme [Inkscape](https://inkscape.org/en/)).

J'ai utilisé le format d'image *png*, utile pour les images de type graphique, avec des changements de couleurs drastiques. Pour les photos, vous préférerez le format *jpg*. Des éditeurs demanderont peut-être des formats vectoriels comme *pdf* ou *eps*. Si vous ne trouvez pas de moyen de modifié un aspect du graphique dans le code (bouger des étiquettes ou des légendes, ajouter des éléments graphiques), vous pouvez exporter votre graphique en format svg (par la commande `svg()`. Ce format vectoriel peut être ouvert avec des logiciels de dessin vectoriel comme le logiciel libre [Inkscape](inkscape.org).

Le module de base de R comprend une panoplie d'autres particularités que je ne couvrirai pas ici, en faveur du module `ggplot2`.

## La grammaire graphique ggplot2

Brièvement, une grammaire graphique permet de schématiser (ma traduction de *to map*) des données sur des attributs esthétiques sur des géométries. Avec cette définition, nous avons 3 composantes.

0. **Les données**. Votre tableau est bien sûr un argument nécessaire pour générer le graphique.
0. **Les marqueurs**. Un terme abstrait pour désigner les points, les lignes, les polygones, les barres, les flèches, etc.
0. **Les attributs encodés**. La position, la dimension, la couleur ou la forme que prendront les géométries. En ggplot2, on les nomme les *aesthetics*.
0. **Les attributs globaux**. Les attributs sont globaux lorsqu'ils sont constant (ils ne dépendent pas d'une variable). Les valeurs par défaut conviennent généralement, mais certains attributs peuvent être spécifiés: par exemple la forme ou la couleur des points, le type de ligne.
0. **Les thèmes**. Le thème du graphique peut être spécifié dans son ensemble, c'est-à-dire en utilisant un thème prédéfini, mais l'on peut modifier certains détails.

Le flux de travail pour créer un graphique à partir d'une grammaire ressemble donc à ceci:

```
Avec mon tableau,
Créer un marqueur (
encoder(position X = colonne A,
position Y = colonne B,
couleur = colonne C)
forme globale = 1)
Avec un thème noir et blanc
```

Le module tidyverse installera des modules utilisés de manière récurrente dans ce cours, comme ggplot2, dplyr, tidyr et readr. Je vous recommande de l'installer et de le charger au début de vos sessions de travail.

```{r}
library("tidyverse")
```

Le *tidyverse* est le nom d'une nouvelle méthode de travail en R. Par rapport à l'approche classique, l'approche tidyverse est plus intuitive et mieux adaptée pour l'ensemble des tâches à accomplir en science des données. Les deux approches peuvent tout-à-fait être combinées. Nous utiliserons préférablement le tidyverse pour ce cours. 

## Mon premier ggplot

Pour notre premier exercice, je vais charger un tableau depuis le fichier de données [`abalone.data`](https://github.com/ajschumacher/gadsdc1/blob/master/dataset_research/clara_abalone.md) depuis un dépôt sur internet. Je n'irai pas dans les détails sur les tableaux de données, puisque c'est le sujet du prochain chapitre. Le fichier de données porte sur un escargot de mer et comprend le sexe (M: mâle, F: femelle et I: enfant), des poids et dimensions des individus observés, et le nombre d'anneaux comptés dans la coquille.

```{r}
abalone <- read_csv("data/abalone.csv")
```

Inspectons l'entête du tableau avec la fonction `head()`.

```{r}
head(abalone)
```

Suivant la grammaire graphique ggplot2, on pourra créer ce graphique de points comprenant les attributs suivants suivants.

0. `data = abalone`, le fichier de données.
0. `mapping = aes(...)`, spécifié comme attribut de la fonction `ggplot()`, cet encodage (ou `aes`thetic) reste l'encodage par défaut pour tous les marqueurs du graphique. Toutefois, l'encodage `mapping = aes()` peut aussi être spécifié dans la fonction du marqueur (par exemple `geom_point()`). Dans l'encodage global du graphique, on place en x la longueur de la coquille (`x = LongestShell`) et on place en y le poids de la coquille (`y = ShellWeight`).
0. Pour ajouter un marqueur, on utilise le `+`. Généralement, on change aussi de ligne.
0. Le marqueur ajouté est un point, `geom_point()`, dans lequel on spécifie un encodage de couleur sur la variable Type (`colour = Type`) et un encodage de dimension du point sur la variable rings (`size = Rings`). L'attribut `alpha = 0.5` se situe hors du mapping et de la fonction `aes()`: c'est un attribut identique pour tous les points.

```{r}
ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) +
  geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5)
```

Il existe plusieurs types de marqueurs:

- `geom_point` pour les points
- `geom_line` pour les lignes
- `geom_bar` pour les diagrammes en barre et `geom_histogram` pour les histogrammes
- `geom_boxplot` pour les boxplots
- `geom_errorbar`, `geom_pointrange` ou `geom_crossbar` pour les marges d'erreur
- `geom_map` pour les cartes
- etc.

Il existe plusieurs attributs d'encodage:

- la position `x`, `y` et `z` (`z` pertinent notamment pour le marqueur `geom_tile()`)
- la taille `size`
- la forme des points `shape`
- la couleur `colour`, qui peut être discrète ou continue
- le type de ligne `linetype`
- la transparence `alpha`
- et d'autres types spécialisés que vous retrouverez dans la documentation des marqueurs

Les types de marqueurs et leurs encodages sont décrits dans la [documentation de ggplot2](https://ggplot2.tidyverse.org/), qui fournit des  feuilles aide-mémoire qu'il est commode d'imprimer et d'afficher près de soi.

[![](https://www.rstudio.com/wp-content/uploads/2015/01/ggplot2-cheatsheet-2.1-600x464.png)](https://github.com/rstudio/cheatsheets/raw/master/data-visualization-2.1.pdf)
Aide-mémoire de ggplot2, source: https://www.rstudio.com/resources/cheatsheets/

#### Les facettes

Dans ggplot2, les `facet`ttes sont un type spécial d'encodage utilisés pour définir des grilles de graphique. Elles prennent deux formes:

- Le collage, `facet_wrap()`. Une variable catégorielle est utilisée pour segmenter les graphiques en plusieurs graphiques, qui sont placés l'un à la suite de l'autre dans un arrangement spécifié par un nombre de colonne ou un nombre de ligne.
- La grille, `facet_grid()`. Une ou deux variables segmentent les graphiques selon les colonnes et les lignes.

Les facettes peuvent être spécifiées n'importe où dans la chaîne de commande de ggplot2, mais conventionnellement, on les place tout de suite après la fonction `ggplot()`.

```{r}
ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) +
  facet_wrap(~Type, ncol=2) +
  geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5)
```

La fonction `cut()` permet de discrétiser des variables continues en catégories ordonnées - les fonctions peuvent être utilisées à l'intérieur de la fonction ggplot.

```{r}
ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) +
  facet_grid(Type ~ cut(Rings, breaks = seq(0, 30, 5))) +
  geom_point(mapping = aes(colour = Type), alpha = 0.5)
```

Par défaut, les axes des facettes, ainsi que leurs dimensions, sont les mêmes. Une telle représentation permet de comparer les facets sur une même échelle. Les axes peuvent être définis selon les données avec l'argument `scales`, tandis que l'espace des facettes peut être conditionné selon l'argument `space` - pour plus de détails, [voir la fiche de documentation](https://ggplot2.tidyverse.org/reference/facet_grid.html).

**Exercice**. Personnalisez le graphique avec les données `abalone` en remplaçant les variables et en réorganisant les facettes.

### Plusieurs sources de données

Il peut arriver que les données pour générer un graphique proviennent de plusieurs tableaux. Lorsqu'on ne spécifie pas la source du tableau dans un marqueur, la valeur par défaut est le tableau spécifier dans l'amorce `ggplot()`. Il est néanmoins possible de définir une source personnalisée pour chaque marqueur en spécifiant `data = ...` comme argument du marqueur.

```{r}
abalone_siteA <- data.frame(LongestShell = c(0.3, 0.8, 0.7),
                            ShellWeight = c(0.05, 0.81, 0.77))

ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) +
  geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5) +
  geom_point(data = abalone_siteA, size = 8, shape = 4)
```

### Exporter avec style

Le fond gris est une marque distinctive de ggplot2. Il n'est toutefois pas apprécié de tout le monde. D'autres thèmes dits *complets* peuvent être utilisés ([liste des thèmes complets](https://ggplot2.tidyverse.org/reference/ggtheme.html)). Les thèmes complets sont appelés avant la fonction `theme()`, qui permet d'effectuer des ajustements précis dont la liste exhaustive se trouve [dans la documentation de ggplot2](https://ggplot2.tidyverse.org/reference/theme.html).

Vous pouvez aussi personnaliser le titre des axes (`xlab()` et `ylab()`), leur limites (`xlim()` et `ylim()`) ou spécifier un titre global (`ggtitle()`).

Pour exporter un ggplot, on pourra utiliser les commandes de R `png()`, `svg()` ou `pdf()`, ou les outils de RStudio. Toutefois, ggplot2 offre la fonction `ggsave()`, que l'on place en remorque du graphique, en spécifiant les dimensions (`width` et `height`) ainsi que la résolution (`dpi`). La résolution d'un graphique destiné à la publication est typiquement de plus de 300 dpi.

```{r}
ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) +
  geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5) +
  xlab("Length (mm)") +
  ylab("Shell weight (g)") +
  ggtitle("Abalone") +
  xlim(c(0, 1)) +
  theme_classic() +
  theme(axis.title = element_text(size=20),
        axis.text = element_text(size=20),
        axis.text.y = element_text(size=20, angle=90, hjust=0.5),
        legend.box = "horizontal")
ggsave("images/abalone.png", width = 8, height = 8, dpi = 300)
```

Nous allons maintenant couvrir différents types de graphiques, accessibles selon différents marqueurs:

- les nuages de points
- les diagrammes en ligne
- les boxplots
- les histogrammes
- les diagrammes en barres

### Nuages de points

L'exemple précédent est un nuage de points, que nous avons généré avec le marqueur `geom_point()`, qui a déjà été passablement introduit. L'exploration de ces données a permis de détecter une croissance exponentielle du poids de la coquille en fonction de sa longueur. Il est clair que les abalones juvéniles (Type I) sont plus petits et moins lourds, mais nous devrons probablement procéder à des tests statistiques pour vérifier s'il y a des différences entre mâles et femelles.

Le graphique étant très chargé, nous avons utilisé des stratégies pour l'alléger en utilisant de la transparence et des facettes. Le marqueur `geom_jitter()` peut permettre de mieux apprécier la dispersion des points en ajoutant une dispersion randomisée en x ou en y.

```{r}
ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) +
  geom_jitter(mapping = aes(colour = Type, size = Rings), alpha = 0.5, width = 0.05, height=0.1)
```

Dans ce cas-ci, ça ne change pas beaucoup, mais retenons-le pour la suite.

### Diagrammes en lignes

Les lignes sont utilisées pour exprimer des liens entre une suite d'information. Dans la plupart des cas, il s'agit d'une suite d'information dans le temps que l'on appelle les séries temporelles. En l’occurrence, les lignes devraient être évitées si la séquence entre les variables n'est pas évidente. Nous allons utiliser un tableau de données de R portant sur la croissance des orangers.

```{r}
data(Orange)
head(Orange)
```

La première colonne spécifie le numéro de l'arbre mesuré, la deuxième son âge et la troisième sa circonférence. Le marqueur `geom_line()` permet de tracer la tendance de la circonférence selon l'âge. En encodant la couleur de la ligne à l'arbre, nous pourrons tracer une ligne pour chacun d'entre eux.

```{r}
ggplot(data = Orange, mapping = aes(x = age, y = circumference)) +
  geom_line(aes(colour = Tree))
```

La légende ne montre pas les numéros d'arbre en ordre croissance. En effet, la légende (tout comme les facettes) classe les catégories prioritairement selon l'ordre des catégories si elles sont ordinales, ou par ordre alphabétique si les catégories sont nominales. Inspectons la colonne `Tree` en inspectant le tableau avec la commande `str()` - la commande `glimpse()` du tidyverse donne un sommaire moins complet que `str()`.

```{r}
str(Orange)
```

En effet, la colonne `Tree` est un facteur ordinal dont les niveaux sont dans le même ordre que celui la légende.

### Les histogrammes

Nous avons vu les histogrammes dans la brève section sur les fonctions graphiques de base dans R: il s'agit de segmenter l'axe des x en incréments, puis de présenter sur l'axe de y le nombre de données que l'on retrouve dans cet incrément. Le marqueur à utiliser est `geom_histogram()`.

Revenons à nos escargots. Comment présenteriez-vous la longueur de la coquille selon la variable `Type`? Selon des couleurs ou des facettes? La couleur, dans le cas des histogrammes, est celle du pourtour des barres. Pour colorer l'intérieur des barres, l'argument à utiliser est `fill`.

```{r}
ggplot(data = abalone, mapping = aes(x = LongestShell)) +
  geom_histogram(mapping = aes(fill = Type), colour = 'black')
```

On n'y voit pas grand chose. Essayons plutôt les facettes.

```{r}
ggplot(data = abalone, mapping = aes(x = LongestShell)) +
  facet_grid(Type ~ .) +
  geom_histogram()
```

Les facettes permettent maintenant de bien distinguer la distribution des longueur des juvéniles. L'argument `bins`, tout comme l'argument `breaks` du module graphique de base, permet de spécifier le nombre d'incréments, ce qui peut être très utile en exploration de données.

```{r}
ggplot(data = abalone, mapping = aes(x = LongestShell)) +
  facet_grid(Type ~ .) +
  geom_histogram(bins=60, colour = 'white')
```

Le nombre d'incréments est un paramètre qu'il ne faut pas sous-estimer. À preuve, ce tweet de [@NicholasStrayer](https://twitter.com/NicholasStrayer):

<blockquote class="twitter-tweet" data-lang="fr"><p lang="en" dir="ltr">Histograms are fantastic, but make sure your bin-width/number is chosen well. This is the _exact_ same data, plotted with different bin-widths. Notice that the pattern doesn&#39;t necessarily get clearer as bin num increases. <a href="https://twitter.com/hashtag/dataviz?src=hash&amp;ref_src=twsrc%5Etfw">#dataviz</a> <a href="https://t.co/3MhSFwTVPH">pic.twitter.com/3MhSFwTVPH</a></p>&mdash; Nick Strayer (@NicholasStrayer) <a href="https://twitter.com/NicholasStrayer/status/1026893778404225024?ref_src=twsrc%5Etfw">7 août 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

### Boxplots

Les boxplots sont une autre manière de visualiser des distributions. L'astuce est de créer une boîte qui s'étant du premier quartile (valeur où l'on retrouve 25% de données dont la valeur est inférieure) au troisième quartile  (valeur où l'on retrouve 75% de données dont la valeur est inférieure). Une barre à l'intérieur de cette boîte est placée à la médiane (qui est en fait le second quartile). De part et d'autre de la boîte, on retrouve des lignes spécifiant l'étendue hors quartile. Cette étendue peut être déterminée de plusieurs manières, mais dans le cas de ggplot2, il s'agit de 1.5 fois l'étendue de la boîte (l'*écart interquartile*). Au-delà de ces lignes, on retrouve les points représentant les valeurs extrêmes. Le marqueur à utiliser est `geom_boxplot()`. L'encodage x est la variable catégorielle et l'encodage y est la variable continue.

```{r}
ggplot(data = abalone, mapping = aes(x = Type, y = LongestShell)) +
  geom_boxplot()
```

**Exercice**. On suggère parfois de présenter les mesures sur les boxplots. Utiliser `geom_jitter` avec un bruit horizontal.

### Les diagrammes en barre

Les diagrammes en barre représente une variable continue associée à une catégorie. Les barres sont généralement horizontales et ordonnées. Nous y reviendrons à la fin de ce chapitre, mais retenez pour l'instant que dans tous les cas, les diagrammes en barre doivent inclure le zéro pour éviter les mauvaises interprétations.

Pour les diagrammes en barre, nous allons utiliser les données de l'union internationale pour la conservation de la nature [distribuées par l'OCDE](https://stats.oecd.org/Index.aspx?DataSetCode=WILD_LIFE).

```{r}
# Certaines  colonnes de caractère sont considérées comme booléennes
# mieux vaut définir leur type pour s'assurer que le bon type
# soit attribué
especes_menacees <- read_csv('data/WILD_LIFE_09012019174644084.csv',
                             col_types = list("c", "c", "c", "c",
                                              "c", "c", "c", "c",
                                              "d", "c", "c", "c",
                                              "d", "c", "c"))
head(especes_menacees)
```

L'exercice consiste à créer un diagramme en barres horizontales du nombre d'espèces menacées de manière critique pour les 10 pays qui en contiennent le plus. Je vais effectuer quelques opérations sur ce tableau afin d'en arriver avec un tableau que nous pourrons convenablement mettre en graphique: n'y portez pas trop attention pour l'instant: ces opérations sont un avant-goût du prochain chapitre.

Nous allons filtrer le tableau pour obtenir les espèces critiquement menacées, sélectionner seulement le pays et le nombre d'espèces, les grouper par pays, additionner toutes les espèces pour chaque pays, les placer en ordre descendant et enfin sélectionner les 10 premiers. Comme vous le voyez, la création de graphique est liée de près avec la manipulation des tableaux!

```{r}
especes_crit <- especes_menacees %>%
  filter(IUCN == 'CRITICAL') %>%
  dplyr::select(Country, Value) %>%
  group_by(Country)  %>%
  summarise(n_critical_species = sum(Value)) %>%
  arrange(desc(n_critical_species)) %>%
  head(10)
especes_crit
```

Le premier type de diagramme en barre que nous allons couvrir est obtenu par le marqueur `geom_col()`.

```{r}
ggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) +
  geom_col()
```

Ce graphique est perfectible. Les barres sont verticales et non ordonnées. Souvenons-nous que ggplot2 ordonne par ordre alphabétique si aucun autre ordre est spécifié. Nous pouvons changer l'ordre en changeant l'ordre des niveaux de la variable `Country` selon le nombre d'espèces. Il s'avère que cet ordre est déjà défini par la colonne `especes_crit$Country`.

```{r}
especes_crit$Country <- factor(especes_crit$Country, levels = especes_crit$Country)
```

Pour faire pivoter le graphique, nous ajoutons `coord_flip()` à la séquence.

```{r}
ggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) +
  geom_col() +
  coord_flip()
```

Nous n'y sommes pas encore. Normalement, la valeur la plus élevée se retrouve en haut! Pas de problème, il s'agit de renverser le vecteur d'ordre avec la fonction `rev()`.

```{r}
especes_crit$Country <- factor(especes_crit$Country, levels = rev(especes_crit$Country))
ggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) +
  geom_col() +
  coord_flip()
```

Une autre méthode, `geom_bar()`, est un raccourcis permettant de compter le nombre d’occurrence d'une variable unique. Par exemple, dans le tableau abalone, le nombre de fois que chaque niveau de la variable Type

```{r}
ggplot(data = abalone, mapping = aes(x = Type)) +
  geom_bar() + 
  coord_flip()
```

Personnellement, je préfère passer par un diagramme en lignes avec le marqueur `geom_segment()`. Cela me donne la flexibilité pour définir un largeur de trait et éventuellement d'ajouter un point au bout pour en faire un [diagramme en suçon](https://www.r-graph-gallery.com/lollipop-plot/).

```{r}
ggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) +
  geom_segment(mapping = aes(xend=Country, yend = 0), lwd = 2) +
  geom_point(size=6) +
  coord_flip() +
  theme_bw()
```

Les diagrammes en barre peuvent être placés en relation avec d'autres. Reprenons notre manipulation de données précédente, mais en incluant tous les pays.

```{r}
especes_pays_iucn <- especes_menacees %>%
  filter(IUCN %in% c('ENDANGERED', 'VULNERABLE','CRITICAL')) %>%
  dplyr::select(IUCN, Country, Value) %>%
  group_by(Country, IUCN) %>%
  summarise(n_species = sum(Value)) %>%
  group_by(Country) %>%
  mutate(n_tot = sum(n_species)) %>%
  arrange(desc(n_tot))
head(especes_pays_iucn)
```

Pour l'ordre des pays, jouons un peu avec R.

```{r}
ordre_pays <- especes_pays_iucn %>%
  dplyr::select(Country, n_tot) %>%
  unique() %>%
  arrange(n_tot) %>%
  dplyr::select(Country)

especes_pays_iucn$Country <- factor(especes_pays_iucn$Country,
                                    levels = pull(ordre_pays))
```

Pour placer les barres les unes à côté des autres, nous spécifions `position = "dodge"`.

```{r}
ggplot(data = especes_pays_iucn, mapping = aes(x = Country, y = n_species)) +
  geom_col(aes(fill=IUCN), position = "dodge") +
  coord_flip()
```

Il est parfois plus pratique d'utiliser les facettes.

```{r}
ggplot(data = especes_pays_iucn, mapping = aes(x = Country, y = n_species)) +
  facet_grid(IUCN ~ .) +
  geom_col() +
  coord_flip()
```

### Exporter un graphique

Plus besoin d'utiliser la fonction `png()` en mode ggplot2. Utilisons plutôt `ggsave()`.

```{r}
ggplot(data = especes_pays_iucn, mapping = aes(x = Country, y = n_species)) +
  facet_grid(IUCN ~ .) +
  geom_col(aes(fill=IUCN)) +
  coord_flip()
ggsave("images/especes_pays_iucn.png", width = 6, height = 8, dpi = 300)
```


## Les graphiques comme outil d'exploration des données

La plupart des graphiques que vous créerez ne seront pas destinés à être publiés, mais serviront d'outil d'exploration des données. Le jeu de données datasaurus, présenté en [début de chapitre](#Pourquoi-explorer-grapiquement%3F), permet de saisir l'importance des outils graphiques pour bien comprendre les données.

```{r}
datasaurus = read_tsv('data/DatasaurusDozen.tsv')
head(datasaurus)
```

Projetons d'abord les coordonnées x et y sur un graphique. J'utilise FacetGrid ici, sachant que ce sera utile pour l'exploration.

```{r}
ggplot(data = datasaurus, mapping = aes(x = x, y = y)) +
  geom_point()
```

Ce graphique pourrait ressembler à une distribution binormale, ou *un coup de 12 dans une porte de grange*. Mais on aperçoit des données alignées, parfois de manière rectiligne, parfois en forme d'ellipse. Le tableau `datasaurus` a une colonne d'information supplémentaire. Utilisons-la comme catégorie pour générer des couleurs différente.

```{r}
ggplot(data = datasaurus, mapping = aes(x = x, y = y)) +
  geom_point(mapping = aes(colour = dataset))
```

Ce n'est pas vraiment plus clair. Il y a toutefois des formes qui se dégage, comme des ellipse et des lignes. Et si je regarde bien, j'y vois une étoile. La catégorisation pourrait-elle être mieux utilisée si on segmentait par facettes au lieu de des couleurs?

```{r}
ggplot(data = datasaurus, mapping = aes(x = x, y = y)) +
  facet_wrap(~dataset, nrow=2) +
  geom_point(size = 0.5) +
  coord_equal()
```

Voilà! Fait intéressant, ni les statistiques, ni les algorithmes de regroupement ne nous auraient été utiles pour différencier les groupes!

### Des graphiques interactifs!

Les graphiques sont traditionnellement des images statiques. Toutefois, les graphiques n'étant pas dépendants de supports papiers peuvent être utilisés de manière différente, en ajoutant une couche d’interaction. Conçue à Montréal, plotly est un module graphique interactif en soi. Il peut être utilisé grâce à son outil web, tout comme il peut être interfacé avec R, Python, javascript, etc. Mais ce qui retient notre attention ici est son interface avec ggplot2.

Les graphiques ggplot2 peuvent être enregistrés en tant qu'objets. Il peuvent conséquemment être manipulés par des fonctions. La fonction ggplotly permet de rendre votre ggplot interactif.

```{r}
library("plotly")
especes_crit_bar <- ggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) +
  geom_segment(mapping = aes(xend=Country, yend = 0), lwd = 2) +
  geom_point(size=6) +
  coord_flip()
# ggplotly(especes_crit_bar) # erreur en Rmd
```

### Des extensions de ggplot2

ggplot2 est un module graphique élégant et polyvalent. Il a pourtant bien des limitations. Justement, le module est conçu pour être implémenté avec des extensions. Vous en trouverez plusieurs sur [ggplot2-exts.org](http://www.ggplot2-exts.org/gallery/), mais en trouverez de nombreuses autres en cherchant avec le terme ggplot2 sur [github.com](https://github.com/search?q=ggplot2), probablement la plate-forme (voire un réseau social) de développement de logiciels la plus utilisée dans le monde. En voici quelques unes.

- [cowplot](https://github.com/wilkelab/cowplot) permet de créer des graphiques prêts pour la publication, par exemple en créant des grilles de plusieurs ggplots, en les numérotant, etc.
- Si les thèmes de base ne vous conviennent pas, vous en trouverez d'autres en installant [ggthemes](https://github.com/jrnold/ggthemes).
- [ggmap](https://github.com/dkahle/ggmap) et [ggspatial](https://github.com/paleolimbot/ggspatial) sont deux extensions pour créer des cartes. Un chapitre sur les données spatiales est en développement.
- [ggtern](http://www.ggtern.com/) permet de créer des diagrammes ternaires, qui sont utiles pour la visualisation de proportions incluant trois composantes. Ce sujet est couvert au chapitre 6, en développement.

### Aller plus loin avec ggplot2

- [Claus O. Wilke](@ClausWilke) est professeur en biologie intégrative à l'Université du Texas à Austin. Son livre [Fundamentals of Data Visualization](https://serialmentor.com/dataviz/) est un guide théorique et pratique pour la visualisation de données avec ggplot2.
- Le site [data-to-viz.com](https://www.data-to-viz.com/) vous accompagne dans le choix du graphique à créer selon vos données.
- Le site [r-graph-gallery.com](https://www.r-graph-gallery.com/) offre des recettes pour créer des graphiques avec ggplot2.

## Choisir les bonnes couleurs

*La couleur est une information*. Les couleurs devraient être sélectionnées d'abord pour être lisibles par les personnes ne percevant pas les couleurs, selon le support (apte à être photocopié, lisible à l'écran, lisible sur des documents imprimés en noir et blanc) et selon le type de données.
- Données continues ou catégorielles ordinales: gradient (transition graduelle d'une couleur à l'autre), séquence (transition saccadée selon des groupes de données continues) ou divergentes (transition saccadée d'une couleur à l'autre vers des couleurs divergentes, par exemple orange vers blanc vers bleu).
- Données catégorielles nominales: couleurs éloignées d'une catégorie à une autre (plus il y a de catégories, plus les couleurs sont susceptibles de se ressembler).

<img src="images/03_colorbrewer2.png" width="200">
Capture d'écran de [colorbrewer2.org](http://colorbrewer2.org), qui propose des palettes de couleurs pour créer des cartes, mais l'information est pertinente pour tout type de graphique.

## Règles particulières

> Les mauvais graphiques peuvent survenir à cause de l'ignorance, bien sûr, mais souvent ils existent pour la même raison que la boeuferie [*bullhist*] verbale ou écrite. Parfois, les gens ne se soucient pas de la façon dont ils présentent les données aussi longtemps que ça appuie leurs arguments et, parfois, ils ne se soucient pas que ça porte à confusion tant qu'ils ont l'air impressionnant. $-$ Carl Bergstorm et Jevin West, [Calling Bullshit Read-Along Week 6: Data Visualization](https://graphpaperdiaries.com/2017/04/09/calling-bs-read-along-week-6-data-visualization/)

Une représentation visuelle est un outil tranchant qui peut autant présenter un état véritable des données qu'une perspective trompeuse. Bien souvent, une ou plusieurs des 5 qualités ne sont pas respectées. Les occasions d'erreur ne manquent pas - j'en ferai mention dans la section *Choisir le bon type de graphique*. Pour l'instant, notons quelques règles particulières.

### Ne tronquez pas inutilement l'axe des $y$

Tronquer l'axe vertical peut amener à porter de fausses conclusions. 

<center>
<img src="https://i2.wp.com/flowingdata.com/wp-content/uploads/2015/08/bar-plots1.png?w=1800">
Effets sur la perception d'utiliser différentes références. Source: Yau (2015), [Real Chart Rules to Follow](https://flowingdata.com/2015/08/11/real-chart-rules-to-follow/).
</center>

La règle semble simple: les diagrammes en barre (utilisés pour représenter une grandeur) devraient toujours présenter le 0 et les diagrammes en ligne (utilisés pour présenter des tendances) ne requiert pas nécessairement le zéro ((Bergstrom et West, Calling bullshit: Misleading axes on graphs)[http://callingbullshit.org/tools/tools_misleading_axes.html]). Mais le zéro n'est pas toujours lié à une quantité particulière, par exemple, la température ou un log-ratio. De plus, avec un diagramme en ligne on pourra toujours magnifier des tendances en zoomant sur une variation somme toute mineure. On arrive donc moins à une règle qu'une qualité d'un bon graphique, en particulier la qualité no 1 de Cairo: offrir une représentation honnête des données. Par exemple, Nathan Yau, auteur du blogue Flowing Data, [propose](https://flowingdata.com/2015/08/31/bar-chart-baselines-start-at-zero/) de présenter des résultats de manière relative à la mesure initiale. C'est d'ailleurs ce qui a été fait pour générer le graphique de Michael Mann et al., ci-dessus, où le zéro correspond à la moyenne des températures enregistrées entre 1961 et 1990.

Il peut être tentant de tronquer l'axe des $y$ lorsque l'on désire superposer deux axes verticaux. Souvent, l'utilisation de plusieurs axes verticaux amène une perception de causalité dans des situations de [fausses corrélations](http://www.tylervigen.com/spurious-correlations). On ne devrait jamais utiliser plusieurs axes verticaux. Ja-mais.

### Utilisez un encrage proportionnel

Cette règle a été proposée par Edward Tufte dans [Visual Display of Quantitative Information](https://ariane25.bibl.ulaval.ca/ariane/wicket/detail?c=ariane&m=S&rq.ct=PE&rq.fa=false&rq.r.esc=false&rq.r.l[0].c=TI&rq.r.l[0].ex=false&rq.r.l[0].op=AND&rq.r.l[0].v=Visual+Display+of+Quantitative+Information&rq.r.la=*&rq.r.loc=*&rq.r.pft=false&rq.r.ta=*&rq.r.td=*&rq.rows=2&rq.st=1). Une des raisons pour lesquelles on évite de tronquer l'axe des $y$ en particulier pour les diagrammes en barre est que l'aire représentant une mesure (la quantité d'"encre" nécessaire pour la dessiner) devrait être proportionnelle à sa magnitude. Les diagrammes en barre sont particulièrement sensibles à cette règle, étant donnée que la largeur des barres peuvent amplifier l'aire occupée. Deux solutions dans ce cas: (1) utiliser des barres minces ou (2) préférer des "diagrammes de points" (*dot charts*, à ne pas confondre aux nuages de points).

L'encrage a beau être proportionnel, la difficulté que les humains éprouvent à comparer la dimension des cercles, et *a fortiori* la dimension de parties de cercle, donne peu d'avantage à utiliser des diagrammes en pointe de tarte, souvent utilisés pour illustrer des proportions. Nathan Yau [suggère](https://flowingdata.com/2015/08/11/real-chart-rules-to-follow/) de les utiliser avec suspicions et d'explorer d'[autres options](https://flowingdata.com/2009/11/25/9-ways-to-visualize-proportions-a-guide/).

![](https://i0.wp.com/flowingdata.com/wp-content/uploads/2015/08/pies.png?w=1800&ssl=1)

Pour comparer deux proportions, une avenue intéressante est le diagramme en pente, suggéré notamment par [Ann K. Emery](http://annkemery.com/avoiding-diagonal-text/#).

![](images/03_ann-emery-slope-chart.png)

Par extension, le diagramme en pente devient un diagramme en ligne lorsque plusieurs types de proportions sont comparées, ou lorsque des proportions évoluent selon des données continuent.


De la même manière, les [diagrammes en bulles](https://datavizcatalogue.com/methods/bubble_chart.html) ne devraient pas être représentatifs de la quantité, mais plutôt de contextualiser des données. Justement, le graphique tiré des données de *Gap minder* présenté plus haut est une contextualisation: l'aire d'un cercle ne permet pas de saisir la population d'un pays, mais de comparer grossièrement la population d'un pays par rapport aux autres.


### Publiez vos données

Vous avez peut-être déjà feuilleté un article et voulu avoir accès aux données incluses dans un graphique. Il existe des outils pour digitaliser des graphiques pour en extraire les données. Mais le processus est fastidieux, long, souvent peu précis. De plus en plus, les chercheurs sont encouragés à publier leurs données et leurs calculs. Matplotlib et Seaborn sont des outils graphiques classiques qui devraient être accompagnés des données et calculs ayant servi à les générer. Mais ce n'est pas idéal non plus. En revanche, les outils graphiques modernes comme Plotly et Altair peuvent être exportés en code javascipt, qui contient toutes les informations sur les données et la manière de les représenter graphiquement. Ce chapitre a pour objectif de vous familiariser avec les outils de base les plus communément utilisés en calcul scientifique avec Python, mais je vous encourage à explorer la nouvelle génération d'outils graphiques.


### Évitez de distraire avec des décorations futiles

À venir.

### Visitez www.junkcharts.typepad.com de temps à autre 

Le statisticien et blogueur Kaiser Fung s'affaire quotidiennement à proposer des améliorations à de mauvais graphiques sur son blogue [Junk Charts](www.junkcharts.typepad.com).

<!--chapter:end:04_visualisation.Rmd-->

---
title: "Biostatistiques bayésiennes"
author: "Serge-Étienne Parent"
date: "`r format(Sys.Date())`"
output: github_document
---

# Introduction à l'analyse bayésienne en écologie {#chapitre-biostats-bayes}

 ***
️\ **Objectifs spécifiques**:

**Ce chapitre est un extra. Il ne fait pas partie des objectifs du cours. Il ne sera pas évalué.**

À la fin de ce chapitre, vous

- serez en mesure de définir ce que sont les statistiques bayésiennes
- serez en mesure de calculer des statistiques descriptives de base en mode bayésien avec le module [greta](https://greta-stats.org/).

 ***

Les statistiques bayésiennes forment une trousse d'outils à garder dans votre pack sack.

## Qu'est-ce que c'est?

En deux mots: modélisation probabiliste. Un approche de modélisation probabiliste se servant au mieux de l'information disponible. Pour calculer les probabilités d'une variable inconnu en mode bayésien, nous avons besoin:

* De données
* D'un modèle
* D'une idée plus ou moins précise du résultat avant d'avoir analysé les données

De manière plus formelle, le théorème de Bayes (qui forme la base de l'analyse bayéseienne), dit que la distribution de probabilité des paramètres d'un modèle (par exemple, la moyenne ou une pente) est proportionnelle à la mutliplication de la distribution de probabilité estimée des paramètres et la distribution de probabilité émergeant des données.

Plus formellement,

$$P\left(\theta | y \right) = \frac{P\left(y | \theta \right) \times P\left(\theta\right)}{P\left(y \right)}$$,

où $P\left(\theta | y \right)$ $-$ la probabilité d'obtenir des paramètres $\theta$ à partir des données $y$ $-$ est la distribution de probabilité *a posteriori*, calculée à partir de votre *a prioti* $P\left(\theta\right)$ $-$ la probabilité d'obtenir des paramètres $\theta$ sans égard aux données, selon votre connaissance du phénomène $-$ et vos données observées $P\left(y | \theta \right)$ $-$ la probabilité d'obtenir les données $y$ étant donnés les paramètres $\theta$ qui régissent le phénomène. $P\left(y\right)$, la probabilité d'observer les données, est appellée la *vraissemblance marginale*, et assure que la somme des probabilités est nulle.

## Pourquoi l'utiliser?

Avec la notion fréquentielle de probabilité, on teste la probabilité d'observer les données recueillies étant donnée l'absence d'effet réel (qui est l'hypothèse nulle généralement adoptée). La notion bayésienne de probabilité combine la connaissance que l'on a d'un phénomène et les données observées pour estimer la probabilité qu'il existe un effet réel. En d'autre mots, les stats fréquentielles testent si les données concordent avec un modèle du réel, tandis que les stats bayésiennes évaluent, selon les données, la probabilité que le modèle soit réel.

Le hic, c'est que lorsqu'on utilise les statistiques fréquentielles pour répondre à une question bayésienne, on s'expose à de mauvaises interprétations. Par exemple, lors d'un projet considérant la vie sur Mars, les stats fréquentielles évalueront si les données recueillies sont conformes ou non avec l'hypothèse de la vie sur Mars. Par contre, pour évaluer la *probabilité de l'existance de vie sur Mars*, on devra passer par les stats bayésiennes (exemple tirée du billet [Dynamic Ecology -- Frequentist vs. Bayesian statistics: resources to help you choose](https://dynamicecology.wordpress.com/2011/10/11/frequentist-vs-bayesian-statistics-resources-to-help-you-choose/)).

## Comment l'utiliser?

Bien que la formule du théorème de Bayes soit plutôt simple, calculer une fonction *a posteriori* demandera de passer par des algorithmes de simulation, ce qui pourrait demander une bonne puissance de calcul, et des outils appropriés. R comporte une panoplie d'outils pour le calcul bayésien générique ([rstan](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started), [rjags](https://cran.r-project.org/web/packages/rjags/index.html), [MCMCpack](https://cran.r-project.org/web/packages/MCMCpack/index.html), etc.), et d'autres outils pour des besoins particuliers ([brms: R package for Bayesian generalized multivariate non-linear multilevel models using Stan](https://github.com/paul-buerkner/brms)). Nous utiliserons ici le module générique [`greta`](https://greta-stats.org/), qui permet de générer de manière conviviale plusieurs types de modèles bayésiens.

-----------------------------------------------------------------------
Pour installer greta, vous devez préalablement installer Python, gréé des modules tensorflow et tensorflow-probability en suivant [le guide](https://greta-stats.org/articles/get_started.html). En somme, vous devez d'abord installer greta (`install.packages("greta")`). Puis vous devez installer une distribution de Python -- je vous suggère [Anaconda](https://www.anaconda.com/download) (~500 Mo) ou [Miniconda](https://conda.io/miniconda.html) pour une installation minimale (~60 Mo). Enfin, lancez les commandes suivantes (une connection internet est nécessaire pour télécharger les modules).

```
install_tensorflow(method = "conda")

reticulate::conda_install("r-tensorflow", "tensorflow-probability", pip = TRUE)
```
-----------------------------------------------------------------------

## Faucons pélerins

Empruntons un exemple du livre [Introduction to WinBUGS for Ecologists: A Bayesian Approach to Regression, ANOVA and Related Analyses](https://www.elsevier.com/books/introduction-to-winbugs-for-ecologists/kery/978-0-12-378605-0), de Marc Kéry et examinons la masse de faucons pélerins. Mais alors que Marc Kéry utilise WinBUGS, un logiciel de résolution de problème en mode bayésien, nous utiliserons greta.

![](https://upload.wikimedia.org/wikipedia/commons/thumb/2/21/Falco_peregrinus_-_01.jpg/1024px-Falco_peregrinus_-_01.jpg)
Source: [Wikimedia Commons](https://commons.wikimedia.org/wiki/File:Falco_peregrinus_-_01.jpg)

Pour une première approche, nous allons estimer la masse moyenne d'une population de faucons pélerins.

À titre de données, générons des nombres aléatoires. Cette stratégie permet de valider les statistiques en les comparant aux paramètre que l'on impose. Ici, nous imposons une moyenne de 600 grammes et un écart-type de 30 grammes. Générons une séries de données avec 20 échantillons.

```{r}
library("tidyverse")
set.seed(5682)
y20 <- rnorm(n = 20, mean=600, sd = 30)
y200 <- rnorm(n = 200, mean=600, sd = 30)
par(mfrow = c(1, 2))
hist(y20, breaks=5)
hist(y200, breaks=20)
```

Je crée une fonction qui retourne la moyenne et l'erreur sur la moyenne ou sur la distribution. Calculons les statistiques classiques.

```{r}
confidence_interval <- function(x, on="deviation", distribution="t", level=0.95) {
  m <- mean(x)
  se <- sd(x)
  n <- length(x)
  if (distribution == "t") {
    error <- se * qt((1+level)/2, n-1)
  } else if (distribution == "normal") {
    error <- se * qnorm((1+level)/2)
  }
  if (on == "error") {
    error <- error/sqrt(n)
  }
  return(c(ll = m-error, mean = m, ul = m+error))
}
```

```{r}
print("Déviation, 95%")
print(round(confidence_interval(y20, on='deviation', level=0.95), 2))

print("Erreur, 95%")
print(round(confidence_interval(y20, on='error', level=0.95), 2))

print("Écart-type")
print(round(sd(y20), 2))
```

En faisant cela, nous prenons pour acquis que les données sont distribuées normalement. En fait, nous savons qu'elles devraient l'être pour de grands échantillons, puisque nous avons nous-même généré les données. Par contre, comme observateur par exemple de la série de 20 données générées, la distribution est définitivement asymétrique. Sous cet angle, la moyenne, ainsi que l'écart-type, pourraient être des paramètres biaisés. Nous pouvons justifier le choix d'une loi normale par des connaissances a priori des distributions de masse parmi des espèces d'oiseau. Ou bien transformer les données pour rendre leur distribution normale (chapitre \@ref(chapitre-explorer)).

## Statistiques d'une population

### greta

En mode bayésien, nous devons définir la connaissance *a priori* sous forme de variables aléatoires non-observées selon une distribution. Prenons l'exemple des faucons pélerins. Disons que nous ne savons pas à quoi ressemble la moyenne du groupe a priori. Nous pouvons utiliser un a priori vague, où la masse moyenne peut prendre n'importe quelle valeur entre 0 et 2000 grammes, sans préférence: nous lui imposons donc un a priori selon une distribution uniforme. Idem pour l'écart-type

```{r}
library("greta")
library("DiagrammeR")
library ("bayesplot")
library("tidybayes")
param_mean <- uniform(min = 0, max = 2000)
param_sd <- uniform(min = 0, max = 100)
```

La fonction a porteriori inclue la fonction de vraissemblance ainsi que la connaissancew a priori.

```{r}
distribution(y20) <- normal(param_mean, param_sd)
```

Le tout forme un modèle pour apprécier y, la masse des faucons pélerins.

```{r}
m <- model(param_mean, param_sd)
plot(m)
```

**Légende**:

![](images/5-1_legende.png)


Nous pouvons enfin lancer le modèle .

```{r}
draws <- mcmc(m, n_samples = 1000)
```

L'inspection de l'échantillonnage peut être effectuée grâce au module bayesplot.

```{r}
mcmc_combo(draws, combo = c("hist", "trace"))
```

L'échantillonnage semble stable. Voyons la distribution a posteriori des paramètres.

```{r}
draws_tidy <- draws %>%
  spread_draws(param_mean, param_sd)

print("Moyenne:")
confidence_interval(x = draws_tidy$param_mean, on = "deviation", distribution = "normal", level = 0.95)

print("Écart-type:")
confidence_interval(x = draws_tidy$param_sd, on = "deviation", distribution = "normal", level = 0.95)
```

L'*a priori* étant vague, les résultats de l'analyse bayésienne sont comparables aux statistiques fréquentielles.

```{r}
print("Erreur, 95%")
print(round(confidence_interval(y20, on='error', level=0.95), 2))
```

Les résultats des deux approches peuvent néanmoins être interprétés de manière différente. En ce qui a trait à la moyenne:

- **Fréquentiel**. Il y a une probabilité de 95% que mes données aient été générées à partir d'une moyenne se situant entre 584 et 614 grammes.

- **Bayésien**. Étant donnée mes connaissances (vagues) de la moyenne et de l'écart-type avant de procéder à l'analyse (*a priori*), il y a une probabilité de 95% que la moyenne de la masse de la population se situe entre 583 et 614 grammes.

Nous avons maintenant une idée de la distribution de moyenne de la population. Mais, rarement, une analyse s'arrêtera à ce stade. Il arrive souvent que l'on doive comparer les pparamètres de deux, voire plusieurs groupes. Par exemple, comparer des populations vivants dans des écosystèmes différents, ou comparer un traitement à un placébo. Ou bien, comparer, dans une même population de faucons pélerins, l'envergure des ailes des mâles et celle des femelles.

## Test de t: Différence entre des groupes

Pour comparer des groupes, on exprime généralement une hypothèse nulle, qui typiquement pose qu'il n'y a pas de différence entre les groupes. Puis, on choisit un test statistique **pour déterminer si les distributions des données observées sont plausibles dans si l'hypothèse nulle est vraie**.

En d'autres mots, le test statistique exprime la probabilité que l'on obtienne les données obtenues s'il n'y avait pas de différence entre les groupes. 

Par exemple, si 

1. vous obtenez une *p-value* de moins de 0.05 après un test de comparaison et
2. l'hypothèse nulle pose qu'il n'y a pas de différence entre les groupes,

cela signifie qu'il y a une probabilité de 5% que vous ayiez obtenu ces données s'il n'y avait en fait pas de différence entre les groupe. Il serait donc peu probable que vos données euent été générées comme telles s'il n'y avait en fait pas de différence.

```{r}
n_f <- 30
moy_f <- 105
n_m <- 20
moy_m <- 77.5
sd_fm <- 2.75

set.seed(21526)
envergure_f <- rnorm(mean=moy_f, sd=sd_fm, n=n_f)
envergure_m <- rnorm(mean=moy_m, sd=sd_fm, n=n_m)

envergure_f_df <- data.frame(Sex = "Female", Wingspan = envergure_f)
envergure_m_df <- data.frame(Sex = "Male", Wingspan = envergure_m)
envergure_df <- rbind(envergure_f_df, envergure_m_df)

envergure_df %>%
  ggplot(aes(x=Wingspan)) +
  geom_histogram(aes(y=..density.., fill=Sex)) +
  geom_density(aes(linetype=Sex, y=..density..))
```

Et les statistiques des deux groupesL

```{r}
envergure_df %>%
  group_by(Sex) %>%
  summarise(mean = mean(Wingspan),
            sd = sd(Wingspan),
            n = n())
```

Évaluer s'il y a une différence significative peut se faire avec un test de t (ou de Student).

```{r}
t.test(envergure_f, envergure_m)
```

La probabilité que les données ait été générées de la sorte si les deux groupes n'était semblables est très faible (`p-value < 2.2e-16`). On obtiendrait sensiblement les mêmes résultats avec une régression linéaire.

```{r}
linmod <- lm(Wingspan ~ Sex, envergure_df)
summary(linmod)
```

Le modèle linéaire est plus informatif. Il nous apprend que l'envergure des ailes des mâles est en moyenne plus faible de 28.0 cm que celle des femelles...

```{r}
confint(linmod, level = 0.95)
```

... avec un intervalle de confiance entre -29.6 cm à -26.4 cm.

Utilisons l'information dérivée de statistiques classiques dans nos a priori. Oui-oui, on peut faire ça. Mais attention, un a priori trop précis ou trop collé sur nos données orientera le modèle vers une solution préalablement établie: ce qui constituerait aucune avancée par rapport à l'*a priori*. Nous allons utiliser a priori pour les deux groupes la moyenne des deux groupes, et comme dispersion la moyenne le double de l'écart-type. Rappelons que cet écart-type est l'a priori de écart-type sur la moyenne, non pas de la population.

Procédons à la création d'un modèle greta. Nous utiliserons la régression linéaire préférablement au test de t.

```{r}
is_female <- model.matrix(~envergure_df$Sex)[, 2]
```


```{r}
int <- normal(600, 30)
coef <- normal(30, 10)
sd <- cauchy(0, 10, truncation = c(0, Inf))

mu <- int + coef * is_female

distribution(envergure_df$Wingspan) <- normal(mu, sd)

m <- model(int, coef, sd, mu)
plot(m)
```

Go!

```{r}
draws <- mcmc(m, n_samples = 1000)
```

Et les résultats.

```{r}
mcmc_combo(draws, combo = c("dens", "trace"), pars = c("int", "coef", "sd"))
```


```{r}
draws_tidy <- draws %>%
  spread_draws(int, coef, sd)
draws_tidy
```

```{r}
print("Intercept:")
confidence_interval(x = draws_tidy$int, on = "deviation", distribution = "normal", level = 0.95)

print("Pente:")
confidence_interval(x = draws_tidy$coef, on = "deviation", distribution = "normal", level = 0.95)
```

## Pour aller plus loin

Le module greta est conçu et maintenu par [Nick Golding](https://github.com/goldingn), du Quantitative & Applied Ecology Group de l'University of Melbourne, Australie. La [documentation de greta](https://greta-stats.org/) offre des [recettes](https://greta-stats.org/articles/example_models.html) pour toutes sortes d'analyses en écologie.

Les livres de Mark Kéry, bien que rédigés pour les calculs en langage R et WinBUGS, offre une approche bien structurée et traduisible en greta, qui est plus moderne que WinBUGS.

- [Introduction to WinBUGS for Ecologists (2010)](https://www.amazon.com/Introduction-WinBUGS-Ecologists-Bayesian-regression/dp/0123786053)
- [Bayesian Population Analysis using WinBUGS: A Hierarchical Perspective (2011)](https://www.amazon.com/Bayesian-Population-Analysis-using-WinBUGS/dp/0123870208)
- [Applied Hierarchical Modeling in Ecology: Analysis of distribution, abundance and species richness in R and BUGS (2015)](https://www.amazon.com/Applied-Hierarchical-Modeling-Ecology-distribution/dp/0128013788)

<!--chapter:end:05-1_biostats_bayes.Rmd-->

---
title: "Biostatistiques"
author: "Serge-Étienne Parent"
date: "`r format(Sys.Date())`"
output: github_document
---

# Biostatistiques {#chapitre-biostats}

 ***
️\ **Objectifs spécifiques**:

À la fin de ce chapitre, vous

- serez en mesure de définir les concpets de base en statistique: population, échantillon, variable, probabilité et distribution
- serez en mesure de calculer des statistiques descriptives de base: moyenne et écart-type, quartiles, maximum et minimum
- comprendrez les notions de test d'hypothèse, d'effet et de p-value, ainsi qu'éviter les erreurs communes dans leur interprétation
- saurez effectuer une modélisation statistique linéaire simple, multiple et mixte, entre autre sur des catégories
- saurez effectuer une modélisation statistique non linéaire simple, multiple et mixte

 ***


Aux chapitres précédents, nous avons vu comment visualiser, organiser et manipuler des tableaux de données. La statistique est une collection de disciplines liées à la collecte, l’organisation, l'analyse, l'interprétation et la présentation de données. Les biostatistiques est l'application de ces disciplines à la biosphère.

Dans [*Principles and procedures of statistics: A biometrical approach*](https://www.amazon.com/Principles-Procedures-Statistics-Biometrical-Approach/dp/0070610282), Steel, Torie et Dickey (1997) définissent les statistiques ainsi:

> Les statistiques forment la science, pure et appliquée, de la création, du développement, et de l'application de techniques par lesquelles l'incertitude de l'induction inférentielle peut être évaluée. (ma traduction)

Alors que l'inférence consiste à généraliser des observations sur des échantillons à l'ensemble d'une population, l'induction est un type de raisonnement qui permet de généraliser des observations en théories. Les statistiques permettent d'évaluer l'incertitude découlant du processus qui permet d'abord de passer de l'échantillon à la population représenté par cet échantillon, puis de passer de cette représentation d'une population en lois générales la concernant.

La définition de Whitlock et Schuluter (2015), dans [The Analysis of Biological Data](http://whitlockschluter.zoology.ubc.ca/), est plus simple, insistant sur l'inférence:

> La statistique est l'étude des méthodes pour décrire et mesures des aspects de la nature à partir d'échantillons. (ma traduction)

Les statistiques consistent à *faire du sens* (anglicisme assumé) avec des observations dans l'objectif de répondre à une question que vous aurez formulée clairement, préalablement à votre expérience.

<blockquote class="twitter-tweet" data-lang="fr"><p lang="en" dir="ltr">The more time I spend as The Statistician in the room, the more I think the best skill you can cultivate is the ability to remain calm and repeatedly ask &quot;What question are you trying to answer?&quot;</p>&mdash; Bryan Howie (@bryan_howie) <a href="https://twitter.com/bryan_howie/status/1073054519808876544?ref_src=twsrc%5Etfw">13 décembre 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

Le flux de travail conventionnel consiste à collecter des échantillons, transformer les données, effectuer des tests, analyser les résultats, les interpréter et les visualiser. Bien que ces tâches soient complexes, en particulier en ce qui a trait aux tests statistiques, la plupart des opérations statistiques peuvent être effectuées sans l'assistance de statisticien.ne.s... à condition de comprendre suffisamment les concepts utilisés. Ce chapitre à lui seul est trop court pour permettre d'intégrer toutes les connaissances nécessaires à une utilisation raisonnée des statistiques, mais fourni les bases pour aller plus loin. Notez que les erreurs d'interprétation statistiques sont courantes et la consultation de spécialistes n'est souvent pas un luxe.

Dans ce chapitre, nous verrons comment répondre correctement à une question valide et adéquate avec l'aide d'outils de calcul scientifique. Nous couvrirons les notions de bases des distributions et des variables aléatoires qui nous permettront d'effectuer des tests statistiques commun avec R. Nous couvrirons aussi les erreurs communément commises en recherche académique et les moyens simples de les éviter.

Ce chapitre est une introduction aux statistiques avec R, et ne remplacera pas un bon cours de stats.

En plus des modules de base de R nous utiliserons

* les modules de la `tidyverse`,
* le module de données agricoles `agridat`, ainsi que 
* le module `nlme` spécialisé pour la modélisation mixte. 

Avant de survoler les applications statistiques avec R, je vais d'abord et rapidement présenter quelques notions importantes en statistiques: populations et échantillons, variables, probabilités et distributions. Nous allons effectuer des tests d'hypothèse univariés (notamment les tests de *t* et les analyses de variance) et détailler la notion de p-value. Mais avant tout, je vais m'attarder plus longuement aux modèles linéaires généralisés, incluant en particulier des effets fixes et aléatoires (modèles mixtes), qui fournissent une trousse d'analyse polyvalente en analyse multivariée. Je terminerai avec les perspectives multivariés que sont les matrices de covariance et de corrélation.

## Populations et échantillons

Le principe d'inférence consiste à généraliser des conclusions à l'échelle d'une population à partir d'échantillons issus de cette population. Alors qu'une **population** contient tous les éléments étudiés, un **échantillon** d'une population est une observation unique. Une expérience bien conçue fera en sorte que les échantillons sont représentatifs de la population qui, la plupart du temps, ne peut être observée entièrement pour des raisons pratiques.

Les principes d'expérimentation servant de base à la conception d'une bonne méthodologie sont présentés dans le cours [*Dispositifs expérimentaux (BVG-7002)*](https://www.ulaval.ca/les-etudes/cours/repertoire/detailsCours/bvg-7002-dispositifs-experimentaux.html). Également, je recommande le livre *Principes d'expérimentation: planification des expériences et analyse de leurs résultats* de Pierre Dagnelie (2012), [disponible en ligne en format PDF](http://www.dagnelie.be/docpdf/ex2012.pdf). Un bon aperçu des dispositifs expérimentaux est aussi présenté dans [*Introductory Statistics with R*](https://www.springer.com/us/book/9780387790534), de Peter Dalgaard (2008).

Une population est échantillonnée pour induire des **paramètres**: un rendement typique dans des conditions météorologiques, édaphiques et managériales données, la masse typique des faucons pèlerins, mâles et femelles, le microbiome typique d'un sol agricole ou forestier, etc. Une **statistique** est une estimation d'un paramètre calculée à partir des données, par exemple une moyenne et un écart-type.

Par exemple, la moyenne ($\mu$) et l'écart-type ($\sigma$) d'une population sont estimés par les moyennes ($\bar{x}$) et écarts-types ($s$) calculés sur les données issues de l'échantillonnage.

Chaque paramètre est liée à une perspective que l'on désire connaître chez une population. Ces angles d'observations sont les **variables**.

## Les variables

Nous avons abordé au chapitre 4 la notion de *variable* par l'intermédiaire d'une donnée. Une variable est l'observation d'une caractéristique décrivant un échantillon et qui est susceptible de varier d'un échantillon à un autre. Si les observations varient en effet d'un échantillon à un autre, on parlera de variable aléatoire. Même le hasard est régit par certaines loi: ce qui est aléatoire dans une variable peut être décrit par des **lois de probabilité**, que nous verrons plus bas.

Mais restons aux variables pour l'instant. Par convention, on peut attribuer aux variables un symbole mathématique. Par exemple, on peut donner à la masse volumique d'un sol (qui est le résultat d'une méthodologie précise) le symbole $\rho$. Lorsque l'on attribue une valeur à $\rho$, on parle d'une donnée. Chaque donnée d'une observation a un indice qui lui est propre, que l'on désigne souvent par $i$, que l'on place en indice $\rho_i$. Pour la première donnée, on a $i=1$, donc $\rho_1$. Pour un nombre $n$ d'échantillons, on aura $\rho_1$, $\rho_2$, $\rho_3$, ..., $\rho_n$, formant le vecteur $\rho = \left[\rho_1, \rho_2, \rho_3, ..., \rho_n \right]$.

En R, une variable est associée à un vecteur ou une colonne d'un tableau.

```{r}
rho <- c(1.34, 1.52, 1.26, 1.43, 1.39) # matrice 1D
data <- data.frame(rho = rho) # tableau
data
```

Il existe plusieurs types de variables, qui se regroupe en deux grandes catégories: les **variables quantitatives** et les **variables qualitatives**.

### Variables quantitatives

Ces variables peuvent être continuent dans un espace échantillonnal réel ou discrètes dans un espace échantillonnal ne considérant que des valeurs fixes. Notons que la notion de nombre réel est toujours une approximation en sciences expérimentales comme en calcul numérique, étant donnée que l'on est limité par la précision des appareils comme par le nombre d'octets à utiliser. Bien que les valeurs fixes des distributions discrètes ne soient pas toujours des valeurs entières, c'est bien souvent le cas en biostatistiques comme en démographie, où les décomptes d'individus sont souvent présents (et où la notion de fraction d'individus n'est pas acceptée).

### Variables qualitatives

On exprime parfois qu'une variable qualitative est une variable impossible à mesurer numériquement: une couleur, l'appartenance à espèce ou à une série de sol. Pourtant, dans bien des cas, les variables qualitatives peut être encodées en variables quantitatives. Par exemple, on peut accoler des pourcentages de sable, limon et argile à un loam sableux, qui autrement est décrit par la classe texturale d'un sol. Pour une couleur, on peut lui associer des pourcentages de rouge, vert et bleu, ainsi qu'un ton. En ce qui a trait aux variables ordonnées, il est possible de supposer un étalement. Par exemple, une variable d'intensité faible-moyenne-forte peut être transformée linéairement en valeurs quantitatives -1, 0 et 1. Attention toutefois, l'étalement peut parfois être quadratique ou logarithmique. Les séries de sol peuvent être encodées par la proportion de gleyfication ([Parent et al., 2017](https://www.frontiersin.org/articles/10.3389/fenvs.2017.00081/full#B4)). Quant aux catégories difficilement transformables en quantités, on pourra passer par l'**encodage catégoriel**, souvent appelé *dummyfication*, qui nous verrons plus loin.

## Les probabilités

> « Nous sommes si éloignés de connaître tous les agens de la nature, et leurs divers modes d'action ; qu'il ne serait pas philosophique de nier les phénomènes, uniquement parce qu'ils sont inexplicables dans l'état actuel de nos connaissances. Seulement, nous devons les examiner avec une attention d'autant plus scrupuleuse, qu'il paraît plus difficile de les admettre ; et c'est ici que le calcul des probabilités devient indispensable, pour déterminer jusqu'à quel point il faut multiplier les observations ou les expériences, afin d'obtenir en faveur des agens qu'elles indiquent, une probabilité supérieure aux raisons que l'on peut avoir d'ailleurs, de ne pas les admettre. » — Pierre-Simon de Laplace

Une probabilité est la vraisemblance qu'un évènements se réalise chez un échantillon. Les probabilités forment le cadre des systèmes stochastiques, c'est-à-dire des systèmes trop complexes pour en connaître exactement les aboutissants, auxquels ont attribue une part de hasard. Ces systèmes sont prédominants dans les processus vivants.

On peut dégager deux perspectives sur les probabilités: l'une passe par une interprétation fréquentielle, l'autre bayésienne. L'interprétation **fréquentielle** représente la fréquence des occurrences après un nombre infini d’évènements. Par exemple, si vous jouez à pile ou face un grand nombre de fois, le nombre de pile sera égal à la moitié du nombre de lancés. Il s'agit de l'interprétation communément utilisée.

L'interprétation **bayésienne** vise à quantifier l'incertitude des phénomènes. Dans cette perspective, plus l'information s'accumule, plus l'incertitude diminue. Cette approche gagne en notoriété notamment parce qu'elle permet de décrire des phénomènes qui, intrinsèquement, ne peuvent être répétés infiniment (absence d'asymptote), comme celles qui sont bien définis dans le temps ou sur des populations limités.

L'approche fréquentielle teste si les données concordent avec un modèle du réel, tandis que l'approche bayésienne évalue la probabilité que le modèle soit réel. Une erreur courante consiste à aborder des statistiques fréquentielles comme des statistiques bayésiennes. Par exemple, si l'on désire évaluer la probabilité de l’existence de vie sur Mars, on devra passer par le bayésien, car avec les stats fréquentielles, l'on devra plutôt conclure si les données sont conforme ou non avec l'hypothèse de la vie sur Mars (exemple tirée du blogue [Dynamic Ecology](https://dynamicecology.wordpress.com/2011/10/11/frequentist-vs-bayesian-statistics-resources-to-help-you-choose/)).

Des rivalités factices s'installent enter les tenants des différentes approches, dont chacune, en réalité, répond à des questions différentes dont il convient réfléchir sur les limitations. Bien que les statistiques bayésiennes soient de plus en plus utilisées, nous ne couvrirons dans ce chapitre que l'approche fréquentielle. L'approche bayésienne est néanmoins traitée dans le document complémentaire statistiques_bayes.ipynb (section en développement).

## Les distributions

Une variable aléatoire peut prendre des valeurs selon des modèles de distribution des probabilités. Une distribution est une fonction mathématique décrivant la probabilité d'observer une série d'évènements. Ces évènements peuvent être des valeurs continues, des nombres entiers, des catégories, des valeurs booléennes (Vrai/Faux), etc. Dépendamment du type de valeur et des observations obtenues, on peut associer des variables à différentes lois de probabilité. Toujours, l'aire sous la courbe d'une distribution de probabilité est égale à 1.

En statistiques inférentielle, les distributions sont les modèles, comprenant certains paramètres comme la moyenne et la variance pour les distributions normales, à partir desquelles les données sont générées.

Il existe deux grandes familles de distribution: **discrètes** et **continues**. Les distributions discrètes sont contraintes à des valeurs prédéfinies (finies ou infinies), alors que les distributions continues prennent nécessairement un nombre infinie de valeur, dont la probabilité ne peut pas être évaluée ponctuellement, mais sur un intervalle.

L'**espérance** mathématique est une fonction de tendance centrale, souvent décrite par un paramètre. Il s'agit de la moyenne d'une population pour une distribution normale. La **variance**, quant à elle, décrit la variabilité d'une population, i.e. son étalement autour de l'espérance. Pour une distribution normale, la variance d'une population est aussi appelée variance, souvent présentée par l'écart-type.

### Distribution binomiale

En tant que scénario à deux issues possibles, des tirages à pile ou face suivent une loi binomiale, comme toute variable booléenne prenant une valeur vraie ou fausse. En biostatistiques, les cas communs sont la présence/absence d'une espèce, d'une maladie, d'un trait phylogénétique, ainsi que les catégories encodées. Lorsque l'opération ne comprend qu'un seul échantillon (i.e. un seul tirage à pile ou face), il s'agit d'un cas particulier d'une loi binomiale que l'on nomme une loi de *Bernouilli*.

Pour 25 tirages à pile ou face indépendants (i.e. dont l'ordre des tirages ne compte pas), on peut dessiner une courbe de distribution dont la somme des probabilités est de 1. La fonction `dbinom` est une fonctions de distribution de probabilités. Les fonctions de distribution de probabilités discrètes sont appelées des fonctions de masse.

```{r}
library("tidyverse")
x <- 0:25
y <- dbinom(x = x, size = 25, prob = 0.5)
print(paste('La somme des probabilités est de', sum(y)))
ggplot(data = data.frame(x, y), mapping = aes(x, y)) +
  geom_segment(aes(x = x, xend = x, y = 0, yend = y), color = "grey50") +
  geom_point()
```

### Distribution de Poisson

La loi de Poisson (avec un P majuscule, introduite par le mathématicien français Siméon Denis Poisson et non pas l'animal) décrit des distributions discrètes de probabilité d'un nombre d’évènements se produisant dans l'espace ou dans le temps. Les distributions de Poisson décrive ce qui tient du décompte. Il peut s'agir du nombre de grenouilles traversant une rue quotidiennement, du nombre de plants d'asclépiades se trouvant sur une terre cultivée, ou du nombre d’évènements de précipitation au mois de juin, etc. La distribution de Poisson n'a qu'un seul paramètre, $\lambda$, qui décrit tant la moyenne des décomptes.

Par exemple, en un mois de 30 jours, et une moyenne de 8 évènements de précipitation pour ce mois, on obtient la distribution suivante.

```{r}
x <- 1:30
y <- dpois(x, lambda = 8)
print(paste('La somme des probabilités est de', sum(y)))
ggplot(data = data.frame(x, y), mapping = aes(x, y)) +
  geom_segment(aes(x = x, xend = x, y = 0, yend = y), color = "grey50") +
  geom_point()
```

### Distribution uniforme

La distribution la plus simple est probablement la distribution uniforme. Si la variable est discrète, chaque catégorie est associé à une probabilité égale. Si la variable est continue, la probabilité est directement proportionnelle à la largeur de l'intervalle.  On utilise rarement la distribution uniforme en biostatistiques, sinon pour décrire des *a priori* vagues pour l'analyse bayésienne (ce sujet est traité dans le document 5.1_bayes.ipynb). Nous utilisons la fonction `dunif`. À la différence des distributions discrètes, les fonctions de distribution de probabilités continues sont appelées des fonctions de densité d'une loi de probabilité (*probability density function*).

```{r}
increment <- 0.01
x <- seq(-4, 4, by = increment)
y1 <- dunif(x, min = -3, max = 3)
y2 <- dunif(x, min = -2, max = 2)
y3 <- dunif(x, min = -1, max = 1)

print(paste('La somme des probabilités est de', sum(y3 * increment)))

gg_unif <- data.frame(x, y1, y2, y3) %>% gather(variable, value, -x)

ggplot(data = gg_unif, mapping = aes(x = x, y = value)) +
  geom_line(aes(colour = variable))
```

### Distribution normale

La plus répandue de ces lois est probablement la loi normale, parfois nommée loi gaussienne et plus rarement loi laplacienne. Il s'agit de la distribution classique en forme de cloche.

La loi normale est décrite par une moyenne, qui désigne la tendance centrale, et une variance, qui désigne l'étalement des probabilités autours de la moyenne. La racine carrée de la variance est l'écart-type.

Les distributions de mesures exclusivement positives (comme le poids ou la taille) sont parfois avantageusement approximées par une loi **log-normale**, qui est une loi normale sur le logarithme des valeurs: la moyenne d'une loi log-normale est la moyenne géométrique.

```{r}
increment <- 0.01
x <- seq(-10, 10, by = increment)
y1 <- dnorm(x, mean = 0, sd = 1)
y2 <- dnorm(x, mean = 0, sd = 2)
y3 <- dnorm(x, mean = 0, sd = 3)

print(paste('La somme des probabilités est de', sum(y3 * increment)))

gg_norm <- data.frame(x, y1, y2, y3) %>% gather(variable, value, -x)

ggplot(data = gg_norm, mapping = aes(x = x, y = value)) +
  geom_line(aes(colour = variable))
```

Quelle est la probabilité d'obtenir le nombre 0 chez une observation continue distribuée normalement dont la moyenne est 0 et l'écart-type est de 1? Réponse: 0. La loi normale étant une distribution continue, les probabilités non-nulles ne peuvent être calculés que sur des intervalles. Par exemple, la probabilité de retrouver une valeur dans l'intervalle entre -1 et 2 est calculée en soustrayant la probabilité cumulée à -1 de la probabilité cumulée à 2.

```{r}
increment <- 0.01
x <- seq(-5, 5, by = increment)
y <- dnorm(x, mean = 0, sd = 1)

prob_between <- c(-1, 2)

gg_norm <- data.frame(x, y)
gg_auc <- gg_norm %>%
  filter(x > prob_between[1], x < prob_between[2]) %>%
  rbind(c(prob_between[2], 0)) %>%
  rbind(c(prob_between[1], 0))

ggplot(data.frame(x, y), aes(x, y)) +
  geom_polygon(data = gg_auc, fill = '#71ad50') + # #71ad50 est un code de couleur format hexadécimal
  geom_line()

prob_norm_between <- pnorm(q = prob_between[2], mean = 0, sd = 1) - pnorm(q = prob_between[1], mean = 0, sd = 1)
print(paste("La probabilité d'obtenir un nombre entre", 
            prob_between[1], "et", 
            prob_between[2], "est d'environ", 
            round(prob_norm_between, 2) * 100, "%"))
```

La courbe normale peut être utile pour évaluer la distribution d'une population. Par exemple, on peut calculer les limites de région sur la courbe normale qui contient 95% des valeurs possibles en tranchant 2.5% de part et d'autre de la moyenne. Il s'agit ainsi de l'intervalle de confiance sur la déviation de la distribution.

```{r}
increment <- 0.01
x <- seq(-5, 5, by = increment)
y <- dnorm(x, mean = 0, sd = 1)

alpha <- 0.05
prob_between <- c(qnorm(p = alpha/2, mean = 0, sd = 1),
                  qnorm(p = 1 - alpha/2, mean = 0, sd = 1))

gg_norm <- data.frame(x, y)
gg_auc <- gg_norm %>%
  filter(x > prob_between[1], x < prob_between[2]) %>%
  rbind(c(prob_between[2], 0)) %>%
  rbind(c(prob_between[1], 0))

ggplot(data = data.frame(x, y), mapping = aes(x, y)) +
  geom_polygon(data = gg_auc, fill = '#71ad50') + # #71ad50 est un code de couleur format hexadécimal
  geom_line() +
  geom_text(data = data.frame(x = prob_between,
                              y = c(0, 0),
                              labels = round(prob_between, 2)),
            mapping = aes(label = labels))
```

On pourrait aussi être intéressé à l'intervalle de confiance sur la moyenne. En effet, la moyenne suit aussi une distribution normale, dont la tendance centrale est la moyenne de la distribution, et dont l'écart-type est noté *erreur standard*. On calcule cette erreur en divisant la variance par le nombre d'observation, ou en divisant l'écart-type par la racine carrée du nombre d'observations. Ainsi, pour 10 échantillons:

```{r}
increment <- 0.01
x <- seq(-5, 5, by = increment)
y <- dnorm(x, mean = 0, sd = 1)

alpha <- 0.05
prob_between <- c(qnorm(p = alpha/2, mean = 0, sd = 1) / sqrt(10),
                  qnorm(p = 1 - alpha/2, mean = 0, sd = 1) / sqrt(10))

gg_norm <- data.frame(x, y)
gg_auc <- gg_norm %>%
  filter(x > prob_between[1], x < prob_between[2]) %>%
  rbind(c(prob_between[2], 0)) %>%
  rbind(c(prob_between[1], 0))

ggplot(data = data.frame(x, y), mapping = aes(x, y)) +
  geom_polygon(data = gg_auc, fill = '#71ad50') + # #71ad50 est un code de couleur format hexadécimal
  geom_line() +
  geom_text(data = data.frame(x = prob_between,
                              y = c(0, 0),
                              labels = round(prob_between, 2)),
            mapping = aes(label = labels))
```

## Statistiques descriptives

On a vu comment générer des statistiques sommaires en R avec la fonction `summary()`. Reprenons les données d'iris.

```{r}
data("iris")
summary(iris)
```

Pour précisément effectuer une moyenne et un écart-type sur un vecteur, passons par les fonctions `mean()` et `sd()`.

```{r}
mean(iris$Sepal.Length)
sd(iris$Sepal.Length)
```

Pour effectuer un sommaire de tableau piloté par une fonction, nous passons par la gamme de fonctions `summarise()`, de dplyr. Dans ce cas, avec `group_by()`, nous fragmentons le tableau par espèce pour effectuer un sommaire sur toutes les variables.

```{r}
iris %>%
  group_by(Species) %>%
  summarise_all(mean)
```

Vous pourriez être intéressé par les quartiles à 25, 50 et 75%. Mais la fonction `summarise()` n'autorise que les fonctions dont la sortie est d'un seul objet, alors faisons sorte que l'objet soit une liste - lorsque l'on imbrique une fonction `funs`, le tableau à insérer dans la fonction est indiqué par un `.`.

```{r}
iris %>%
  group_by(Species) %>%
  summarise_all(funs(list(quantile(.))))
```

En mode programmation classique de R, on pourra générer les quartiles à la pièce.

```{r}
quantile(iris$Sepal.Length[iris$Species == 'setosa'])
quantile(iris$Sepal.Length[iris$Species == 'versicolor'])
quantile(iris$Sepal.Length[iris$Species == 'virginica'])
```

La fonction `table()` permettra d'obtenir des décomptes par catégorie, ici par plages de longueurs de sépales. Pour obtenir les proportions du nombre total, il s'agit d'encapsuler le tableau croisé dans la fonction `prop.table()`.

```{r}
tableau_croise <- table(iris$Species, 
                        cut(iris$Sepal.Length, breaks = quantile(iris$Sepal.Length)))
tableau_croise
```

```{r}
prop.table(tableau_croise)
```

## Tests d'hypothèses à un et deux échantillons

Un test d'hypothèse permet de décider si une hypothèse est confirmée ou rejetée à un seuil de probabilité prédéterminé.

Cette section est inspirée du chapitre 5 de [Dalgaard, 2008](https://www.springer.com/us/book/9780387790534).

----

#### Information: l'hypothèse nulle

Les tests d'hypothèse évalue des *effets* statistiques (qui ne sont pas nécessairement des effets de causalité). L'effet à évaluer peut être celui d'un traitement, d'indicateurs météorologiques (e.g. précipitations totales, degré-jour, etc.), de techniques de gestion des paysages, etc. Une recherche est menée pour évaluer l'hypothèse que l'on retrouve des différences entre des unités expérimentales. Par convention, l'**hypothèse nulle** (écrite $H_0$) est l'hypothèse qu'il n'y ait pas d'effet (c'est l'hypothèse de l'avocat du diable 😈) à l'échelle de la population (et non pas à l'échelle de l'échantillon). À l'inverse, l'**hypothèse alternative** (écrite $H_1$) est l'hypothèse qu'il y ait un effet à l'échelle de la population.

----

À titre d'exercice en stats, on débute souvent par en testant si deux vecteurs de valeurs continues proviennent de populations à moyennes différentes ou si un vecteur de valeurs a été généré à partir d'une population ayant une moyenne donner. Dans cette section, nous utiliserons la fonction `t.test()` pour les tests de t et la fonction `wilcox.test()` pour les tests de Wilcoxon (aussi appelé de Mann-Whitney).

### Test de t à un seul échantillon

Nous devons assumer, pour ce test, que l'échantillon est recueillit d'une population dont la distribution est normale, $\mathcal{N} \sim \left( \mu, \sigma^2 \right)$, et que chaque échantillon est indépendant l'un de l'autre. L'hypothèse nulle est souvent celle de l'avocat du diable, c'est-à-dire: ici, que $\mu = \bar{x}$. L'erreur standard sur la moyenne (ESM) de l'échantillon, $\bar{x}$ est calculée comme suit.

$$ESM = \frac{s}{\sqrt{n}}$$

où $s$ est l'écart-type de l'échantillon et $n$ est le nombre d'échantillons.

Pour tester l'intervalle de confiance de l'échantillon, on multiplie l'ESM par l'aire sous la courbe de densité couvrant une certaine proportion de part et d'autre de l'échantillon. Pour un niveau de confiance de 95%, on retranche 2.5% de part et d'autre.

```{r}
set.seed(33746)
x <- rnorm(20, 16, 4)

level <-  0.95
alpha <- 1-level

x_bar <- mean(x)
s <- sd(x)
n <- length(x)

error <- qnorm(1 - alpha/2) * s / sqrt(n)
error
```

intervalle de confiance est l'erreur de par et d'autre de la moyenne.

```{r}
c(x_bar - error, x_bar + error)
```

Si la moyenne de la population est de 16, un nombre qui se situe dans l'intervalle de confiance on accepte l'hypothèse nulle au seuil 0.05. Si le nombre d'échantillon est réduit (généralement < 30), on passera plutôt par une distribution de t, avec $n-1$ degrés de liberté.

```{r}
error <- qt(1 - alpha/2, n-1) * s / sqrt(n)
c(x_bar - error, x_bar + error)
```

Plus simplement, on pourra utiliser la fonction `t.test()` en spécifiant la moyenne de la population. Nous avons généré 20 données avec une moyenne de 16 et un écart-type de 4. Nous savons donc que la vraie moyenne de l'échantillon est de 16. Mais disons que nous testons l'hypothèse que ces données sont tirées d'une population dont la moyenne est 18 (et implicitement que sont écart-type est de 4).

```{r}
t.test(x, mu = 18)
```

La fonction retourne la valeur de t (*t-value*), le nombre de degrés de liberté ($n-1 = 19$), une description de l'hypothèse alternative (`alternative hypothesis: true mean is not equal to 18`), ainsi que l'intervalle de confiance au niveau de 95%. Le test contient aussi la *p-value*. Bien que la *p-value* soit largement utilisée en science

----

#### Information: la *p-value*

La *p-value*, ou valeur-p ou p-valeur, est utilisée pour trancher si, oui ou non, un résultat est **significatif** (en langage scientifique, le mot significatif ne devrait être utilisé *que* lorsque l'on réfère à un test d'hypothèse statistique). Vous retrouverez des *p-value* partout en stats. Les *p-values* indiquent la confiance que l'hypothèse nulle soit vraie, selon les données et le modèle statistique utilisées.

> La p-value est la probabilité que les données aient été générées pour obtenir un effet équivalent ou plus prononcé si l'hypothèse nulle est vraie.

Une *p-value* élevée indique que le modèle appliqué à vos données concordent avec la conclusion que l'hypothèse nulle est vraie, et inversement si la *p-value* est faible. Le seuil arbitraire utilisée en écologie et en agriculture, comme dans plusieurs domaines, est 0.05.

Les six principes de l'[American Statistical Association](https://phys.org/news/2016-03-american-statistical-association-statement-significance.html) guident l'interprétation des *p-values*. [ma traduction]

0. Les *p-values* indique l'ampleur de l’incompatibilité des données avec le modèle statistique
0. Les *p-values* ne mesurent pas la probabilité que l'hypothèse étudiée soit vraie, ni la probabilité que les données ont été générées uniquement par la chance.
0. Les conclusions scientifiques et décisions d'affaire ou politiques ne devraient pas être basées sur si une *p-value* atteint un seuil spécifique.
0. Une inférence appropriée demande un rapport complet et transparent.
0. Une *p-value*, ou une signification statistique, ne mesure pas l'ampleur d'un effet ou l'importance d'un résultat.
0. En tant que tel, une *p-value* n'offre pas une bonne mesure des évidences d'un modèle ou d'une hypothèse.

Cet encadré est inspiré d'un [billet de blogue de Jim Frost](https://blog.minitab.com/blog/adventures-in-statistics-2/how-to-correctly-interpret-p-values) et d'un [rapport de l'American Statistical Association](https://phys.org/news/2016-03-american-statistical-association-statement-significance.html).

----

Dans le cas précédent, la *p-value* était de 0.01014. Pour aider notre interprétation, prenons l'hypothèse alternative: `true mean is not equal to 18`. L'hypothèse nulle était bien que *la vraie moyenne est égale à 18*. Insérons la *p-value* dans la définition: la probabilité que les données aient été générées pour obtenir un effet équivalent ou plus prononcé si l'hypothèse nulle est vraie est de 0.01014. Il est donc très peu probable que les données soient tirées d'un échantillon dont la moyenne est de 18. Au seuil de signification de 0.05, on rejette l'hypothèse nulle et l'on conclu qu'à ce seuil de confiance, l'échantillon ne provient pas d'une population ayant une moyenne de 18.

----

### Attention: mauvaises interprétations des *p-values*

> "La p-value n'a jamais été conçue comme substitut au raisonnement scientifique" [Ron Wasserstein, directeur de l'American Statistical Association](https://phys.org/news/2016-03-american-statistical-association-statement-significance.html) [ma traduction]. 

**Un résultat montrant une p-value plus élevée que 0.05 est-il pertinent?**

Lors d'une conférence, Dr Evil ne présentent que les résultats significatifs de ses essais au seuil de 0.05. Certains essais ne sont pas significatifs, mais bon, ceux-ci ne sont pas importants... En écartant ces résultats, Dr Evil commet 3 erreurs:

1. La *p-value* n'est pas un bon indicateur de l'importance d'un test statistique. L'importance d'une variable dans un modèle devrait être évaluée par la valeur de son coefficient. Son incertitude devrait être évaluée par sa variance. Une manière d'évaluer plus intuitive la variance est l'écart-type ou l'intervalle de confiance. À un certain seuil d'intervalle de confiance, la p-value traduira la probabilité qu'un coefficient soit réellement nul ait pu générer des données démontrant un coefficient égal ou supérieur.
1. Il est tout aussi important de savoir que le traitement fonctionne que de savoir qu'il ne fonctionne pas. Les résultats démontrant des effets sont malheureusement davantage soumis aux journaux et davantage publiés que ceux ne démontrant pas d'effets ([Decullier et al., 2005]( https://doi.org/10.1136/bmj.38488.385995.8F )).
1. Le seuil de 0.05 est arbitraire.

----


----

#### Attention au *p-hacking*

Le *p-hacking* (ou *data dredging*) consiste à manipuler les données et les modèles pour faire en sorte d'obtenir des *p-values* favorables à l'hypothèse testée et, éventuellement, aux conclusions recherchées. **À éviter dans tous les cas. Toujours. Toujours. Toujours.**

Vidéo suggérée (en anglais).

[![p-hacking](images/05_p-hacking.png)](https://youtu.be/0Rnq1NpHdmw)

----

### Test de Wilcoxon à un seul échantillon

Le test de t suppose que la distribution des données est normale... ce qui est rarement le cas, surtout lorsque les échantillons sont peu nombreux. Le test de Wilcoxon ne demande aucune supposition sur la distribution: c'est un test non-paramétrique basé sur le tri des valeurs.

```{r}
wilcox.test(x, mu = 18)
```

Le `V` est la somme des rangs positifs. Dans ce cas, la *p-value* est semblable à celle du test de t, et les mêmes conclusions s'appliquent.

### Tests de t à deux échantillons

Les tests à un échantillon servent plutôt à s'exercer: rarement en aura-t-on besoin en recherche, où plus souvent, on voudra comparer les moyennes de deux unités expérimentales. L'expérience comprend donc deux séries de données continues, $x_1$ et $x_2$, issus de lois de distribution normale $\mathcal{N} \left( \mu_1, \sigma_1^2 \right)$ et $\mathcal{N} \left( \mu_2, \sigma_2^2 \right)$, et nous testons l'hypothèse nulle que $\mu_1 = \mu_2$. La statistique t est calculée comme suit.

$$t = \frac{\bar{x_1} - \bar{x_2}}{ESDM}$$

L'ESDM est l'erreur standard de la différence des moyennes:

$$ESDM = \sqrt{ESM_1^2 + ESM_2^2}$$

Si vous supposez que les variances sont identiques, l'erreur standard (s) est calculée pour les échantillons des deux groupes, puis insérée dans le calcul des ESM. La statistique t sera alors évaluée à $n_1 + n_2 - 2$ degrés de liberté. Si vous supposez que la variance est différente (*procédure de Welch*), vous calculez les ESM avec les erreurs standards respectives, et la statistique t devient une approximation de la distribution de t avec un nombre de degrés de liberté calculé à partir des erreurs standards et du nombre d'échantillon dans les groupes: cette procédure est considérée comme plus prudente ([Dalgaard, 2008](https://www.springer.com/us/book/9780387790534), page 101).

Prenons les données d'iris pour l'exemple en excluant l'iris setosa étant donnée que les tests de t se restreignent à deux groupes. Nous allons tester la longueur des pétales.

```{r}
iris_pl <- iris %>% 
    filter(Species != "setosa") %>%
    select(Species, Petal.Length)
sample_n(iris_pl, 5)
```

Dans la prochaine cellule, nous introduisons l'*interface-formule* de R, où l'on retrouve typiquement le `~`, entre les variables de sortie à gauche et les variables d'entrée à droite. Dans notre cas, la variable de sortie est la variable testée, `Petal.Length`, qui varie en fonction du groupe `Species`, qui est la variable d'entrée (variable explicative) - nous verrons les types de variables plus en détails dans la section [Les modèles statistiques](#Les-mod%C3%A8les-statistiques), plus bas.

```{r}
t.test(formula = Petal.Length ~ Species,
       data = iris_pl, var.equal = FALSE)
```

Nous obtenons une sortie similaire aux précédentes. L'intervalle de confiance à 95% exclu le zéro, ce qui est cohérent avec la p-value très faible, qui nous indique le rejet de l'hypothèse nulle au seuil 0.05. Les groupes ont donc des moyennes de longueurs de pétale significativement différentes.

----

#### Enregistrer les résultats d'un test

Il est possible d'enregistrer un test dans un objet.

```{r}
tt_pl <- t.test(formula = Petal.Length ~ Species,
                data = iris_pl, var.equal = FALSE)
summary(tt_pl)
str(tt_pl)
```

----

### Comparaison des variances

Pour comparer les variances, on a recours au test de F (F pour Fisher).

```{r}
var.test(formula = Petal.Length ~ Species,
         data = iris_pl)
```

Il semble que l'on pourrait relancer le test de t sans la procédure Welch, avec `var.equal = TRUE`.

### Tests de Wilcoxon à deux échantillons

Cela ressemble au test de t!

```{r}
wilcox.test(formula = Petal.Length ~ Species,
       data = iris_pl, var.equal = TRUE)
```

### Les tests pairés

Les tests pairés sont utilisés lorsque deux échantillons proviennent d'une même unité expérimentale: il s'agit en fait de tests sur la différences entre deux observations.

```{r}
set.seed(2555)

n <- 20
avant <- rnorm(n, 16, 4)
apres <- rnorm(n, 18, 3)
```

Il est important de spécifier que le test est pairé, la valeur par défaut de `paired` étant `FALSE`.

```{r}
t.test(avant, apres, paired = TRUE)
```

L'hypothèse nulle qu'il n'y ait pas de différence entre l'avant et l'après traitement est acceptée au seuil 0.05.

**Exercice**. Effectuer un test de Wilcoxon pairé.

## L'analyse de variance

L'analyse de variance consiste à comparer des moyennes de plusieurs groupe distribués normalement et de même variance. Cette section sera élaborée prochainement plus en profondeur. Considérons-la pour le moment comme une régression sur une variable catégorielle.

```{r}
pl_aov <- aov(Petal.Length ~ Species, iris)
summary(pl_aov)
```

La prochaine section, justement, est vouée aux modèles statistiques explicatifs, qui incluent la régression.

## Les modèles statistiques

La modélisation statistique consiste à lier de manière explicite des variables de sortie $y$ (ou variables-réponse ou variables dépendantes) à des variables explicatives $x$ (ou variables prédictives / indépendantes / covariables). Les variables-réponse sont modélisées par une fonction des variables explicatives ou prédictives.

Pourquoi garder les termes *explicatives* et *prédictives*? Parce que les modèles statistiques (basés sur des données et non pas sur des mécanismes) sont de deux ordres. D'abord, les modèles **prédictifs** sont conçus pour prédire de manière fiable une ou plusieurs variables-réponse à partir des informations contenues dans les variables qui sont, dans ce cas, prédictives. Ces modèles sont couverts dans le chapitre 11 de ce manuel (en développement). Lorsque l'on désire tester des hypothèses pour évaluer quelles variables expliquent la réponse, on parlera de modélisation (et de variables) **explicatives**. En inférence statistique, on évaluera les *corrélations* entre les variables explicatives et les variables-réponse. Un lien de corrélation n'est pas un lien de causalité. L'inférence causale peut en revanche être évaluée par des [*modèles d'équations structurelles*](https://www.amazon.com/Cause-Correlation-Biology-Structural-Equations/dp/1107442591), sujet qui fera éventuellement partie de ce cours.

Cette section couvre la modélisation explicative. Les variables qui contribuent à créer les modèles peuvent être de différentes natures et distribuées selon différentes lois de probabilité. Alors que les modèles linéaires simples (*lm*) impliquent une variable-réponse distribuée de manière continue, les modèles linéaires généralisés peuvent aussi expliquer des variables de sorties discrètes.

Dans les deux cas, on distinguera les variables fixes et les variables aléatoires. Les **variables fixes** sont des les variables testées lors de l'expérience: dose du traitement, espèce/cultivar, météo, etc. Les **variables aléatoires** sont les sources de variation qui génèrent du bruit dans le modèle: les unités expérimentales ou le temps lors de mesures répétées. Les modèles incluant des effets fixes seulement sont des modèles à effets fixes. Généralement, les modèles incluant des variables aléatoires incluent aussi des variables fixes: on parlera alors de modèles mixtes. Nous couvrirons ces deux types de modèle.

### Modèles à effets fixes

Les tests de t et de Wilcoxon, explorés précédemment, sont des modèles statistiques à une seule variable. Nous avons vu dans l'*interface-formule* qu'une variable-réponse peut être liée à une variable explicative avec le tilde `~`. En particulier, le test de t est régression linéaire univariée (à une seule variable explicative) dont la variable explicative comprend deux catégories. De même, l'anova est une régression linéaire univariée dont la variable explicative comprend plusieurs catégories. Or l'interface-formule peut être utilisé dans plusieurs circonstance, notamment pour ajouter plusieurs variables de différents types: on parlera de régression multivariée.

La plupart des modèles statistiques peuvent être approximés comme une combinaison linéaire de variables: ce sont des modèles linéaires. Les modèles non-linéaires impliquent des stratégies computationnelles complexes qui rendent leur utilisation plus difficile à manœuvrer.

Un modèle linéaire univarié prendra la forme $y = \beta_0 + \beta_1 x + \epsilon$, où $\beta_0$ est l'intercept et $\beta_1$ est la pente et $\epsilon$ est l'erreur.

Vous verrez parfois la notation $\hat{y} = \beta_0 + \beta_1 x$. La notation avec le chapeau $\hat{y}$ exprime qu'il s'agit des valeurs générées par le modèle. En fait, $y = \hat{y} - \epsilon$.

#### Modèle linéaire univarié avec variable continue

Prenons les données [`lasrosas.corn`](https://rdrr.io/cran/agridat/man/lasrosas.corn.html) incluses dans le module `agridat`, où l'on retrouve le rendement d'une production de maïs à dose d'azote variable, en Argentine.

```{r}
library("agridat")
data("lasrosas.corn")
sample_n(lasrosas.corn, 10)
```

Ces données comprennent plusieurs variables. Prenons le rendement (`yield`) comme variable de sortie et, pour le moment, ne retenons que la dose d'azote (`nitro`) comme variable explicative: il s'agit d'une régression univariée. Les deux variables sont continuent. Explorons d'abord le nuage de points de l'une et l'autre.

```{r}
ggplot(data = lasrosas.corn, mapping = aes(x = nitro, y = yield)) +
    geom_point()
```

L'hypothèse nulle est que la dose d'azote n'affecte pas le rendement, c'est à dire que le coefficient de pente et nul. Une autre hypothèse est que l'intercept est nul: donc qu'à dose de 0, rendement de 0. Un modèle linéaire à variable de sortie continue est créé avec la fonction `lm()`, pour *linear model*.

```{r}
modlin_1 <- lm(yield ~ nitro, data = lasrosas.corn)
summary(modlin_1)
```

Le diagnostic du modèle comprend plusieurs informations. D'abord la formule utilisée, affichée pour la traçabilité. Viens ensuite un aperçu de la distribution des résidus. La médiane devrait s'approcher de la moyenne des résidus (qui est toujours de 0). Bien que le -3.079 peut sembler important, il faut prendre en considération de l'échelle de y, et ce -3.079 est exprimé en terme de rendement, ici en quintaux (i.e. 100 kg) par hectare. La distribution des résidus mérite d'être davantage investiguée. Nous verrons cela un peu plus tard.

Les coefficients apparaissent ensuite. Les estimés sont les valeurs des effets. R fournit aussi l'erreur standard associée, la valeur de t ainsi que la p-value (la probabilité d'obtenir cet effet ou un effet plus extrême si en réalité il y avait absence d'effet). L'intercept est bien sûr plus élevé que 0 (à dose nulle, on obtient 65.8 quintaux par hectare en moyenne). La pente de la variable `nitro` est de ~0.06: pour chaque augmentation d'un kg/ha de dose, on a obtenu ~0.06 quintaux/ha de plus de maïs. Donc pour 100 kg/ha de N, on a obtenu un rendement moyen de 6 quintaux de plus que l'intercept. Soulignons que l'ampleur du coefficient est très important pour guider la fertilisation: ne rapporter que la p-value, ou ne rapporter que le fait qu'elle est inférieure à 0.05 (ce qui arrive souvent dans la littérature), serait très insuffisant pour l'interprétation des statistiques. La p-value nous indique néanmoins qu'il serait très improbable qu'une telle pente ait été générée alors que celle-ci est nulle en réalité. Les étoiles à côté des p-values indiquent l'ampleur selon l'échelle `Signif. codes` indiquée en-dessous du tableau des coefficients.

Sous ce tableau, R offre d'autres statistiques. En outre, les R² et R² ajustés indiquent si la régression passe effectivement par les points. Le R² prend un maximum de 1 lorsque la droite passe exactement sur les points.

Enfin, le test de F génère une p-value indiquant la probabilité que les coefficients de pente ait été générés si les vrais coefficients étaient nuls. Dans le cas d'une régression univariée, cela répète l'information sur l'unique coefficient.

On pourra également obtenir les intervalles de confiance avec la fonction `confint()`.

```{r}
confint(modlin_1, level = 0.95)
```

Ou soutirer l'information de différentes manières, comme avec la fonction `coefficients()`.

```{r}
coefficients(modlin_1)
```

Également, on pourra exécuter le modèle sur les données qui ont servi à le générer:

```{r}
predict(modlin_1)[1:5]
```

Ou sur des données externes.

```{r}
nouvelles_donnees <- data.frame(nitro = seq(from = 0, to = 100, by = 5))
predict(modlin_1, newdata = nouvelles_donnees)[1:5]
```

#### Analyse des résidus

Les résidus sont les erreurs du modèle. C'est le vecteur $\epsilon$, qui est un décalage entre les données et le modèle. Le R² est un indicateur de l'ampleur du décalage, mais une régression linéaire explicative en bonne et due forme devrait être accompagnée d'une analyse des résidus. On peut les calculés par $\epsilon = y - \hat{y}$, mais aussi bien utiliser la fonction `residuals()`.

```{r}
res_df <- data.frame(nitro = lasrosas.corn$nitro,
                     residus_lm = residuals(modlin_1), 
                     residus_calcul = lasrosas.corn$yield - predict(modlin_1))
sample_n(res_df, 10)
```

Dans une bonne régression linéaire, on ne retrouvera pas de structure identifiable dans les résidus, c'est-à-dire que les résidus sont bien distribués de part et d'autre du modèle de régression.

```{r}
ggplot(res_df, aes(x = nitro, y = residus_lm)) +
  geom_point() +
  labs(x = "Dose N", y = "Résidus") +
  geom_hline(yintercept = 0, col = "red", size = 1)
```

Bien que le jugement soit subjectif, on peut dire avec confiance qu'il n'y a pas structure particulière. En revanche, on pourrait générer un $y$ qui varie de manière quadratique avec $x$, un modèle linéaire montrera une structure évidente.

```{r}
set.seed(36164)
x <- 0:100
y <- 10 + x*1 + x^2 * 0.05 + rnorm(length(x), 0, 50)
modlin_2 <- lm(y ~ x)
ggplot(data.frame(x, residus = residuals(modlin_2)),
       aes(x = x, y = residus)) +
  geom_point() +
  labs(x = "x", y = "Résidus") +
  geom_hline(yintercept = 0, col = "red", size = 1)
```

De même, les résidus ne devraient pas croître avec $x$.

```{r}
set.seed(3984)
x <- 0:100
y <-  10 + x + x * rnorm(length(x), 0, 2)
modlin_3 <- lm(y ~ x)
ggplot(data.frame(x, residus = residuals(modlin_3)),
       aes(x = x, y = residus)) +
  geom_point() +
  labs(x = "x", y = "Résidus") +
  geom_hline(yintercept = 0, col = "red", size = 1)
```

On pourra aussi inspecter les résidus avec un graphique de leur distribution. Reprenons notre modèle de rendement du maïs.

```{r}
ggplot(res_df, aes(x = residus_lm)) +
  geom_histogram(binwidth = 2, color = "white") +
  labs(x = "Residual")
```

L'histogramme devrait présenter une distribution normale. Les tests de normalité comme le test de Shapiro-Wilk peuvent aider, mais ils sont généralement très sévères.

```{r}
shapiro.test(res_df$residus_lm)
```

L'hypothèse nulle que la distribution est normale est rejetée au seuil 0.05. Dans notre cas, il est évident que la sévérité du test n'est pas en cause, car les résidus semble générer trois ensembles. Ceci indique que les variables explicatives sont insuffisantes pour expliquer la variabilité de la variable-réponse.

#### Régression multiple

Comme c'est le cas pour bien des phénomènes en écologie, le rendement d'une culture n'est certainement pas expliqué seulement par la dose d'azote.

Lorsque l'on combine plusieurs variables explicatives, on crée un modèle de régression multivariée, ou une régression multiple. Bien que les tendances puissent sembler non-linéaires, l'ajout de variables et le calcul des coefficients associés reste un problème d'algèbre linéaire.

On pourra en effet généraliser les modèles linéaires, univariés et multivariés, de la manière suivante.

$$ y = X \beta + \epsilon $$

où:

$X$ est la matrice du modèle à $n$ observations et $p$ variables.

$$ X = \left( \begin{matrix} 
1 & x_{11} & \cdots & x_{1p}  \\ 
1 & x_{21} & \cdots & x_{2p}  \\ 
\vdots & \vdots & \ddots & \vdots  \\ 
1 & x_{n1} & \cdots & x_{np}
\end{matrix} \right) $$

$\beta$ est la matrice des $p$ coefficients, $\beta_0$ étant l'intercept qui multiplie la première colonne de la matrice $X$.

$$ \beta = \left( \begin{matrix} 
\beta_0  \\ 
\beta_1  \\ 
\vdots \\ 
\beta_p 
\end{matrix} \right) $$

$\epsilon$ est l'erreur de chaque observation.

$$ \epsilon = \left( \begin{matrix} 
\epsilon_0  \\ 
\epsilon_1  \\ 
\vdots \\ 
\epsilon_n
\end{matrix} \right) $$

#### Modèles linéaires univariés avec variable catégorielle **nominale**

Une variable catégorielle nominale (non ordonnée) utilisée à elle seule dans un modèle comme variable explicative, est un cas particulier de régression multiple. En effet, l'**encodage catégoriel** (ou *dummyfication*) transforme une variable catégorielle nominale en une matrice de modèle comprenant une colonne désignant l'intercept (une série de 1) désignant la catégorie de référence, ainsi que des colonnes pour chacune des autres catégories désignant l'appartenance (1) ou la non appartenance (0) de la catégorie désignée par la colonne.

##### L'encodage catégoriel

Une variable à $C$ catégories pourra être déclinée en $C$ variables dont chaque colonne désigne par un 1 l'appartenance au groupe de la colonne et par un 0 la non-appartenance. Pour l'exemple, créons un vecteur désignant le cultivar de pomme de terre.

```{r}
data <- data.frame(cultivar = c('Superior', 'Superior', 'Superior', 'Russet', 'Kenebec', 'Russet'))
model.matrix(~cultivar, data)
```

Nous avons trois catégories, encodées en trois colonnes. La première colonne est un intercept et les deux autres décrivent l'absence (0) ou la présence (1) des cultivars Russet et Superior. Le cultivar Kenebec est absent du tableau. En effet, en partant du principe que l'appartenance à une catégorie est mutuellement exclusive, c'est-à-dire qu'un échantillon ne peut être assigné qu'à une seule catégorie, on peut déduire une catégorie à partir de l'information sur toutes les autres. Par exemple, si `cultivar_Russet` et `cultivar_Superior` sont toutes deux égales à $0$, on conclura que `cultivar_Kenebec` est nécessairement égal à $1$. Et si l'un d'entre `cultivar_Russet` et `cultivar_Superior` est égal à $1$, `cultivar_Kenebec` est nécessairement égal à $0$. L'information contenue dans un nombre $C$ de catégorie peut être encodée dans un nombre $C-1$ de colonnes. C'est pourquoi, dans une analyse statistique, on désignera une catégorie comme une référence, que l'on détecte lorsque toutes les autres catégories sont encodées avec des $0$: cette référence sera incluse dans l'intercept. La catégorie de référence par défaut en R est celle la première catégorie dans l'ordre alphabétique. On pourra modifier cette référence avec la fonction `relevel()`.

```{r}
data$cultivar <- relevel(data$cultivar, ref = "Superior")
model.matrix(~cultivar, data)
```

Pour certains modèles, vous devrez vous assurer vous-même de l'encodage catégoriel. Pour d'autre, en particulier avec l'*interface par formule* de R, ce sera fait automatiquement.

##### Exemple d'application

Prenons la topographie du terrain, qui peut prendre plusieurs niveaux.

```{r}
levels(lasrosas.corn$topo)
```

Explorons le rendement selon la topographie.

```{r}
ggplot(lasrosas.corn, aes(x = topo, y = yield)) +
    geom_boxplot()
```

Les différences sont évidentes, et la modélisation devrait montrer des effets significatifs.

L'encodage catégoriel peut être visualisé en générant la matrice de modèle avec la fonction `model.matrix()` et l'interface-formule - sans la variable-réponse.

```{r}
model.matrix(~ topo, data = lasrosas.corn) %>% 
    tbl_df() %>% # tbl_df pour transformer la matrice en tableau
    sample_n(10) 
```

Dans le cas d'un modèle avec une variable catégorielle nominale seule, l'intercept représente la catégorie de référence, ici `E`. Les autres colonnes spécifient l'appartenance (1) ou la non-appartenance (0) de la catégorie pour chaque observation.

Cette matrice de modèle utilisée pour la régression donnera un intercept, qui indiquera l'effet de la catégorie de référence, puis les différences entre les catégories subséquentes et la catégorie de référence.

```{r}
modlin_4 <- lm(yield ~ topo, data = lasrosas.corn)
summary(modlin_4)
```

Le modèle linéaire est équivalent à l'anova, mais les résultats de `lm` sont plus élaborés.

```{r}
summary(aov(yield ~ topo, data = lasrosas.corn))
```

L'analyse de résidus peut être effectuée de la même manière.

#### Modèles linéaires univariés avec variable catégorielle **ordinale**

Bien que j'introduise la régression sur variable catégorielle ordinale à la suite de la section sur les variables nominales, nous revenons dans ce cas à une régression simple, univariée. Voyons un cas à 5 niveaux.

```{r}
statut <- c("Totalement en désaccord", 
            "En désaccord",
            "Ni en accord, ni en désaccord",
            "En accord",
            "Totalement en accord")
statut_o <- factor(statut, levels = statut, ordered=TRUE)
model.matrix(~statut_o) # ou bien, sans passer par model.matrix, contr.poly(5) où 5 est le nombre de niveaux
```

La matrice de modèle a 5 colonnes, soit le nombre de niveaux: un intercept, puis 4 autres désignant différentes valeurs que peuvent prendre les niveaux. Ces niveaux croient-ils linéairement? De manière quadratique, cubique ou plus loin dans des distributions polynomiales?

```{r}
modmat_tidy <- data.frame(statut, model.matrix(~statut_o)[, -1]) %>%
    gather(variable, valeur, -statut)
modmat_tidy$statut <- factor(modmat_tidy$statut, 
                             levels = statut, 
                             ordered=TRUE)
ggplot(data = modmat_tidy, mapping = aes(x = statut, y = valeur)) +
    facet_wrap(. ~ variable) +
    geom_point() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Règle générale, pour les variables ordinales, on préférera une distribution linéaire, et c'est l'option par défaut de la fonction `lm()`. L'utilisation d'une autre distribution peut être effectuée à la mitaine en utilisant dans le modèle la colonne désirée de la sortie de la fonction `model.matrix()`.

#### Régression multiple à plusieurs variables

Reprenons le tableau de données du rendement de maïs.

```{r}
head(lasrosas.corn)
```

Pour ajouter des variables au modèle dans l'interface-formule, on additionne les noms de colonne. La variable `lat` désigne la latitude, la variable `long` désigne la latitude et la variable `bv` (*brightness value*) désigne la teneur en matière organique du sol (plus `bv` est élevée, plus faible est la teneur en matière organique).

```{r}
modlin_5 <- lm(yield ~ lat + long + nitro + topo + bv,
               data = lasrosas.corn)
summary(modlin_5)
```

L'ampleur des coefficients est relatif à l'échelle de la variable. En effet, un coefficient de 5541 sur la variable `lat` n'est pas comparable au coefficient de la variable `bv`, de -0.5089, étant donné que les variables ne sont pas exprimées avec la même échelle. Pour les comparer sur une même base, on peut centrer (soustraire la moyenne) et réduire (diviser par l'écart-type).

```{r}
scale_vec <- function(x) as.vector(scale(x)) # la fonction scale génère une matrice: nous désirons un vecteur

lasrosas.corn_sc <- lasrosas.corn %>%
    mutate_at(c("lat", "long", "nitro", "bv"), 
                 scale_vec)

modlin_5_sc <- lm(yield ~ lat + long + nitro + topo + bv,
               data = lasrosas.corn_sc)
summary(modlin_5_sc)
```

Typiquement, les variables catégorielles, qui ne sont pas mises à l'échelle, donneront des coefficients plus élevées, et devrons être évaluées entre elles et non comparativement aux variables mises à l'échelle. Une manière conviviale de représenter des coefficients consiste à créer un tableau (fonction `tibble()`) incluant les coefficients ainsi que leurs intervalles de confiance, puis à les porter graphiquement.

```{r}
intervals <- tibble(Estimate = coefficients(modlin_5_sc)[-1], # [-1] enlever l'intercept
                    LL = confint(modlin_5_sc)[-1, 1], # [-1, ] enlever la première ligne, celle de l'intercept
                    UL = confint(modlin_5_sc)[-1, 2],
                    variable = names(coefficients(modlin_5_sc)[-1])) 
intervals
```

```{r}
ggplot(data = intervals, mapping = aes(x = Estimate, y = variable)) +
    geom_vline(xintercept = 0, lty = 2) +
    geom_segment(mapping = aes(x = LL, xend = UL, 
                               y = variable, yend = variable)) +
    geom_point() +
    labs(x = "Coefficient standardisé", y = "")
```

On y voit qu'à l'exception de la variable `long`, tous les coefficients sont différents de 0. Le coefficient `bv` est négatif, indiquant que plus la valeur de `bv` est élevé (donc plus le sol est pauvre en matière organique), plus le rendement est faible. Plus la latitude est élevée (plus on se dirige vers le Nord de l'Argentine), plus le rendement est élevé. La dose d'azote a aussi un effet statistique positif sur le rendement.

Quant aux catégories topographiques, elles sont toutes différentes de la catégorie `E`, ne croisant pas le zéro. De plus, les intervalles de confiance ne se chevauchant pas, on peut conclure en une différence significative d'une à l'autre. Bien sûr, tout cela au seuil de confiance de 0.05.

On pourra retrouver des cas où l'effet combiné de plusieurs variables diffère de l'effet des deux variables prises séparément. Par exemple, on pourrait évaluer l'effet de l'azote et celui de la topographie dans un même modèle, puis y ajouter une interaction entre l'azote et la topographie, qui définira des effets supplémentaires de l'azote selon chaque catégorie topographique. C'est ce que l'on appelle une interaction.

Dans l'interface-formule, l’interaction entre l'azote et la topographie est notée `nitro:topo`. Pour ajouter cette interaction, la formule deviendra `yield ~ nitro + topo + nitro:topo`. Une approche équivalente est d'utiliser le raccourci `yield ~ nitro*topo`.

```{r}
modlin_5_sc <- lm(yield ~ nitro*topo,
               data = lasrosas.corn_sc)
summary(modlin_5_sc)
```

Les résultats montre des effets de l'azote et des catégories topographiques, mais il y a davantage d'incertitude sur les interactions, indiquant que l'effet statistique de l'azote est sensiblement le même indépendamment des niveaux topographiques.

----

#### Attention à ne pas surcharger le modèle

Il est possible d'ajouter des interactions doubles, triples, quadruples, etc. Mais plus il y a d’interactions, plus votre modèle comprendra de variables et vos tests d'hypothèse perdront en puissance statistique.

----

#### Les modèles linéaires généralisés

Dans un modèle linéaire ordinaire, un changement constant dans les variables explicatives résulte en un changement constant de la variable-réponse. Cette supposition ne serait pas adéquate si la variable-réponse était un décompte, si elle est booléenne ou si, de manière générale, la variable-réponse ne suivait pas une distribution continue. Ou, de manière plus spécifique, il n'y a pas moyen de retrouver une distribution normale des résidus? On pourra bien sûr transformer les variables (sujet du chapitre 6, en développement). Mais il pourrait s'avérer impossible, ou tout simplement non souhaitable de transformer les variables. Le modèle linéaire généralisé (MLG, ou *generalized linear model* - GLM) est une généralisation du modèle linéaire ordinaire chez qui la variable-réponse peut être caractérisé par une distribution de Poisson, de Bernouilli, etc.

Prenons d'abord cas d'un décompte de vers fil-de-fer (`worms`) retrouvés dans des parcelles sous différents traitements (`trt`). Les décomptes sont typiquement distribué selon une loi de Poisson.

```{r}
cochran.wireworms %>% ggplot(aes(x = worms)) + geom_histogram()
```

Explorons les décomptes selon les traitements.

```{r}
cochran.wireworms %>% ggplot(aes(x = trt, y = worms)) + geom_boxplot()
```

Les traitements semble à première vue avoir un effet comparativement au contrôle. Lançons un MLG avec la fonction `glm()`, et spécifions que la sortie est une distribution de Poisson.

```{r}
modglm_1 <- glm(worms ~ trt, cochran.wireworms, family = "poisson")
summary(modglm_1)
```

Il est très probable (p-value de ~0.66) qu'un intercept de 0.18 ayant une erreur standard de 0.4082 ait été généré depuis une population dont l'intercept est nul: autrement dit, le contrôle n'a probablement pas eu d'effet. Quant aux autres traitements, leurs effets sont tous significatifs au seuil 0.05, mais peuvent-ils être considérés comme équivalents?

```{r}
intervals <- tibble(Estimate = coefficients(modglm_1), # [-1] enlever l'intercept
                    LL = confint(modglm_1)[, 1], # [-1, ] enlever la première ligne, celle de l'intercept
                    UL = confint(modglm_1)[, 2],
                    variable = names(coefficients(modglm_1))) 
intervals
```

```{r}
ggplot(data = intervals, mapping = aes(x = Estimate, y = variable)) +
    geom_vline(xintercept = 0, lty = 2) +
    geom_segment(mapping = aes(x = LL, xend = UL, 
                               y = variable, yend = variable)) +
    geom_point() +
    labs(x = "Coefficient", y = "")
```

Les intervalles de confiance se superposant, on ne peut pas conclure qu'un traitement est lié à une réduction plus importante de vers qu'un autre, au seuil 0.05.

Maintenant, à défaut de trouver un tableau de données plus approprié, prenons le tableau `mtcars`, qui rassemble des données sur des modèles de voitures. La colonne `vs`, pour v-shaped, inscrit 0 si les pistons sont droit et 1 s'ils sont placés en V dans le moteur. Peut-on expliquer la forme des pistons selon le poids du véhicule (`wt`)?

```{r}
mtcars %>% sample_n(6)
```

```{r}
mtcars %>% 
    ggplot(aes(x = wt, y = vs)) + geom_point()
```

Il semble y avoir une tendance: les véhicules plus lourds ont plutôt des pistons droits (`vs = 0`). Vérifions cela.

```{r}
modglm_2 <- glm(vs ~ wt, data = mtcars, family = binomial)
summary(modglm_2)
```

**Exercice**. Analyser les résultats.

#### Les modèles non-linéaires

La hauteur d'un arbre en fonction du temps n'est typiquement pas linéaire. Elle tend à croître de plus en plus lentement jusqu'à un plateau. De même, le rendement d'une culture traité avec des doses croissantes de fertilisants tend à atteindre un maximum, puis à se stabiliser.

Ces phénomènes ne peuvent pas être approximés par des modèles linéaires. Examinons les données du tableau `engelstad.nitro`.

```{r}
engelstad.nitro %>% sample_n(10)
```

```{r}
engelstad.nitro %>%
    ggplot(aes(x = nitro, y = yield)) +
        facet_grid(year ~ loc) +
        geom_line() +
        geom_point()
```

Le modèle de Mitscherlich pourrait être utilisé.

$$ y = A \left( 1 - e^{-R \left( E + x \right)} \right) $$

où $y$ est le rendement, $x$ est la dose, $A$ est l'asymptote vers laquelle la courbe converge à dose croissante, $E$ est l'équivalent de dose fourni par l'environnement et $R$ est le taux de réponse.

Explorons la fonction.

```{r}
mitscherlich_f <- function(x, A, E, R) {
    A * (1 - exp(-R*(E + x)))
}

x <- seq(0, 350, by = 5)
y <- mitscherlich_f(x, A = 75, E = 30, R = 0.02)

ggplot(tibble(x, y), aes(x, y)) +
    geom_point(data = engelstad.nitro, aes(x = nitro, y = yield)) +
    geom_line() + ylim(c(0, 100))
```

**Exercice**. Changez les paramètres pour visualiser comment la courbe réagit.

Nous pouvons décrire le modèle grâce à l'interface formule dans la fonction `nls()`. Notez que les modèles non-linéaires demandent des stratégies de calcul différentes de celles des modèles linéaires. En tout temps, nous devons identifier des valeurs de départ raisonnables pour les paramètres dans l'argument `start`. Vous réussirez rarement à obtenir une convergence du premier coup avec vos paramètres de départ. Le défi est d'en trouver qui permettront au modèle de converger. Parfois, le modèle ne convergera jamais. D'autres fois, il convergera vers des solutions différentes selon les variables de départ choisies.
<
```{r}
#modnl_1 <- nls(yield ~ A * (1 - exp(-R*(E + nitro))),
#                data = engelstad.nitro,
#                start = list(A = 50, E = 10, R = 0.2))
```

Le modèle ne converge pas. Essayons les valeurs prises plus haut, lors de la création du graphique, qui semblent bien s'ajuster.

```{r}
modnl_1 <-  nls(yield ~ A * (1 - exp(-R*(E + nitro))),
                data = engelstad.nitro,
                start = list(A = 75, E = 30, R = 0.02))
```

Bingo! Voyons maintenant le sommaire.

```{r}
summary(modnl_1)
```

Les paramètres sont significativement différents de zéro au seuil 0.05, et donnent la courbe suivante.

```{r}
x <- seq(0, 350, by = 5)
y <- mitscherlich_f(x,
                    A = coefficients(modnl_1)[1],
                    E = coefficients(modnl_1)[2],
                    R = coefficients(modnl_1)[3])

ggplot(tibble(x, y), aes(x, y)) +
    geom_point(data = engelstad.nitro, aes(x = nitro, y = yield)) +
    geom_line() + ylim(c(0, 100))
```

Et les résidus...

```{r}
tibble(res = residuals(modnl_1)) %>%
    ggplot(aes(x = res)) + geom_histogram(bins = 20)
```

```{r}
tibble(nitro = engelstad.nitro$nitro, res = residuals(modnl_1)) %>%
    ggplot(aes(x = nitro, y = res)) + 
        geom_point() +
        geom_hline(yintercept = 0, colour = "red")
```

Les résidus ne sont pas distribués normalement, mais semble bien partagés de part et d'autre de la courbe.

### Modèles à effets mixtes

Lorsque l'on combine des variables fixes (testées lors de l'expérience) et des variables aléatoire (variation des unités expérimentales), on obtient un modèle mixte. Les modèles mixtes peuvent être univariés, multivariés, linéaires ordinaires ou généralisés ou non linéaires.

À la différence d'un effet fixe, un effet aléatoire sera toujours distribué normalement avec une moyenne de 0 et une certaine variance. Dans un modèle linéaire où l'effet aléatoire est un décalage d'intercept, cet effet s'additionne aux effets fixes:

$$ y = X \beta + Z b + \epsilon $$

où:

$Z$ est la matrice du modèle à $n$ observations et $p$ variables aléatoires. Les variables aléatoires sont souvent des variables nominales qui subissent un encodage catégoriel.

$$ Z = \left( \begin{matrix} 
z_{11} & \cdots & z_{1p}  \\ 
z_{21} & \cdots & z_{2p}  \\ 
\vdots & \ddots & \vdots  \\ 
z_{n1} & \cdots & z_{np}
\end{matrix} \right) $$

$b$ est la matrice des $p$ coefficients aléatoires.

$$ b = \left( \begin{matrix} 
b_0  \\ 
b_1  \\ 
\vdots \\ 
b_p 
\end{matrix} \right) $$

Le tableau `lasrosas.corn`, utilisé précédemment, contenait trois répétitions effectués au cours de deux années, 1999 et 2001. Étant donné que la répétition R1 de 1999 n'a rien à voir avec la répétition R1 de 2001, on dit qu'elle est **emboîtée** dans l'année.

Le module `nlme` nous aidera à monter notre modèle mixte.

```{r}
library("nlme")

mmodlin_1 <- lme(fixed = yield ~ lat + long + nitro + topo + bv,
                 random = ~ 1|year/rep,
                 data = lasrosas.corn)

```

À ce stade vous devriez commencer à être familier avec l'interface formule et vous deviez saisir l'argument `fixed`, qui désigne l'effet fixe. L'effet aléatoire, `random`, suit un tilde `~`. À gauche de la barre verticale `|`, on place les variables désignant les effets aléatoire sur la pente. Nous n'avons pas couvert cet aspect, alors nous le laissons à `1`. À droite, on retrouve un structure d'emboîtement désignant l'effet aléatoire: le premier niveau est l'année, dans laquelle est emboîtée la répétition.

```{r}
summary(mmodlin_1)
```

La sortie est semblable à celle de la fonction `lm()`.

#### Modèles mixtes non-linéaires

Le modèle non linéaire créé plus haut liait le rendement à la dose d'azote. Toutefois, les unités expérimentales (le site `loc` et l'année `year`) n'étaient pas pris en considération. Nous allons maintenant les considérer. 

Nous devons décider la structure de l'effet aléatoire, et sur quelles variables il doit être appliqué - la décision appartient à l'analyste. Il me semble plus convenable de supposer que le site et l'année affectera le rendement maximum plutôt que l'environnement et le taux: les effets aléatoires seront donc affectés à la variable `A`. Les effets aléatoires n'ont pas de structure d'emboîtement. L'effet de l'année sur A sera celui d'une pente et l'effet de site sera celui de l'intercept. La fonction que nous utiliserons est `nlme()`.

```{r}
mm <- nlme(yield ~ A * (1 - exp(-R*(E + nitro))),
           data = engelstad.nitro, 
           start = c(A = 75, E = 30, R = 0.02), 
           fixed = list(A ~ 1, E ~ 1, R ~ 1), 
           random = A ~ year | loc)
summary(mm)
```

Les modèles mixtes non linéaires peuvent devenir très complexes lorsque les paramètres, par exemple A, E et R, sont eux-même affectés linéairement par des variables (par exemple `A ~ topo`). Pour aller plus loin, consultez [Parent et al. (2017) ](https://doi.org/10.3389/fenvs.2017.00081) ainsi que les [calculs associés à l'article](https://github.com/essicolo/site-specific-multilevel-modeling-of-potato-response-to-nitrogen-fertilization). Ou écrivez-moi un courriel pour en discuter!

**Note**. L'interprétation de p-values sur les modèles mixtes est controversée. À ce sujet, ??? Bates a écrit une longue lettre à la communauté de développement du module `lme4`, une alternative à `nlme`, qui remet en cause l'utilisation des p-values, [ici](https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html). De plus en plus, pour les modèles mixtes, on se tourne vers les statistiques bayésiennes, couvertes dans l'annexe de cette section (`statitiques_bayes.ipynb`, en développement). À cet effet, le module [`brms`](https://github.com/paul-buerkner/brms) automatise bien des aspects de la modélisation mixte bayésienne.

### Aller plus loin

#### Statistiques générales:
- [The analysis of biological data](https://ariane25.bibl.ulaval.ca/ariane/wicket/detail?c=ariane&m=S&rq.ct=PE&rq.fa=false&rq.r.esc=false&rq.r.l%5B0%5D.c=TI&rq.r.l%5B0%5D.ex=false&rq.r.l%5B0%5D.op=AND&rq.r.l%5B0%5D.v=the+analysis+of+biological+data&rq.r.la=*&rq.r.loc=*&rq.r.pft=true&rq.r.ta=*&rq.r.td=*&rq.rows=5&rq.st=1)

#### Statistiques avec R

- Disponibles en version électronique à la bibliothèque de l'Université Laval:
    - Introduction aux statistiques avec R: [Introductory statistics with R](https://ariane25.bibl.ulaval.ca/ariane/wicket/detail?c=ariane&m=S&rq.ct=PE&rq.fa=false&rq.r.esc=false&rq.r.l%5B0%5D.c=TI&rq.r.l%5B0%5D.ex=false&rq.r.l%5B0%5D.op=AND&rq.r.l%5B0%5D.v=Introductory+statistics+with+R&rq.r.la=*&rq.r.loc=*&rq.r.pft=true&rq.r.ta=*&rq.r.td=*&rq.rows=1&rq.st=0)
    - Approfondir les statistiques avec R: [The R Book, Second edition](https://ariane25.bibl.ulaval.ca/ariane/wicket/detail?c=ariane&m=S&rq.ct=PE&rq.fa=false&rq.r.esc=false&rq.r.l%5B0%5D.c=TI&rq.r.l%5B0%5D.ex=false&rq.r.l%5B0%5D.op=AND&rq.r.l%5B0%5D.v=the+r+book&rq.r.la=*&rq.r.loc=*&rq.r.pft=true&rq.r.ta=*&rq.r.td=*&rq.rows=15&rq.st=2)
    - Approfondir les modèles à effets mixtes avec R: [Mixed Effects Models and Extensions in Ecology with R](https://ariane25.bibl.ulaval.ca/ariane/wicket/detail?c=ariane&m=S&rq.ct=PE&rq.fa=false&rq.r.esc=false&rq.r.l%5B0%5D.c=TI&rq.r.l%5B0%5D.ex=false&rq.r.l%5B0%5D.op=AND&rq.r.l%5B0%5D.v=Mixed+Effects+Models+and+Extensions+in+Ecology+with+R&rq.r.la=*&rq.r.loc=*&rq.r.pft=false&rq.r.ta=*&rq.r.td=*&rq.rows=2&rq.st=1)
- [ModernDive](https://moderndive.com/index.html), un livre en ligne offrant une approche moderne avec le package `moderndive`.

<!--chapter:end:05_biostats.Rmd-->

---
title: "Prétraitement"
author: "Serge-Étienne Parent"
date: "`r format(Sys.Date())`"
output: github_document
---

# Explorer R {#chapitre-explorer}

L'apprentissage de R peut être étourdissant. Cette section est une petite pause fourre-tout qui vous introduira aux nombreuses possibilités de R.

 ***
️\ **Objectifs spécifiques**:

À la fin de ce chapitre, vous

- serez en mesure d'identifier les sources d'information principales sur le développement de R et de ses modules
- comprendrez l'importance du prétraitement des données, en particulier dans le cadre de l'analyse de données compositionnelles, et saurez effectuer un prétraitement adéquat
- saurez comment acquérir des données météo d'Environnement Canada avec le module [weathercan](http://ropensci.github.io/weathercan/)
- saurez identifier les modules d'analyse de sols ([soiltexture](https://github.com/julienmoeys/soiltexture) et [aqp](http://ncss-tech.github.io/aqp/docs/))
- saurez créer des cartes
- etc.

 ***

Pour certains, le langage R est un labyrinthe. Pour d'autres, c'est une myriade de portes ouvertes. Si vous lisez ce manuel, vous vous êtes peut-être engagé dans un labyrinthe dans l'objectif d'y trouver la clé qui dévérouillera une porte bien précise qui mène à un trésor, un objet magique... ou un diplôme. Peut-être aussi prendrez-vous le goût d'errer dans ce labyrinthe, explorant ses débouchés, pour y dénicher au hasard des petits outils et des débouchés.

![](images/06_zelda.gif)

Séquence du jeu vidéo *The legend of Zelda*.

Cette section est un amalgame de plusieurs outils de R pertinents en analyse écologique.

## R sur le web

Dans un environnement de travail en évolution rapide et constante, il est difficile de considérer ses compétences comme étant abouties. Rester informé sur le développement de R vous permettra de dénicher de résoudre des problèmes persistants de manière plus efficace ou par de nouvelles avenues, et vous offrira même l'occasion de dénicher des problèmes dont vous ne soupçonniez pas l'existance.  Plusieurs sources d'information vous permettront de vous tenir à jour sur le développement de R, de ses environnement de travail (RStudio, Jupyter, Atom, etc.) et des nouveaux modules qui s'y greffent. Plus largement, vous gagnerez à vous informer sur les dernières tendances en calcul scientifique sur d'autres plate-forme que R (Python, Javascript, Julia, etc.). Évidemment, nos tâches quotidiennes ne nous permettent pas de tout suivre. Même si vous pouviez n'attrapper qu'1% du défilement, ce sera déjà 1% de plus que rien du tout.

Je vous propose une liste de ressources. Ne vous y tenez surtout pas: discartez ce qui ne vous convient pas, et partez à l'aventure!

![](images/06_hobbit.gif)

The Hobbit: An Unexpected Journey, Peter Jackson (2012)

### GitHub

Nous verrons au chapitre \@ref(chapitre-git) l'importane d'utilser des outils d'archivage et de suivi de version, comme *git*, dans le déploiement de la *science ouverte*. Pour l'instant, retenons que [GitHub](https://www.github.com) est une plate-forme *git* en ligne acquise par Microsoft qui est devenue un réseau social de développement informatique. [De nombreux modules de R y sont développés](https://github.com/topics/r). Au chapitre \@ref(chapitre-git), vous serez invités à y ouvrir un compte et à y archiver du contenu. Vous pourrez alors suivre le développement de projets et suivre les travaux des personnes qui vous semblent d'intérêt.

### Nouvelles
Le site d'aggrégation [R-bloggers](https://www.r-bloggers.com/), mis à jour quotidiennement, republie des articles en anglais tirés d'un peu partout sur la toile. On y trouve principalement des tutoriels et des annonces de nouveaux développement. Deux fois par mois, l'organisation [rOpenSci](https://news.ropensci.org/) offre un portrait de l'univ-R, ce que [R Weekely](https://rweekly.org/) offre de manière hebdomadaire (l'information sera probablement redondante). Le tidyverse a quant à lui son propre [blogue](https://www.tidyverse.org/articles/).

### Twitter
Le *hashtag* `#rstats` rassemble sur [Twitter](https://twitter.com/hashtag/rstats?src=hash) ce qui se tweete sur le sujet. On y retrouve les comptes de [R-bloggers](https://twitter.com/Rbloggers), [RStudio](https://twitter.com/rstudio) et [rOpenSci](https://twitter.com/rOpenSci). Certaines communauté y sont aussi actives, comme [R4DS online learning community](https://twitter.com/R4DScommunity), qui partage des nouvelles sur R, et [R-Ladies Global](https://twitter.com/RLadiesGlobal), qui vise à amener davantage de diversité à la communauté de R. Des comptes thématiques comme [Daily R Cheatsheets](https://twitter.com/daily_r_sheets) et [One R Package a Day](https://twitter.com/RLangPackage) permettent de découvrir quotidiennement de nouvelles possibilités. Enfin, plusieurs personnes contribuent positivement à la communauté R. [Hadley Wickham](https://twitter.com/hadleywickham) brille parmi les étoiles de R. Les comptes de [Mara Averick](https://twitter.com/dataandme), [Claus Wilke](https://twitter.com/ClausWilke) et [David Robinson](https://twitter.com/drob) sont aussi intéressants.

### Des questions?

Bien que davantage voués à la résolution de problème qu' à l'exploration de nouvelles opportunités, [Stackoverflow](https://stackoverflow.com/questions/tagged/r) et [Cross Validated](https://stats.stackexchange.com/questions/tagged/r) sont des plate-forme prisées. De plus, la liste de courriels [r-sig-ecology](https://www.mail-archive.com/r-sig-ecology@r-project.org/info.html) permet des échanges entre professionnels et novices en analyse de données écologiques avec R.

### Mise en garde

Les modules de R sont développés par quiconque le veut bien: leur qualité n'est pas nécessairement auditée. Souvent, ils ne sont vérifiés que par une vigilance communautaire: dans ce cas, vous êtes les cobailles. Ce qui n'est pas nécessairement une mauvaise chose, mais cela nécessite de prendre ses précautions. Dans sa conférence [How to be a resilient R user](https://maelle.github.io/fluctuat_nec_mergitur), [Maëlle Salmon](https://twitter.com/ma_salmon) propose quelques guides pour juger de la qualité d'un module.

**1. Le module est-il activement développé?**

Bien!

![](images/06_2019-01-14-facebook-prophet.png)

Attention!

![](images/06_2019-01-14_mlammens_meteR.png)

**2. Le module est-il bien testé?**

Vérifiez si le module a fait l'objet d'une publication scientifique, s'il a été utilisé avec succès dans la litérature ou dans des documents crédibles.

**3. Le module est-il bien documenté?**

Un site internet dédié est-il utilisé pour documenter l'utilisation du module? Les fichiers d'aide sont-ils complets, et sont-ils de bonne qualité?

**4. Le module est-il largement utilisé?**

Un module peu populaire n'est pas nécessaissairement de mauvaise qualité: peut-être est-il seulement destiné à des applications de niche. S'il n'est pas un indicateur à lui seul de la solidité ou la validité d'un module, une masse critique indique que le module a passé sous la surveillance de plusieurs utilisateurs. Dans GitHub, ceci peu être évalué par le nombre d'étoiles attribué au module (équivalent à un J'aime).

![](images/06_peu-etoiles.png) ![](images/06_bcp-etoiles.png)

**5. Le module est-il développé par une personne ou une organisation crédible?**

On peut affirmer sans trop se compromettre que l'équipe de RStudio développe des modules de confiance. Tout comme il faudrait se méfier d'un module développé par une personne anonyme.

Le module [packagemetrics](https://github.com/ropenscilabs/packagemetrics) permet d'évaluer ces critères.

```{r}
library("packagemetrics")
pm <- package_list_metrics(c("dplyr", "ggplot2", "vegan", "greta"))
metrics_table(pm)
```

### Prendre tout ça en note

Un logiciel de prise de notes (comme [Evernote](http://evernote.com/), [OneNote](http://onenote.com/), [Notion](http://notion.so), [Simplenote](https://simplenote.com), [Turtl](https://turtlapp.com/), etc.) pourrait vous être utile pour retrouver l'information soutirée de vos flux d'information.

## Quelques outils en écologie mathématique avec R

### Prétraitement des données

Il arrive souvent ques les données brutes ne soient pas exprimées de manière appropriée ou optimale pour l'analyse statistique ou la modélisation. Vous devrez alors effectuer un prétraitement sur ces données. En particulier, si vos données forment une partie d'un tout (exprimées en pourcentages ou fractions), vous devriez probablement utiliser les outils de l'**analyse compositionnelle**. Avant de les aborder, nous allons traiter des transformations de base.

#### Standardisation

La standardisation consiste à centrer vos données à une moyenne de 0 et à les échelonner à une variance de 1, c'est-à-dire

$$x_{standard} = \frac{x - \bar{x}}{\sigma}$$

où $\bar{x}$ est la moyenne du vecteur $x$ et où $\sigma$ est son écart-type.

Ce prétraitement des données peut s'avérér utile lorsque la modélisation tient compte de l'échelle de vos mesures (par exemple, les paramètres de régression vus au chapitre \@ref(chapitre-biostats) ou les distances que nous verrons au chapitre \@ref(chapitre-ordination)). En effet, les pentes d'une régression linéaire multiple ne pourront être comparées entre elles que si elles sont une même échelle. Par exemple, on veut modéliser la consommation en miles au gallon (`mpg`) de voitures en fonction de leur puissance (`hp`), le temps en secondes pour parcourir un quart de mile (`qsec`) et le nombre de cylindre.

```{r}
data(mtcars)
modl <- lm(mpg ~ hp + qsec + cyl, mtcars)
summary(modl)
```

Les pentes signifie que la distance parcourue par gallon d'essence diminue de 0.03552 miles au gallon pour chaque HP, de 0.89242 par seconde au quart de mile et de 2.2696 par cyclindre additionnel. L'interprétation est conviviale à cette échelle. Mais lequel de ces effets est le plus important? L `t value` indique que ce seraient les cylindres. Mais pour juger l'importance en terme de pente, il vaudrait mieux standardiser.

```{r}
library("tidyverse")
mtcars_sc <- mtcars %>%
  apply(., 2, function(x) (x-mean(x))/sd(x)) %>% 
  as_tibble() # ou bien scale(mtcars, center = TRUE, scale = TRUE)
modl_sc <- lm(mpg ~ hp + qsec + cyl, mtcars_sc)
summary(modl_sc)
```

Les valeurs des pentes ne peuvent plus être interprétées directement, mais peuvent maintenant être comparées entre elles. Dans ce cas, le nombre de cilyndres a en effet une importance plus grande que la puissance et le temps pour parcourir un 1/4 de mile.

Les algorithmes basés sur des distances auront, de même, avantage à être standardisés.

#### Normalisation 

#### Analyse compositionnelle

### Acquérir des données météo

### Analyse de sols

### Cartographier

<!--chapter:end:06_pretraitement.Rmd-->

---
title: "Association, partitionnement et ordination"
author: "Serge-Étienne Parent"
date: "`r format(Sys.Date())`"
output: github_document
---

# Association, partitionnement et ordination {#chapitre-ordination}

 ***
️\ **Objectifs spécifiques**:

À la fin de ce chapitre, vous

- serez en mesure d'effectuer des calculs permettant de mesurer des différence entre des observations, des groupes d'observation ou des variables observées
- serez en mesure d'effection des analyses de partitionnement hiérarchiques et non-hiérarchiques
- serez en mesure d'effectuer des calculs d'ordination à l'aide des techniques de réduction d'axe communes: analyse en composante principale, analyse discriminante linéaire, l'analyse de correspondance, l'analyse factorielle, l'analyse en coordonnées principales et l'analyse de redondance.

 ***

Introduction...

```{r}
library("tidyverse")
```


## Espaces d'analyse

### Abondance et occurence

L'abondance est le décompte d'espèces observées, tandis que l'occurence est la présence ou l'absence d'une espèce. Le tableau suivant contient des données d'abondance.

```{r}
abundance <- tibble('Bruant familier' = c(1, 0, 0, 3),
                        'Citelle à poitrine rousse' = c(1, 0, 0, 0),
                        'Colibri à gorge rubis' = c(0, 1, 0, 0),
                        'Geai bleu' = c(3, 2, 0, 0),
                        'Bruant chanteur' = c(1, 0, 5, 2),
                        'Chardonneret' = c(0, 9, 6, 0),
                        'Bruant à gorge blanche' = c(1, 0, 0, 0),
                        'Mésange à tête noire' = c(20, 1, 1, 0),
                        'Jaseur boréal' = c(66, 0, 0, 0))
```

Ce tableau peut être rapidement transformé en données d'occurence, qui ne comprennent que l'information booléenne de présence (noté 1) et d'absence (noté 0).

```{r}
occurence <- abundance %>%
  transmute_all(funs(if_else(. > 0, 1, 0)))
occurence
```

L'**espace des espèces** (ou des variables ou descripteurs) est celui où les espèces forment les axes et où les sites sont positionnés dans cet espace. Il s'agit d'une perspective en *mode R*, qui permet principalement d'identifier quels espèces se retrouvent plus courrament ensemble.


```{r}
library("scatterplot3d")
species <- c("Bruant chanteur", "Chardonneret", "Mésange à tête noire")
x <- abundance %>% pull(species[1])
y <- abundance %>% pull(species[2])
z <- abundance %>% pull(species[3])
scatterplot3d(x, y, z, angle = 20, asp = 0.3,
              xlab = species[1], ylab = species[2], zlab = species[3])
```

Dans l'**espace des sites** (ou les échantillons ou objets), on transpose la matrice d'abondance. On passe ici en *mode Q*, où chaque point est une espèce, et où l'on peut observer quels échantillons sont similaires.

```{r}
site1 <- t(abundance)[, 1]
site2 <- t(abundance)[, 2]
site3 <- t(abundance)[, 3]
scatterplot3d(site1, site2, site3, angle = 20, asp = 10,
              xlab = "Site 1", ylab = "Site 2", zlab = "Site 3")
```

### Environnement

L'**espace de l'environnement** comprend souvent un autre tableau contenant l'information sur l'environnement où se trouve les espèces: les coordonnées et l'élévation, la pente, le pH du sol, la pluviométrie, etc.

## Analyse d'association

Nous utiliserons le terme *association* come une **mesure pour quantifier la ressemblance ou la différence entre deux objets (échantillons) ou variables (descripteurs)**.

Alors que la corrélation et la covariance sont des mesures d'association entre des variables (analyse en *mode R*), la **similarité** et la **distance** sont deux types de une mesure d'association entre des objets (analyse en *mode Q*). Une distance de 0 est mesuré chez deux objets identiques. La distance augmente au fur et à mesure que les objets sont dissociés. Une similarité ayant une valeur de 0 indique aucune association, tandis qu'une valeur de 1 indique une association parfaite. À l'opposé, la dissimilarité est égale à 1-similarité.

La distance peut être liée à la similarité par la relation:

$$distance=\sqrt{1-similarité}$$

ou

$$distance=\sqrt{dissimilarité}$$

La racine carrée permet, pour certains indices de similarité, d'obtenir des propriétés euclédiennes. Pour plus de détails, voyez le tableau 7.2 de [Legendre et Legendre (2012)](https://www.elsevier.com/books/numerical-ecology/legendre/978-0-444-53868-0).

Les matrices d'association sont généralement présentées comme des matrices carrées, dont les dimensions sont égales au nombre d'objets (*mode Q*) ou de vrariables (*mode R*) dans le tableau. Chaque élément ("cellule") de la matrice est un indice d'association entre un objet (ou une variable) et un autre. Ainsi, la diagonale de la matrice est un vecteur nul (distance ou dissimilarité) ou unitaire (similarité), car elle correspond à l'association entre un objet et lui-même. 

Puisque l'association entre A et B est la même qu'entre B et A, et puisque la diagonale retourne une valeur convenue, il est possible d'exprimer une matrice d'association en mode "compact", sous forme de vecteur. Le vecteur d'association entre des objets A, B et C contiendra toute l'information nécessaire en un vecteur de trois chiffres, `[AB, AC, BC]`, plutôt qu'une matrice de dimension $3 \times 3$. L'impact sur la mémoire vive peut être considérable pour les calculs comprenant de nombreuses dimensions.

En R, les calculs de similarité et de distances peuvent être effectués avec le module `vegan`. La fonction `vegdist` permet de calculer les indices d'association en forme carrée.

Nous verons plus tard les méthodes de mesure de similarité et de distance plus loin. Pour l'instant, utilisons la méthode de *Jaccard* pour une démonstration sur des données d'occurence.

```{r}
library("vegan")
vegdist(occurence, method = "jaccard",
        diag = TRUE, upper = TRUE)
```

Remarquez que `vegdist` retourne une matrice dont la diagonale est de 0 (on l'affiche en spécifiant `diag = TRUE`). La diagonale est l'association d'un objet avec lui-même. Or la similarité d'un objet avec lui-même devrait être de 1! En fait, par convention `vegdist` retourne des dissimilarités, non pas des similarités. La matrice de distance serait donc calculée en extrayant la racine carrée des éléments de la matrice de dissimilarité:

```{r}
dissimilarity <- vegdist(occurence, method = "jaccard",
                        diag = TRUE, upper = TRUE)
distance <- sqrt(dissimilarity)
distance
```

Dans le chapitre sur l'analyse compositionnelle, nous avons abordé les significations différentes que peuvent prendre le zéro. L'information fournie par un zéro peut être différente selon les circonstances. Dans le cas d'une variable continue, un zéro signifie généralement une mesure sous le seuil de détection. Deux tissus dont la concentration en cuivre est nulle ont une afinité sous la perspective de la concentration en cuivre. Dans le cas de mesures d'abondance (décompte) ou d'occurence (présence-absence), on pourra décrire comme similaires deux niches écologiques où l'on retrouve une espèce en particulier. Mais deux sites où l'on de retouve pas d'ours polaires ne correspondent pas nécessairement à des niches similaires! En effet, il peut exister de nombreuses raisons écologiques et méthodologiques pour lesquelles l'espèces ou les espèces n'ont pas été observées. C'est le problème des **double-zéros** (espèces non observées à deux sites), problème qui est amplifié avec les grilles comprenant des espèces rares.

La ressemblance entre des objets comprenant des données continues devrait être calculée grâce à des indicateurs *symétriques*. Inversement, les affinités entre les objets décrits par des données d'abondance ou d'occurence susceptibles de générer des problèmes de double-zéros devraient être évaluées grâce à des indicateurs *asymétriques*. Un défi supplémentaire arrive lorsque les données sont de type mixte.

Nous utiliserons la convention de `scipy` et nous calculerons la dissimilarité, non pas la similarité. Les mesures de dissimilarité sont calculées sur des données d'abondance ou des données d'occurence. Notons qu'il existe beaucoup de confusion dans la littérature sur la manière de nommer les dissimilarités (ce qui n'est pas le cas des distances, dont les noms sont reconnus). Dans les sections suivantes, nous noterons la dissimilarité avec un $d$ minuscule et la distance avec un $D$ majuscule.

### Association entre objets (mode Q)

#### Objets: Abondance

La **dissimilarité de Bray-Curtis** est asymétrique. Elle est aussi appelée l'indice de Steinhaus, de Czekanowski ou de Sørensen. Il est important de s'assurer de bien s'entendre la méthode à laquelle on fait référence. L'équation enlève toute ambiguité. La dissimilarité de Bray-Curtis entre les points A et B est calculée comme suit.

$$d_{AB} =  \frac {\sum \left| A_{i} - B_{i} \right| }{\sum \left(A_{i}+B_{i}\right)}$$

Utilisons `vegdist` pour générer les matrices d'association. Le format "liste" de R est pratique pour enregistrer la collection d'objets, dont les matrice d'association que nous allons créer dans cette section.

```{r}
associations_abund <- list()
associations_abund[['BrayCurtis']] <- vegdist(abundance, method = "bray")
associations_abund[['BrayCurtis']]
```

La dissimilarité de Bray-Curtis est souvent utilisée dans la littérature. Toutefois, la version originale de Bray-Curtis n'est pas tout à fait métrique (semimétrique). Conséquemment, la **dissimilarité de Ruzicka** (une variante de la dissimilarité de Jaccard pour les données d'abondance) est métrique, et devrait probablement être préféré à Bary-Curtis ([Oksanen, 2006](http://ocw.um.es/ciencias/geobotanica/otros-recursos-1/documentos/vegantutorial.pdf)).

$$d_{AB, Ruzicka} =  \frac { 2 \times d_{AB, Bray-Curtis} }{1 + d_{AB, Bray-Curtis}}$$

```{r}
associations_abund[['Ruzicka']] <- associations_abund[['BrayCurtis']] * 2 / (1 + associations_abund[['BrayCurtis']])
```

La **dissimilarité de Kulczynski** (aussi écrit Kulsinski) est asymétrique et semimétrique, tout comme celle de Bray-Curtis. Elle est calculée comme suit.

$$d_{AB} = 1-\frac{1}{2} \times \left[ \frac{\sum min(A_i, B_i)}{\sum A_i} + \frac{\sum min(A_i, B_i)}{\sum B_i} \right]$$

```{r}
associations_abund[['Kulczynski']] <- vegdist(abundance, method = "kulczynski")
```

Une approche commune pour mesurer l'association entre sites décrits par des données d'abondance est la **distance de Hellinger**. Notez qu'il s'agit ici d'une distance, non pas d'une dissimilarité. Pour l'obtenir, on doit d'abord diviser chaque donnée d'abondance par l'abondance totale pour chaque site pour obtenir les espèces en tant que proportions, puis on extrait la racine carrée de chaque élément. Enfin, on calcule la distance euclidienne entre les proportions de chaque site. Pour rappel, une distance euclidienne est la généralisation en plusieurs dimensions du théorème de Pythagore, $c = \sqrt{a^2 + b^2}$.

$$D_{AB} = \sqrt {\sum \left( \frac{A_i}{\sum A_i} - \frac{B_i}{\sum B_i} \right)^2}$$

------------------ -----------------------------------------------
😱\ **Attention**   La distance d'Hellinger hérite des biais liées aux données compositionnelles. Elle peut être substitiée par une matrice de distances d'Aitchison.

------------------------------------------------------------------

```{r}
associations_abund[['Hellinger']] <- dist(decostand(abundance, method="hellinger"))
```

Toute comme la distance d'Hellinger, la **distance de chord** est calculée par une distance euclidienne sur des données d'abondance transformées de sorte que chaque ligne ait une longueur (norme) de 1.

```{r}
associations_abund[['Chord']] <- dist(decostand(abundance, method="normalize"))
```

La **métrique du chi-carré**, ou $\chi$-carré, ou chi-square, donne davantage de poids aux espèces rares qu'aux espèces communes. Son utilisation est recommandée lorsque les espèces rares sont de bons indicateurs de conditions écologiques particulières ([Legendre et Legendre, 2012](https://www.elsevier.com/books/numerical-ecology/legendre/978-0-444-53868-0), p. 308).

$$  d_{AB} = \sqrt{\sum _j \frac{1}{\sum y_j} \left( \frac{A_j}{\sum A} - \frac{B_j}{\sum B} \right)^2 }  $$

La métrique peut être transformée en distance en la multipliant par la racine carrée de la somme totale des espèces dans la matric d'abondance ($X$).

$$ D_{AB} = \sqrt{\sum X} \times d_{AB} $$


```{r}
associations_abund[['ChiSquare']] <- dist(decostand(abundance, method="chi.square"))
```

Une mannière visuellement plus intéressante de présenter une matrice d'association est un graphique de type *heatmap*.

```{r}
associations_abund_df <- list()

for (i in 1:length(associations_abund)) {
  associations_abund_df[[i]] <- data.frame(as.matrix(associations_abund[[i]]))
  colnames(associations_abund_df[[i]]) <- rownames(associations_abund_df[[i]])
  associations_abund_df[[i]]$row <- rownames(associations_abund_df[[i]])
  associations_abund_df[[i]] <- associations_abund_df[[i]] %>% gather(key=row)
  associations_abund_df[[i]]$column = rep(1:4, 4)
  associations_abund_df[[i]]$dist <- names(associations_abund)[i]
}
associations_abund_df <- do.call(rbind, associations_abund_df)

ggplot(associations_abund_df, aes(x=row, y=column)) +
  facet_wrap(. ~ dist, nrow = 2) +
  geom_tile(aes(fill = value)) +
  geom_text(aes(label = round(value, 2))) +
  scale_fill_gradient2(low = "#00ccff", mid = "#aad400", high = "#ff0066", midpoint = 2) +
  labs(x="Site", y="Site")
```

Peu importe le type d'association utilisée, les *heatmaps* montrent les mêmes tendances. Les assocaitions de dissimilarité (Bray-Curtis, Kulczynski et Ruzicka) s'étalent de 0 à 1, tandis que les distances (Chi-Square, Chord et Hellinger) partent de zéro, mais n'ont pas de limite supérieure. On note les plus grandes différences entre les sites 2 et 4, tandis que les sites 2 et 3 sont les plus semblables pour toutes les mesures d'association à l'exception de la dissimilarité de Kulczynski.

#### Objets: Occurence (présence-absence)

Des indices d'association différents devraient être utilisés lorsque des données sont compilées sous forme booléenne. En général, les tableaux de données d'occurence seront compilés avec des 1 (présence) et des 0 (absence).

La **similarité de Jaccard** entre le site A et le site B est la proportion de double 1 (présences de 1 dans A et B) parmi les espèces. La dissimilarié est la proportion complémentaire (comprenant [1, 0], [0, 1] et [0, 0]). La distance de Jaccard est la racine carrée de la dissimilarité.

```{r}
associations_occ <- list()
associations_occ[['Jaccard']] <- vegdist(occurence, method = "jaccard")
```

Les **distances d'Hellinger, de chord et de chi-carré** sont aussi appropriées pour les calculs de distances sur des tableaux d'occurence.


```{r}
associations_occ[['Hellinger']] <- dist(decostand(occurence, method="hellinger"))
associations_occ[['Chord']] <- dist(decostand(occurence, method="normalize"))
associations_occ[['ChiSquare']] <- dist(decostand(occurence, method="chi.square"))
```

Graphiquement,

```{r}
associations_occ_df <- list()

for (i in 1:length(associations_occ)) {
  associations_occ_df[[i]] <- data.frame(as.matrix(associations_occ[[i]]))
  colnames(associations_occ_df[[i]]) <- rownames(associations_occ_df[[i]])
  associations_occ_df[[i]]$row <- rownames(associations_occ_df[[i]])
  associations_occ_df[[i]] <- associations_occ_df[[i]] %>% gather(key=row)
  associations_occ_df[[i]]$column = rep(1:4, 4)
  associations_occ_df[[i]]$dist <- names(associations_occ)[i]
}
associations_occ_df <- do.call(rbind, associations_occ_df)

ggplot(associations_occ_df, aes(x=row, y=column)) +
  facet_wrap(. ~ dist) +
  geom_tile(aes(fill = value)) +
  geom_text(aes(label = round(value, 2))) +
  scale_fill_gradient2(low = "#00ccff", mid = "#aad400", high = "#ff0066", midpoint = 1) +
  labs(x="Site", y="Site")

```

Il est attendu que les matrices d'association sur l'occurence sont semblables à celles sur l'abondance. Dans ce cas-ci, la distance d'Hellinger donne des résultats semblables à la dissimilarité de Jaccard.

#### Objets: Données quantitatives

Les données quantitative en écologie peuvent décrire l'état de l'environnement: le climat, l'hydrologie, l'hydrogéochimie, la pédologie, etc. En règle générale, les coordonnées des sites ne sot pas des variables environnementales, à que l'on soupçonne la coordonnée elle-même d'être responsable d'effets sur notre système: mais il s'agira la plupart du temps d'effets confondants (par exemple, on peut mesurer un effet de lattitude sur le rendement des agrumes, mais il s'agira probablement avant tout d'effets dus aux conditions climatiques, qui elles changent en fonction de la lattitude). D'autre types de données quantitative pouvant être appréhendées par des distances sont les traits phénologiques, les ionomes, les génomes, etc.

La **distance euclidienne** est la racine carrée de la somme des carrés des distances sur tous les axes. Il s'agit d'une application multidimensionnelle du théorème de Pythagore. La **distance d'Aitchison**, couverte dans le chapitre 6, est une distance euclidienne calculée sur des données compositionnelles préalablement transformées. La distance euclidienne est sensible aux unités utilisés: utiliser des milimètres plutôt que des mètres enflera la distance euclidienne. Il est recommandé de porter une attention particulière aux unités, et de standardiser les données au besoin (par exemple, en centrant la moyenne à zéro et en fixant l'écart-type à 1).

On pourrait, par exemple, mesurer la distance entre des observations des dimensions de différentes espèces d'iris. Ce tableau est inclu dans R par défaut.

```{r}
data(iris)
iris %>% sample_n(5)
```

Les mesures du tableau sont en centimètres. Pour éviter de donner davantage de poids aux longueur des sépales et en même temps de négliger la largeur des pétales, nous allons standardiser le tableau.

```{r}
iris_sc <- iris %>%
  select(-Species) %>% 
  scale(.)%>% 
  as_tibble(.) %>% 
  mutate(Species = iris$Species) 
iris_sc
```

Pour les comparaisons des dimensions, prenons la moyenne des dimensions (mises à l'échelle) par espèce.

```{r}
iris_means <- iris_sc %>%
  group_by(Species) %>%
  summarise_all(mean) %>%
  select(-Species)
iris_means
```

Nous pouvons utiliser la distance euclidienne, commune en géométrie, pour comparer les espèces. La distance euclidienne est calculée comme suit.


$$ \mathcal{E} = \sqrt{\Sigma_i \left( A_i - B_i \right) ^2 } $$

```{r}
associations_cont = list()
associations_cont[['Euclidean']] <- dist(iris_sc %>% select(-Species), method="euclidean")
```

La **distance de Mahalanobis** est semblable à la distance euclidienne, mais qui tient compte de la covariance de la matrice des objets. Cette covariance peut être utilisée pour décrire la structure d'un nuage de points. La figure suivante montre deux points verts qui se trouvent aux extrêmes d'un nuage de point. Ces points ont des distances euclidiennes par rapport au centre différentes: les lignes d'équidistance euclédienne sont tracées en rose. Toutefois, les deux points ont un distance de Mahalanobis égale à partir du centre.

<img src="images/07_eucl-maha.png" width=400>
<p style="text-align: center">Source: [Parent et al. (2012)](https://www.intechopen.com/books/soil-fertility/nutrient-balance-as-paradigm-of-plant-and-soil-chemometricsnutrient-balance-as-paradigm-of-soil-and-).</p>

La diastance de Mahalanobis se calcule comme suit.

$$\mathcal{M} = \sqrt{(A - B)^T S^{-1} (A-B)}$$

Notez qu'il s'agit d'une généralisation de la distance euclidienne, qui équivaut à une distance de Mahalanobis dont la matrice de covariance est une matrice identité.

La distance de Mahalanobis permet de représenter des distances dans un espace fortement corrélé. Elle est courramment utilisée pour détecter les valeurs aberrantes selon des critères de distance à partir du centre d'un jeu de données multivariées.

```{r}
associations_cont[['Mahalanobis']] <- vegdist(iris_sc %>% select(-Species), 'mahalanobis')
```

La **distance de Manhattan** porte aussi le nom de distance de cityblock ou de taxi. C'est la distance que vous devrez parcourir pour vous rendre du point A au point B à Manhattan, c'est-à-dire selon une séquence de tronçons perpendiculaires.

$$ D_{AB} = \sum _i \left| A_i - B_i \right| $$

La distance de Manhattan est appropriée lorsque les gradients (changements d'un état à l'autre ou d'une région à l'autre) ne permettent pas des changements simultanés. Mieux vaut standardiser les variables pour éviter qu'une dimension soit prépondérante.

```{r}
associations_cont[['Manhattan']] <- vegdist(iris_sc %>% select(-Species), 'manhattan')
```

Graphiquement

```{r}
associations_cont_df <- list()

for (i in 1:length(associations_cont)) {
  associations_cont_df[[i]] <- data.frame(as.matrix(associations_cont[[i]]))
  colnames(associations_cont_df[[i]]) <- rownames(associations_cont_df[[i]])
  associations_cont_df[[i]]$row <- rownames(associations_cont_df[[i]])
  associations_cont_df[[i]] <- associations_cont_df[[i]] %>% gather(key=row)
  associations_cont_df[[i]]$column = rep(1:nrow(iris), nrow(iris))
  associations_cont_df[[i]]$dist <- names(associations_cont)[i]
}
associations_cont_df <- do.call(rbind, associations_cont_df)

ggplot(associations_cont_df, aes(x=row, y=column)) +
  facet_wrap(. ~ dist) +
  geom_tile(aes(fill = value), colour = NA) +
  #geom_text(aes(label = round(value, 2))) +
  scale_fill_gradient2(low = "#00ccff", mid = "#aad400", high = "#ff0066", midpoint = 5) +
  labs(x="Site", y="Site")
```


Le tableau `iris` est ordonné par espèce. Les distances euclidienne et de Manhattan permettent aisément de distinguer les espèces selon les dimensions des pétales et des sépales. Toutefois, l'utilsation de la covariance avec la distance de Mahalanobis crée des distinction moins tranchées.

#### Objets: Données mixtes

Les données catégorielles ordinales peuvent être transformées en données continues par gradations linéaires ou quadratiques. Les données catégorielles nominales, quant à elles, peuvent être *dummyfiées* en données similaires à des occurences. Attention toutefois: contrairement à la régression linéaire qui demande d'exclure une catégorie, la *dummyfication* doit inclure toutes les catégories. Le comportement par défaut de la fonction `pandas.get_dummies` est de garder toutes les catégories. La **similarité de Gower** a été développée pour mesurer des associations entre des objets dont les données sont mixtes: booléennes, catégorielles et continues. La similarité de Gower est calculée en additionnant les distances calculées par colonne, individuellement. Si la colonne est booléenne, on utilise les distances de Jaccard (qui exclue les double-zéro) de manière univariée: une variable à la fois. Pour les variables continues, on utilise la distance de Manhattan divisée par la plage de valeurs de la variable (pour fin de standardisation). Puisqu'elle hérite de la particularité de la distance de Manhattan et de la similarité de Jaccard univariée, la **similarité de Gower** reste une combinaison linéaire de distances univariées.


```{r}
X <- tibble(ID = 1:8,
            age = c(21, 21, 19, 30, 21, 21, 19, 30),
            gender = c('M','M','N','M','F','F','F','F'),
            civil_status = c('MARRIED','SINGLE','SINGLE','SINGLE','MARRIED','SINGLE','WIDOW','DIVORCED'),
            salary = c(3000.0,1200.0 ,32000.0,1800.0 ,2900.0 ,1100.0 ,10000.0,1500.0),
            children = c(TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE),
            available_credit = c(2200,100,22000,1100,2000,100,6000,2200))
X
```

Il faut préalablement *dummifier* les variables catégorielles nominales.

```{r}
X_dum <- model.matrix(~ 0 + ., X[, -1])
X_dum
```

Calculons la dissimilarité de Gower (cette fois le graphique est fait avec `pheatmap`).

```{r}
library("pheatmap")
d_gow <- as.matrix(vegdist(X_dum, 'gower'))
colnames(d_gow) <- rownames(d_gow) <- X$ID
pheatmap(d_gow)
```

Les dendrogrammes apparaissants sur les axes du graphique sont issus d'un processus de partitionnement basé sur la distance, que nous verrons plus loin dans ce chapiter. Les profils des clients 4 et 7, ainsi que ceux des clients 3 et 7 diffèrent le plus. Les profils 3 et 4 sont néanmoins plutôt différents.

### Associations entre variables (mode R)

Il existe de nombreuses approches pour mesurer les associations entre variables. La plus connue est la corrélation. Mais les données d'abondance et d'occurence demandent des approches différentes.

#### Variables: Abondance

La distance du chi-carré est suggérée par [Borcard et al. (2011)](http://www.springer.com/us/book/9781441979759).

```{r}
abundance_r <- t(abundance)
D_chisq_R <- as.matrix(dist(decostand(abundance_r, method="chi.square")))
pheatmap(D_chisq_R, display_numbers = round(D_chisq_R, 2))
```

Des coabondances sont notables pour la mésange à tête noire, le jaseur boréal, la citelle à poitrine rousse et le bruant à gorge blanche (tache bleu au centre).

#### Variables: Occurence

La dissimilarité de Jaccard peut être utilisée.

```{r}
occurence_r <- t(occurence)
D_jacc_R <- as.matrix(vegdist(occurence_r, method = "jaccard"))
pheatmap(D_jacc_R, display_numbers = round(D_jacc_R, 2))
```

Des cooccurences sont notables pour le jaseur boréal, la citelle à poitrine rousse et le bruant à gorge blanche (tache bleu au centre).

#### Variables: Quantités

La matrice des corrélations de Pearson peut être utilisée pour les données continues. Quant aux variables ordinales, elles devraient idéalement être liées linéairement ou quadratiquement. Si ce n'est pas le cas, c'est-à-dire que les catégories sont ordonnées par rang seulement, vous pourrez avoir recours aux coefficients de corrélation de Spearman ou de Kendall.

```{r}
iris_cor <- iris %>%
  select(-Species) %>%
  cor(.)
pheatmap(iris_cor, cluster_rows = FALSE, cluster_cols = FALSE,
         display_numbers = round(iris_cor, 2))
```

### Conclusion sur les associations

Il n'existe pas de règle claire pour déterminer quelle technique d'association utiliser. Cela dépend en premier lieu de vos données. Vous sélectionnerez votre méthode d'association selon le type de données que vous abordez, la question à laquelle vous désirez répondre ainsi l'expérience dans la littérature comme celle de vos collègues scientifiques. S'il n'existe pas de règle clair, c'est qu'il existe des dizaines de méthodes différentes, et la plupart d'entre elles vous donneront une perspective juste et valide. Il faut néanmoins faire attention pour éviter de sélectionner les méthodes qui ne sont pas appropriées. 

## Partitionnement

Les données suivantes ont été générées par [Leland McInnes](https://github.com/scikit-learn-contrib/hdbscan/blob/master/notebooks/clusterable_data.npy) (Tutte institute of mathematics, Ottawa). Êtes-vous en mesure d'identifier des groupes? Combien en trouvez-vous?

```{r}
df_mcinnes <- read_csv("data/clusterable_data.csv", col_names = c("x", "y"), skip = 1)
ggplot(df_mcinnes, aes(x=x, y=y)) + geom_point() + coord_fixed()
```

En 2D, l'oeil humain peut facilement détecter les groupes. En 3D, c'est toujours possible, mais au-delà de 3D, le partitionnement cognitive devient rapidement maladroite. Les algorithmes sont alors d'une aide précieuse. Mais ils transportent en pratique tout un baggage de limitations. Quel est le critère d'association entre les groupes? Combien de groupe devrions-nous créer? Comment distinguer une donnée trop bruitée pour être classifiée?

Le partitionnement de données (*clustering* en anglais), et inversement leur regroupement, permet de créer des ensembles selon des critères d'association. On suppose donc que Le partitionnement permet de créer des groupes selon l'information que l'on fait émerger des données. Il est conséquemment entendu que les données ne sont pas catégorisées à priori: **il ne s'agit pas de prédire la catégorie d'un objet, mais bien de créer des catégories à partir des objets** par exemple selon leurs dimensions, leurs couleurs, leurs signature chimique, leurs comportements, leurs gènes, etc. 

Plusieurs méthodes sont aujourd'hui offertes aux analystes pour partitionner leurs données. Dans le cadre de ce manuel, nous couvrirons ici deux grandes tendances dans les algorithmes.

1. *Méthodes hiérarchique et non hiérarchiques*. Dans un partitionnement hiérarchique, l'ensemble des objets forme un groupe, comprenant des sous-regroupements, des sous-sous-regroupements, etc., dont les objets forment l'ultime partitionnement. On pourra alors identifier comment se décline un partitionnement. À l'inverse, un partitionnement non-hiérarchique des algorhitmes permettent de créer les groupes non hiérarchisés les plus différents que possible.

2. *Membership exclusif ou flou*. Certaines techniques attribuent à chaque une classe unique: l'appartenance sera indiquée par un 1 et la non appartenance par un 0. D'autres techniques vont attribuer un membership flou où le degré d'appartenance est une variable continue de 0 à 1. Parmi les méthodes floues, on retrouve les méthodes probabilistes.

### Évaluation d'un partitionnement

Le choix d'une technique de partitionnement parmi de [nombreuses disponibles](http://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods), ainsi que le choix des paramètres gouvernant chacune d'entre elles, est avant tout basé sur ce que l'on désire définir comme étant un groupe, ainsi que la manière d'interpréter les groupes. En outre, **le nombre de groupe à départager est *toujours* une décision de l'analyste**. Néanmoins, on peut se fier [des indicateurs de performance de partitionnement](http://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation). Parmis ceux-ci, retenons le score [silouhette](http://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient) ainsi que l'[indice de Calinski-Harabaz](http://scikit-learn.org/stable/modules/clustering.html#calinski-harabaz-index).

#### Score silouhette

En anglais, le *h* dans silouhette se trouve après le *l*: on parle donc de *silhouette coefficient* pour désigner le score de chacun des objets dans le partitionnement. Pour chaque objet, on calcule la distance moyenne qui le sépare des autres points de son groupe ($a$) ainsi que la distance moyenne  qui le sépare des points du groupe le plus rapproché.

$$s = \frac{b-a}{max \left(a, b \right)}$$

Un coefficient de -1 indique le pire classement, tandis qu'un coefficient de 1 indique le meilleur classement. La moyenne des coefficients silouhette est le score silouhette.

#### Indice de Calinski-Harabaz

L'indice de Calinski-Harabaz est proportionnel au ratio des dispersions intra-groupe et la moyenne des dispersions inter-groupes. Plus l'indice est élevé, mieux les groupes sont définis. La mathématique est décrite [dans la documentation de scikit-learn](http://scikit-learn.org/stable/modules/clustering.html#calinski-harabaz-index).

**Note**. Les coefficients silouhette et l'indice de Calinski-Harabaz sont plus appropriés pour les formes de groupes convexes (cercles, sphères, hypersphères) que pour les formes irrégulières (notamment celles obtenues par la DBSCAN, discutée ci-desssous).

### Partitionnement non hiérarchique

Il peut arriver que vous n'ayez pas besoin de comprendre la structure d'agglomération des objets (ou variables).  Plusieurs techniques de partitionnement non hiérarchique [sont disponibles dans le module scikit-learn](http://scikit-learn.org/stable/modules/clustering.html). On s'intéressera en particulier à celles-ci.

**Kmeans** (`sklearn.cluster.Kmeans`). L'objectif des kmeans est de minimiser la distance euclédienne entre un nombre prédéfini de *k* groupes exclusifs.

1. L'algorhitme commence par placer une nombre *k* de centroides au hasard dans l'espace d'un nombre *p* de variables (vous devez fixer *k*, et *p* est le nombre de colonnes de vos données).
2. Ensuite, chaque objet est étiquetté comme appartenant au groupe du centroid le plus près.
3. La position du centroide est déplacée à la moyenne de chaque groupe.
4. Recommencer à partir de l'étape 2 jusqu'à ce que l'assignation des objets aux groupes ne change plus.

![](https://media.giphy.com/media/12vVAGkaqHUqCQ/giphy.gif)
<center>Source: [David Sheehan](https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/)</center>

La technique des kmeans suppose que les groupes ont des distributions multinormales - représentées par des cercles en 2D, des sphères en 3D, des hypersphères en plus de 3D. Cette limitation est problématique lorsque les groupes se présentent sous des formes irrégulières, comme celles du nuage de points de Leland McInnes, présenté plus haut. De plus, la technique classique des kmeans est basée sur des distances euclidiennes: l'utilisation des kmeans n'est appropriée pour les données comprenant beaucoup de zéros, comme les données d'abondance, qui devraient préalablement être transformées en variables centrées et réduites ([Legendre et Legendre, 2012](https://www.elsevier.com/books/numerical-ecology/legendre/978-0-444-53868-0)). La technique des **mixtures gaussiennes** (*gaussian mixtures*, `sklearn.mixture.GaussianMixture`) est une généralisation des kmeans permettant d'intégrer la covariance des groupes. Les groupes ne sont plus des hyper-sphères, mais des hyper-ellipsoïdes.

**DBSCAN**. La technique DBSCAN (* **D**ensity-**B**ased **S**patial **C**lustering of **A**pplications with **N**oise*) sousentend que les groupes sont composés de zones où l'on retrouve plus de points (zones denses) séparées par des zones de faible densité. Pour lancer l'algorithme, nous devons spécifier une mesure d'association critique (distance ou dissimilarité) *d* ainsi qu'un nombre de point critique *k* dans le voisinage de cette distance.

1. L'algorithme comme étiqueter chaque point selon l'une de ces catégories:

    - *Noyau*: le point a au moins *k* points dans son voisinage, c'est-à-dire à une distance inférieure ou égale à *d*.
    - *Bordure*: le point a moins de *k* points dans son voisinage, mais l'un de des points voisins est un *noyau*.
    - *Bruit*: le cas échéant. Ces points sont considérés comme des outliers.

    <img src="images/dbscan_1.svg" width=600>

2. Les noyaux distancés de *d* ou moins sont connectés entre eux en englobant les bordures.

    <img src="images/dbscan_2.svg" width=600>

Le nombre de groupes est prescrit par l'algorithme DBSCAN, qui permet du coup de détecter des données trop bruitées pour être classées.

[Damiani et al. (2014)](https://doi.org/10.1145/2666310.2666417) a développé une approche utilisant la technique DBSCAN pour partitionner des zones d'escale pour les flux de populations migratoires.

#### Application

`---------

Dans scikit-learn, on définit d'abord le modèle (par exemple `Kmeans(...)`), puis on l'applique à nos données (`fit(...)`), enfin on applique le modèle sur des données (`predict(...)`). Certaines fonctions utilisent toutefois le raccourcis `fit_predict`. Chaque algorithme doit être ajusté avec les paramètres qui convient. De nombreux paramètres par défaut sont utilisés dans les exécutions ci-dessous. Lors de travaux de recherche, l'utilsation d'un argument ou d'un autre dans une fonction doit être justifié: qu'un paramètre soit utilisé par défaut dans une fonction n'est a priori pas une justification convainquante.


``{r}
mcinnes_kmeans_labels <- kmeans(x=df_mcinnes, centers = 3)$cl
silhouette(mcinnes_kmeans_labels, dist = vegdist(df_mcinnes, method = "euclidean"))
```



-----

D

<!--chapter:end:07_association-partition-ordination.Rmd-->

---
title: "Détection de valeurs aberrantes et imputation"
author: "Serge-Étienne Parent"
date: "`r format(Sys.Date())`"
output: github_document
---

# Détection de valeurs aberrantes et imputation {#chapitre-outliers}


 ***
️\ **Objectifs spécifiques**:

À la fin de ce chapitre, vous

- saurez comment détecter des valeurs aberrantes en mode univarié et multivarié
- saurez comment procéder à l'imputation de valeurs manquantes en mode univarié et multivarié

 ***



<!--chapter:end:08_outliers-imputation.Rmd-->

---
title: "Les séries temporelles"
author: "Serge-Étienne Parent"
date: "`r format(Sys.Date())`"
output: github_document
---

# Les séries temporelles {#chapitre-temps}

 ***
️\ **Objectifs spécifiques**:

À la fin de ce chapitre, vous

- saurez comment importer et manipuler des données temporelles (utiliser le format de date, filtrer, effectuer des sommaires, aggréger des données, etc.)
- effectuer une régression sur une série temporelle

 ***


- À développer

<!--chapter:end:09_series-temporelles.Rmd-->

---
title: "Science ouverte et suivi de version"
author: "Serge-Étienne Parent"
date: "`r format(Sys.Date())`"
output: github_document
---

# Science ouverte et suivi de version {#chapitre-git}

 ***
️\ **Objectifs spécifiques**:

À la fin de ce chapitre, vous

- saurez exprimer l'importance et les enjeux de la science ouverte
- saurez arranger vos données (format csv) et votre code (format notebook) afin de rendre vos recherches reproductibles
- saurez comment créer un dépôt sur GitHub, puis administrer son développement

 ***

<!--chapter:end:10_github.Rmd-->

---
title: "Autoapprentissage"
author: "Serge-Étienne Parent"
date: "`r format(Sys.Date())`"
output: github_document
---

# Autoapprentissage {#chapitre-ml}

 ***
️\ **Objectifs spécifiques**:

À la fin de ce chapitre, vous

- saurez établir un plan de modélisation par autoapprentissage
- saurez définir le sous-apprentissage et le surapprentissage
- serez en mesure d'effectuer un autoapprentissage avec les techniques des *k*-proches voisins, les arbres de décision, les forêts aléatoires, les réseaux neuronnaux et les processus gaussiens

 ***

Plusieurs cas d'espèces en sciences et génies peuvent être approchés en liant un variable avec une ou plusieurs autres à l'aide de régressions linéaires, polynomiales, sinusoïdales, exponentielle, sigmoïdales, [etc](https://dl.sciencesocieties.org/publications/aj/pdfs/107/2/786). Encore faut-il s'assurer que ces formes préétablies représentent le phénomène de manière fiable.

Lorsque la forme de la réponse est difficile à envisager, en particulier dans des cas non-linéaires ou impliquant plusieurs variables, on pourra faire appel à des modèles dont la structure n'est pas contrôlée par une équation rigide gouvernée par des paramètres (comme la pente ou l'intercept).

L'**autoapprentissage**, apprentissage automatique, ou *machine learning*, vise à détecter des structures complexes émergeant d'ensembles de données à l'aide des mathématiques et de processus automatisés afin de prédire l'émergence de futures occurrences. Comme ensemble de techniques empiriques, l'autoapprentissage est un cas particulier de l'**intelligence artificielle**, qui elle inclut aussi les mécanismes déterministes et des ensembles d'opérations logiques. Par exemple, les premiers ordinateurs à compétitionner aux échecs se basaient sur des règles de logique (si la reine noire est positionnée en c3 et qu'un le fou blanc est en position f6 et que ... alors bouge la tour en g5 - j'écris n'importe quoi). Il s'agissait d'intelligence artificielle, mais pas d'autoapprentissage. L'autoapprentissage passera davantage par la simulation de nombreuses parties et dégagera la structure optimale pour l'emporter considérant les positions des pièces sur l'échiquier.

## Objectifs

* Comprendre les applications possibles de l'autoapprentissage
* Comprendre le flux de travail d'une opération d'autoapprentissage
* Comprendre les principes soutenant les techniques des *k* plus proches voisins, des arbres décisionnels, des réseaux neuronaux et des processus gaussiens.

Plus spécifiquement, vous devrez à la fin de cette section être en mesure de prédire une variable catégorie ou numérique à partir de données observées.

## Lexique
L'autoapprentissage possède son jargon particulier. Puisque certains termes peuvent porter à confusion, voici quelques définitions de termes que j'utiliserai dans ce chapitre.

- **Réponse**. La variable que l'on cherche à obtenir. Il peut s'agir d'une variable continue comme d'une variable catégorielle. On la nomme aussi la *cible*.
- **Prédicteur**. Une variable utilisée pour prédire une réponse. Les prédicteurs sont des variables continues. Les prédicteurs de type catégoriel doivent préalablement être dummifiés (voir chapitre 5). On nomme les prédicteurs les *entrées*.
- **Apprentissage supervisé** et **non-supervisé**. Si vous avez suivi le cours jusqu'ici, vous avez déjà utilisé des outils entrant dans la grande famille de l'apprentissage automatique. La régression linéaire, par exemple, vise à minimiser l'erreur sur la réponse en optimisant les coefficients de pente et l'intercept. Un apprentissage supervisé a une cible, comme c'est le cas de la régression linéaire. En revanche, un apprentissage non supervisé n'en a pas: on laisse l'algorithme le soin de détecter des structures intéressantes. Nous avons déjà utilisé cette approche. Pensez-y un peu... l'analyse en composante principale ou en coordonnées principales, ainsi que le partitionnement hiérarchique ou non sont des exemples d'apprentissage non supervisé. En revanche, l'analyse de redondance a une réponse. L'analyse discriminante aussi, bien que sa réponse soit catégorielle. L'apprentissage non supervisé ayant déjà été couvert au chapitre 7, ce chapitre ne s'intéresse qu'à l'apprentissage supervisé.
- **Régression** et **Classification**. Alors que la régression est un type d'apprentissage automatique pour les réponses continues, la classification vise à prédire une réponse catégorielle. Il existe des algorithmes uniquement application à la régression, uniquement applicables à la classification, et plusieurs autres adaptable aux deux situations.
- **Données d'entraînement** et **données de test**. Lorsque l'on génère un modèle, on désire qu'il sache comment réagir à ses prédicteurs. Cela se fait avec des données d'entraînement, sur lesquelles on **calibre** et **valide** le modèle. Les données de test servent à vérifier si le modèle est en mesure de prédire des réponses sur lesquelles il n'a pas été entraîné.
- **Fonction de perte**. Une fonction qui mesure l'erreur d'un modèle.

## Démarche

La première tâche est d'explorer les données, ce que nous avons couvert au chapitres 3 et 4.

### Prétraitement
Pour la plupart des techniques d'autoapprentissage, le choix de l'échelle de mesure est déterminant sur la modélisation subséquente. Par exemple, un algorithme basé sur la distance comme les *k* plus proches voisins ne mesurera pas les mêmes distances entre deux observations si l'on change l'unité de mesure d'une variable du mètre au kilomètre. Il est donc important d'effectuer, ou d'envisager la possibilité d'effectuer un prétraitement sur les données. Je vous réfère au chapitre 6 (en développement) pour plus de détails sur le prétraitement.

### Entraînement et test

Vous connaissez peut-être l'expression sportive "avoir l'avantage du terrain". Il s'agit d'un principe prétendant que les athlètes performent mieux en terrain connu. Idem pour les modèles phénoménologiques. Il est possible qu'un modèle fonctionne très bien sur les données avec lesquelles il a été entraîné, mais très mal sur des données externes. De mauvaises prédictions effectuées à partir d'un modèle qui semblait bien se comporter peut mener à des décisions qui, pourtant prises de manière confiante, se révèlent fallacieuses au point d'aboutir à de graves conséquences. C'est pourquoi, **en mode prédictif, on doit évaluer la précision et la justesse d'un modèle sur des données qui n'ont pas été utilisés dans son entraînement**.

En pratique, il convient de séparer un tableau de données en deux: un tableau d’entraînement et un tableau de test. Il n'existe pas de standards sur le ratio à utiliser. Cela dépend de la prudence de l'analyse et de l'ampleur de son tableau de données. Certaines personnes préférerons couper le tableau à 50%. D'autres préférerons réserver le deux-tiers des données pour l'entraînement, ou 70%, 75%. Rarement, réservera-t-on moins plus de 50% et moins de 20% à la phase de test.

Si les données sont peu équilibrées (par exemple, on retrouve peu de données de l'espèce $A$, que l'on retrouve peu de données à un pH inférieur à 5 ou que l'on a peu de données croisées de l'espèce $A$ à ph inférieur à 5), il y a un danger qu'une trop grande part, voire toute les données, se retrouvent dans le tableau d'entraînement (certaines situations ne seront ainsi pas testées) ou dans le tableau de test (certaines situations ne seront pas couvertes par le modèle). L'analyste doit s'assurer de séparer le tableau au hasard, mais de manière consciencieuse.

### Sousapprentissage et surapprentissage

Une difficulté en modélisation phénoménologique est ce qui tient de la structure et ce qui tient du bruit. Lorsque l'on considère une structure comme du bruit, on est dans un cas de sousapprentissage. Lorsque, au contraire, on interprète du bruit comme une structure, on est en cas de surapprentissage. Les graphiques suivant présentent ces deux cas, avec au centre un cas d'apprentissage conforme.

```{r}
set.seed(35473)
n <- 50
x <- seq(0, 20, length = n) 
y <- 500 + 0.4 * (x-10)^3 + rnorm(n, mean=10, sd=80) # le bruit est généré par rnorm()

par(mfrow = c(1, 3))
plot(x, y, main = "Sousapprentissage", col = "#46c19a", pch=16)
lines(x, predict(lm(y~x)), col = "#b94a73")

plot(x, y, main = "Apprentissage conforme", col = "#46c19a", pch=16)
lines(x, 
      predict(lm(y~x + I(x^2) + I(x^3))),
      col = "#b94a73")

plot(x, y, main = "Surapprentissage", col = "#46c19a", pch=16)
lines(x, 
      predict(lm(y~x + I(x^2) + I(x^3) + I(x^4) + 
                   I(x^5) + I(x^6) + I(x^7) + I(x^8) +
                   I(x^9) + I(x^10) + I(x^11) + I(x^12) +
                   I(x^13) + I(x^14) + I(x^15) + I(x^16))),
      col = "#b94a73")

```

Afin d'éviter les cas de *mésapprentissage* on peut avoir recours à la validation croisée.

### Validation croisée

Souvent confondue avec le fait de séparer le tableau en phases d'entraînement et de test, la validation croisée est un principe incluant plusieurs algorithmes qui consiste à entraîner le modèle sur un échantillonnage aléatoire des données d'entraînement.

La technique la plus utilisée est le *k-fold*, où l'on sépare aléatoirement le tableau d'entraînement en un nombre *k* de tableaux. À chaque étape de la validation croisée, on calibre le modèle sur tous les tableaux sauf un, puis on valide le modèle sur le tableau exclu. La performance du modèle en entraînement est jugée sur les validations.

### Choix de l'algorithme d'apprentissage

Face aux centaines d’algorithmes d'apprentissages qui vous sont offertes, choisir l'algorithme ou les algorithmes adéquats pour vos données n'est pas facile. Ce choix sera motivé par les tenants et aboutissants des algorithmes, votre expérience, l'expérience de la littérature, l'expérience de vos collègues. Une approche raisonnable est de tester plusieurs modèles et d'approfondir si ce n'est déjà fait la mathématique des options retenues. Il existe des algorithmes génétiques, qui ne sont pas couverts ici, permettent de sélectionner des modèles d'autoapprentissages optimaux. Un de ces algorithmes est offert par le module Python [`tpot`](https://epistasislab.github.io/tpot/).

### Déploiement
RData, Shiny

----

En résumé,

1. Explorer les données
1. Sélectionner des algorithmes
1. Effectuer un prétraitement
1. Créer un ensemble d'entraînement et un ensemble de test
1. Lisser les données sur les données d'entraînement avec validation croisée
1. Tester le modèle
1. Déployer le modèle

## Algorithmes

Il existe des centaines d'algorithmes d'apprentissage. Je n'en couvrirai que quatre, qui me semblent être appropriés pour la modélisation phénoménologique des systèmes vivants, et utilisables pour la régression et la classification.

- Les k plus proches voisins 
- Les arbres de décision
- Les réseaux neuronaux
- Les processus gaussiens

## L'autoapprentissage en R

Plusieurs options sont disponibles.

1. Les modules que l'on retrouve en R pour l'autoapprentissage sont nombreux, et parfois spécialisés. Il est possible de les utiliser individuellement.
1. Chacun de ces modules fonctionne à sa façon. Le module `caret` de R a été conçu pour donner accès à des centaines de fonctions d'autoapprentissage via une interface commune.
1. Le module `mlr` occupe sensiblement le même créneau que `caret`, mais utilise plutôt une approche par objets connectés. Au moment d'écrire ces lignes, `mlr` est peu documenté, donc *a priori* plus complexe à prendre en main.
1. En Python, le module `scikit-learn` offre un interface unique pour l'utilisation de nombreuses techniques d'autoapprentissage. Il est possible d'appeler des fonctions de Python à partir de R grâce au module `reticulate`.

Dans ce chapitre, nous verrons comment fonctionnent certains algorithmes sélectionnés, puis nous les appliquerons avec le module respectif qui m'a semblé le plus approprié. Vous remarquerez néanmoins des références récurrentes aux modules de Python. En ce moment, la force de R réside dans la gestion des tableaux, les tests statistiques, l'exploration heuristique et la visualisation de données. Néanmoins, Python le surpasse pour l'autoapprentissage...

```{r}
library("tidyverse") # évidemment
library("caret")
```

## Les *k* plus proches voisins

> [![Les voisins, une pièce de Claude Meunier here](images/11_les-voisins.jpg)](https://youtu.be/-RpYi_Vuviw?t=6m40s)

> "Le... l'idée en arrière pour être... euh... simpliste, là c'est que c'est un peu de... euhmm... de la vitamine de vinyle." - Georges (Les voisins, une pièce de Claude Meunier)

Pour dire comme Georges, le... l'idée en arrière des KNN pour être... euh... *simpliste*, c'est qu'un objet va ressembler à ce qui se trouve dans son voisinage. Les KNN se basent en effet sur une métrique de distance pour rechercher un nombre *k* de points situés à proximité de la mesure. Les *k* points les plus proches sont retenus, *k* étant un entier non nul à optimiser. Un autre paramètre parfois utilisé est la distance maximale des voisins à considérer: un voisin trop éloigné pourra être discarté. La réponse attribuée à la mesure est calculée à partir de la réponse des *k* voisins retenus. Dans le cas d'une régression, on utiliser généralement la moyenne. Dans le cas de la classification, la mesure prendra la catégorie qui sera la plus présente chez les *k* plus proches voisins.

L'algorithme des *k* plus proches voisins est relativement simple à comprendre. Certains pièges sont, de même, peuvent être contournés facilement. Imaginez que vous rechercher les points les plus rapprochés dans un système de coordonnées géographiques où les coordonnées $x$ sont exprimées en mètres et les coordonnées $y$, en centimètres. Vous y projetez trois points.

```{r}
data <- data.frame(X = c(0, 1, 0),
                   Y = c(0, 0, 1),
                   row.names = c('A', 'B', 'C'))
options(repr.plot.width = 4, repr.plot.height = 4)
par(pty="s")
plot(data, cex=3,
     xlab = 'Position X (m)', ylab = 'Position Y (cm)')
text(data, labels = rownames(data))
```

Techniquement la distance A-B est 100 plus élevée que la distance A-C, mais l'algorithme ne se soucie pas de la métrique que vous utilisez. Il est primordial dans ce cas d'utiliser la même métrique. Cette stratégie est évidente lorsque les variables sont comparables. C'est rarement le cas, que ce soit lorsque l'on compare des dimensions physionomiques (la longueur d'une phalange ou celle d'un fémur) mais lorsque les variables incluent des mélanges de longueurs, des pH, des décomptes, etc., il est important de bien identifier la métrique et le type de distance qu'il convient le mieux d'utiliser. En outre, la standardisation des données à une moyenne de zéro et à un écart-type de 1 est une approche courrament utilisée.

### Exemple d'application

Pour ce premier exemple, je présenterai un cheminement d'autoapprentissage, du prétraitement au test.

```{r}
# ionome
```

## Les arbres décisionnels

![Les Ents, tiré du film le Seigneur des anneaux](images/11_Entmoot.jpg)

Un arbre décisionnel est une collection hiérarchisée de décisions, le plus souvent binaires. Chaque embranchement est un test à vrai ou faux sur une variable. La réponse, que ce soit une catégorie ou une valeur numérique, se trouve au bout de la dernière branche. Les suites de décisions sont organisées de manière à ce que la précision de la réponse soit optimisée.

Par exemple, ...

## Les réseaux neuronaux

Après les KNN et les random forests, nous passons au domaine plus complexe des réseaux neuronaux. Le terme *réseau neuronal* est une métaphore liée à une perception que l'on avait du fonctionnement du cerveau humain lorsque la technique des réseaux neuronaux a été développée dans les années 1950. Un réseau neuronal comprend une série de boîtes d'entrées liée à des fonctions qui transforment et acheminent successivement l'information jusqu'à la sortie d'une ou plusieurs réponse. Il existe plusieurs formes de réseaux neuronnaux, dont la plus simple manifestation est le *perceptron multicouche*. Dans l'exemple suivant, on retrouve 4 variables d'entrée et trois variables de sortie entre lesquelles on retrouve 5 couches dont le nombre de neurones varient entre 3 et 6.

![](images/11_deep_neural_network.png)

Source: [Neural designer](https://www.neuraldesigner.com/)

Entre la première couche de neurones (les variables prédictives) et la dernière couche (les variables réponse), on retrouve des *couches cachées*. Chaque neurone est relié à tous les neurones de la couche suivante.

Les liens sont des poids, qui peuvent prendre des valeurs dans l'ensemble des nombres réels. À chaque neurone suivant la première couche, on fait la somme des poids multipliés par la sortie du neurone. Le nombre obtenu entre dans chaque neurone de la couche. Le neurone est une fonction, souvent très simple, qui transforme le nombre. La fonction plus utilisée est probablement la fonction ReLU, pour *rectified linear unit*, qui expulse le même nombre aux neurones de la prochaine couche s'il est positif: sinon, il expulse un zéro.

**Exercice**. Si tous les neurones sont des fonctions ReLU, calculez la sortie de ce petit réseau neuronal.

<img src="images/11_nn_ex1_Q.jpg" width="600px">

Vous trouverez la réponse sur l'image `images/11_nn_ex1_R.jpg`.

Il est aussi possible d'ajouter un *biais* à chaque neurone, qui est un nombre réel additionné à la somme des neurones pondérée par les poids.

L'optimisation les poids pour chaque lien et les biais pour chaque neurone (grâce à des algorithmes dont le fonctionnement sort du cadre de ce cours) constitue le processus d'apprentissage. Avec l'aide de logiciels et de modules spécialisés, la construction de réseaux de centaines de neurones organisés en centaines de couches vous permettra de capter des patrons complexes dans des ensembles de données.

Vous avez peut-être déjà entendu parler d'apprentissage profond (ou *deep learning*). Il s'agit simplement d'une appellation des réseaux neuronaux modernisé pour insister sur la présence de plusieurs couches de neurones. C'est un terme à la mode.

### Les réseaux neuronaux sur R avec Keras

Plusieurs modules sont disponibles sur R pour l'apprentissage profond. Certains utilisent le module [H2O.ia](https://github.com/h2oai/h2o-3), propulsé en Java, d'autres utilisent plutôt [Keras](https://keras.rstudio.com/), propulsé en Python par l'intermédiaire de tensorflow. J'ai une préférence pour Keras, puisqu'il supporte les réseaux neuronaux classiques (perceptrons multicouche) autant que convolutifs ou récurrents. Keras pourrait néanmoins être difficile à installer sur Windows, où Python ne vient pas par défaut. Sur Windows, Keras ne fonctionne qu'avec Anaconda: vous devez donc installez [Anaconda ou Miniconda](https://www.anaconda.com/download/#windows) (Miniconda offre une installation minimaliste).

Pour installer Keras, il suffit d'installer le module (avec devtools pour obtenir la version la plus récente, i.e. `devtools::install_github("rstudio/keras")`), puis de lancer la fonction `install_keras()`. Dans mon cas, j'utilise conda.

```{r}
library("keras")
#install_keras(method = "conda")
```

La fonction `install_keras()` installera keras dans un environnement virtuel nommé `r-tensorflow`. Si vous utilisez conda, vous y accéderez avec cette commande.

```{r}
use_condaenv("r-tensorflow")
```

Chargeons les données.

```{r}
abalone <- read_csv("http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data",
                    col_names = c('sex', 'length', 'diameter', 'height', 'whole.weight', 'shucked.weight', 'viscera.weight', 'shell.weight', 'rings'))
```

Prenons soin de segmenter nos données en entraînement et en test.

```{r}
set.seed(8453668)
abal_tr_index <- createDataPartition(y=abalone$sex, p = 0.75, list = FALSE)
```

Nous pouvons ainsi créer nos tableaux d'entraînement et de test pour les variables prédictives.

```{r}
x <- abalone %>% select(-sex) %>% as.matrix()
x_tr <- x[abal_tr_index, ]
x_te <- x[-abal_tr_index, ]

```

Pour les variables-réponse, keras exige une transformation préalable. Les réseaux neuronnaux sont aptes à générer des sorties multiples. Lors de la prédiction d'une catégorie, nous devons générée des sorties multiples qui permettront de décider de l'appartenance exclusive à une catégorie ou une autre. Nous avons abordé l'encodage catégoriel aux chapitres \@ref(chapitre-biostats) et \@ref(chapitre-explorer). C'est exactement ce que nous ferons ici, mais en utilisant les fonctions du module keras.

D'abord nous allons transformer les catégories de la colonne `sex` en nombres entiers. Étant donnée que keras désire que la première catégorie commence à 0 et que la transformation de *factor* à *integer* démarre la première catégorie à 1, nous soustrayons 1 de toutes les valeurs.

```{r}
y_num <- as.integer(as.factor(abalone$sex)) - 1
```

Nous encodons ensuite ces valeurs entières en plusieurs colonnes catégorielles.

```{r}
y <- to_categorical(y_num)
head(y)
```

Nous pouvons finalement générer nos sorties pour l'entraînement et le test.

```{r}
y_tr <- y[abal_tr_index, ]
y_te <- y[-abal_tr_index, ]
```

Le module keras construit des modèles avec l'opérateur pipe `%>%`. Nous couvrirons ici seulement les modèles séquentiels, le plus communs, qui permettent de construire notre séquence de neuronnes. Le modèle séquentiel est initié de cette manière.

```{r}
keras_model <- keras_model_sequential() 
```

Une fois inité, nous lui ajoutons des couches avec `layer_dense()`, une fonction qui définit principalement le nombre de neuronne de la couche (`units =`) et la fonction d'activation (`activation =`). Entre chaque couche, je coupe 20% des synapses (`layer_dropout(rate = 0.2)`): une technique pour éviter le surlissage. Enfin, la couche de sortie comporte trois neuronnes, un par catégorie, avec l'activation `"softmax"`, qui permet de générer une sortie catégorielle.

```{r}
keras_model %>% 
  layer_dense(units = 256, activation = 'relu') %>% 
  layer_dropout(rate = 0.2) %>% 
  layer_dense(units = 256 * 2, activation = 'relu') %>% 
  layer_dropout(rate = 0.2) %>% 
  layer_dense(units = 256 * 2, activation = 'relu') %>% 
  layer_dropout(rate = 0.2) %>% 
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dense(units = 3, activation = 'softmax')
```

Il n'existe pas de règle stricte sur le nombre de couche et le nombre de noeud par couche. Il est néanmoins conseillé de générer d'abord un modèle simple, puis au besoin de le complexifier graduellement en terme de nombre de noeuds, puis de nombre de couches. Le module [`autokeras`](https://autokeras.com/), disponible seulement en Python, est conçu pour optimiser un modèle Keras - je vous laisse le soin d'explorer ses capcités.

Le modèle étant maintenant créé, on doit le compiler en spécifiant la fonction de perte (la fonction qui permet de mesurer la performance d'un modèle) ainsi que d'autres option.

```{r}
keras_model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)
```

Enfin, le modèle est lissé sur les données en paramétrant le solveur.

```{r}
history <- keras_model %>% fit(
  x_tr, y_tr, 
  epochs = 5,
  batch_size = 128, 
  validation_split = 0.25
)

plot(history)
```

Les informations du modèle lissé se trouvent dans l'objet du modèle, ici `keras_model`. Évaluons le modèle sur nos données de test.

```{r}
keras_model %>% evaluate(x_te, y_te)
```


```{r}
pred_te <- keras_model %>% predict_classes(x_te)
pred_te[pred_te == 0] <- 'F'
pred_te[pred_te == 1] <- 'I'
pred_te[pred_te == 2] <- 'M'
pred_te <- factor(pred_te, levels = c("F", "I", "M"))

confusionMatrix(factor(abalone$sex[-abal_tr_index]),
                pred_te)
```

#### Pour aller plus loin

En une heure divisée en [4 vidéos](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi), Grant Sanderson explique les réseaux neuronaux de manière intuitive. En ce qui a trait à Keras, je recommande le livre [Deep learning with R, de François Allaire](https://www.safaribooksonline.com/library/view/deep-learning-with/9781617295546/?ar), auquel vous avez accès avec un IDUL de l'Université Laval. Si vous vous sentez à l'aise à utiliser Keras avec le langage Python, je vous recommande le cours gratuit en ligne [*Applications of deep neural networks*, de Jeff Heaton](https://www.youtube.com/watch?v=sRy26qWejOI&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN).

Des types de réseaux neuronaux spécialisés ont été développés. Je les présente sans aller dans les détails.

- **Réseaux neuronaux convolutif**. Ce type de réseau neuronal est surtout utilisé en reconnaissance d'image. Les couches de neurones convolutifs possèdent, en plus des fonctions des perceptrons classiques, des filtres permettant d'intégrer les variables descriptives connexes à l'observation: dans le cas d'une image, il s'agit de scanner les pixels au pourtour du pixel traité. [Une brève introduction sur Youtube](https://www.youtube.com/watch?v=YRhxdVk_sIs).
- **Réseaux neuronaux récurrents**. Prédire des occurrences futures à partir de séries temporelles implique que la réponse au temps t dépend non seulement de conditions externes, mais aussi le la réponse au temps t-1. Les réseaux neuronaux récurrents. Vous devrez ajouter des neurones particuliers pour cette tâche, qui pourra être pris en charge par Keras grâce aux couches de type [*Long Short-Term Memory network*, ou LSTM](https://www.youtube.com/watch?v=UnclHXZszpw).
- **Réseaux neuronaux probabilistes**. Les réseaux neuronaux non-probabilistes offre une estimation de la variable réponse. Mais quelle est la crédibilité de la réponse selon les variables descriptives? Question qui pourrait se révéler cruciale en médecine ou en ingénierie, à la laquelle on pourra répondre en mode probabiliste. Pour ce faire, on pose des distributions *a priori* sur les poids du réseau neuronal. Le module [`edward`](http://edwardlib.org/), programmé et distribué en Python, offre cette possibilité. Vous pourrez accéder à `edward` grâce au module `reticulate`, mais à ce stade mieux vaudra basculer en Python. Pour en savoir davantage, considérez [cette conférence de Andrew Rowan](https://www.youtube.com/watch?v=I09QVNrUS3Q).

## Les processus gaussiens

Les sorties des techniques que sont les KNN, les arbres ou les forêts ainsi que les réseaux neuronaux sont (classiquement) des nombres réels ou des catégories. Dans les cas où la crédibilité de la réponse est importante, il devient pertinent que la sortie soit probabiliste: les prédictions seront alors présentées sous forme de distributions de probabilité. Dans le cas d’une classification, la sortie du modèle sera un vecteur de probabilité qu’une observation appartienne à une classe ou à une autre. Dans celui d’une régression, on obtiendra une distribution continue.

Les **processus gaussiens** tirent profit des statistiques bayésiennes pour effectuer des prédictions probabilistes. D’autres techniques peuvent être utilisées pour effectuer des prédictions probabilistes, comme les [réseaux neuronaux probabilistes](http://edwardlib.org/iclr2017), que j'ai introduits précédemment.

Bien que les processus gaussiens peuvent être utilisés pour la classification, son fonctionnement s'explique favorablement, de manière intuitive, pas la régression.

### Un approche intuitive

Ayant acquis de l'expérience en enseignement des processus gaussiens, [John Cunningham](http://stat.columbia.edu/~cunningham/) a développé une approche intuitive permettant de saisir les mécanismes des processus gaussiens. lors de conférences disponible sur YouTube ([1](https://youtu.be/BS4Wd5rwNwE), [2](https://www.youtube.com/watch?v=Jv25sg-IYHU)), il aborde le sujet par la nécessité d'effectuer une régression non-linéaire.

Générons d'abord une variable prédictive `x`, l'heure, et une variable réponse `y`, le rythme cardiaque d'un individu en battements par minute (bpm).

```{r}
x <- c(7, 8, 10, 14, 17)
y <- c(61, 74, 69, 67, 78)

plot(x, y, xlab="Heure", ylab="Rythme cardiaque (bpm)")
abline(v=12, lty=3, col='gray50');text(12, 67, '?', cex=2)
abline(v=16, lty=3, col='gray50');text(16, 72, '?', cex=2)
```

Poser un problème par un processus gaussien, c'est se demander les valeurs crédibles qui pourraient être obtenues hors du domaine d'observations (par exemple, dans la figure ci-dessus, à `x=12` et `x=16`)? Ou bien, de manière plus générale, *quelles fonctions ont pu générer les variables réponse à partir d'une structure dans les variables prédictives?*

Les distributions normales, que nous appellerons *gaussiennes* dans cette section par concordance avec le terme *processus gaussien*, sont particulièrement utiles pour répondre à cette question.

Nous avons vu précédemment ce que sont les distributions de probabilité: des outils mathématiques permettant d'appréhender la structure des processus aléatoires. Une distribution gaussienne représente une situation où l'on tire au hasard des valeurs continues. Une distribution gaussienne de la variable aléatoire $X$ de moyenne $0$ et de variance de $1$ est notée ainsi:

$$ X \sim \mathcal{N} \left( 0, 1\right)$$

Par exemple, une courbe de distribution gaussienne du rythme cardiaque à 7:00 pourrait prendre la forme suivante.

$$ bpm \sim \mathcal{N} \left( 65, 5\right)$$

En `R`:

```{r}
x_sequence <- seq(50, 80, length=100)
plot(x_sequence,
     dnorm(x_sequence, mean=65, sd=5),
     type="l",
     xlab="Rythme cardiaque (bpm)",
     ylab="Densité")
```

Une distribution **bi**normale, un cas particulier de la distribution **multi**normale, comprendra deux vecteurs, $x_1$ et $x_2$. Elle aura donc deux moyennes. Puisqu'il s'agit d'une distribution binormale, et non pas deux distributions normales, les deux variables ne sont pas indépendantes et l'on utilisera une matrice de covariance au lieu de deux variances indépendantes.

$$
\binom{x_1}{x_2} \sim \mathcal{N}
\Bigg( 
\binom{\mu_1}{\mu_2},
\left[ {\begin{array}{cc}
\Sigma_{x_1} & \Sigma_{x_1,x_2} \\
\Sigma_{x_1,x_2}^T & \Sigma_{x_2} \\
\end{array} } \right]
\Bigg)
$$

La matrice $\Sigma$, dite de *variance-covariance*, indique sur sa diagonale les variances des variables ($\Sigma_{x_1}$ et $\Sigma_{x_2}$). Les covariances $\Sigma_{x_1,x_2}$ et $\Sigma_{x_1,x_2}^T$ sont symétriques et indiquent le lien entre les variables.

On pourrait supposer que le rythme cardiaque à 8:00 soit corrélé avec celui à 7:00. Mises ensembles, les distributions gaussiennes à 7:00 et à 8:00 formeraient une distribution gaussienne binormale.

$$
\binom{bpm_7}{bpm_8} \sim \mathcal{N}
\Bigg( 
\binom{65}{75},
\left[ {\begin{array}{cc}
10 & 6 \\
6 & 15 \\
\end{array} } \right]
\Bigg)
$$

En `R`:

```{r}
library("ellipse")
means_vec <- c(65, 75)
covariance_mat <- matrix(c(10, 6, 6, 15), ncol=2)
par(pty='s')
plot(ellipse(x=covariance_mat, centre=means_vec, levels=0.95), 
     type='l',
     xlab="Rythme cardiaque à 7:00 (bpm)",
     ylab="Rythme cardiaque à 8:00 (bpm)")
#lines(ellipse(x=covariance_mat, centre=means_vec, level=0.8))
```

On peut se poser la question: étant donnée que $x_1 = 68$, quelle serait la distribution de $x_2$? Dans ce cas bivariée, la distribution marginale serait univariée, mais dans le cas multivarié en $D$ dimensions, la distribution marginale où l'on spécifie $m$ variables serait de $D-m$. de  Une propriété fondamentale d'une distribution gaussienne est que peu importe l'endroit où l'angle selon lequel on la tranche, la distribution marginale sera aussi gaussienne. Lorsque l'on retranche une ou plusieurs variables en spécifiant la valeur qu'elles prennent, on applique un *conditionnement* à la distribution.

```{r}
library("condMVNorm")

condition_x1 <- 61 # changer ce chiffre pour visualiser l'effet

cond_parameters <- condMVN(mean=means_vec, sigma=covariance_mat,
                           dependent=2, given=1, X.given=condition_x1)
cond_mean <- cond_parameters$condMean
cond_sd <- sqrt(cond_parameters$condVar)
x2_sequence <- seq(50, 90, length=100)
x2_dens <- dnorm(x2_sequence, mean=cond_mean, sd=cond_sd)

par(pty='s')
plot(ellipse(x=covariance_mat, centre=means_vec, levels=0.95), type='l',
     xlab="Rythme cardiaque à 7:00 (bpm)",
     ylab="Rythme cardiaque à 8:00 (bpm)")
abline(v=condition_x1, col='#f8ad00', lwd=2, lty=2)
lines(x=condition_x1 + x2_dens*40, y=x2_sequence, col="#f8ad00", lwd=2)
lines(x = c(condition_x1, condition_x1),
      y = c(cond_mean-cond_sd, cond_mean+cond_sd),
      lwd=3, col='#46c19a')
points(condition_x1, cond_mean, 
       col='#46c19a', pch=16, cex=2)

n_sample <- 20
points(x = rep(condition_x1, n_sample),
       y = rnorm(n_sample, cond_mean, cond_sd),
       pch=4, col = rgb(0, 0, 0, 0.5))
```

Les points sur l'axe (symbole x) conditionnés sont des échantillons tirés au hasard dans la distribution conditionnée.

Une autre manière de visualiser la distribution gaussienne binormale est de placer $x_1$ et $x_2$ côte à côte en abscisse, avec leur valeur en ordonnée. Le bloc de code suivant peut sembler lourd au premier coup d’œil: pas de panique, il s'agit surtout d'instructions graphiques. Vous pouvez vous amuser à changer les paramètres de la distribution binormale (section 1) ainsi que la valeur de $x_1$ à laquelle est conditionnée la distribution de $x_2$ (section 2).

```{r}
source("lib/plot_matrix.R")

# 1. Distribution
means_vec <- c(65, 65)
covariance_mat <- matrix(c(10, 6, 6, 15), ncol=2)

# 2. Condition
condition_x1 <- 61 # changer ce chiffre pour visualiser l'effet

# 3. Densité conditionnée
cond_parameters <- condMVN(mean=means_vec, sigma=covariance_mat,
                           dependent=2, given=1, X.given=condition_x1)
cond_mean <- cond_parameters$condMean
cond_sd <- sqrt(cond_parameters$condVar)
x2_sequence <- seq(50, 90, length=100)
x2_dens <- dnorm(x2_sequence, mean=cond_mean, sd=cond_sd)
x2_draw <- rnorm(1, cond_mean, cond_sd)

# 4. Graphiques
options(repr.plot.width = 8, repr.plot.height = 5)
layout(matrix(c(1,2,3,3), nrow=2), widths=c(1,2))
par(mar=c(4, 4, 1, 1), pty='s')

## 4.1 Ellipse
plot(ellipse(x=covariance_mat, centre=means_vec, levels=0.95), 
     type='l', xlab="BPM à 7:00", ylab="BPM à 8:00")
abline(v=condition_x1, col='#f8ad00', lwd=1)
lines(x=condition_x1 + x2_dens*40, y=x2_sequence, col="#f8ad00", lwd=1)
lines(x = c(condition_x1, condition_x1),
      y = c(cond_mean-cond_sd, cond_mean+cond_sd),
      lwd=2, col='#46c19a')
points(condition_x1, cond_mean, 
       col='#46c19a', pch=16, cex=1)
points(condition_x1, x2_draw, pch=16, col="#b94a73")

## 4.2 Covariance
plot_matrix(covariance_mat)

## 4.3 Série
plot(c(1, 2), c(condition_x1, x2_draw), xlim=c(0, 6), ylim=c(55, 75), type='l',
     xlab="Indice de la variable", ylab="Rythme cardiaque (bpm)")
points(1, condition_x1, pch=16, col='#46c19a', cex=3)
points(2, x2_draw, pch=16, col='#b94a73', cex=3)
```

Les valeurs que peuvent prendre le rythme cardiaque en $x_2$ sont tirées aléatoirement d'une distribution conditionnée. Sautons maintenant au cas multinormal, incluant 6 variables (*hexanormal*!). Afin d'éviter de composer une matrice de covariance à la mitaine, je me permets de la générer avec une fonction. Cette fonction particulière est nommée *fonction de base radiale* ou *exponentiel de la racine*.

$$K_{RBF} \left( x_i, x_j \right) = \sigma^2 exp \left( -\frac{\left( x_i - x_j \right)^2}{2 l^2}  \right) $$

```{r}
RBF_kernel <- function(x, sigma, l) {
  n <- length(x)
  k <- matrix(ncol = n, nrow = n)
  for (i in 1:n) {
    for (j in 1:n) {
      k[i, j] = sigma^2 * exp(-1/(2*l^2) * (x[i] - x[j])^2)
    }
  }
  colnames(k) <- paste0('x', 1:n)
  rownames(k) <- colnames(k)
  return(k)
}
```

Dans la fonction `RBF_kernel`, `x` désigne les dimensions, `sigma` désigne un écart-type commun à chacune des dimensions et `l` est la longueur désignant l'amplification de la covariance entre des dimensions éloignées (dans le sens que la première dimension est éloignée de la dernière). Pour 6 dimensions, avec un écart-type de 4 et une longueur de 2.

```{r}
covariance_6 <- RBF_kernel(1:6, sigma=4, l=2)
round(covariance_6, 2)
```

Changez la valeur de `l` permet de bien saisir son influence sur la matrice de covariance. Avec un `l` de 1, la covariance entre $x_1$ et $x_6$ est pratiquement nulle: elle est un peut plus élevée avec `l=2`. Pour reprendre l'exemple du rythme cardiaque, on devrait en effet s'attendre à retrouver une plus grande corrélation entre celles mesurées aux temps 4 et 5 qu'entre les temps 1 et 6.

De même que dans la situation où nous avions une distribution binormale, nous pouvons conditionner une distribution multinormale. Dans l'exemple suivant, je conditionne la distribution multinormale de 6 dimensions en spécifiant les valeurs prises par les deux premières dimensions. Le résultat du conditionnement est une distribution en 4 dimensions. Puisqu'il est difficile de présenter une distribution en 6D, le graphique en haut à gauche ne comprend que les dimensions 1 et 6. Remarquez que la corrélation entre les dimensions 1 et 6 est faible, en concordance avec la matrice de covariance générée par la fonction `RBF_kernel`. Lancez plusieurs fois le code et voyez ce qui advient des échantillonnages dans les dimensions 3 à 6 selon le conditionnement en 1 et 2.

```{r}
library("MASS")

# 1. Distribution
means_vec <- rep(65, 6)
covariance_mat <- covariance_6

# 2. Condition
conditions_x <- c(61, 74) # changer ces chiffres pour visualiser l'effet

# 3. Densité conditionnée
cond_parameters <- condMVN(mean=means_vec, sigma=covariance_mat, 
                           dependent.ind = 3:6, given.ind=1:2,
                           X.given=conditions_x)
cond_mean <- cond_parameters$condMean
cond_sd <- sqrt(cond_parameters$condVar)
x6_sequence <- seq(50, 90, length=100)
x6_dens <- dnorm(x2_sequence, mean=cond_mean[4], sd=cond_sd[4, 4])

x_3.6_draw <- mvrnorm(n = 1, mu = cond_mean, Sigma = cond_sd^2)

# 4. Graphiques
layout(matrix(c(1,2,3,3), nrow=2), widths=c(1,2))
par(mar=c(4, 4, 1, 1))

## 4.1 Ellipse
plot(ellipse(x=covariance_mat[c(1, 6), c(1, 6)], centre=means_vec[c(1, 6)], levels=0.95), 
     type='l', xlab="BPM à 7:00", ylab="BPM à 8:00")
abline(v=conditions_x[1], col='#f8ad00', lwd=1)
lines(x=condition_x1 + x6_dens*40, y=x2_sequence, col="#f8ad00", lwd=1)
lines(x = c(conditions_x[1], conditions_x[1]),
      y = c(cond_mean[4]-cond_sd[4, 4], cond_mean[4]+cond_sd[4, 4]),
      lwd=2, col='#46c19a')
points(conditions_x[1], cond_mean[4],
       col='#46c19a', pch=16, cex=1)
points(conditions_x[1], x_3.6_draw[4], pch=16, col="#b94a73")

## 4.2 Covariance
plot_matrix(covariance_mat, cex=0.8)

## 4.3 Série
plot(1:6, c(conditions_x, x_3.6_draw), xlim=c(0, 6), ylim=c(60, 85), type='l',
     xlab="Indice de la variable", ylab="Rythme cardiaque (bpm)")
points(c(1, 2), conditions_x, pch=16, col='#46c19a', cex=3)
points(3:6, x_3.6_draw, pch=16, col='#b94a73', cex=3)
```

La structure de la covariance assure que les dimensions proches prennent des valeurs similaires, assurant une courbe lisse et non en dents de scie. Pourquoi s'arrêter à 6 dimensions? Prenons-en plusieurs, puis générons plus d'un échantillon. Ensuite, utilisons ces simulations pour de calculer la moyenne et l'écart-type de chacune des dimensions.

```{r}
# 1. Distribution
n <- 20
means_vec <- rep(65, n)
covariance_mat <- RBF_kernel(x = 1:n, sigma = 10, l = 2)

# 2. Condition
conditions_x <- c(61, 74) # changer ces chiffres pour visualiser l'effet

# 3. Densité conditionnée
cond_parameters <- condMVN(mean=means_vec, sigma=covariance_mat, 
                           dependent.ind = 3:n, given.ind=1:2,
                           X.given=conditions_x)
cond_mean <- cond_parameters$condMean
cond_sd <- cond_parameters$condVar

# 4. Graphiques
par(mar=c(4, 4, 1, 1))

## 4.3 Série
plot(0, 0, xlim=c(0, n), ylim=c(40, 95), type='l',
     xlab="Indice de la variable", ylab="Rythme cardiaque (bpm)")

samples <- 50
x_3.n_draw <- mvrnorm(n = samples, mu = cond_mean, Sigma = cond_sd)
for (i in 1:samples) {
  lines(1:n, c(conditions_x, x_3.n_draw[i, ]), col = rgb(0, 0, 0, 0.15))
}
x_3.n_draw_mean <- apply(x_3.n_draw, 2, mean)
x_3.n_draw_sd <- apply(x_3.n_draw, 2, stats::sd)

lines(1:n, c(conditions_x, x_3.n_draw_mean), lwd = 2)
lines(1:n, c(conditions_x, x_3.n_draw_mean + x_3.n_draw_sd), col = "#b94a73", lwd = 2)
lines(1:n, c(conditions_x, x_3.n_draw_mean - x_3.n_draw_sd), col = "#b94a73", lwd = 2)
points(c(1, 2), conditions_x, pch=16, col='#46c19a', cex=2)
```

Revenons au rythme cardiaque. On pourra utiliser le conditionnement aux temps observés, soit 7:00, 8:00, 10:00, 14:00 et 17:00 pour estimer la distribution à 12:00 et 16:00, où à des dimensions artificielles quelconques ici fixées aux demi-heures.

```{r}
# 1. Distribution
n <- 21
means_vec <- rep(65, n)
covariance_mat <- RBF_kernel(x = 1:n, sigma = 5, l = 2)

# 2. Condition
conditions_x <- c(61, 74, 69, 67, 78)
conditions_indices <- c(1, 3, 7, 15, 21)
dependent_indices <- (1:20)[! 1:20 %in% conditions_indices]

# 3. Densité conditionnée
cond_parameters <- condMVN(mean=means_vec, sigma=covariance_mat, 
                           dependent.ind = dependent_indices,
                           given.ind=conditions_indices,
                           X.given=conditions_x)
cond_mean <- cond_parameters$condMean
cond_sd <- cond_parameters$condVar
samples <- 100
x_draw <- mvrnorm(n = samples, mu = cond_mean, Sigma = cond_sd)
means_draw <- apply(x_draw, 2, mean)
sd_draw <- apply(x_draw, 2, stats::sd)

# 4. Graphiques
par(mar=c(4, 4, 1, 1))

## 4.1 Combiner les prédictions
bpm <- rep(NA, n)
bpm[conditions_indices] <- conditions_x
bpm[dependent_indices] <- means_draw

bpm_sd <- rep(NA, n)
bpm_sd[conditions_indices] <- 0
bpm_sd[dependent_indices] <- sd_draw


## 4.2 Combiner les tirages et les données
x_draw_all <- matrix(ncol = n, nrow = samples)
for (i in 1:length(conditions_x)) x_draw_all[, conditions_indices[i]] <- conditions_x[i]
x_draw_all[, dependent_indices] <- x_draw


## 4.3 Série
plot(1:n, bpm, xlim=c(0, n), ylim=c(40, 90), type='l', lwd = 2,
     xlab="Indice de la variable", ylab="Rythme cardiaque (bpm)")
for (i in 1:samples) {
  lines(1:n, x_draw_all[i, ], col = rgb(0, 0, 0, 0.1))
}
lines(1:n, bpm+bpm_sd, col = "#b94a73", lwd = 2)
lines(1:n, bpm-bpm_sd, col = "#b94a73", lwd = 2)
points(conditions_indices, bpm[conditions_indices], pch=16, col='#46c19a', cex=2)

```

Comme on devrait s'y attendre, la régression résultant de la mise en indices de la distribution est précise aux mesures, et imprécise aux indices peu garnis en mesures. Nous avions utilisé 21 dimensions. **Lorsque l'on généralise la procédure à une quantité infinie de dimensions, on obtient un *processus gaussien*.** 

![](https://media.giphy.com/media/12R2bKfxceemNq/giphy.gif)

L'indice de la variable devient ainsi une valeur réelle. Un processus gaussien, $\mathcal{GP}$, est défini par une fonction de la moyenne, $m \left( x \right)$, et une autre de la covariance que l'on nomme *noyau* (ou *kernel*), $K \left( x, x' \right)$. Un processus gaussien est noté de la manière suivante:

$$\mathcal{GP} \sim \left( m \left( x \right), K \left( x, x' \right) \right)$$

La fonction définissant la moyenne peut être facilement écartée en s'assurant de centrer la variable réponse à zéro ($y_{centré} = y - \hat{y}$). Ainsi, par convention, on spécifie une fonction de moyenne comme retournant toujours un zéro. Quant au noyau, il peut prendre différentes fonctions de covariance ou combinaisons de fonctions de covariance. Règle générale, on utilisera un noyau permettant de définir deux paramètres: la hauteur ($\sigma$) et la longueur de l'ondulation ($l$).

```{r}
hyperparameters <- expand.grid(l=c(1, 3, 9), sigma=1:3)

# Graphique
n <- 100

samples_list <- list()
for (i in 1:nrow(hyperparameters)) {
  sample <- mvrnorm(n = 1, mu = rep(0, n), 
                  Sigma = RBF_kernel(x=1:n,
                                     sigma = hyperparameters$sigma[i],
                                     l = hyperparameters$l[i]))
  samples_list[[i]] <- data.frame(sigma = paste("sigma =", hyperparameters$sigma[i]),
                                  l = paste("l =", hyperparameters$l[i]),
                                  x = 1:n,
                                  sample = sample)
  
}
samples_df <- bind_rows(samples_list)
samples_df %>%
  ggplot(mapping = aes(x = x, y = sample)) +
  geom_line() +
  facet_grid(l ~ sigma)
```

On pourra ajouter à ce noyau un bruit blanc, c'est-à-dire une variation purement aléatoire, sans covariance (noyau générant une matrice diagonale).

Le noyau devient ainsi un *a priori*, et le processus gaussien conditionné aux données devient un *a posteriori* probabiliste.

Finalement, les processus gaussiens peuvent être extrapolés à plusieurs variables descriptives.

### Les processus gaussiens en `R`

Pas de souci, vous n'aurez pas à programmer vos propres fonctions pour lancer des processus gaussiens. Vous pourrez [passer par `caret`](https://topepo.github.io/caret/train-models-by-tag.html#gaussian-process). Vous pourriez, comme c'est le cas avec les réseaux neuronnaux, obtenir davantage de contrôle sur l'autoapprentissage en utilisant directement la fonction `gausspr` du package `kernlab`.


```{r}
library(kernlab)
x <- c(7, 8, 10, 14, 17)
y <- c(61, 74, 69, 67, 78)
y_sc <- (y - mean(y)) / sd(y)

m <- gausspr(x, y_sc, kernel = 'rbfdot',
             kpar = list(sigma = 4),
             variance.model = TRUE, scaled = TRUE,
             var = 0.01,
             cross = 2)

xtest <- seq(6, 18, by = 0.1)
y_sc_pred_mean <- predict(m, xtest, type="response")
y_pred_mean <- y_sc_pred_mean * sd(y) + mean(y)
y_sc_pred_sd <- predict(m, xtest, type="sdeviation")
y_pred_sd <- y_sc_pred_sd * sd(y)

plot(x, y, xlim = c(6, 18), ylim = c(45, 90))
lines(xtest, y_pred_mean)
lines(xtest, y_pred_mean + y_pred_sd, col="red")
lines(xtest, y_pred_mean - y_pred_sd, col="red")
abline(v=12, lty=3, col='gray50');text(12, 67, '?', cex=2)
abline(v=16, lty=3, col='gray50');text(16, 72, '?', cex=2)
```

### Application pratique

Les processus gaussiens sont utiles pour effectuer des prédictions sur des phénomène sur lesquels on désire éviter de se commettre sur la structure. Les séries temporelles ou les signaux spectraux en sont des exemples. Aussi, j'ai utilisé les processus gaussiens pour modéliser des courbes de réponse aux fertilisants.

EXEMPLE...

Prédiction spatiale:
- https://www.sciencedirect.com/science/article/pii/S2211675316300033
- https://stackoverflow.com/questions/43618633/multi-output-spatial-statistics-with-gaussian-processes






<!--chapter:end:11_autoapprentissage.Rmd-->

---
title: "Les données spatiales"
author: "Serge-Étienne Parent"
date: "`r format(Sys.Date())`"
output: github_document
---

# Les données spatiales {#chapitre-geo}

 ***
️\ **Objectifs spécifiques**:

À la fin de ce chapitre, vous

- serez familiers avec les notions de base géomatique: systèmes géodésiques, projections et données géoréférencées
- saurez utiliser R comme outil d'analyse spatiale (donnée associées à des points, lignes, polygones)
- saurez cartographier des données géoréférencées avec ggplot et Leaflet
- serez en mesure d'effectuer un autoapprentissage spatial avec les techniques des *k*-proches voisins et les processus gaussiens
- serez aptes à aborder une analyse géostatistique
- serez aptes à aborder une modélisation de distribution des espèces

 ***



<!--chapter:end:12_donnees-spatiales.Rmd-->

---
title: "Modélisation déterministe"
author: "Serge-Étienne Parent"
date: "`r format(Sys.Date())`"
output: github_document
---

# Modélisation déterministe {#chapitre-ode}

***
️\ **Objectifs spécifiques**:

À la fin de ce chapitre, vous

- saurez définir une équation différentielle ordinaire et une équation différentielle partielle
- saurez aptes à détecter un problème impliquant le besoin d'utiliser des équations différentielles
- serez en mesure d'effectuer une modélisation impliquant un système d'EDO en contexte écologique

De plus, en extra (non évalué, objectif incertain), vous
- serez en mesure d'effectuer une modélisation par différences finies impliquant une EDP simple en contexte écologique

***


On se réfère à la modélisation mécanistique lorsque des principes théoriques guident une modélisation, à l'inverse de la modélisation phénoménologique, qui est guidée par les données. Il existe de nombreuses techniques de modélisation mécanistique, mais la plupart sont guidées par les équations différentielles.

## Équations différentielles
Les équations différentielles permettent la résolution de problèmes impliquant des gradients dans le temps et dans l'espace. On les utilise pour modéliser la dynamique des populations, la thermodynamique, l'écoulement de l'eau dans les sols, le transport des solutés, etc. On en distingue deux grandes catégories: les équations différentielles ordinaires et partielles.

**Équations différentielles ordinaires (EDO)**. Les équations différentielles ordinaires s'appliquent sur des fonctions s'appliquant à une seule variables, qui est souvent le temps. On pourra suivre, par exemple, l'évolution de la température en un point, en fonction du temps à partir d'une condition initiale. Parfois, plusieurs EDO sont utilisées conjointement pour créer un système d'EDO que l'on pourra nommé un *système dynamique*. Les solutions analytiques des EDO sont parfois relativement faciles à résoudre, mais les ordinateurs permettent des résolutions numériques en quelques lignes de code.

**Équations différentielles partielles (EDP)**. Dans ce cas, ce sont plusieurs variables qui sont différenciées dans la même fonction. Il peut s'agir des coordonnées dans l'espace $[x, y, z]$ (régime permanent), qui peuvent aussi être appliqués à différents pas de temps (régime transitoire). Le problème sera délimité non pas seulement par des conditions initiales, mais aussi par des conditions aux frontières du modèle. Puisque que les solutions analytiques des EDP peuvent rarement être développées, on utilisera pratiquement toujours des approches numériques que sont principalement les méthodes de résolution par différences finies ou par éléments finis.

## Les équations différentielles ordinaires en modélisation écologique

L'évolution des populations dans le temps peut être abordée à l'aide de systèmes d'équations différentielles. Une simple équation décrivant la croissance d'une population peut être couplée à des schémas d'exploitation de cette population, que ce soit une exploitation forestière, une terre fourragère ou un territoire de chasse. On pourra aussi faire interagir des populations dans des schémas de relations biologiques. Ces processus peuvent être implémentés avec des processus aléatoires pour générer des schémas probabilistes. De plus, les biostatistiques et l'autoapprentissage peuvent être mis à contribution afin de calibrer les modèles.

### Évolution d'une seule population en fonction du temps

La croissance d'une population (ou de sa densité) isolée en fonction du temps dépend des conditions qui lui offre son environnement. Dans le cas de la biomasse d'une culture à croissance constante, le taux de croissance est toujours le même.

$$ \frac{d 🌿 }{dt} = c $$

$$ \int_0^t c dt = \int_{🌿_0}^{🌿(t)} ~d🌿 $$

$$ ct = 🌿(t) - 🌿_0$$

$$ 🌿(t) = 🌿_0 + ct $$

```{r}
par(mar=c(4, 4, 1, 1), ps=10)

y0 <- 2
c <- 2 # exprimé en individu / pas de temps
times <- seq(0, 6, 0.1)
y <- y0 + c * times
plot(times, y, 'l', xlab="An",  ylab="Population", ylim=c(0, max(y)))
text(max(times), max(y), round(max(y)))
```

Dans le cas d'une population qui se reproduit, une formulation simple modélise une évolution linéaire associée à un taux de natalité $n$ et un taux de mortalité $m$, où $r = n-m$ est le taux de croissance de la population d'une population de lapins 🐰 en fonction du temps $t$.

$$ \frac{d🐰}{dt} = n🐰 - m🐰 = r🐰 $$

$$ \int_0^t dt = \int_{🐰_0}^{🐰(t)} \frac{1}{r🐰} ~d🐰 $$

$$ t = \frac{1}{r} ln(🐰) \bigg\rvert_{🐰_0}^{🐰(t)} $$

$$ rt = ln \left( \frac{🐰(t)}{🐰_0} \right) $$

$$ 🐰(t) = 🐰_0 exp(rt) $$

La vitesse de croissance est constante pour une population constante, mais la croissance de la population est exponentielle étant donnée que chaque nouvel individu se reproduit.

```{r}
par(mar=c(4, 4, 1, 1), ps=10)

y0 <- 10
r <- 0.2 # exprimé en individu / pas de temps
times <- seq(0, 10, 0.1)
y <- y0 * exp(r*times)
plot(times, y, 'l', xlab="An",  ylab="Lapin", ylim=c(0, max(y)))
text(max(times), max(y), round(max(y)))
```

De 10 lapins au départ, nous en avons un peu plus de 75 après 10 ans... et près de 5 milliards après 100 ans! En fait, la capacité de support d'une population étant généralement limitée, on peut supposer que le taux de natalité décroit et que le taux de mortalité croit linéairement avec l'effectif.

$$ n(🐰) = \alpha - \beta 🐰 $$
$$ m(🐰) = \gamma + \delta 🐰 $$

On aura donc

$$ \frac{d🐰}{dt} = 🐰 \left( \alpha - \beta 🐰 \right) - 🐰 \left( \gamma + \delta 🐰 \right) = r🐰 \left( 1 - \frac{🐰}{K} \right) $$

où $r = \alpha - \gamma$ est l'ordonnée à l'origine du taux de croissance (théorique, lorsque la population est nulle) et $K = \frac{\alpha-\gamma}{\beta + \delta}$ est la capacité limite du milieu de subsistance. On pourra s'aider d'un logiciel de calcul symbolique comme `sympy` ou [`maxima`](https://andrejv.github.io/wxmaxima/) pour en tirer une solution analytique. Mais à ce point, nous utiliserons une approximation numérique. Nous utiliserons le module `deSolve`.

```{r}
library("deSolve")
```

`deSolve` demande de définir les paramètres de l'EDO ou du système d'EDO. Nous devons d'abord spécifier à quels pas de temps notre EDO doit être approximée. J'étends la plage de temps à 30 ans pour bien visualiser la courbe de croissance.

```{r}
times <- seq(0, 30, by = 0.5)
```

Les conditions initiales du système d'EDO sont aussi définies dans un vecteur. La seule condition initiale de notre EDO est le nombre initial de lapin.

```{r}
y0 <- c(lapin = 10)
```

On définira les paramètres dans un vecteur `p`. Dans notre cas, nous avons $r$, le taux de croissance à l'origine et $K$, la capacité de support de l'écosystème. Il est préférable de nommer les paramètres du vecteur pour éviter les erreurs.

```{r}
p <- c(r = 0.2, K = 40)
```

Enfin, une fonction définit l'EDO avec, comme entrées, les pas de temps, les conditions initiales et les paramètres. La sortie de la fonction est un vecteur des dérivées emboîtés dans une liste (lisez le fichier d'aide de la fonction `ode` pour les détails en lançant `?ode`).

```{r}
model_logistic <- function(t, y, p) {
  lapin <- y[1]
  dlapin_dt <- p[1] * lapin * (1 - lapin/p[2])
  return(list(c(dlapin_dt)))
}
```

Une fois que les pas de temps, les conditions initiales, les paramètres et le modèle sont définis, on les spécifie comme arguments dans la fonction `ode`. La sortie de la fonction `ode` est une matrice dont la première colonne comprend les pas de temps imposés, et les autres colonnes sont les dérivées spécifiées à la sortie de la fonction `ode`.


```{r}
lapin_t <- ode(y = y0, times = times, model_logistic, p)
head(lapin_t)
```

```{r}
par(mar=c(4, 4, 1, 1), ps=10)
plot(lapin_t[, 1], lapin_t[, 2], type='l', xlab="An",  ylab="Lapin", ylim=c(0, max(lapin_t[, 2])))
```

**Exercice**. Que ce passerait-il si le taux de croissance était négatif? Profitez-en pour changer les paramètres `r` et `K`.

**Exercice**. D'autres formulations existent pour exprimer des taux de croissance (Gompertz, Allee, etc.). En outre la formulation de Gompertz s'écrit comme suit.

$$ \frac{d🐰}{dt} = r🐰 \left( ln \frac{K}{🐰} \right) $$

Entrer cet EDO dans `R` avec `deSolve`.

### Population exploitée

L'exploitation d'une population peut être effectuée de différentes manières. D'abord, le prélèvement peut être effectué de manière constante, par exemple dans un élevage ou par la chasse ou la cueillette. Ajoutons un prélèvement constant dans une courbe de croissance logistique.

$$ \frac{d🐰}{dt} = r🐰 \left( 1 - \frac{🐰}{K} \right) - Q $$

où $Q$ est le quota, ou le prélèvement constant.

On pourra aussi effectuer un prélèvement proportionnel à la population.

$$ \frac{d🐰}{dt} = r🐰 \left( 1 - \frac{🐰}{K} \right) - E🐰 $$

où $E$ est l'effort d'exploitation.

Ou bien effectuer une série de prélèvement ponctuels, comme la récolte de plantes fourragères.

$$ \frac{d🌿}{dt} = c - \left[ 🌿 - \gamma \right] \bigg\rvert_{t=a, b, c, d, e, ...} $$

où $\gamma$ est le reste de la biomasse après la récolte et $t=a, b, c, d, e, ...$ sont les pas de temps où le bloc entre les crochets est actif, c'est-à-dire la période de récolte. La solution analytique d'une culture à croissance constante est plutôt facile à déduire.

Les fonctions de prélèvement peuvent être modulées à votre guise.

Prenons pour l'exemple un prélèvement constant et une croissance logistique.

```{r}
p <- c(r = 0.2, K = 40, Q = 1)

model_logistic_expl <- function(t, y, p) {
  lapin <- y[1]
  dlapin_dt <- p[1] * lapin * (1 - lapin/p[2]) - p[3]
  return(list(c(dlapin_dt)))
}

lapin_t <- ode(y = y0, times = times, model_logistic_expl, p)
par(mar=c(4, 4, 1, 1), ps=10)
plot(lapin_t[, 1], lapin_t[, 2], type='l', xlab="An",  ylab="Lapin", ylim=c(0, max(lapin_t[, 2])))
```

**Exercice**. Modéliser avec un prélèvement proportionnel.

L'**exploitation ponctuelle**, comme la récolte ou l'administration d'une série de traitements, implique l'utilisation d'approches intermittentes. `deSolve` ignore les changements dans les variables d'état (`y`) tels que définis dans les dérivés. Pour ce faire, nous devons avoir recours à des évènements dans le jargon de `deSolve`. Ces évènements doivent être spécifiés dans un `data.frame` ou une liste. Il est difficile de trouver un exemple générique pour modéliser des évènements. Pour en savoir davantage, je vous invite donc à consulter la fiche d'aide `?events`.

Dans notre cas, nous allons modéliser une récolte de plantes fourragères. La récolte est déclenchée lorsque le rendement atteint 2 t/ha, et laisser 0.3 t/ha au sol pour assurer le renouvellement pour les coupes subséquentes. Définissons d'abord les entrées du modèles.

```{r}
times <- seq(0, 120, 0.1)
p <- c(r = 0.1, K = 2.5)
y0 <- c(champ = 0.1)
```

Nous devons définir une fonction root, dont la sortie est une valeur qui déclenchera un évènement lorsque la valeur sera nulle. Dans notre cas, la valeur correspond simplement au rendement moins 2, la quantité au champ y[1]. Notez que d'autres stratégies peuvent être utilisées pour déclencher une récolte, par exemple le pourcentage de floraison qui demanderait des simulations plus poussées.

```{r}
recolte_root <- function(t, y, p) y[1]-2
```


Puis, lorsque la fonction root est déclenchée, l’évènement ramène la quantité au champs à 1 t/ha, une quantité qui permet de relancer la croissance.

```{r}
recolte_event <- function(t, y, p) {
    y[1] <- 0.3
    return(y)
}
```

La fonction du modèle est telle qu'utilisée auparavant: une fonction logistique.

```{r}
recolte <- function(t, y, p) {
    champ <- y[1]
    dchamp_dt <- p[1] * champ * (1 - champ/p[2])
    return(list(c(dchamp_dt)))
}
```

La fonction `ode` est lancée en entrant les fonction `root` et `events`.

```{r}
out <- ode(times = times, y = y0, func = recolte, parms = p,
           rootfun = recolte_root,
           events = list(func = recolte_event, root = TRUE),
          method="impAdams")
plot(out)
```

Nous pourrons organiser deux récoltes de 1.7 t/ha et une de 2 t/ha pour terminer la saison.

**Exercice**. Qu'adviendrait-il si vous laissiez 0.15 t/ha au champ au lieu de 0.3? Ou si vous laissiez 1 t/ha? Ou si vous déclenchiez une récolte à 2.3 t/ha?

**Défi**. Pouvez-vous modéliser l'ensilage?

### Interactions biologiques

Les interactions biologiques entre deux espèces à un stade de croissance défini peuvent prendre différentes formes, du mutualisme (les deux espèces bénéficient de la relation) à la compétition (les deux espèces se nuisent) en passant par la prédation ou le parasitisme (une espèce bénéficie de l'autre en lui nuisant) ou le neutralisme (aucun effet). Ces effets sont décrits dans [Pringle (2016)](https://doi.org/10.1371/journal.pbio.2000891) en un tableau synthèse.


<center>
<img src="images/13_journal.pbio.2000891.g001.png" width=600px>
Source: Pringle, E.G. 2016. Orienting the Interaction Compass: Resource Availability as a Major Driver of Context Dependence. Plos Biology. https://doi.org/10.1371/journal.pbio.2000891
</center>

Ces interactions peuvent être décrite mathématiquement dans des systèmes d'EDO, ou EDO couplées. Le cas d'étude le plus courant reprend le système d'équation prédateur-proie de **Lotka-Volterra**, deux auteurs ayant développé de manière indépendante des équations similaires respectivement en 1925 et 1926.

Les équations de Lotka-Volterra supposent une croissance illimitée des deux espèces: les proies 🐰 se reproduisent par elles-mêmes ($\alpha 🐰$), tandis que les prédateurs 🦊 croissent selon la disponibilité des proies ($\delta 🐰🦊$). À l'inverse, la mortalité des proies dépend du nombre de prédateurs ($- \beta 🐰🦊$), mais la mortalité des prédateurs est indépendante des proies ($- \gamma 🦊$). On obtient ainsi un système d'équation.

$$\frac{d🐰}{dt} = \alpha 🐰 - \beta 🐰🦊 = 🐰 \left( \alpha - \beta 🦊 \right)$$

$$\frac{d🦊}{dt} = \delta 🐰🦊 - \gamma 🦊 = 🦊 \left( \delta 🐰 - \gamma \right) $$

À l'équilibre de 🐰, c'est-à-dire où $\frac{d🐰}{dt} = 0$, on retrouve $🐰=0$ ou $🦊 = \frac{\alpha}{\beta}$. De même, à l'équilibre de 🦊, on retrouve $🦊=0$ ou $🐰 = \frac{\gamma}{\delta}$. En termes mathématiques, ces équilibre sont des isoclines, des points d'inflexion dans le système d'EDO.

Nous allons résoudre les équations de Lotka-Volterra avec `deSolve`. Rappelons-nous que nous devons définir des pas de temps où approximer les populations (`times`), des conditions initiales (`y0`) et des paramètres (`p`).

```{r}
times <- seq(0, 30, by = 0.1)

y0 <- c(lapin = 3, renard = 1)

p <- c(alpha = 2, # taux de croissance des lapins (naissance - mortalité, 1/an)
      beta = 0.8, # taux de prédation des lapins (renard / an)
      delta = 0.1, # taux de conversion lors de la prédation (lapin / renard)
      gamma = 0.2) # mortalité naturelle des renards (1/an)
```

On peut calculer d'emblée les isoclines.

```{r}
lapin_iso <- p[4]/p[3]
renard_iso <- p[1]/p[2]
```

Nous devons ensuite créer notre modèle.

```{r}
modele_LV <- function(t, y, p) {
  lapin = y[1]
  renard = y[2]
  dlapin_dt = p[1] * lapin - p[2] * lapin * renard
  drenard_dt = p[3] * lapin * renard - p[4] * renard
  return(list(c(dlapin_dt, drenard_dt)))
}
```

Lançons l'approximation.

```{r}
effectifs_t = ode(y = y0, times = times, modele_LV, p)
head(effectifs_t)
```

```{r}
par(mar=c(4, 4, 1, 1), ps=10)

plot(effectifs_t[, 1], effectifs_t[, 2], type = 'l', ylim = c(0, max(effectifs_t[, 2])),
     xlab = 'Temps', ylab = "Nombre d'individus") # lapins
lines(effectifs_t[, 1], effectifs_t[, 3], col = 'red')

legend(x=4, y=12, legend=c("Lapins", "Renards"), col=c("black", "red"),
       lty=c(1, 1), cex=1.2)
```

Lorsque la population de lapins croit, celle des renards croit à retardement jusqu'à ce que la population de lapin diminue jusqu'à être presque éteinte. Dans ces conditions, la population de renard ne peut plus être soutenue, et décroit, ce qui en retour donne l'opportunité de la population de lapins de resurgir.

```{r}
par(mar=c(4, 4, 1, 1), ps=10)

plot(effectifs_t[, 2], effectifs_t[, 3], type = 'l', xlab = "Nombre lapins",
     ylab= "Nombre de renards",
     xlim = c(0, max(effectifs_t[, 2])), ylim = c(0, max(effectifs_t[, 3])))

# isoclines
abline(v=lapin_iso, lty=2, col="black")
abline(h=renard_iso, lty=2, col="red")
points(lapin_iso, renard_iso)

# condition initiale
points(y0[1], y0[2], pch = 16)
```

Les conditions initiales sont responsables de l'amplitude des cycles. En faisant les faisant varier et en portant graphiquement les vecteurs de flux, on peut mieux apprécier l'importance des isoclines, qui séparent la direction que prend la relation entre deux espèces.

```{r}
effectifs_i <- list()
lapin_0 <- 1:30
for (i in 1:length(lapin_0)) {
    y0[1] <- lapin_0[i]
    effectifs_i[[i]] <- ode(y = y0, times = times, modele_LV, p)
    offsets <- effectifs_i[[i]][-1, -1] - effectifs_i[[i]][-nrow(effectifs_i[[i]]), -1]
    colnames(offsets) <- c("d_lapin", "d_renard")
    effectifs_i[[i]] <- cbind(effectifs_i[[i]][-1, ], offsets)
}
effectifs_df <- do.call(rbind.data.frame, effectifs_i)
```

```{r}
library("plotrix")

plot(effectifs_df[, 2], effectifs_df[, 3], type = 'n', xlab = "Nombre lapins",
     ylab= "Nombre de renards",
     xlim = c(0, max(effectifs_df[, 2])), ylim = c(0, max(effectifs_df[, 3])))

# isoclines
abline(v=lapin_iso, lty=2, col="black")
abline(h=renard_iso, lty=2, col="red")
points(lapin_iso, renard_iso)

vectorField(u=effectifs_df[, 4], v=effectifs_df[, 5],
            xpos=effectifs_df[, 2], ypos=effectifs_df[, 3],
            scale=0.1, headspan=0.05,
            vecspec="lonlat")
```

Nous avons modélisé une relation biologique de prédation. Il existe dans la littérature une panoplie de modèles d'EDO pour décrire les relations biologiques, qui peuvent être modélisés entre plusieurs espèces pour créer des réseaux trophiques complexes. Toutefois, la difficulté de collecter des données en quantité et en qualité suffisante rendent ces modèles difficiles à appréhender.

**Exercice**. Qu’adviendrait-il des populations si l'on prenait plutôt un profil de croissance logistique chez les lapins?

$$\frac{d🐰}{dt} = r🐰 \left( 1-\frac{x}{K} \right) - \beta 🐰🦊 $$

$$\frac{d🦊}{dt} = \delta 🐰🦊 - \gamma 🦊 $$

**Exercice**. Modéliser une compétition interspécifique où chaque population croit de manière logistique.

$$\frac{d🐁}{dt} = r_1 🐁 \left( 1-\frac{🐁}{K_1} -\alpha \frac{🐀}{K_1} \right) $$

$$\frac{d🐁}{dt} = r_2 🐀 \left( 1-\frac{🐀}{K_2} -\beta \frac{🐁}{K_2} \right) $$

où $r_1$ et $r_2$ sont les taux de croissances respectifs des 🐁 et des 🐀, ainsi que $K_1$ et que $K_2$ sont les capacités de support des 🐁 et des 🐀. Le coefficient $\alpha$ décrit l'ampleur de la compétition de 🐀 sur 🐁 et le coefficient $\beta$ décrit l'ampleur de la compétition de 🐁 sur 🐀 ($\alpha$ et $\beta$ sont >= 0).

**Exercice**. Les interactions biologiques forment une bonne introduction aux systèmes d'équations différentielles ordinaires. On fait néanmoins souvent référence aux équations de [Lorenz (1963)](https://journals.ametsoc.org/doi/abs/10.1175/1520-0469%281963%29020%3C0130:DNF%3E2.0.CO;2), qui a développé un système d'EDO chaotique depuis trois équations,

$$ X' = aX + YZ, $$
$$ Y' = b \left(Y-Z\right), $$
$$ Z' =  -XY + cY - Z, $$

où $X$ est la température horizontale, $Y$ est la température verticale, $Z$ est le flux de chaleur convectif, et où l'on retrouve les paramètres $a = -8/3$, $b=-10$ et $c=28$.

Résoudre les équations de Lorents avec `deSolve`. Porter graphiquement les relations entre X, Y et Z.

## Les équations différentielles partielles en modélisation écologique

Contrairement aux EDO, la solution des équations différentielle partielles (EDP) dépend de plus d'une variable indépendante. Typiquement, elles dépendent de coordonnées spatiales. Elles peuvent aussi dépendre du temps. Dans cette section, nous allons explorer les régimes permanents, c'est-à-dire indépendants du temps, en utilisant la méthode des différences finies. Nous allons aussi explorer les problèmes transitoires, qui eux dépendent du temps, en utilisant la méthode des lignes.

À venir...

<!--chapter:end:13_modelisation-deterministe.Rmd-->

