[
["chapitre-ordination.html", "9 Association, partitionnement et ordination 9.1 Espaces d‚Äôanalyse 9.2 Analyse d‚Äôassociation 9.3 Partitionnement 9.4 Ordination", " 9 Association, partitionnement et ordination Ô∏è Objectifs sp√©cifiques: √Ä la fin de ce chapitre, vous serez en mesure d‚Äôeffectuer des calculs permettant de mesurer des diff√©rence entre des observations, des groupes d‚Äôobservation ou des variables observ√©es serez en mesure d‚Äôeffection des analyses de partitionnement hi√©rarchiques et non-hi√©rarchiques serez en mesure d‚Äôeffectuer des calculs d‚Äôordination √† l‚Äôaide des techniques de r√©duction d‚Äôaxe communes: analyse en composante principale, l‚Äôanalyse de correspondance, l‚Äôanalyse en coordonn√©es principales, analyse discriminante lin√©aire, l‚Äôanalyse de redondance et l‚Äôanalyse canonique des correspondances. Les donn√©es √©cologiques incluent g√©n√©ralement plusieurs variables qui doivent √™tre analys√©es conjointement. Les techniques pour l‚Äôanalyse multivari√©e de donn√©es √©cologiques ont grandi en nombre et en complexit√©, laissant √©merger l‚Äô√©cologie num√©rique, un nouveau domaine d‚Äô√©tude scientifique initi√© par Pierre Legendre et Louis Legendre dont l‚Äôouvrage Numerical Ecology, aujourd‚Äôhui √† sa troisi√®me √©dition, reste un incontournable pour qui s‚Äôint√©resse aux math√©matiques sous-jacentes au domaine. Pour la r√©daction de ces notes, c‚Äôest toutefois le livre Numerical ecology with R, √©crit par Borcard et al. (2011) pour offrir un guide √† qui voudrait une approche plus appliqu√©e. L‚Äô√©cologie num√©rique sera effleur√©e dans ce chapitre, qui introduit √† trois concepts. Les associations permettent de quantifier la ressemblance ou la diff√©rence entre deux observation (√©chantillons) ou variables (descripteurs). Lorsque l‚Äôon a plus de deux variables ou plus de deux site, nous obtenons des matrices d‚Äôassociation. Le partitionnement permet de regrouper des observations ou des variables selon des m√©triques d‚Äôassociation. L‚Äôordination vise par l‚Äôinterm√©diaire de techniques de r√©duction d‚Äôaxe √† mettre de l‚Äôordre dans des donn√©es dont le nombre √©lev√© de variables peut amener √† des difficult√©s d‚Äôappr√©ciation et d‚Äôinterpr√©taion. library(&quot;tidyverse&quot;) ## -- Attaching packages ------------------------------------------------------ tidyverse 1.3.0 -- ## v ggplot2 3.3.0 v purrr 0.3.4 ## v tibble 3.0.0 v dplyr 0.8.5 ## v tidyr 1.0.2 v stringr 1.4.0 ## v readr 1.3.1 v forcats 0.5.0 ## -- Conflicts --------------------------------------------------------- tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() 9.1 Espaces d‚Äôanalyse 9.1.1 Abondance et occurence L‚Äôabondance est le d√©compte d‚Äôesp√®ces observ√©es, tandis que l‚Äôoccurence est la pr√©sence ou l‚Äôabsence d‚Äôune esp√®ce. Le tableau suivant contient des donn√©es d‚Äôabondance. abundance &lt;- tibble(&#39;Bruant familier&#39; = c(1, 0, 0, 3), &#39;Citelle √† poitrine rousse&#39; = c(1, 0, 0, 0), &#39;Colibri √† gorge rubis&#39; = c(0, 1, 0, 0), &#39;Geai bleu&#39; = c(3, 2, 0, 0), &#39;Bruant chanteur&#39; = c(1, 0, 5, 2), &#39;Chardonneret&#39; = c(0, 9, 6, 0), &#39;Bruant √† gorge blanche&#39; = c(1, 0, 0, 0), &#39;M√©sange √† t√™te noire&#39; = c(20, 1, 1, 0), &#39;Jaseur bor√©al&#39; = c(66, 0, 0, 0)) Ce tableau peut √™tre rapidement transform√© en donn√©es d‚Äôoccurence, qui ne comprennent que l‚Äôinformation bool√©enne de pr√©sence (not√© 1) et d‚Äôabsence (not√© 0). occurence &lt;- abundance %&gt;% transmute_all(~if_else(. &gt; 0, 1, 0)) L‚Äôespace des esp√®ces (ou des variables ou descripteurs) est celui o√π les esp√®ces forment les axes et o√π les sites sont positionn√©s dans cet espace. Il s‚Äôagit d‚Äôune perspective en mode R, qui permet principalement d‚Äôidentifier quels esp√®ces se retrouvent plus courrament ensemble. abundance %&gt;% select(`Bruant chanteur`, Chardonneret, `M√©sange √† t√™te noire`) ## # A tibble: 4 x 3 ## `Bruant chanteur` Chardonneret `M√©sange √† t√™te noire` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 20 ## 2 0 9 1 ## 3 5 6 1 ## 4 2 0 0 Dans l‚Äôespace des sites (ou les √©chantillons ou objets), on transpose la matrice d‚Äôabondance. On passe ici en mode Q, o√π chaque point est une esp√®ce, et o√π l‚Äôon peut observer quels √©chantillons sont similaires. abundance %&gt;% t() ## [,1] [,2] [,3] [,4] ## Bruant familier 1 0 0 3 ## Citelle √† poitrine rousse 1 0 0 0 ## Colibri √† gorge rubis 0 1 0 0 ## Geai bleu 3 2 0 0 ## Bruant chanteur 1 0 5 2 ## Chardonneret 0 9 6 0 ## Bruant √† gorge blanche 1 0 0 0 ## M√©sange √† t√™te noire 20 1 1 0 ## Jaseur bor√©al 66 0 0 0 9.1.2 Environnement L‚Äôespace de l‚Äôenvironnement comprend souvent un autre tableau contenant l‚Äôinformation sur l‚Äôenvironnement o√π se trouve les esp√®ces: les coordonn√©es et l‚Äô√©l√©vation, la pente, le pH du sol, la pluviom√©trie, etc. 9.2 Analyse d‚Äôassociation Nous utiliserons le terme association comme une mesure pour quantifier la ressemblance ou la diff√©rence entre deux objets (√©chantillons) ou variables (descripteurs). Alors que la corr√©lation et la covariance sont des mesures d‚Äôassociation entre des variables (analyse en mode R), la similarit√© et la distance sont deux types de une mesure d‚Äôassociation entre des objets (analyse en mode Q). Une distance de 0 est mesur√©e chez deux objets identiques. La distance augmente au fur et √† mesure que les objets sont dissoci√©s. Une similarit√© ayant une valeur de 0 indique aucune association, tandis qu‚Äôune valeur de 1 indique une association parfaite. √Ä l‚Äôoppos√©, la dissimilarit√© est √©gale √† 1-similarit√©. La distance peut √™tre li√©e √† la similarit√© par la relation: \\[distance=\\sqrt{1-similarit√©}\\] ou \\[distance=\\sqrt{dissimilarit√©}\\] La racine carr√©e permet, pour certains indices de similarit√©, d‚Äôobtenir des propri√©t√©s eucl√©diennes. Pour plus de d√©tails, voyez le tableau 7.2 de Legendre et Legendre (2012). Les matrices d‚Äôassociation sont g√©n√©ralement pr√©sent√©es comme des matrices carr√©es, dont les dimensions sont √©gales au nombre d‚Äôobjets (mode Q) ou de vrariables (mode R) dans le tableau. Chaque √©l√©ment (‚Äúcellule‚Äù) de la matrice est un indice d‚Äôassociation entre un objet (ou une variable) et un autre. Ainsi, la diagonale de la matrice est un vecteur nul (distance ou dissimilarit√©) ou unitaire (similarit√©), car elle correspond √† l‚Äôassociation entre un objet et lui-m√™me. Puisque l‚Äôassociation entre A et B est la m√™me qu‚Äôentre B et A, et puisque la diagonale retourne une valeur convenue, il est possible d‚Äôexprimer une matrice d‚Äôassociation en mode ‚Äúcompact‚Äù, sous forme de vecteur. Le vecteur d‚Äôassociation entre des objets A, B et C contiendra toute l‚Äôinformation n√©cessaire en un vecteur de trois chiffres, [AB, AC, BC], plut√¥t qu‚Äôune matrice de dimension \\(3 \\times 3\\). L‚Äôimpact sur la m√©moire vive peut √™tre consid√©rable pour les calculs comprenant de nombreuses dimensions. En R, les calculs de similarit√© et de distances peuvent √™tre effectu√©s avec le module vegan. La fonction vegdist permet de calculer les indices d‚Äôassociation en forme carr√©e. Nous verons plus tard les m√©thodes de mesure de similarit√© et de distance plus loin. Pour l‚Äôinstant, utilisons la m√©thode de Jaccard pour une d√©monstration sur des donn√©es d‚Äôoccurence. library(&quot;vegan&quot;) ## Loading required package: permute ## Loading required package: lattice ## This is vegan 2.5-6 vegdist(occurence, method = &quot;jaccard&quot;, diag = TRUE, upper = TRUE) ## 1 2 3 4 ## 1 0.0000000 0.7777778 0.7500000 0.7142857 ## 2 0.7777778 0.0000000 0.6000000 1.0000000 ## 3 0.7500000 0.6000000 0.0000000 0.7500000 ## 4 0.7142857 1.0000000 0.7500000 0.0000000 Remarquez que vegdist retourne une matrice dont la diagonale est de 0 (on l‚Äôaffiche en sp√©cifiant diag = TRUE). La diagonale est l‚Äôassociation d‚Äôun objet avec lui-m√™me. Or la similarit√© d‚Äôun objet avec lui-m√™me devrait √™tre de 1! En fait, par convention vegdist retourne des dissimilarit√©s, non pas des similarit√©s. La matrice de distance serait donc calcul√©e en extrayant la racine carr√©e des √©l√©ments de la matrice de dissimilarit√©: dissimilarity &lt;- vegdist(occurence, method = &quot;jaccard&quot;, diag = TRUE, upper = TRUE) distance &lt;- sqrt(dissimilarity) distance ## 1 2 3 4 ## 1 0.0000000 0.8819171 0.8660254 0.8451543 ## 2 0.8819171 0.0000000 0.7745967 1.0000000 ## 3 0.8660254 0.7745967 0.0000000 0.8660254 ## 4 0.8451543 1.0000000 0.8660254 0.0000000 Dans le chapitre sur l‚Äôanalyse compositionnelle, nous avons abord√© les significations diff√©rentes que peuvent prendre le z√©ro. L‚Äôinformation fournie par un z√©ro peut √™tre diff√©rente selon les circonstances. Dans le cas d‚Äôune variable continue, un z√©ro signifie g√©n√©ralement une mesure sous le seuil de d√©tection. Deux tissus dont la concentration en cuivre est nulle ont une afinit√© sous la perspective de la concentration en cuivre. Dans le cas de mesures d‚Äôabondance (d√©compte) ou d‚Äôoccurence (pr√©sence-absence), on pourra d√©crire comme similaires deux niches √©cologiques o√π l‚Äôon retrouve une esp√®ce en particulier. Mais deux sites o√π l‚Äôon de retouve pas d‚Äôours polaires ne correspondent pas n√©cessairement √† des niches similaires! En effet, il peut exister de nombreuses raisons √©cologiques et m√©thodologiques pour lesquelles l‚Äôesp√®ces ou les esp√®ces n‚Äôont pas √©t√© observ√©es. C‚Äôest le probl√®me des double-z√©ros (esp√®ces non observ√©es √† deux sites), probl√®me qui est amplifi√© avec les grilles comprenant des esp√®ces rares. La ressemblance entre des objets comprenant des donn√©es continues devrait √™tre calcul√©e gr√¢ce √† des indicateurs sym√©triques. Inversement, les affinit√©s entre les objets d√©crits par des donn√©es d‚Äôabondance ou d‚Äôoccurence susceptibles de g√©n√©rer des probl√®mes de double-z√©ros devraient √™tre √©valu√©es gr√¢ce √† des indicateurs asym√©triques. Un d√©fi suppl√©mentaire arrive lorsque les donn√©es sont de type mixte. Nous utiliserons la convention de vegan et nous calculerons la dissimilarit√©, non pas la similarit√©. Les mesures de dissimilarit√© sont calcul√©es sur des donn√©es d‚Äôabondance ou des donn√©es d‚Äôoccurence. Notons qu‚Äôil existe beaucoup de confusion dans la litt√©rature sur la mani√®re de nommer les dissimilarit√©s (ce qui n‚Äôest pas le cas des distances, dont les noms sont reconnus). Dans les sections suivantes, nous noterons la dissimilarit√© avec un \\(d\\) minuscule et la distance avec un \\(D\\) majuscule. 9.2.1 Association entre objets (mode Q) 9.2.1.1 Objets: Abondance La dissimilarit√© de Bray-Curtis est asym√©trique. Elle est aussi appel√©e l‚Äôindice de Steinhaus, de Czekanowski ou de S√∏rensen. Il est important de s‚Äôassurer de bien s‚Äôentendre la m√©thode √† laquelle on fait r√©f√©rence. L‚Äô√©quation enl√®ve toute ambiguit√©. La dissimilarit√© de Bray-Curtis entre les points A et B est calcul√©e comme suit. \\[d_{AB} = \\frac {\\sum \\left| A_{i} - B_{i} \\right| }{\\sum \\left(A_{i}+B_{i}\\right)}\\] Utilisons vegdist pour g√©n√©rer les matrices d‚Äôassociation. Le format ‚Äúliste‚Äù de R est pratique pour enregistrer la collection d‚Äôobjets, dont les matrice d‚Äôassociation que nous allons cr√©er dans cette section. associations_abund &lt;- list() associations_abund[[&#39;BrayCurtis&#39;]] &lt;- vegdist(abundance, method = &quot;bray&quot;) associations_abund[[&#39;BrayCurtis&#39;]] ## 1 2 3 ## 2 0.9433962 ## 3 0.9619048 0.4400000 ## 4 0.9591837 1.0000000 0.7647059 La dissimilarit√© de Bray-Curtis est souvent utilis√©e dans la litt√©rature. Toutefois, la version originale de Bray-Curtis n‚Äôest pas tout √† fait m√©trique (semim√©trique). Cons√©quemment, la dissimilarit√© de Ruzicka (une variante de la dissimilarit√© de Jaccard pour les donn√©es d‚Äôabondance) est m√©trique, et devrait probablement √™tre pr√©f√©r√© √† Bary-Curtis (Oksanen, 2006). \\[d_{AB, Ruzicka} = \\frac { 2 \\times d_{AB, Bray-Curtis} }{1 + d_{AB, Bray-Curtis}}\\] associations_abund[[&#39;Ruzicka&#39;]] &lt;- associations_abund[[&#39;BrayCurtis&#39;]] * 2 / (1 + associations_abund[[&#39;BrayCurtis&#39;]]) La dissimilarit√© de Kulczynski (aussi √©crit Kulsinski) est asym√©trique et semim√©trique, tout comme celle de Bray-Curtis. Elle est calcul√©e comme suit. \\[d_{AB} = 1-\\frac{1}{2} \\times \\left[ \\frac{\\sum min(A_i, B_i)}{\\sum A_i} + \\frac{\\sum min(A_i, B_i)}{\\sum B_i} \\right]\\] associations_abund[[&#39;Kulczynski&#39;]] &lt;- vegdist(abundance, method = &quot;kulczynski&quot;) Une approche commune pour mesurer l‚Äôassociation entre sites d√©crits par des donn√©es d‚Äôabondance est la distance de Hellinger. Notez qu‚Äôil s‚Äôagit ici d‚Äôune distance, non pas d‚Äôune dissimilarit√©. Pour l‚Äôobtenir, on doit d‚Äôabord diviser chaque donn√©e d‚Äôabondance par l‚Äôabondance totale pour chaque site pour obtenir les esp√®ces en tant que proportions, puis on extrait la racine carr√©e de chaque √©l√©ment. Enfin, on calcule la distance euclidienne entre les proportions de chaque site. Pour rappel, une distance euclidienne est la g√©n√©ralisation en plusieurs dimensions du th√©or√®me de Pythagore, \\(c = \\sqrt{a^2 + b^2}\\). \\[D_{AB} = \\sqrt {\\sum \\left( \\frac{A_i}{\\sum A_i} - \\frac{B_i}{\\sum B_i} \\right)^2}\\] üò± Attention La distance d‚ÄôHellinger h√©rite des biais li√©es aux donn√©es compositionnelles. Elle peut √™tre substiti√©e par une matrice de distances d‚ÄôAitchison. associations_abund[[&#39;Hellinger&#39;]] &lt;- dist(decostand(abundance, method=&quot;hellinger&quot;)) Toute comme la distance d‚ÄôHellinger, la distance de chord est calcul√©e par une distance euclidienne sur des donn√©es d‚Äôabondance transform√©es de sorte que chaque ligne ait une longueur (norme) de 1. associations_abund[[&#39;Chord&#39;]] &lt;- dist(decostand(abundance, method=&quot;normalize&quot;)) La m√©trique du chi-carr√©, ou \\(\\chi\\)-carr√©, ou chi-square, donne davantage de poids aux esp√®ces rares qu‚Äôaux esp√®ces communes. Son utilisation est recommand√©e lorsque les esp√®ces rares sont de bons indicateurs de conditions √©cologiques particuli√®res (Legendre et Legendre, 2012, p. 308). \\[ d_{AB} = \\sqrt{\\sum _j \\frac{1}{\\sum y_j} \\left( \\frac{A_j}{\\sum A} - \\frac{B_j}{\\sum B} \\right)^2 } \\] La m√©trique peut √™tre transform√©e en distance en la multipliant par la racine carr√©e de la somme totale des esp√®ces dans la matric d‚Äôabondance (\\(X\\)). \\[ D_{AB} = \\sqrt{\\sum X} \\times d_{AB} \\] associations_abund[[&#39;ChiSquare&#39;]] &lt;- dist(decostand(abundance, method=&quot;chi.square&quot;)) Une manni√®re visuellement plus int√©ressante de pr√©senter une matrice d‚Äôassociation est un graphique de type heatmap. associations_abund_df &lt;- list() for (i in 1:length(associations_abund)) { associations_abund_df[[i]] &lt;- data.frame(as.matrix(associations_abund[[i]])) colnames(associations_abund_df[[i]]) &lt;- rownames(associations_abund_df[[i]]) associations_abund_df[[i]]$row &lt;- rownames(associations_abund_df[[i]]) associations_abund_df[[i]] &lt;- associations_abund_df[[i]] %&gt;% gather(key=row) associations_abund_df[[i]]$column = rep(1:4, 4) associations_abund_df[[i]]$dist &lt;- names(associations_abund)[i] } associations_abund_df &lt;- do.call(rbind, associations_abund_df) ggplot(associations_abund_df, aes(x=row, y=column)) + facet_wrap(. ~ dist, nrow = 2) + geom_tile(aes(fill = value)) + geom_text(aes(label = round(value, 2))) + scale_fill_gradient2(low = &quot;#00ccff&quot;, mid = &quot;#aad400&quot;, high = &quot;#ff0066&quot;, midpoint = 2) + labs(x=&quot;Site&quot;, y=&quot;Site&quot;) Peu importe le type d‚Äôassociation utilis√©e, les heatmaps montrent les m√™mes tendances. Les assocaitions de dissimilarit√© (Bray-Curtis, Kulczynski et Ruzicka) s‚Äô√©talent de 0 √† 1, tandis que les distances (Chi-Square, Chord et Hellinger) partent de z√©ro, mais n‚Äôont pas de limite sup√©rieure. On note les plus grandes diff√©rences entre les sites 2 et 4, tandis que les sites 2 et 3 sont les plus semblables pour toutes les mesures d‚Äôassociation √† l‚Äôexception de la dissimilarit√© de Kulczynski. 9.2.1.2 Objets: Occurence (pr√©sence-absence) Des indices d‚Äôassociation diff√©rents devraient √™tre utilis√©s lorsque des donn√©es sont compil√©es sous forme bool√©enne. En g√©n√©ral, les tableaux de donn√©es d‚Äôoccurence seront compil√©s avec des 1 (pr√©sence) et des 0 (absence). La similarit√© de Jaccard entre le site A et le site B est la proportion de double 1 (pr√©sences de 1 dans A et B) parmi les esp√®ces. La dissimilari√© est la proportion compl√©mentaire (comprenant [1, 0], [0, 1] et [0, 0]). La distance de Jaccard est la racine carr√©e de la dissimilarit√©. associations_occ &lt;- list() associations_occ[[&#39;Jaccard&#39;]] &lt;- vegdist(occurence, method = &quot;jaccard&quot;) Les distances d‚ÄôHellinger, de chord et de chi-carr√© sont aussi appropri√©es pour les calculs de distances sur des tableaux d‚Äôoccurence. associations_occ[[&#39;Hellinger&#39;]] &lt;- dist(decostand(occurence, method=&quot;hellinger&quot;)) associations_occ[[&#39;Chord&#39;]] &lt;- dist(decostand(occurence, method=&quot;normalize&quot;)) associations_occ[[&#39;ChiSquare&#39;]] &lt;- dist(decostand(occurence, method=&quot;chi.square&quot;)) Graphiquement, associations_occ_df &lt;- list() for (i in 1:length(associations_occ)) { associations_occ_df[[i]] &lt;- data.frame(as.matrix(associations_occ[[i]])) colnames(associations_occ_df[[i]]) &lt;- rownames(associations_occ_df[[i]]) associations_occ_df[[i]]$row &lt;- rownames(associations_occ_df[[i]]) associations_occ_df[[i]] &lt;- associations_occ_df[[i]] %&gt;% gather(key=row) associations_occ_df[[i]]$column = rep(1:4, 4) associations_occ_df[[i]]$dist &lt;- names(associations_occ)[i] } associations_occ_df &lt;- do.call(rbind, associations_occ_df) ggplot(associations_occ_df, aes(x=row, y=column)) + facet_wrap(. ~ dist) + geom_tile(aes(fill = value)) + geom_text(aes(label = round(value, 2))) + scale_fill_gradient2(low = &quot;#00ccff&quot;, mid = &quot;#aad400&quot;, high = &quot;#ff0066&quot;, midpoint = 1) + labs(x=&quot;Site&quot;, y=&quot;Site&quot;) Il est attendu que les matrices d‚Äôassociation sur l‚Äôoccurence sont semblables √† celles sur l‚Äôabondance. Dans ce cas-ci, la distance d‚ÄôHellinger donne des r√©sultats semblables √† la dissimilarit√© de Jaccard. 9.2.1.3 Objets: Donn√©es quantitatives Les donn√©es quantitative en √©cologie peuvent d√©crire l‚Äô√©tat de l‚Äôenvironnement: le climat, l‚Äôhydrologie, l‚Äôhydrog√©ochimie, la p√©dologie, etc. En r√®gle g√©n√©rale, les coordonn√©es des sites ne sot pas des variables environnementales, √† que l‚Äôon soup√ßonne la coordonn√©e elle-m√™me d‚Äô√™tre responsable d‚Äôeffets sur notre syst√®me: mais il s‚Äôagira la plupart du temps d‚Äôeffets confondants (par exemple, on peut mesurer un effet de lattitude sur le rendement des agrumes, mais il s‚Äôagira probablement avant tout d‚Äôeffets dus aux conditions climatiques, qui elles changent en fonction de la lattitude). D‚Äôautre types de donn√©es quantitative pouvant √™tre appr√©hend√©es par des distances sont les traits ph√©nologiques, les ionomes, les g√©nomes, etc. La distance euclidienne est la racine carr√©e de la somme des carr√©s des distances sur tous les axes. Il s‚Äôagit d‚Äôune application multidimensionnelle du th√©or√®me de Pythagore. La distance d‚ÄôAitchison, couverte dans le chapitre 8, est une distance euclidienne calcul√©e sur des donn√©es compositionnelles pr√©alablement transform√©es. La distance euclidienne est sensible aux unit√©s utilis√©s: utiliser des milim√®tres plut√¥t que des m√®tres enflera la distance euclidienne. Il est recommand√© de porter une attention particuli√®re aux unit√©s, et de standardiser les donn√©es au besoin (par exemple, en centrant la moyenne √† z√©ro et en fixant l‚Äô√©cart-type √† 1). On pourrait, par exemple, mesurer la distance entre des observations des dimensions de diff√©rentes esp√®ces d‚Äôiris. Ce tableau est inclu dans R par d√©faut. data(iris) iris %&gt;% sample_n(5) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.2 3.5 1.5 0.2 setosa ## 2 5.9 3.2 4.8 1.8 versicolor ## 3 4.6 3.6 1.0 0.2 setosa ## 4 6.3 2.3 4.4 1.3 versicolor ## 5 5.0 3.4 1.6 0.4 setosa Les mesures du tableau sont en centim√®tres. Pour √©viter de donner davantage de poids aux longueur des s√©pales et en m√™me temps de n√©gliger la largeur des p√©tales, nous allons standardiser le tableau. iris_sc &lt;- iris %&gt;% select(-Species) %&gt;% scale(.)%&gt;% as_tibble(.) %&gt;% mutate(Species = iris$Species) iris_sc ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 -0.898 1.02 -1.34 -1.31 setosa ## 2 -1.14 -0.132 -1.34 -1.31 setosa ## 3 -1.38 0.327 -1.39 -1.31 setosa ## 4 -1.50 0.0979 -1.28 -1.31 setosa ## 5 -1.02 1.25 -1.34 -1.31 setosa ## 6 -0.535 1.93 -1.17 -1.05 setosa ## 7 -1.50 0.786 -1.34 -1.18 setosa ## 8 -1.02 0.786 -1.28 -1.31 setosa ## 9 -1.74 -0.361 -1.34 -1.31 setosa ## 10 -1.14 0.0979 -1.28 -1.44 setosa ## # ... with 140 more rows Pour les comparaisons des dimensions, prenons la moyenne des dimensions (mises √† l‚Äô√©chelle) par esp√®ce. iris_means &lt;- iris_sc %&gt;% group_by(Species) %&gt;% summarise_all(mean) %&gt;% select(-Species) iris_means ## # A tibble: 3 x 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -1.01 0.850 -1.30 -1.25 ## 2 0.112 -0.659 0.284 0.166 ## 3 0.899 -0.191 1.02 1.08 Nous pouvons utiliser la distance euclidienne, commune en g√©om√©trie, pour comparer les esp√®ces. La distance euclidienne est calcul√©e comme suit. \\[ \\mathcal{E} = \\sqrt{\\Sigma_i \\left( A_i - B_i \\right) ^2 } \\] associations_cont = list() associations_cont[[&#39;Euclidean&#39;]] &lt;- dist(iris_sc %&gt;% select(-Species), method=&quot;euclidean&quot;) La distance de Mahalanobis est semblable √† la distance euclidienne, mais qui tient compte de la covariance de la matrice des objets. Cette covariance peut √™tre utilis√©e pour d√©crire la structure d‚Äôun nuage de points. La diastance de Mahalanobis se calcule comme suit. \\[\\mathcal{M} = \\sqrt{(A - B)^T S^{-1} (A-B)}\\] Notez qu‚Äôil s‚Äôagit d‚Äôune g√©n√©ralisation de la distance euclidienne, qui √©quivaut √† une distance de Mahalanobis dont la matrice de covariance est une matrice identit√©. La distance de Mahalanobis permet de repr√©senter des distances dans un espace fortement corr√©l√©. Elle est courramment utilis√©e pour d√©tecter les valeurs aberrantes selon des crit√®res de distance √† partir du centre d‚Äôun jeu de donn√©es multivari√©es. associations_cont[[&#39;Mahalanobis&#39;]] &lt;- vegdist(iris_sc %&gt;% select(-Species), &#39;mahalanobis&#39;) La distance de Manhattan porte aussi le nom de distance de cityblock ou de taxi. C‚Äôest la distance que vous devrez parcourir pour vous rendre du point A au point B √† Manhattan, c‚Äôest-√†-dire selon une s√©quence de tron√ßons perpendiculaires. \\[ D_{AB} = \\sum _i \\left| A_i - B_i \\right| \\] La distance de Manhattan est appropri√©e lorsque les gradients (changements d‚Äôun √©tat √† l‚Äôautre ou d‚Äôune r√©gion √† l‚Äôautre) ne permettent pas des changements simultan√©s. Mieux vaut standardiser les variables pour √©viter qu‚Äôune dimension soit pr√©pond√©rante. associations_cont[[&#39;Manhattan&#39;]] &lt;- vegdist(iris_sc %&gt;% select(-Species), &#39;manhattan&#39;) Avant de pr√©senter les r√©sultats des esp√®ces d‚Äôiris, voici une repr√©sentation des distances euclidiennes (rouge), de Mahalanobis (bleu) et de Manhattan (vert), chacune de 1 et 2 unit√©s √† partir du centre et, pour ce qui est de la distance de Mahalanobis, selon la covariance. library(&quot;car&quot;) ## Loading required package: carData ## ## Attaching package: &#39;car&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## recode ## The following object is masked from &#39;package:purrr&#39;: ## ## some library(&quot;MASS&quot;) ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## select select &lt;- dplyr::select # √©viter les conflits de fonctions entre MASS et dplyr filter &lt;- dplyr::filter sigma &lt;- matrix(c(1, 0.6, 0.6, 1), ncol = 2) # matrice de covariance mu &lt;- c(0, 0) # centre data &lt;- mvrnorm(n = 100, mu, sigma) # g√©n√©rer des donn√©es plot(data, ylim = c(-2, 2), xlim = c(-2, 2), asp = 1) ## cercles t &lt;- seq(0,2*pi,length=100) c1 &lt;- t(rbind(mu[2] + sin(t)*1, mu[1] + cos(t)*1)) c2 &lt;- t(rbind(mu[2] + sin(t)*2, mu[1] + cos(t)*2)) lines(c1, lwd = 2, col = &quot;red&quot;) lines(c2, lwd = 2, col = &quot;red&quot;) ## ellipses e1 &lt;- ellipse(mu, sigma, radius=1, add=TRUE) e2 &lt;- ellipse(mu, sigma, radius=2, add=TRUE) ## carr√©s lines(c(1, 0, -1, 0, 1), c(0, 1, 0, -1, 0), lwd = 2, col = &quot;green&quot;) lines(c(2, 0, -2, 0, 2), c(0, 2, 0, -2, 0), lwd = 2, col = &quot;green&quot;) Et, graphiquement, les r√©sultats des distances des iris. associations_cont_df &lt;- list() for (i in 1:length(associations_cont)) { associations_cont_df[[i]] &lt;- data.frame(as.matrix(associations_cont[[i]])) colnames(associations_cont_df[[i]]) &lt;- rownames(associations_cont_df[[i]]) associations_cont_df[[i]]$row &lt;- rownames(associations_cont_df[[i]]) associations_cont_df[[i]] &lt;- associations_cont_df[[i]] %&gt;% gather(key=row) associations_cont_df[[i]]$column = rep(1:nrow(iris), nrow(iris)) associations_cont_df[[i]]$dist &lt;- names(associations_cont)[i] } associations_cont_df &lt;- do.call(rbind, associations_cont_df) ggplot(associations_cont_df, aes(x=row, y=column)) + facet_wrap(. ~ dist) + geom_tile(aes(fill = value), colour = NA) + #geom_text(aes(label = round(value, 2))) + scale_fill_gradient2(low = &quot;#00ccff&quot;, mid = &quot;#aad400&quot;, high = &quot;#ff0066&quot;, midpoint = 5) + labs(x=&quot;Site&quot;, y=&quot;Site&quot;) Le tableau iris est ordonn√© par esp√®ce. Les distances euclidienne et de Manhattan permettent ais√©ment de distinguer les esp√®ces selon les dimensions des p√©tales et des s√©pales. Toutefois, l‚Äôutilsation de la covariance avec la distance de Mahalanobis cr√©e des distinction moins tranch√©es. 9.2.1.4 Objets: Donn√©es mixtes Les donn√©es cat√©gorielles ordinales peuvent √™tre transform√©es en donn√©es continues par gradations lin√©aires ou quadratiques. Les donn√©es cat√©gorielles nominales, quant √† elles, peuvent √™tre encod√©es (encodage cat√©goriel) en donn√©es similaires √† des occurences. Attention toutefois: contrairement √† la r√©gression lin√©aire qui demande d‚Äôexclure une cat√©gorie, l‚Äôencodage cat√©goriel doit inclure toutes les cat√©gories. Le comportement par d√©faut de la fonction model.matrix est d‚Äôexclure la cat√©gorie de r√©f√©rence: on doit sp√©cifier que l‚Äôintercept est de z√©ro, c‚Äôest-√†-dire model.matrix(~ + categorie). La similarit√© de Gower a √©t√© d√©velopp√©e pour mesurer des associations entre des objets dont les donn√©es sont mixtes: bool√©ennes, cat√©gorielles et continues. La similarit√© de Gower est calcul√©e en additionnant les distances calcul√©es par colonne, individuellement. Si la colonne est bool√©enne, on utilise les distances de Jaccard (qui exclue les double-z√©ro) de mani√®re univari√©e: une variable √† la fois. Pour les variables continues, on utilise la distance de Manhattan divis√©e par la plage de valeurs de la variable (pour fin de standardisation). Puisqu‚Äôelle h√©rite de la particularit√© de la distance de Manhattan et de la similarit√© de Jaccard univari√©e, la similarit√© de Gower reste une combinaison lin√©aire de distances univari√©es. X &lt;- tibble(ID = 1:8, age = c(21, 21, 19, 30, 21, 21, 19, 30), gender = c(&#39;M&#39;,&#39;M&#39;,&#39;N&#39;,&#39;M&#39;,&#39;F&#39;,&#39;F&#39;,&#39;F&#39;,&#39;F&#39;), civil_status = c(&#39;MARRIED&#39;,&#39;SINGLE&#39;,&#39;SINGLE&#39;,&#39;SINGLE&#39;,&#39;MARRIED&#39;,&#39;SINGLE&#39;,&#39;WIDOW&#39;,&#39;DIVORCED&#39;), salary = c(3000.0,1200.0 ,32000.0,1800.0 ,2900.0 ,1100.0 ,10000.0,1500.0), children = c(TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE), available_credit = c(2200,100,22000,1100,2000,100,6000,2200)) X ## # A tibble: 8 x 7 ## ID age gender civil_status salary children available_credit ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 1 21 M MARRIED 3000 TRUE 2200 ## 2 2 21 M SINGLE 1200 FALSE 100 ## 3 3 19 N SINGLE 32000 TRUE 22000 ## 4 4 30 M SINGLE 1800 TRUE 1100 ## 5 5 21 F MARRIED 2900 TRUE 2000 ## 6 6 21 F SINGLE 1100 TRUE 100 ## 7 7 19 F WIDOW 10000 FALSE 6000 ## 8 8 30 F DIVORCED 1500 TRUE 2200 Il faut pr√©alablement proc√©der √† l‚Äôencodage cat√©goriel pour les variables cat√©gorielles nominales. X_dum &lt;- model.matrix(~ 0 + ., X[, -1]) X_dum ## age genderF genderM genderN civil_statusMARRIED civil_statusSINGLE ## 1 21 0 1 0 1 0 ## 2 21 0 1 0 0 1 ## 3 19 0 0 1 0 1 ## 4 30 0 1 0 0 1 ## 5 21 1 0 0 1 0 ## 6 21 1 0 0 0 1 ## 7 19 1 0 0 0 0 ## 8 30 1 0 0 0 0 ## civil_statusWIDOW salary childrenTRUE available_credit ## 1 0 3000 1 2200 ## 2 0 1200 0 100 ## 3 0 32000 1 22000 ## 4 0 1800 1 1100 ## 5 0 2900 1 2000 ## 6 0 1100 1 100 ## 7 1 10000 0 6000 ## 8 0 1500 1 2200 ## attr(,&quot;assign&quot;) ## [1] 1 2 2 2 3 3 3 4 5 6 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$gender ## [1] &quot;contr.treatment&quot; ## ## attr(,&quot;contrasts&quot;)$civil_status ## [1] &quot;contr.treatment&quot; ## ## attr(,&quot;contrasts&quot;)$children ## [1] &quot;contr.treatment&quot; Calculons la dissimilarit√© de Gower (cette fois le graphique est fait avec pheatmap). library(&quot;pheatmap&quot;) d_gow &lt;- as.matrix(vegdist(X_dum, &#39;gower&#39;)) colnames(d_gow) &lt;- rownames(d_gow) &lt;- X$ID pheatmap(d_gow) Les dendrogrammes apparaissants sur les axes du graphique sont issus d‚Äôun processus de partitionnement bas√© sur la distance, que nous verrons plus loin dans ce chapiter. Les profils des clients 4 et 7, ainsi que ceux des clients 3 et 7 diff√®rent le plus. Les profils 3 et 4 sont n√©anmoins plut√¥t diff√©rents. 9.2.2 Associations entre variables (mode R) Il existe de nombreuses approches pour mesurer les associations entre variables. La plus connue est la corr√©lation. Mais les donn√©es d‚Äôabondance et d‚Äôoccurence demandent des approches diff√©rentes. 9.2.2.1 Variables: Abondance La distance du chi-carr√© est sugg√©r√©e par Borcard et al. (2011). abundance_r &lt;- t(abundance) D_chisq_R &lt;- as.matrix(dist(decostand(abundance_r, method=&quot;chi.square&quot;))) pheatmap(D_chisq_R, display_numbers = round(D_chisq_R, 2)) Des coabondances sont notables pour la m√©sange √† t√™te noire, le jaseur bor√©al, la citelle √† poitrine rousse et le bruant √† gorge blanche (tache bleu au centre). 9.2.2.2 Variables: Occurence La dissimilarit√© de Jaccard peut √™tre utilis√©e. occurence_r &lt;- t(occurence) D_jacc_R &lt;- as.matrix(vegdist(occurence_r, method = &quot;jaccard&quot;)) pheatmap(D_jacc_R, display_numbers = round(D_jacc_R, 2)) Des cooccurences sont notables pour le jaseur bor√©al, la citelle √† poitrine rousse et le bruant √† gorge blanche (tache bleu au centre). 9.2.2.3 Variables: Quantit√©s La matrice des corr√©lations de Pearson peut √™tre utilis√©e pour les donn√©es continues. Quant aux variables ordinales, elles devraient id√©alement √™tre li√©es lin√©airement ou quadratiquement. Si ce n‚Äôest pas le cas, c‚Äôest-√†-dire que les cat√©gories sont ordonn√©es par rang seulement, vous pourrez avoir recours aux coefficients de corr√©lation de Spearman ou de Kendall. iris_cor &lt;- iris %&gt;% select(-Species) %&gt;% cor(.) pheatmap(cor(iris[, -5]), cluster_rows = FALSE, cluster_cols = FALSE, display_numbers = round(iris_cor, 2)) 9.2.3 Conclusion sur les associations Il n‚Äôexiste pas de r√®gle claire pour d√©terminer quelle technique d‚Äôassociation utiliser. Cela d√©pend en premier lieu de vos donn√©es. Vous s√©lectionnerez votre m√©thode d‚Äôassociation selon le type de donn√©es que vous abordez, la question √† laquelle vous d√©sirez r√©pondre ainsi l‚Äôexp√©rience dans la litt√©rature comme celle de vos coll√®gues scientifiques. S‚Äôil n‚Äôexiste pas de r√®gle clair, c‚Äôest qu‚Äôil existe des dizaines de m√©thodes diff√©rentes, et la plupart d‚Äôentre elles vous donneront une perspective juste et valide. Il faut n√©anmoins faire attention pour √©viter de s√©lectionner les m√©thodes qui ne sont pas appropri√©es. 9.3 Partitionnement Les donn√©es suivantes ont √©t√© g√©n√©r√©es par Leland McInnes (Tutte institute of mathematics, Ottawa). √ätes-vous en mesure d‚Äôidentifier des groupes? Combien en trouvez-vous? df_mcinnes &lt;- read_csv(&quot;data/clusterable_data.csv&quot;, col_names = c(&quot;x&quot;, &quot;y&quot;), skip = 1) ## Parsed with column specification: ## cols( ## x = col_double(), ## y = col_double() ## ) ggplot(df_mcinnes, aes(x=x, y=y)) + geom_point() + coord_fixed() En 2D, l‚Äôoeil humain peut facilement d√©tecter les groupes. En 3D, c‚Äôest toujours possible, mais au-del√† de 3D, le partitionnement cognitive devient rapidement maladroite. Les algorithmes sont alors d‚Äôune aide pr√©cieuse. Mais ils transportent en pratique tout un baggage de limitations. Quel est le crit√®re d‚Äôassociation entre les groupes? Combien de groupe devrions-nous cr√©er? Comment distinguer une donn√©e trop bruit√©e pour √™tre classifi√©e? Le partitionnement de donn√©es (clustering en anglais), et inversement leur regroupement, permet de cr√©er des ensembles selon des crit√®res d‚Äôassociation. On suppose donc que Le partitionnement permet de cr√©er des groupes selon l‚Äôinformation que l‚Äôon fait √©merger des donn√©es. Il est cons√©quemment entendu que les donn√©es ne sont pas cat√©goris√©es √† priori: il ne s‚Äôagit pas de pr√©dire la cat√©gorie d‚Äôun objet, mais bien de cr√©er des cat√©gories √† partir des objets par exemple selon leurs dimensions, leurs couleurs, leurs signature chimique, leurs comportements, leurs g√®nes, etc. Plusieurs m√©thodes sont aujourd‚Äôhui offertes aux analystes pour partitionner leurs donn√©es. Dans le cadre de ce manuel, nous couvrirons ici deux grandes tendances dans les algorithmes. M√©thodes hi√©rarchique et non hi√©rarchiques. Dans un partitionnement hi√©rarchique, l‚Äôensemble des objets forme un groupe, comprenant des sous-regroupements, des sous-sous-regroupements, etc., dont les objets forment l‚Äôultime partitionnement. On pourra alors identifier comment se d√©cline un partitionnement. √Ä l‚Äôinverse, un partitionnement non-hi√©rarchique des algorhitmes permettent de cr√©er les groupes non hi√©rarchis√©s les plus diff√©rents que possible. Membership exclusif ou flou. Certaines techniques attribuent √† chaque objet une classe unique: l‚Äôappartenance sera indiqu√©e par un 1 et la non appartenance par un 0. D‚Äôautres techniques vont attribuer un membership flou o√π le degr√© d‚Äôappartenance est une variable continue de 0 √† 1. Parmi les m√©thodes floues, on retrouve les m√©thodes probabilistes. 9.3.1 √âvaluation d‚Äôun partitionnement Le choix d‚Äôune technique de partitionnement parmi de nombreuses disponibles, ainsi que le choix des param√®tres gouvernant chacune d‚Äôentre elles, est avant tout bas√© sur ce que l‚Äôon d√©sire d√©finir comme √©tant un groupe, ainsi que la mani√®re d‚Äôinterpr√©ter les groupes. En outre, le nombre de groupe √† d√©partager est toujours une d√©cision de l‚Äôanalyste. N√©anmoins, on peut se fier des indicateurs de performance de partitionnement. Parmis ceux-ci, retenons le score silouhette ainsi que l‚Äôindice de Calinski-Harabaz. 9.3.1.1 Score silouhette En anglais, le h dans silouhette se trouve apr√®s le l: on parle donc de silhouette coefficient pour d√©signer le score de chacun des objets dans le partitionnement. Pour chaque objet, on calcule la distance moyenne qui le s√©pare des autres points de son groupe (\\(a\\)) ainsi que la distance moyenne qui le s√©pare des points du groupe le plus rapproch√©. \\[s = \\frac{b-a}{max \\left(a, b \\right)}\\] Un coefficient de -1 indique le pire classement, tandis qu‚Äôun coefficient de 1 indique le meilleur classement. La moyenne des coefficients silouhette est le score silouhette. 9.3.1.2 Indice de Calinski-Harabaz L‚Äôindice de Calinski-Harabaz est proportionnel au ratio des dispersions intra-groupe et la moyenne des dispersions inter-groupes. Plus l‚Äôindice est √©lev√©, mieux les groupes sont d√©finis. La math√©matique est d√©crite dans la documentation de scikit-learn, un module d‚Äôanalyse et autoapprentissage sur Python. Note. Les coefficients silouhette et l‚Äôindice de Calinski-Harabaz sont plus appropri√©s pour les formes de groupes convexes (cercles, sph√®res, hypersph√®res) que pour les formes irr√©guli√®res (notamment celles obtenues par la DBSCAN, discut√©e ci-desssous). 9.3.2 Partitionnement non hi√©rarchique Il peut arriver que vous n‚Äôayez pas besoin de comprendre la structure d‚Äôagglom√©ration des objets (ou variables). Plusieurs techniques de partitionnement non hi√©rarchique sont disponibles sur R. On s‚Äôint√©ressera en particulier aux k-means et au dbscan. 9.3.2.1 Kmeans L‚Äôobjectif des kmeans est de minimiser la distance eucl√©dienne entre un nombre pr√©d√©fini de k groupes exclusifs. L‚Äôalgorhitme commence par placer une nombre k de centroides au hasard dans l‚Äôespace d‚Äôun nombre p de variables (vous devez fixer k, et p est le nombre de colonnes de vos donn√©es). Ensuite, chaque objet est √©tiquett√© comme appartenant au groupe du centroid le plus pr√®s. La position du centroide est d√©plac√©e √† la moyenne de chaque groupe. Recommencer √† partir de l‚Äô√©tape 2 jusqu‚Äô√† ce que l‚Äôassignation des objets aux groupes ne change plus. Source: David Sheehan La technique des kmeans suppose que les groupes ont des distributions multinormales - repr√©sent√©es par des cercles en 2D, des sph√®res en 3D, des hypersph√®res en plus de 3D. Cette limitation est probl√©matique lorsque les groupes se pr√©sentent sous des formes irr√©guli√®res, comme celles du nuage de points de Leland McInnes, pr√©sent√© plus haut. De plus, la technique classique des kmeans est bas√©e sur des distances euclidiennes: l‚Äôutilisation des kmeans n‚Äôest appropri√©e pour les donn√©es comprenant beaucoup de z√©ros, comme les donn√©es d‚Äôabondance, qui devraient pr√©alablement √™tre transform√©es en variables centr√©es et r√©duites (Legendre et Legendre, 2012). La technique des mixtures gaussiennes (gaussian mixtures) est une g√©n√©ralisation des kmeans permettant d‚Äôint√©grer la covariance des groupes. Les groupes ne sont plus des hyper-sph√®res, mais des hyper-ellipso√Ødes. 9.3.2.1.1 Application Nous pouvons utilis√© la fonction kmeans de R. Toutefois, puisque l‚Äôon d√©sire ici effectuer des tests de partitionnement pour plusieurs nombres de groupes, nous utiliserons cascadeKM, du module vegan. Notez que de nombreux param√®tres par d√©faut sont utilis√©s dans les ex√©cutions ci-dessous. Ces notes de cours ne forment pas un travail de recherche scientifique. Lors de travaux de recherche, l‚Äôutilsation d‚Äôun argument ou d‚Äôun autre dans une fonction doit √™tre justifi√©: qu‚Äôun param√®tre soit utilis√© par d√©faut dans une fonction n‚Äôest a priori pas une justification convainquante. Pour les kmeans, on doit fixer le nombre de groupes. Le graphique des donn√©es de Leland McInnes montrent 6 groupes. Toutefois, il est rare que l‚Äôon puisse visualiser des d√©marquations aussi tranch√©es que celles de l‚Äôexemple, qui plus est dans des cas o√π l‚Äôon doit traiter de plus de deux dimensions. Je vais donc lancer le partitionnement en boucle pour plusieurs nombres de groupes, de 3 √† 10 et pour chaque groupe, √©valuer le score silouhette et de Calinski-Habaraz. J‚Äôutilise un argument random_state pour m‚Äôassurer que les groupes seront les m√™mes √† chaque fois que la cellule sera lanc√©e. library(&quot;vegan&quot;) mcinnes_kmeans &lt;- cascadeKM(df_mcinnes, inf.gr = 3, sup.gr = 10, criterion = &quot;calinski&quot;) str(mcinnes_kmeans) ## List of 4 ## $ partition: int [1:2309, 1:8] 2 2 2 2 2 2 2 2 2 2 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:2309] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. ..$ : chr [1:8] &quot;3 groups&quot; &quot;4 groups&quot; &quot;5 groups&quot; &quot;6 groups&quot; ... ## $ results : num [1:2, 1:8] 85.1 2164.5 61.4 2294.6 51.4 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:2] &quot;SSE&quot; &quot;calinski&quot; ## .. ..$ : chr [1:8] &quot;3 groups&quot; &quot;4 groups&quot; &quot;5 groups&quot; &quot;6 groups&quot; ... ## $ criterion: chr &quot;calinski&quot; ## $ size : int [1:10, 1:8] 561 1243 505 NA NA NA NA NA NA NA ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:10] &quot;Group 1&quot; &quot;Group 2&quot; &quot;Group 3&quot; &quot;Group 4&quot; ... ## .. ..$ : chr [1:8] &quot;3 groups&quot; &quot;4 groups&quot; &quot;5 groups&quot; &quot;6 groups&quot; ... ## - attr(*, &quot;class&quot;)= chr &quot;cascadeKM&quot; L‚Äôobjet mcinnes_kmeans, de type cascadeKM, peut √™tre visualis√© directement avec la fonction plot. plot(mcinnes_kmeans) On obtient un maximum de Calinski √† 4 groupes, qui correspons √† la deuxi√®me simulation effectu√©e de 3 √† 10. Examinons les scores silouhette (module: cluster). library(&quot;cluster&quot;) asw &lt;- c() for (i in 1:ncol(mcinnes_kmeans$partition)) { mcinnes_kmeans_silhouette &lt;- silhouette(mcinnes_kmeans$partition[, i], dist = vegdist(df_mcinnes, method = &quot;euclidean&quot;)) asw[i] &lt;- summary(mcinnes_kmeans_silhouette)$avg.width } plot(3:10, asw, type = &#39;b&#39;) Le score silouhette maximum est √† 3 groupes. La forme des groupes n‚Äô√©tant pas convexe, il fallait s‚Äôattendre √† ce que indicateurs maximaux pour les deux indicateurs soient diff√©rents. C‚Äôest d‚Äôailleurs souvent le cas. Cet exemple supporte que le choix du nombre de groupe √† d√©partager repose sur l‚Äôanalyste, non pas uniquement sur les indicateurs de performance. Choisissons 6 groupes, puisque que c‚Äôest visuellement ce que l‚Äôon devrait chercher pour ce cas d‚Äô√©tude. kmeans_group &lt;- mcinnes_kmeans$partition[, 4] mcinnes_kmeans$partition %&gt;% head(3) ## 3 groups 4 groups 5 groups 6 groups 7 groups 8 groups 9 groups 10 groups ## 1 2 2 5 2 5 6 1 2 ## 2 2 2 4 3 5 3 1 2 ## 3 2 1 5 2 6 6 9 4 df_mcinnes %&gt;% mutate(kmeans_group = kmeans_group) %&gt;% # ajouter une colonne de regoupement ggplot(aes(x=x, y=y)) + geom_point(aes(colour = factor(kmeans_group))) + coord_fixed() L‚Äôalgorithme kmeans est loin d‚Äô√™tre statisfaisant. Cela est attendu, puisque les kmeans recherchent des distribution gaussiennes sur des groupes vraisemblablement non-gaussiens. Nous pouvons cr√©er un graphique silouhette pour nos 6 groupes. Notez qu‚Äô√† cause d‚Äôun bogue, il n‚Äôest pas possible de pr√©senter les donn√©es clairement lorsqu‚Äôelles sont nombreuses. sil &lt;- silhouette(mcinnes_kmeans$partition[, 6], dist = vegdist(df_mcinnes[, ], method = &quot;euclidean&quot;)) sil &lt;- sortSilhouette(sil) plot(sil, col = &#39;black&#39;) 9.3.2.2 DBSCAN La technique DBSCAN (* Density-Based Spatial Clustering of Applications with Noise) sousentend que les groupes sont compos√©s de zones o√π l‚Äôon retrouve plus de points (zones denses) s√©par√©es par des zones de faible densit√©. Pour lancer l‚Äôalgorithme, nous devons sp√©cifier une mesure d‚Äôassociation critique (distance ou dissimilarit√©) d* ainsi qu‚Äôun nombre de point critique k dans le voisinage de cette distance. L‚Äôalgorithme commence par √©tiqueter chaque point selon l‚Äôune de ces cat√©gories: Noyau: le point a au moins k points dans son voisinage, c‚Äôest-√†-dire √† une distance inf√©rieure ou √©gale √† d. Bordure: le point a moins de k points dans son voisinage, mais l‚Äôun de des points voisins est un noyau. Bruit: le cas √©ch√©ant. Ces points sont consid√©r√©s comme des outliers. Les noyaux distanc√©s de d ou moins sont connect√©s entre eux en englobant les bordures. Le nombre de groupes est prescrit par l‚Äôalgorithme DBSCAN, qui permet du coup de d√©tecter des donn√©es trop bruit√©es pour √™tre class√©es. Damiani et al. (2014) a d√©velopp√© une approche utilisant la technique DBSCAN pour partitionner des zones d‚Äôescale pour les flux de populations migratoires. 9.3.2.2.1 Application La technique DBSCAN n‚Äôest pas bas√©e sur le nombre de groupe, mais sur la densit√© des points. L‚Äôargument x ne constitue pas les donn√©es, mais une matrice d‚Äôassociation. L‚Äôargument minPts sp√©cifie le nombre minimal de points qui l‚Äôon doit retrouver √† une distance critique d* pour la formation des *noyaux et la propagation des groupes, sp√©cifi√©e dans l‚Äôargument eps. La distance d peut √™tre estim√©e en prenant une fraction de la moyenne, mais on aura volontiers recours √† sont bon jugement. library(&quot;dbscan&quot;) mcinnes_dbscan &lt;- dbscan(x = vegdist(df_mcinnes[, ], method = &quot;euclidean&quot;), eps = 0.03, minPts = 10) dbscan_group &lt;- mcinnes_dbscan$cluster unique(dbscan_group) ## [1] 1 0 2 6 3 4 5 Les param√®tres sp√©cifi√©s donnent 5 groupes (1, 2, ..., 5) et des points trop bruit√©s pour √™tre classifi√©s (√©tiquet√©s 0). Voyons comment les groupes ont √©t√© form√©s. df_mcinnes %&gt;% mutate(dbscan_group = dbscan_group) %&gt;% # ajouter une colonne de regoupement ggplot(aes(x=x, y=y)) + geom_point(aes(colour = factor(dbscan_group))) + coord_fixed() Le partitionnement semble plus conforme √† ce que l‚Äôon recherche. N√©anmoins, DBSCAN cr√© quelques petits groupes ind√©sirables (groupe 6, en rose) ainsi qu‚Äôun grand groupe (violet) qui auraient lieu d‚Äô√™tre partitionn√©. Ces d√©faut pourraient √™tre r√©gl√©s en jouant sur les param√®tres eps et minPts. 9.3.3 Partitionnement hi√©rarchique Les techniques de partitionnement hi√©rarchique sont bas√©es sur les matrices d‚Äôassociation. La technique pour mesurer l‚Äôassociation (entre objets ou variables) d√©terminera en grande partie le paritionnement des donn√©es. Les partitionnements hi√©rarchiques ont l‚Äôavantage de pouvoir √™tre repr√©sent√©s sous forme de dendrogramme (ou arbre) de partition. Un tel dendrogramme pr√©sente des sous-groupes qui se joignent en groupes jusqu‚Äô√† former un seul ensemble. Le partitionnement hi√©rarchique est abondamment utilis√© en phylog√©nie, pour √©tudier les relations de parent√© entre organismes vivants, populations d‚Äôorganismes et esp√®ces. La ph√©n√©tique, branche empirique de la phylog√©n√®se intersp√©cifique, fait usage du partitionnement hi√©rarchique √† partir d‚Äôassociations g√©n√©tiques entre unit√©s taxonomiques. On retrouve de nombreuses ressources acad√©miques en phylog√©n√©tique ainsi que des outils pour R et Python. Toutefois, la phylog√©n√©tique en particulier ne fait pas partie de la pr√©sente itt√©ration de ce manuel. 9.3.3.1 Techniques de partitionnement hi√©rarchique Le partitionnement hi√©rarchique est typiquement effectu√© avec une des quatres m√©thodes suivantes, dont chacune poss√®de ses particularit√©s, mais sont toutes agglom√©ratives: √† chaque √©tape d‚Äôagglom√©ration, on fusionne les deux groupes ayant le plus d‚Äôaffinit√© sur la base des deux sous-groupes les plus rapproch√©s. Single link (single). Les groupes sont agglom√©r√©s sur la base des deux points parmi les groupes, qui sont les plus proches. Complete link (complete). √Ä la diff√©rence de la m√©thode single, on consid√®re comme crit√®re d‚Äôagglom√©ration les √©l√©ments les plus √©loign√©s de chaque groupe. Agglom√©ration centrale. Il s‚Äôagit d‚Äôune fammlle de m√©thode bas√©es sur les diff√©rences entre les tendances centrales des objets ou des groupes. Average (average). Appel√©e UPGMA (Unweighted Pair-Group Method unsing Average), les groupes sont agglom√©r√©s selon un centre calcul√©s par la moyenne et le nombre d‚Äôobjet pond√®re l‚Äôagglom√©ration (le poids des groupes est retir√©). Cette technique est historiquement utilis√©e en bioinformatique pour partitionner des groupes phylog√©n√©tiques (Sneath et Sokal, 1973). Weighted (weighted). La version de average, mais non pond√©r√©e (WPGMA). Centroid (centroid). Tout comme average, mais le centro√Øde (centre g√©om√©trique) est utilis√© au lieu de la moyenne. Accronyme: UPGMC. Median (median). Appel√©e WPGMC. Devinez! ;) Ward (ward). L‚Äôoptimisation vise √† minimiser les sommes des carr√©s par regroupement. 9.3.3.2 Quel outil de partitionnement hi√©rarchique utiliser? Alors que le choix de la matrice d‚Äôassociation d√©pend des donn√©es et de leur contexte, la technique de partitionnement hi√©rarchique peut, quant √† elle, √™tre bas√©e sur un crit√®re num√©rique. Il en existe plusieurs, mais le crit√®re recommand√© pour le choix d‚Äôune technique de partitionnement hi√©rarchique est la corr√©lation coph√©n√©tique. La distance coph√©n√©tique est la distance √† laquelle deux objets ou deux sous-groupes deviennent membres d‚Äôun m√™me groupe. La corr√©lation coph√©n√©tique est la corr√©lation de Pearson entre le vecteur d‚Äôassociation des objets et le vecteur de distances coph√©n√©tiques. 9.3.3.3 Application Les techniques de partitionnement hi√©rarchique pr√©sent√©es ci-dessus sont disponibles dans le module stats de R, qui est charg√© automatiquement lors de l‚Äôouversture de R. Nous allons classifier les dimensions des iris gr√¢ce √† la distance de Manhattan. mcinnes_hclust_distmat &lt;- vegdist(df_mcinnes, method = &quot;manhattan&quot;) clustering_methods &lt;- c(&#39;single&#39;, &#39;complete&#39;, &#39;average&#39;, &#39;centroid&#39;, &#39;ward&#39;) clust_l &lt;- list() coph_corr_l &lt;- c() for (i in seq_along(clustering_methods)) { clust_l[[i]] &lt;- hclust(mcinnes_hclust_distmat, method = clustering_methods[i]) coph_corr_l[i] &lt;- cor(mcinnes_hclust_distmat, cophenetic(clust_l[[i]])) } ## The &quot;ward&quot; method has been renamed to &quot;ward.D&quot;; note new &quot;ward.D2&quot; tibble(clustering_methods, coph_corr = coph_corr_l) %&gt;% ggplot(aes(x = fct_reorder(clustering_methods, -coph_corr), y = coph_corr)) + geom_col() + labs(x = &quot;M√©thode de partitionnement&quot;, y = &quot;Corr√©lation coph√©n√©tique&quot;) La m√©thode average retourne la corr√©lation la plus √©lev√©e. Pour plus de flexibilit√©, ench√¢ssons le nom de la m√©thode dans une variable. Ainsi, en chageant le nom de cette variable, le reste du code sera cons√©quent. names(clust_l) &lt;- clustering_methods best_method &lt;- &quot;average&quot; Le partitionnement hi√©rarchique peut √™tre visualis√© par un dendrogramme. plot(clust_l[[best_method]]) 9.3.3.4 Combien de groupes utiliser? La longueur des lignes verticales est la distance s√©parant les groupes enfants. Bien que la s√©lection du nombre de groupe soit avant tout bas√©e sur les besoins du probl√®me, nous pouvons nous appuyer sur certains outils. La hauteur totale peut servir de crit√®re pour d√©finir un nombre de groupes ad√©quat. On pourra s√©lectionner le nombre de groupe o√π la hauteur se stabilise en fonction du nombre de groupe. On pourra aussi utiliser le graphique silhouette, comprenant une collection de largeurs de silouhette, repr√©sentant le degr√© d‚Äôappartenance √† son groupe. La fonction sklearn.metrics.silhouette_score, du module scikit-learn, s‚Äôen occupe. asw &lt;- c() num_groups &lt;- 3:10 for(i in seq_along(num_groups)) { sil &lt;- silhouette(cutree(clust_l[[best_method]], k = num_groups[i]), mcinnes_hclust_distmat) asw[i] &lt;- summary(sil)$avg.width } plot(num_groups, asw, type = &quot;b&quot;) Le nombre optimal de groupes serait de 5. Coupons le dendrorgamme √† la hauteur correspondant √† 5 groupes avec la fonction cutree. k_opt &lt;- num_groups[which.max(asw)] hclust_group &lt;- cutree(clust_l[[best_method]], k = k_opt) plot(clust_l[[best_method]]) rect.hclust(clust_l[[best_method]], k = k_opt) La classification hi√©rarchique, uniquement bas√©e sur la distance, peut √™tre inappropri√©e pour d√©finir des formes complexes. df_mcinnes %&gt;% mutate(hclust_group = hclust_group) %&gt;% # ajouter une colonne de regoupement ggplot(aes(x=x, y=y)) + geom_point(aes(colour = factor(hclust_group))) + coord_fixed() 9.3.4 Partitionnement hi√©rarchique bas√©e sur la densit√© des points La tecchinque HDBSCAN, dont l‚Äôalgorithme est relativement r√©cent (Campello et al., 2013), permet une partitionnement hi√©rarchique sur le m√™me principe des zones de densit√© de la technique DBSCAN. Le HDBSCAN a √©t√© utilis√©e pour partitionner les lieux d‚Äôescale d‚Äôoiseaux migrateurs en Chine (Xu et al., 2013). Avec DBSCAN, un rayon est fix√© dans une m√©trique appropri√©e. Pour chaque point, on compte le nombre de point voisins, c‚Äôest √† dire le nombre de point se situant √† une distance (ou une dissimilarit√©) √©gale ou inf√©rieure au rayon fix√©. Avec HDBSCAN, on sp√©cifie le nombre de points devant √™tre recouverts et on calcule le rayon n√©cessaire pour les recouvrir. Ainsi, chaque point est associ√© √† un rayon critique que l‚Äôon nommera \\(d_{noyau}\\). La m√©trique initiale est ensuite alt√©r√©e: on remplace les associations entre deux objets A et B par la valeur maximale entre cette association, le rayon critique de A et le rayon critique de B. Cette nouvelle distance est appel√©e la distance d‚Äôatteinte mutuelle: elle accentue les distances pour les points se trouvant dans des zones peu denses. On applique par la suite un algorithme semblable √† la partition hi√©rarchique single link: En s‚Äô√©largissant, les rayons se superposent, chaque superposition de rayon forment graduellement des groupes qui s‚Äôagglom√®rent ainsi de mani√®re hi√©rarchique. Au lieu d‚Äôeffectuer une tranche √† une hauteur donn√©e dans un dendrogramme de partitionnement, la technique HDBSCAN se base sur un dendrogramme condens√© qui discarte les sous-groupes comprenant moins de n objets (\\(n_{gr min}\\)). Dans nouveau dendrogramme, on recherche des groupes qui occupent bien l‚Äôespace d‚Äôanalyse. Pour ce faitre, on utilise l‚Äôinverse de la distance pour cr√©er un indicateur de persistance (semblable √† la similarit√©), \\(\\lambda\\). Pour chaque groupe hi√©rarchique dans le dendrogramme condens√©, on peut calculer la persistance o√π le groupe prend naissance. De plus, pour chaque objet d‚Äôun groupe, on peut aussi calculer une distance √† laquelle il quitte le groupe. La stabilit√© d‚Äôun groupe est la domme des diff√©rences de persistance entre la persistance √† la naissance et les persistances des objets. On descend dans le dendrogramme. Si la somme des stabilit√© des groupes enfants est plus grande que la stabilit√© du groupe parent, on accepte la division. Sinon, le parent forme le groupe. La documentation du module hdbscan pour Python offre une description intuitive et plus exhaustive des principes et algorithme de HDBSCAN. 9.3.4.1 Param√®tres Outre la m√©trique d‚Äôassociation dont nous avons discut√©, HDBSCAN demande d‚Äô√™tre nourri avec quelques param√®tres importants. En particulier, le nombre minimum d‚Äôobjets par groupe, \\(n_{gr min}\\) d√©pend de la quantit√© de donn√©es que vous avez √† votre disposition, ainsi que de la quantit√© d‚Äôobjets que vous jugez suffisante pour cr√©er des groupes. Nous utiliserons l‚Äôimpl√©mentation de HDBSCAN du module dbscan. Si vous d√©sirez davantage d‚Äôoptions, vous pr√©f√©rerez probablement l‚Äôimpl√©mentation du module largeVis. mcinnes_hdbscan &lt;- hdbscan(x = vegdist(df_mcinnes, method = &quot;euclidean&quot;), minPts = 20, gen_hdbscan_tree = TRUE, gen_simplified_tree = FALSE) hdbscan_group &lt;- mcinnes_hdbscan$cluster unique(hdbscan_group) ## [1] 6 0 4 3 5 1 2 Nous avons 6 groupes, num√©rot√©s de 1 √† 6, ainsi que des √©tiquettes identifiant des objets d√©sign√©s comme √©tant du bruit de fond, num√©rot√© 0. Le dendrogramme non condens√© peu √™tre produit. plot(mcinnes_hdbscan$hdbscan_tree) Difficile d‚Äôy voir clair avec autant d‚Äôobjets. L‚Äôobjet mcinnes_hdbscan a un nombre minimum d‚Äôobjets par groupe de 20. Ce qui permet de pr√©senter le dendrogramme de mani√®re condens√©e. plot(mcinnes_hdbscan) Enfin, un aper√ßu des strat√©gies de partitionnement utilis√©s jusqu‚Äôici. clustering_group &lt;- df_mcinnes %&gt;% mutate(kmeans_group, hclust_group, dbscan_group, hdbscan_group) %&gt;% gather(-x, -y, key = &quot;method&quot;, value = &quot;cluster&quot;) ## Warning: attributes are not identical across measure variables; ## they will be dropped clustering_group$cluster &lt;- factor(clustering_group$cluster) clustering_group %&gt;% ggplot(aes(x = x, y = y)) + geom_point(aes(colour = cluster)) + facet_wrap(~method, ncol = 2) + coord_equal() + theme_bw() Clairement, le partitionnement avec HDBSCAN donne les meilleurs r√©sultats. 9.3.5 Conclusion sur le partitionnement Au chapitre 4, nous avons vu avec le jeu de donn√©es ‚Äúdatasaurus‚Äù que la visualisation peut permettre de d√©tecter des structures en segmentant les donn√©es selon des groupes. Or, si les donn√©es n‚Äô√©taient pas √©tiquet√©es, leur structure serait ind√©tectable avec les algorithmes disponibles actuellement. Le partitionnement permet d‚Äôexplorer des donn√©es, de d√©tecter des tendances et de d√©gager des groupes permettant la prise de d√©cision. Plusieurs techniques de partitionnement ont √©t√© pr√©sent√©es. Le choix de la technique sera d√©terminante sur la mani√®re dont les groupes seront partitionn√©s. La d√©finition d‚Äôun groupe variant d‚Äôun cas √† l‚Äôautre, il n‚Äôexiste pas de r√®gle pour prescrire une m√©thode ou une autre. La partitionnement hi√©rarchique a l‚Äôavantage de permetre de visualiser comment les groupes s‚Äôagglom√®rent. Parmi les m√©thodes de partitionnement hi√©rarchique disponibles, les m√©thodes bas√©es sur la densit√© permettent une grande flexibilit√©, ainsi qu‚Äôune d√©tection d‚Äôobservations ne faisant partie d‚Äôaucun goupe. 9.4 Ordination En √©cologie, biologie, agronommie comme en foresterie, la plupart des tableaux de donn√©es comprennent de nombreuses variables: pH, nutriments, climat, esp√®ces ou cultivars, etc. L‚Äôordination vise √† mettre de l‚Äôordre dans des donn√©es dont le nombre √©lev√© de variables peut amener √† des difficult√©s d‚Äôappr√©ciation et d‚Äôinterpr√©taion (Legendre et Legendre, 2012). Plus pr√©cis√©ment, le terme ordination est utilis√© en √©cologie pour d√©signer les techniques de r√©duction d‚Äôaxe. L‚Äôanalyse en composante principale est probablement la plus connue de ces techniques. Mais de nombreuses techniques d‚Äôordination ont √©t√© d√©velopp√©es au cours des derni√®res ann√©es, chacune ayant ses domaines d‚Äôapplication. Les techniques de r√©duction d‚Äôaxe permettent de d√©gager l‚Äôinformation la plus importante en projetant une synth√®se des relations entre les observations et entre les variables. Les techniques ne supposant aucune structure a priori sont dites non-contraignantes: elles ne comprennent pas de tests statistiques. √Ä l‚Äôinverse, les ordinations contraignantes lient des variables descriptives avec une ou plusieurs variables pr√©dictives. La r√©f√©rence en la mati√®re est indiscutablement (Legendre et Legendre, 2012). Cette section en couvrira quelques unes et vous guidera vers la technique la plus appropri√©e pour vos donn√©es. 9.4.1 Ordination non contraignante Cette section couvrira l‚Äôanalyse en composantes principales (ACP), l‚Äôanalyse de correspondance (AC), l‚Äôanalyse factorielle (AF) ainsi que l‚Äôanalyse en coordonn√©es principales (ACoP). M√©thode Distance pr√©serv√©e Variables Analyse en composantes principales (ACP) Distance euclidienne Donn√©es quantitatives, relations lin√©aires (attention aux double-z√©ros) Analyse de correspondance (AC) Distance de \\(\\chi^2\\) Donn√©es non-n√©gatives, dimentionnellement homog√®nes ou binaires, abondance ou occurence Positionnement multidimensionnel (PoMd) Toute mesure de dissimilarit√© Donn√©es quantitatives, qualitatives nominales/ordinales ou mixtes Source: Adapt√© de (Legendre et Legendre, 2012, chapitre 9) 9.4.1.1 Analyse en composantes principales L‚Äôobjectif d‚Äôune ACP est de repr√©senter les donn√©es dans un nombre r√©duit de dimensions repr√©sentant le plus possible la variation d‚Äôun tableau de donn√©es: elle permet de projetter les donn√©es dans un espace o√π les variables sont combin√©es en axes orthogonaux dont le premier axe capte le maximum de variance. L‚ÄôACP peut par exemple √™tre utilis√©e pour analyser des corr√©lations entre variables ou d√©gager l‚Äôinformation la plus pertinente d‚Äôun tableau de donn√©es m√©t√©o ou de signal en un nombre plus retreint de variables. L‚ÄôACP effectue une rotation des axes √† partir du centre (moyenne) du nuage de points effectu√©e de mani√®re √† ce que le premier axe d√©finisse la direction o√π l‚Äôon retrouve la variance maximale. Ce premier axe est une combinaison lin√©aire des variables et forme la premi√®re composante principale. Une fois cet axe d√©finit, on trouve de deuxi√®me axe, orthogonal au premier, o√π l‚Äôon retouve la variance maximale - cet axe forme la deuxi√®me composante principale, et ainsi de suite jusqu‚Äô√† ce que le nombre d‚Äôaxe corresponde au nombre de variables. Les projections des observations sur ces axes principaux sont appel√©s les scores. Les projections des variables sur les axes principaux sont les vecteurs propres (eigenvectors, ou loadings). La variance des composantes principales diminue de la premi√®re √† la derni√®re, et peut √™tre calcul√©e comme une proportion de la variance totale: c‚Äôest le pourcentage d‚Äôinertie. Par convention, on utilise les valeurs propres (eigenvalues) pour mesurer l‚Äôimportance des axes. Si la premi√®re composante principale a une inertie de 50% et la deuxi√®me a une intertie de 30%, la repr√©sentation en 2D des projection repr√©sentera 80% de la variance du nuage de points. L‚Äôh√©t√©rog√©n√©it√© des √©chelles de mesure peut avoir une grande importance sur les r√©sultats d‚Äôune ACP (les donn√©es doivent √™tre dimensionnellement homog√®nes). En effet, la hauteur d‚Äôun ceriser aura une variance plus grande que le diam√®tre d‚Äôune cerise exprim√© dans les m√™mes unit√©s, et cette derni√®re aura plus de variance que la teneur en cuivre d‚Äôune feuille. Il est cons√©quemment avis√© de mettre les donn√©es √† l‚Äô√©chelle en centrant la moyenne √† z√©ro et l‚Äô√©cart-type √† 1 avant de proc√©der √† une ACP. L‚ÄôACP a √©t√© con√ßue pour projetter en un nombre moindre de dimensions des observations dont les distributions sont multinormales. Bien que l‚ÄôACP soit une technique robuste, il est pr√©f√©rable de transformer pr√©alablement les variables dont la distribution est particuli√®rement asym√©triques (Legendre et Legendre, 2012, p. 450). Le cas √©ch√©ant, les valeurs extr√™mes pourraient faire d√©vier les vecteurs propres et biaiser l‚Äôanalyse. En particulier, les donn√©es ACP men√©es sur des donn√©es compositionnelles sont r√©put√©es pour g√©n√©rer des analyses biais√©es (Pawlowsky-Glahn and Egozcue, 2006). Le test de Mardia (Korkmaz, 2014) peut √™tre utilis√© pour tester la multinormalit√©. Une distribution multinormale devrait g√©n√©rer des scores en forme d‚Äôhypersph√®re (en forme de cercle sur un biplot: voir plus loin). 9.4.1.1.1 Vecteurs propres et valeurs propres Une matrice carr√©e (comme une matrice de covariance \\(\\Sigma\\)) multipli√©e par un vecteur propre \\(e\\) est √©gale aux valeurs propres \\(\\lambda\\) multipli√©es par les vecteurs propres \\(e\\). \\[ \\Sigma e = \\lambda e \\] De mani√®re intuitive, les vecteurs propres indiquent l‚Äôorientation de la covariance, et les valeurs propres indique la longueur associ√©e √† cette direction. L‚ÄôACP est bas√©e sur le calcul des vecteurs propres et des valeurs propres de la matrice de covariance des variables. Pour d‚Äôabord obtenir les valeurs propres \\(\\lambda\\), il faut r√©soudre l‚Äô√©quation \\[ det(cov(X) - \\lambda I) = 0 \\], o√π \\(det\\) est l‚Äôop√©ration permettant de calculer le d√©terminant, \\(cov\\) est l‚Äôop√©ration pour calculer la covariance, \\(X\\) est la matrice de donn√©es, \\(\\lambda\\) sont les valeurs propres et \\(I\\) est une matrice d‚Äôidentit√©. Pour \\(p\\) variables dans votre tableau \\(X\\), vous obtiendrex \\(p\\) valeurs propres. Ensuite, on trouve les vecteurs propres en r√©solvant l‚Äô√©quation $ e = e $. Bien qu‚Äôil soit possible d‚Äôeffectuer cette op√©ration √† la main pour des cas tr√®s simples, vous aurez avantage √† utiliser un langage de programmation. Chargeons les donn√©es d‚Äôiris, puis isolons seulement les deux dimensions des s√©pales l‚Äôesp√®ce setosa. data(&quot;iris&quot;) setosa_sepal &lt;- iris %&gt;% filter(Species == &quot;setosa&quot;) %&gt;% select(starts_with(&quot;Sepal&quot;)) setosa_sepal ## Sepal.Length Sepal.Width ## 1 5.1 3.5 ## 2 4.9 3.0 ## 3 4.7 3.2 ## 4 4.6 3.1 ## 5 5.0 3.6 ## 6 5.4 3.9 ## 7 4.6 3.4 ## 8 5.0 3.4 ## 9 4.4 2.9 ## 10 4.9 3.1 ## 11 5.4 3.7 ## 12 4.8 3.4 ## 13 4.8 3.0 ## 14 4.3 3.0 ## 15 5.8 4.0 ## 16 5.7 4.4 ## 17 5.4 3.9 ## 18 5.1 3.5 ## 19 5.7 3.8 ## 20 5.1 3.8 ## 21 5.4 3.4 ## 22 5.1 3.7 ## 23 4.6 3.6 ## 24 5.1 3.3 ## 25 4.8 3.4 ## 26 5.0 3.0 ## 27 5.0 3.4 ## 28 5.2 3.5 ## 29 5.2 3.4 ## 30 4.7 3.2 ## 31 4.8 3.1 ## 32 5.4 3.4 ## 33 5.2 4.1 ## 34 5.5 4.2 ## 35 4.9 3.1 ## 36 5.0 3.2 ## 37 5.5 3.5 ## 38 4.9 3.6 ## 39 4.4 3.0 ## 40 5.1 3.4 ## 41 5.0 3.5 ## 42 4.5 2.3 ## 43 4.4 3.2 ## 44 5.0 3.5 ## 45 5.1 3.8 ## 46 4.8 3.0 ## 47 5.1 3.8 ## 48 4.6 3.2 ## 49 5.3 3.7 ## 50 5.0 3.3 library(&quot;MVN&quot;) ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 ## sROC 0.1-2 loaded setosa_sepal_mvn &lt;- mvn(setosa_sepal, mvnTest = &quot;mardia&quot;) setosa_sepal_mvn$multivariateNormality ## Test Statistic p value Result ## 1 Mardia Skewness 0.759503524380438 0.943793240544741 YES ## 2 Mardia Kurtosis 0.0934600553610254 0.925538081956867 YES ## 3 MVN &lt;NA&gt; &lt;NA&gt; YES Pour consid√©rer la distribution comme multinormale, la p-value de la distortion (Mardia Skewness) et la statistique de Kurtosis (Mardia Kurtosis) doit √™tre √©gale ou plus √©lev√©e que 0.05 (Kormaz, 2019, fiche d‚Äôaide de la fonction mvn de R). C‚Äôest bien le cas pour les donn√©es du tableau setosa_sepal. Retirons de la matrice de covariance les valeurs et vecteurs propres avec la fonction eigen. setosa_eigen &lt;- eigen(cov(setosa_sepal)) setosa_eigenval &lt;- setosa_eigen$values setosa_eigenvec &lt;- setosa_eigen$vectors Le premier vecteur propre correspond √† la premi√®re colonne, et le second √† la deuxi√®me. Les coordonn√©es x et y sont les premi√®res et deuxi√®mes lignes. Les vecteurs propres ont une longueur unitaire (norme de 1). Ils peuvent √™tre mis √† l‚Äô√©chelles √† la racine carr√©e des valeurs propres. setosa_eigenvec_sc &lt;- setosa_eigenvec %*% diag(sqrt(setosa_eigen$values)) Pour effectuer une translation des vecteurs propres au centre du nuage de point, nous avons besoin du centro√Øde. centroid &lt;- setosa_sepal %&gt;% apply(., 2, mean) plot(setosa_sepal, asp = 1) # vecteurs propres brutes lines(x=c(centroid[1], centroid[1] + setosa_eigenvec[1, 1]), y=c(centroid[2], centroid[2] + setosa_eigenvec[2, 1]), col = &quot;green&quot;, lwd = 3) # vecteur propre 1 lines(x=c(centroid[1], centroid[1] + setosa_eigenvec[1, 2]), y=c(centroid[2], centroid[2] + setosa_eigenvec[2, 2]), col = &quot;green&quot;, lwd = 3) # vecteur propre 1 # vecteurs propres √† l&#39;√©chelle lines(x=c(centroid[1], centroid[1] + setosa_eigenvec_sc[1, 1]), y=c(centroid[2], centroid[2] + setosa_eigenvec_sc[2, 1]), col = &quot;red&quot;, lwd = 4) # vecteur propre 1 lines(x=c(centroid[1], centroid[1] + setosa_eigenvec_sc[1, 2]), y=c(centroid[2], centroid[2] + setosa_eigenvec_sc[2, 2]), col = &quot;red&quot;, lwd = 4) # vecteur propre 1 points(x=centroid[1], y=centroid[2], pch = 16, cex = 2, col =&quot;blue&quot;) # centroid On peut observer que, comme je l‚Äôai mentionn√© plus haut, les vecteurs propres indiquent l‚Äôorientation de la covariance, et les valeurs propres indique la longueur associ√©e √† cette direction. 9.4.1.1.2 Biplot Imaginez un nuage de points en 3D, axes y compris. Vous tournez votre nuage de points pour trouver la perspective en 2D qui fera en sorte que vos donn√©es soient les plus dispers√©es possibles. Avec une lampe de poche, vous illuminez votre nuage de points dans l‚Äôaxe de cette perspective: vous venez d‚Äôeffectuer une analyse en composantes principales, et l‚Äôombre des points et des axes sur le mur formera votre biplot. Pour cr√©er un biplot, on juxtapose les descripteurs (variables) en tant que vecteurs propres, repr√©sent√©s par des fl√®ches, et les objets (observations) en tant que scores, repr√©sent√©s par des points. Les r√©sultats d‚Äôune ordination peuvent √™tre pr√©sent√©s selon deux types de biplots (Legendre et Legendre, 2012). Biplot de corr√©lation permettant de visualiser les corr√©lations entre des variables m√©t√©orologiques. Deux types de projection sont courramment utilis√©s. Biplot de distance. Ce type de projection permet de visualiser la position des objets entre eux et par rapport aux descripteurs et d‚Äôappr√©cier la contribution des descripteurs pour cr√©er les composantes principales. Pour cr√©er un biplot de distance, on projette directement les vecteurs propres (\\(U\\)) en guise de descripteurs. Pour ce qui est des objets, on utilise les scores de l‚ÄôACP (\\(F\\)). De cette mani√®re, les distances euclidiennes entre les scores sont des approximations des distances euclidiennes dans l‚Äôespace multidimentionnel, la projection d‚Äôun objet sur un descripteur perpendiculairement √† ce dernier est une approximation de la position de l‚Äôobjet sur le descripteur et la projection d‚Äôun descripteur sur un axe principal est proportionnelle √† sa contribution pour g√©n√©rer l‚Äôaxe. Biplot de corr√©lation. Cette projection permet d‚Äôappr√©cier les corr√©lations entre les descripteurs. Pour ce faire, les objets et les valeurs propres doivent √™tre transform√©s. Pour g√©n√©rer les descripteurs, les vecteurs propres (\\(U\\)) doivent √™tre multipli√©s par la matrice diagonalis√©e de la racine carr√©e des valeurs propres (\\(\\Lambda\\)), c‚Äôest-√†-dire \\(U \\Lambda ^{\\frac{1}{2}}\\). En ce qui a trait aux objets, on multiplie les scores par (\\(F\\)) par la racine carr√©e n√©gative des valeurs propres diagonalis√©es, c‚Äôest-√†-dire \\(F \\Lambda ^{- \\frac{1}{2}}\\). De cette mani√®re, tout comme c‚Äôest le cas pour le biplot de distance, la projection d‚Äôun objet sur un descripteur perpendiculairement √† ce dernier est une approximation de la position de l‚Äôobjet sur le descripteur, la projection d‚Äôun descripteur sur un axe principal est proportionnelle √† son √©cart-type et les angles entre les descripteurs sont proportionnelles √† leur corr√©lation (et non pas leur proximit√©). En d‚Äôautres mots, le bilot de distances devrait √™tre utilis√© pour appr√©cier la distance entre les objets et le biplot de corr√©lation devrait √™tre utilis√© pour appr√©cier les corr√©lations entre les descripteurs. Mais dans tous les cas, le type de biplot utilis√© doit √™tre indiqu√©. Le triplot est une forme apparent√©e au biplot, auquel on ajoute des variables pr√©dictives. Le triplot est utile pour repr√©senter les r√©sultats des ordinations contraignantes comme les analyses de redondance et les analyse de correspondance canoniques. 9.4.1.1.3 Application Bien que l‚ÄôACP puisse √™tre effectu√©e gr√¢ce √† des modules de base de R, nous utiliserons le module vegan. Le tableau varechem comprend des donn√©es issues d‚Äôanalyse de sols identifi√©s par leur composition chimique, leur pH, leur profondeur totale et la profondeur de l‚Äôhumus publi√©es dans V√§re et al. (1995) et export√©es du module vegan. library(&quot;vegan&quot;) data(&quot;varechem&quot;) varechem %&gt;% sample_n(5) ## N P K Ca Mg S Al Fe Mn Zn Mo Baresoil Humdepth ## 1 30.5 24.6 78.7 188.5 55.5 25.3 294.9 123.8 10.1 3.0 0.40 18.60 1.7 ## 2 15.0 48.4 127.4 499.6 75.1 46.9 227.1 32.2 35.1 8.9 0.70 6.07 2.2 ## 3 14.3 62.8 215.2 709.7 102.5 48.6 168.2 32.0 46.9 8.7 0.05 4.40 1.2 ## 4 22.3 47.4 165.9 436.1 64.3 42.3 316.5 200.1 28.2 7.2 0.30 0.01 1.5 ## 5 16.7 55.8 205.3 1169.7 126.3 35.9 253.6 96.4 25.1 8.2 0.05 7.60 1.1 ## pH ## 1 3.1 ## 2 3.0 ## 3 3.2 ## 4 2.9 ## 5 3.6 Comme nous l‚Äôavons vu pr√©cdemment, les donn√©es de concentration sont de type compositionnelles. Les donn√©es compositionnelles du tableau varechem m√©riteraient d‚Äô√™tre transform√©es (Aitchison et Greenacre, 2002). Utilisons les log-ratios centr√©s (clr). library(&quot;compositions&quot;) ## Loading required package: tensorA ## ## Attaching package: &#39;tensorA&#39; ## The following object is masked from &#39;package:base&#39;: ## ## norm ## Loading required package: robustbase ## Loading required package: bayesm ## Welcome to compositions, a package for compositional data analysis. ## Find an intro with &quot;? compositions&quot; ## ## Attaching package: &#39;compositions&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## cor, cov, dist, var ## The following objects are masked from &#39;package:base&#39;: ## ## %*%, scale, scale.default varecomp &lt;- varechem %&gt;% select(-Baresoil, -Humdepth, -pH) %&gt;% mutate(Fv = apply(., 1, function(x) 1e6 - sum(x))) vareclr &lt;- varecomp %&gt;% acomp(.) %&gt;% clr(.) %&gt;% as_tibble() %&gt;% bind_cols(varechem %&gt;% select(Baresoil, Humdepth, pH)) vareclr %&gt;% sample_n(5) ## # A tibble: 5 x 15 ## N P K Ca Mg S Al Fe Mn Zn Mo ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -1.38 -0.525 0.659 1.96 -0.193 -0.767 0.172 -1.49 -0.102 -2.33 -5.16 ## 2 -1.69 -0.624 0.830 1.59 -0.0312 -0.729 0.189 -0.626 -0.331 -2.60 -5.49 ## 3 -0.871 -0.623 0.872 1.92 -0.244 -0.757 0.250 -1.77 -0.862 -2.41 -5.26 ## 4 -1.49 -1.12 0.923 1.47 -0.0470 -0.728 0.973 -0.126 -0.703 -2.62 -5.52 ## 5 -0.881 -0.648 0.723 2.26 0.384 -0.825 -0.347 -2.15 -0.611 -2.26 -5.53 ## # ... with 4 more variables: Fv &lt;dbl&gt;, Baresoil &lt;dbl&gt;, Humdepth &lt;dbl&gt;, pH &lt;dbl&gt; Effectuons l‚ÄôACP. Pour cet exemple, nous standardiserons les donn√©es √©tant donn√©es que les colonnes Baresoil, Humedepth et pH ne sont pas √† la m√™me √©chelle que les colonnes des clr. vareclr_sc &lt;- scale(vareclr) vare_pca &lt;- rda(vareclr_sc) # ou bien rda(vareclr, scale = TRUE, mais la mise √† l&#39;√©chelle pr√©alable est plus explicite) L‚Äôobjet vareclr_pca contient l‚Äôinformation n√©cessaire pour mener notre ACP. summary(vare_pca, scaling = 2) # scaling = 2 pour obtenir les infos pour les biplots de corr√©lation ## ## Call: ## rda(X = vareclr_sc) ## ## Partitioning of variance: ## Inertia Proportion ## Total 15 1 ## Unconstrained 15 1 ## ## Eigenvalues, and their contribution to the variance ## ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Eigenvalue 7.1523 2.4763 2.1122 0.93015 0.57977 0.48786 0.36646 ## Proportion Explained 0.4768 0.1651 0.1408 0.06201 0.03865 0.03252 0.02443 ## Cumulative Proportion 0.4768 0.6419 0.7827 0.84473 0.88338 0.91590 0.94034 ## PC8 PC9 PC10 PC11 PC12 PC13 ## Eigenvalue 0.29432 0.19686 0.15434 0.107357 0.095635 0.042245 ## Proportion Explained 0.01962 0.01312 0.01029 0.007157 0.006376 0.002816 ## Cumulative Proportion 0.95996 0.97308 0.98337 0.990527 0.996902 0.999719 ## PC14 ## Eigenvalue 0.0042200 ## Proportion Explained 0.0002813 ## Cumulative Proportion 1.0000000 ## ## Scaling 2 for species and site scores ## * Species are scaled proportional to eigenvalues ## * Sites are unscaled: weighted dispersion equal on all dimensions ## * General scaling constant of scores: 4.309777 ## ## ## Species scores ## ## PC1 PC2 PC3 PC4 PC5 PC6 ## N 0.1437 0.7606 -0.6792 0.19837 0.1128526 -0.050149 ## P 0.8670 -0.3214 -0.2950 -0.22940 0.1437960 -0.042884 ## K 0.9122 -0.3857 0.2357 0.03469 0.2737020 0.075717 ## Ca 0.9649 -0.3362 -0.2147 0.17757 -0.2188717 0.008051 ## Mg 0.8263 -0.2723 0.1035 0.52135 -0.1495399 -0.342214 ## S 0.8825 -0.3169 0.3539 -0.21216 0.1176279 -0.191386 ## Al -1.0105 -0.2442 0.2146 0.02674 -0.1005560 -0.043569 ## Fe -1.0338 -0.2464 0.1492 0.13162 0.1512218 0.081571 ## Mn 0.9556 0.1041 -0.1256 -0.21300 0.2565831 0.275275 ## Zn 0.7763 -0.1031 -0.3123 -0.36493 -0.5665691 0.153089 ## Mo -0.2152 0.8717 0.4065 -0.33643 -0.2134335 -0.167725 ## Fv 0.2360 0.5776 -0.8112 0.12736 0.1280097 -0.109737 ## Baresoil 0.5147 0.4210 0.4472 0.54980 -0.1438570 0.463148 ## Humdepth 0.7455 0.4379 0.5194 0.16493 0.0004757 -0.273056 ## pH -0.5754 -0.5864 -0.5957 0.23408 -0.1517661 -0.056641 ## ## ## Site scores (weighted sums of species scores) ## ## PC1 PC2 PC3 PC4 PC5 PC6 ## sit1 0.16862 0.423777 0.46731 0.91175 1.10380 1.06421 ## sit2 -0.09705 -0.097482 0.61143 -0.29049 1.14916 0.40622 ## sit3 0.02831 -0.795737 0.74176 -0.19097 -2.43337 -0.81762 ## sit4 1.39081 -0.354376 -0.19377 -0.45160 0.46020 -0.31446 ## sit5 1.30346 0.357866 0.29887 0.76856 0.20913 -0.64145 ## sit6 0.43636 0.495037 1.21722 1.18128 -0.98242 -0.74474 ## sit7 1.07306 0.467575 -0.32245 0.03717 0.13956 -0.64972 ## sit8 0.02545 0.659714 -0.28861 -0.01424 0.47105 0.45173 ## sit9 1.42005 0.007356 -0.29000 -0.78474 0.97592 -0.80263 ## sit10 -0.50638 -0.220909 1.52981 0.26289 0.42135 0.94054 ## sit11 0.45392 0.649297 0.44573 -0.26620 -0.74522 -0.53228 ## sit12 0.18623 0.259640 0.89112 0.21096 -0.51393 2.24361 ## sit13 1.26264 0.225744 -0.96668 -0.69334 0.61990 0.43312 ## sit14 -1.48685 0.739545 -0.20926 1.09256 0.61856 -0.87999 ## sit15 -0.50622 1.108685 -2.61287 -1.00433 -1.35383 1.21964 ## sit16 -1.28653 0.898663 -0.38778 -0.47556 -0.02449 -0.29419 ## sit17 -1.72773 0.476962 -0.48878 0.71156 1.06398 -1.33473 ## sit18 -0.82844 -0.296515 1.20315 -1.49821 -0.18330 1.05231 ## sit19 -1.00247 -0.609253 0.25185 -0.85420 0.71031 0.14854 ## sit20 -0.43405 -0.338912 0.55348 -1.35776 -0.81986 -1.02468 ## sit21 -0.05083 0.122645 -0.04611 -0.56047 -0.26151 -0.98053 ## sit22 0.17891 -2.315489 -0.69084 -0.19547 0.80628 0.04291 ## sit23 -0.46443 -2.592018 -1.21615 1.56359 -0.62334 0.28748 ## sit24 0.46316 0.728185 -0.49843 1.89726 -0.80791 0.72671 La deuxi√®me ligne de Importance of components, Proportion Explained, indique la proportion de la variance totale capt√©e successivement par les axes principaux. Le premier axe principal comporte 47.68% de la variance. Le deuxi√®me axe principal ajoutant une proportion de 16,51%, une repr√©sentation en deux axes principaux pr√©sentent 64.19 % de la variance. prop_expl &lt;- vare_pca$CA$eig / sum(vare_pca$CA$eig) prop_expl ## PC1 PC2 PC3 PC4 PC5 PC6 ## 0.4768180610 0.1650859388 0.1408156459 0.0620101490 0.0386511040 0.0325238535 ## PC7 PC8 PC9 PC10 PC11 PC12 ## 0.0244303815 0.0196215021 0.0131238464 0.0102890284 0.0071571089 0.0063756951 ## PC13 PC14 ## 0.0028163495 0.0002813359 La d√©cision du nombre d‚Äôaxes principaux √† retenir est arbitraire. Elle peut d√©pendre d‚Äôun nombre maximal de param√®tre √† retenir pour √©viter de surdimensionner un mod√®le (curse of dimensionality, section 11) ou d‚Äôun seuil de pourcentage de variance minimal √† retenir, par exemple 75%. Ou bien, vous retiendrez deux composantes principales si vous d√©sirez pr√©senter un seul biplot. L‚Äôapproche de Kaiser-Guttmann (Borcard et al., 2011) consiste √† s√©lectionner les composantes principales dont la valeur propre est sup√©rieure √† leur moyenne. plot(x = 1:length(vare_pca$CA$eig), y = vare_pca$CA$eig, type = &quot;b&quot;, xlab = &quot;Rang de la valeur propre&quot;, ylab = &quot;Valeur propre&quot;) abline(h = mean(vare_pca$CA$eig), col = &quot;red&quot;, lty = 2) L‚Äôapproche du broken stick consiste √† couper un b√¢ton d‚Äôune longueur de 1 en n tranches. La premi√®re tranche est de longueur \\(\\frac{1}{n}\\). La tranche suivante est d‚Äôune longueur de la tranche pr√©c√©dente √† laquelle on aditionne \\(\\frac{1}{longueur~restante}\\). Puis on place les longueurs en ordre d√©croissant. On retient les composantes principales dont les valeurs propres cumul√©es sont plus grandes que le broken stick. broken_stick &lt;- function(x) { bsm &lt;- vector(&quot;numeric&quot;, length = x) bsm[1] &lt;- 1/x for (i in 2:x) { bsm[i] &lt;- bsm[i-1] + 1/(x+1-i) } bsm &lt;- rev(bsm/x) return(bsm) } Le graphique du broken stick: plot(x = 1:length(vare_pca$CA$eig), y = prop_expl, type = &quot;b&quot;, xlab = &quot;Rang de la valeur propre&quot;, ylab = &quot;Valeur propre&quot;) lines(x = 1:length(vare_pca$CA$eig), y = broken_stick(length(vare_pca$CA$eig)), col = &quot;red&quot;, lty = 2) Les approches Kaiser-Guttmann et broken stick sugg√®rent que les trois premi√®res composantes sont suffisantes pour d√©crire la dispersion des donn√©es. Examinons les loadings (vecteurs propres) plus en particulier. Dans le langage du module vegan, les vecteurs propres sont les esp√®ces (species) et les scores sont les sites. vare_eigenvec &lt;- vegan::scores(vare_pca, scaling = 2, display = &quot;species&quot;, choices = 1:(ncol(vareclr)-1)) vare_eigenvec ## PC1 PC2 PC3 PC4 PC5 PC6 ## N 0.1437343 0.7606006 -0.6792046 0.1983670 0.1128526122 -0.050148980 ## P 0.8669892 -0.3213683 -0.2949864 -0.2294036 0.1437959857 -0.042883754 ## K 0.9122089 -0.3857245 0.2356904 0.0346904 0.2737019601 0.075717162 ## Ca 0.9648855 -0.3361651 -0.2147486 0.1775746 -0.2188716732 0.008050762 ## Mg 0.8263327 -0.2723055 0.1035276 0.5213484 -0.1495399242 -0.342213793 ## S 0.8824519 -0.3169039 0.3538854 -0.2121562 0.1176278503 -0.191386377 ## Al -1.0105173 -0.2441785 0.2145614 0.0267422 -0.1005559874 -0.043569364 ## Fe -1.0337676 -0.2463987 0.1491865 0.1316173 0.1512218115 0.081571443 ## Mn 0.9555632 0.1041030 -0.1256178 -0.2130047 0.2565830557 0.275275174 ## Zn 0.7763480 -0.1030878 -0.3122919 -0.3649341 -0.5665691228 0.153089144 ## Mo -0.2152399 0.8717229 0.4064967 -0.3364279 -0.2134335302 -0.167725160 ## Fv 0.2360040 0.5775863 -0.8111953 0.1273582 0.1280096553 -0.109737235 ## Baresoil 0.5147445 0.4209983 0.4472351 0.5497950 -0.1438569673 0.463148072 ## Humdepth 0.7455213 0.4379436 0.5193895 0.1649306 0.0004756685 -0.273056212 ## pH -0.5753858 -0.5863743 -0.5957495 0.2340826 -0.1517660977 -0.056640816 ## PC7 PC8 PC9 PC10 PC11 ## N -0.09111164 -0.06122008 0.315645453 0.08090232 -0.019251478 ## P 0.26894062 0.34111276 0.021124287 0.08756299 -0.045741546 ## K -0.21662612 -0.01641260 0.143099440 -0.08737113 0.183005607 ## Ca 0.03630015 0.04775616 -0.073609828 -0.10601799 0.161460554 ## Mg 0.04617838 -0.12098602 -0.051599273 0.18373857 -0.009862571 ## S -0.26825994 0.15822845 0.038378858 0.05100717 -0.138785063 ## Al -0.22737412 0.10598673 0.040586196 -0.14473132 -0.089462074 ## Fe 0.10553041 -0.09254655 -0.079426433 0.09908706 -0.006376211 ## Mn 0.20224538 -0.19347804 -0.038859808 -0.07637994 -0.083300112 ## Zn -0.12332232 -0.14862229 0.024026151 0.02643462 -0.064973307 ## Mo 0.13788948 0.17165900 0.032981311 0.01419924 0.128814989 ## Fv -0.20911147 0.11289753 -0.281443886 -0.08391004 -0.012456867 ## Baresoil -0.02103009 0.23028292 0.004554036 0.02604286 -0.061147847 ## Humdepth 0.17061078 -0.11310394 0.027515405 -0.23068827 -0.102189307 ## pH 0.19890884 0.12152266 0.150118818 -0.15240317 -0.037691048 ## PC12 PC13 PC14 ## N 0.045420621 0.05020956 0.002340519 ## P 0.145128883 -0.03337551 -0.010109130 ## K 0.002260341 -0.10566808 0.001169065 ## Ca 0.041210064 0.14341793 0.007419161 ## Mg -0.063493608 -0.03782662 -0.023575986 ## S -0.117144869 0.06075094 0.025874035 ## Al 0.058212507 0.01983102 -0.037901576 ## Fe 0.049837173 -0.01169516 0.036048221 ## Mn -0.133353213 0.02679781 -0.021373612 ## Zn 0.051057277 -0.06538348 0.010896560 ## Mo -0.114803631 -0.01989539 -0.001335923 ## Fv -0.020157331 -0.05448619 0.005707928 ## Baresoil -0.019696758 -0.01640490 0.003823725 ## Humdepth 0.109293684 -0.02485030 0.016559206 ## pH -0.153813168 -0.04523353 0.014193061 ## attr(,&quot;const&quot;) ## [1] 4.309777 L‚Äôordre d‚Äôimportance des vecteurs propres est √©tabli en ordre croissant des √©l√©ment des vecteurs propres associ√©es. Un vecteur propre est une combinaison lin√©aire des variables. Par exemple, le premier vecteur propre pointe surtout dans la direction du Fe (-1.497) et de l‚ÄôAl (-1.463). Le deuxi√®me pointe surtout vers le Mo (2.145). Les vecteurs (loadings) d‚Äôun biplot de distance pr√©sentant les des deux premi√®res composantes principales prendront les coordonn√©es des deux premi√®res colonnes. Le vecteur Al aura la coordonn√©e [-1.463 ; -0.601], le vecteur de Fe sera plac√© √† [-1.497 ; -0.606] et le vecteur Mo √† [-0.312 ; 2.145]. Il existe diff√©rentes fonctions d‚Äôaffichage des biplots. Notez que leur longueur peut √™tre magnifi√©e pour am√©liorer la visualisation. Lan√ßons la fonction biplot pour cr√©er un biplot de distance et un autre de corr√©lation. par(mfrow = c(1, 2)) biplot(vare_pca, scaling = 1, main = &quot;Biplot de distance&quot;) biplot(vare_pca, scaling = 2, main = &quot;Biplot de corr√©lation&quot;) Le biplot de distance permet de d√©gager les variables qui expliquent davantage la variabilit√© dans notre tableau: les clr du Fe et de l‚ÄôAl forment en grande partie le premier axe principal, alors que le clr du Mo forme en grande partie le second axe. Le biplot de corr√©lation montre que les clr du Fe et du Al sont corr√©l√©s dans le m√™me sens, mais das le sens contraire du clr du Mn. L‚Äôinformation sur la teneur en Fe et celle de l‚ÄôAl est en grande partie redondante. Toutefois, le clr du Mo est presque ind√©pendant du clr du Fe, ceux-ci √©tant √† angle presque droit (~90¬∞). Ces relations peuvent √™tre explor√©es directement. par(mfrow = c(1, 2)) plot(vareclr$Al, vareclr$Fe) plot(vareclr$Mo, vareclr$Fe) Nous avons mentionn√© que l‚ÄôACP est une rotation. Prenons un second exemple pour bien en saisir les tenants et aboutissants. Le tableau de donn√©es que nous chargerons provient d‚Äôun infographie d‚Äôun dauphin, intitull√©e Bottlenose Dolphin, con√ßu par l‚Äôartiste Tarnyloo. Les points correspondent √† la surface d‚Äôun dauphin. J‚Äôai ajout√© une colonne anatomy, qui indique √† quelle partie anatomique le point appartient. dolphin &lt;- read_csv(&quot;data/07_dolphin.csv&quot;) ## Parsed with column specification: ## cols( ## x = col_double(), ## y = col_double(), ## z = col_double(), ## anatomy = col_character() ## ) dolphin %&gt;% sample_n(5) ## # A tibble: 5 x 4 ## x y z anatomy ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.164 0.944 0.103 Body ## 2 0.0388 -0.542 -0.246 Head ## 3 -0.0268 -0.693 -0.139 Head ## 4 -0.0159 -0.501 -0.199 Head ## 5 -0.167 -0.369 0.00947 Head Voici en vue isom√©trique ce en quoi consiste ce nuage de points. library(&quot;scatterplot3d&quot;) scatterplot3d(x = dolphin$x, y = dolphin$y, z = dolphin$z, pch = 16, cex.symbols = 0.2) Effectuons l‚ÄôACP sur le dauphin. dolph_pca &lt;- rda(dolphin %&gt;% select(x, y, z), scale = FALSE) biplot(dolph_pca, scaling = 2) On n‚Äôy voit pas grand chose, mais si l‚Äôon extrait les scores et que l‚Äôon raccourcit les vecteurs: dolph_scores &lt;- vegan::scores(dolph_pca, display = &quot;sites&quot;) dolph_loads &lt;- vegan::scores(dolph_pca, display = &quot;species&quot;) dolph_loads ## PC1 PC2 ## x -0.02990131 0.01608095 ## y -7.13731672 -1.43221776 ## z -4.56612084 2.23859843 ## attr(,&quot;const&quot;) ## [1] 9.089026 plot(dolph_scores, pch = 16, cex = 0.24, asp = 1, col = factor(dolphin$anatomy)) segments(x0 = rep(0, 3), y0 = rep(0, 3), x = dolph_loads[, 1]/50, y = dolph_loads[, 2]/50, col = &quot;chocolate&quot;, lwd = 4) La meilleure repr√©sentation du dauphin en 2D, selon la variance, est son profil - en effet, il est plus long et haut que large. Note. Une ACP effectue seulement une rotation des points. Les distances euclidiennes entre les points sont maintenues. Note. L‚ÄôACP a √©t√© con√ßue pour projetter en un nombre moindre de dimensions des observations dont les distributions sont multinormales (ce n‚Äôest √©videmment pas le cas du dauphin). Note. Les axes principaux d‚Äôune ACP sont des variables al√©atoires. Elles peuvent √™tre assujetties √† des tests ststistiques, des mod√®les, du partitionnement de donn√©es, etc. Excercice. Effectuez maintenant une ACP avec les donn√©es d‚Äôiris. 9.4.1.2 Analyse de correspondance (AC) L‚Äôanalyse de correspondance (AC) est particuli√®rement appropri√©e pour traiter des donn√©es d‚Äôabondance et d‚Äôoccurence. Tout comme l‚Äôanalyse en composantes principales, les donn√©es apport√©s vers une AC doivent √™tre dimensionnellement homog√®nes, c‚Äôest-√†-dire que chaque variable doit √™tre de m√™me m√©trique: pour des donn√©es d‚Äôabondance, cela signifie que les d√©comptes r√©f√®rent tous au m√™me concept: individus, colonies, surfaces occup√©es, etc. Alors que la distance euclidienne est pr√©serv√©e avec l‚ÄôACP, l‚ÄôAC pr√©serve la distance du \\(\\chi^2\\), qui est insensible aux double-z√©ros. L‚ÄôAC produit \\(min(n,p)-1\\) axes principaux orthogonaux qui captent non pas le maximum de variance, mais la proportion de mesures aux carr√© par rapport √† la somme des carr√©s de la matrice. Le biplot obtenu peut √™tre pr√©sent√© sous forme de biplot de site (scaling 1), o√π la distance du \\(\\chi^2\\) est pr√©serv√©e entre les sites ou biplot d‚Äôesp√®ces (scaling 2), ou la distance du \\(\\chi^2\\) est pr√©serv√©e entre les esp√®ces. L‚ÄôAC h√©rite du coup une propri√©t√© importate de la distance du \\(\\chi^2\\), qui accorde davantage de distance entre un compte de 0 et de 1 qu‚Äôentre 1 et 2, et davantage entre 1 et 2 qu‚Äôentre 2 et 3. Par exemple, sur ces trois sites, on a compt√© un individu A de moins que d‚Äôindividu B. abundance_0123 = tibble(Site = c(&quot;Site 1&quot;, &quot;Site 2&quot;, &quot;Site 3&quot;), A = c(0, 1, 9), B = c(1, 2, 10)) abundance_0123 ## # A tibble: 3 x 3 ## Site A B ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Site 1 0 1 ## 2 Site 2 1 2 ## 3 Site 3 9 10 Pourtant, la distance du \\(\\chi^2\\) est plus √©lev√©e entre le site 1 et le site 2 qu‚Äôentre le site 2 et le site 3. dist(decostand(abundance_0123 %&gt;% select(-Site), method=&quot;chi.square&quot;)) ## 1 2 ## 2 0.6724111 ## 3 0.9555316 0.2831205 La distance du \\(\\chi^2\\) donne davantage d‚Äôimportance aux esp√®ces rares, ce dont une analyse doit tenir compte. Il pourrait √™tre envisageable de retirer d‚Äôun tableau des esp√®ces rare, ou bien pr√©transformer des donn√©es d‚Äôabondance par une transformation de chord ou de Hellinger (tel que discut√© au chapitre 6), puis proc√©der √† une ACP sur ces donn√©es (Legendre et Gallagher, 2001). 9.4.1.2.1 Application Le tableau varespec comprend des donn√©es de surface de couverture de 44 esp√®ces de plantes en lien avec les donn√©es environnementales du tableau varechem. Ces donn√©es ont √©t√© publi√©es dans V√§re et al. (1995) et export√©es du module vegan. data(&quot;varespec&quot;) varespec %&gt;%sample_n(5) ## Callvulg Empenigr Rhodtome Vaccmyrt Vaccviti Pinusylv Descflex Betupube ## 1 0.00 8.92 0 2.42 10.28 0.12 0.02 0.00 ## 2 0.00 3.47 0 0.25 20.50 0.25 0.00 0.00 ## 3 4.47 7.33 0 2.15 4.33 0.10 0.00 0.00 ## 4 0.00 16.00 4 15.00 25.00 0.25 0.50 0.25 ## 5 0.00 12.68 0 0.00 23.73 0.03 0.00 0.00 ## Vacculig Diphcomp Dicrsp Dicrfusc Dicrpoly Hylosple Pleuschr Polypili ## 1 0 0.00 0.00 0.32 0.02 0 21.03 0.02 ## 2 0 0.25 0.00 0.38 0.25 0 4.07 0.00 ## 3 0 0.00 1.02 25.80 0.23 0 18.98 0.00 ## 4 0 0.00 0.25 0.25 3.00 0 2.00 0.00 ## 5 0 0.00 0.00 3.42 0.02 0 19.42 0.02 ## Polyjuni Polycomm Pohlnuta Ptilcili Barbhatc Cladarbu Cladrang Cladstel ## 1 1.58 0.18 0.07 0.27 0.02 7.23 4.95 22.08 ## 2 0.25 0.00 0.25 0.25 0.00 0.46 4.00 84.30 ## 3 0.02 0.00 0.13 0.10 0.00 7.13 14.03 0.02 ## 4 0.25 0.25 0.25 10.00 3.00 0.70 4.70 10.90 ## 5 2.12 0.00 0.17 1.80 0.02 9.08 9.22 0.05 ## Cladunci Cladcocc Cladcorn Cladgrac Cladfimb Cladcris Cladchlo Cladbotr ## 1 0.25 0.10 0.25 0.18 0.10 0.12 0.05 0.02 ## 2 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.00 ## 3 0.82 0.15 0.05 0.22 0.22 0.17 0.00 0.00 ## 4 0.25 0.00 0.05 0.25 0.25 0.25 0.25 0.25 ## 5 0.73 0.08 1.42 0.50 0.17 1.78 0.05 0.05 ## Cladamau Cladsp Cetreric Cetrisla Flavniva Nepharct Stersp Peltapht Icmaeric ## 1 0 0.00 0.00 0.00 0.02 0 0.28 0.00 0.00 ## 2 0 0.25 0.25 0.25 0.67 0 0.00 0.00 0.00 ## 3 0 0.02 0.18 0.08 0.00 0 0.03 0.00 0.07 ## 4 0 0.00 0.00 0.67 0.00 0 0.00 0.00 0.00 ## 5 0 0.00 0.00 0.00 0.02 0 1.58 0.33 0.00 ## Cladcerv Claddefo Cladphyl ## 1 0 0.37 0.00 ## 2 0 0.25 0.25 ## 3 0 0.67 0.00 ## 4 0 0.40 0.00 ## 5 0 1.97 0.00 Pour effectuer l‚ÄôAC, nous utiliserons, comme pour l‚ÄôACP, le module vegan mais cette fois-ci avec la fonction cca. L‚ÄôAC en scaling 1 est effectu√©e sur le tableau des abondances avec les esp√®ces comme colonnes et les sites comme lignes. Les matrices d‚Äôabondance transpos√©es indique les sites o√π chque esp√®ce ont √©t√© d√©nombr√©es: pour une analyse en scaling 2, on effectue une analyse de correspondance sur la matrice d‚Äôabondance (ou d‚Äôoccurence) transpos√©e. Pour chacune des AC, je filtre pour m‚Äôassurer que toutes les lignes contiennent au moins une observation. Ce n‚Äôest pas n√©cessaire dans notre cas, mais je le laisse pour l‚Äôexemple. vare_cca &lt;- cca(varespec %&gt;% filter(rowSums(.) &gt; 0)) summary(vare_cca, scaling = 1) ## ## Call: ## cca(X = varespec %&gt;% filter(rowSums(.) &gt; 0)) ## ## Partitioning of scaled Chi-square: ## Inertia Proportion ## Total 2.083 1 ## Unconstrained 2.083 1 ## ## Eigenvalues, and their contribution to the scaled Chi-square ## ## Importance of components: ## CA1 CA2 CA3 CA4 CA5 CA6 CA7 ## Eigenvalue 0.5249 0.3568 0.2344 0.19546 0.17762 0.12156 0.11549 ## Proportion Explained 0.2520 0.1713 0.1125 0.09383 0.08526 0.05835 0.05544 ## Cumulative Proportion 0.2520 0.4233 0.5358 0.62962 0.71489 0.77324 0.82868 ## CA8 CA9 CA10 CA11 CA12 CA13 CA14 ## Eigenvalue 0.08894 0.07318 0.05752 0.04434 0.02546 0.01710 0.014896 ## Proportion Explained 0.04269 0.03513 0.02761 0.02129 0.01222 0.00821 0.007151 ## Cumulative Proportion 0.87137 0.90650 0.93411 0.95539 0.96762 0.97583 0.982978 ## CA15 CA16 CA17 CA18 CA19 CA20 ## Eigenvalue 0.010160 0.007830 0.006032 0.004008 0.002865 0.0019275 ## Proportion Explained 0.004877 0.003759 0.002896 0.001924 0.001375 0.0009253 ## Cumulative Proportion 0.987855 0.991614 0.994510 0.996434 0.997809 0.9987341 ## CA21 CA22 CA23 ## Eigenvalue 0.0018074 0.0005864 0.0002434 ## Proportion Explained 0.0008676 0.0002815 0.0001168 ## Cumulative Proportion 0.9996017 0.9998832 1.0000000 ## ## Scaling 1 for species and site scores ## * Sites are scaled proportional to eigenvalues ## * Species are unscaled: weighted dispersion equal on all dimensions ## ## ## Species scores ## ## CA1 CA2 CA3 CA4 CA5 CA6 ## Callvulg 0.0303167 -1.597460 0.11455 -2.894569 0.1376073 2.291129 ## Empenigr 0.0751030 0.379305 0.39303 0.023675 0.8568729 -0.400964 ## Rhodtome 1.1052309 1.499299 3.04284 0.120106 3.2324306 -0.283510 ## Vaccmyrt 1.4614812 1.622935 2.72375 0.231688 0.4604556 0.712538 ## Vaccviti 0.1468014 0.313436 0.14696 0.243505 0.6868371 -0.147815 ## Pinusylv -0.4820096 0.588517 -0.36020 -0.127094 0.4064754 0.386604 ## Descflex 1.5348239 1.218806 1.87562 -0.001340 -1.3136979 -0.070731 ## Betupube 0.6694503 1.951826 3.84017 1.389423 7.5959115 -0.244478 ## Vacculig -0.0830789 -1.629259 1.05063 0.802648 -0.3058811 -1.625341 ## Diphcomp -0.5446464 -1.037570 0.52282 0.940275 0.3682126 -1.082929 ## Dicrsp 1.8120408 0.360290 -4.92082 3.088562 1.3867372 0.157815 ## Dicrfusc 1.2704743 -0.562978 -0.39718 -2.929542 0.3848272 -2.408710 ## Dicrpoly 0.7248118 1.409347 0.80341 1.915549 4.5674148 1.295447 ## Hylosple 2.0062408 1.743883 2.27549 0.928884 -3.7648428 2.254851 ## Pleuschr 1.3102086 0.583036 -0.01004 0.137298 -1.1216144 0.200422 ## Polypili -0.3805097 -1.243904 0.54593 1.477188 -0.7276341 -0.387641 ## Polyjuni 1.0133795 0.099043 -2.24697 1.510641 0.7729714 -3.062378 ## Polycomm 0.8468241 1.321773 1.13585 1.140723 2.6836594 -0.605038 ## Pohlnuta -0.0136453 0.589290 -0.35542 0.135481 0.9369707 0.397246 ## Ptilcili 0.4223631 1.598584 3.43474 1.400065 6.3209491 0.198935 ## Barbhatc 0.5018348 2.119334 4.57303 1.693188 8.1101807 0.645995 ## Cladarbu -0.1531729 -1.483884 0.20024 0.193680 0.0734141 0.358926 ## Cladrang -0.5502561 -1.084008 0.40552 0.724060 -0.3357992 -0.335924 ## Cladstel -1.4373146 1.077753 -0.44397 -0.375926 -0.2421525 0.004212 ## Cladunci 0.8151727 -1.006186 -1.82587 -1.389523 1.6046713 3.675908 ## Cladcocc -0.2133215 -0.584429 -0.21434 -0.567886 -0.0003788 -0.145303 ## Cladcorn 0.2631227 -0.177858 -0.44464 0.272422 0.3992282 -0.306738 ## Cladgrac 0.1956947 -0.311167 -0.23894 0.379013 0.4933026 0.037581 ## Cladfimb 0.0009213 -0.161418 0.18463 -0.435908 0.4831233 -0.143751 ## Cladcris 0.3373031 -0.470369 -0.05093 -0.823855 0.7182250 0.636140 ## Cladchlo -0.6200021 1.207278 0.21889 0.426447 1.9506082 0.120722 ## Cladbotr 0.5647242 1.047333 2.65330 0.907734 4.4946805 1.201655 ## Cladamau -0.6598144 -1.512880 0.83251 1.577699 -0.0407227 -1.419139 ## Cladsp -0.8209003 0.476164 -0.49752 -0.998241 -0.2393208 0.390785 ## Cetreric 0.2458192 -0.689228 -1.68427 -0.131681 0.7439412 2.374535 ## Cetrisla -0.3465221 1.362693 0.85897 0.396752 2.7526968 0.396591 ## Flavniva -1.4391907 -0.833589 -0.12919 0.007071 -1.4841375 2.956977 ## Nepharct 1.6813309 0.199484 -4.33509 2.229917 0.9561223 -5.472858 ## Stersp -0.5172793 -2.280900 0.99775 2.377013 -0.8892757 -1.441228 ## Peltapht 0.4035858 -0.043265 0.04538 0.711040 0.1824679 -0.841227 ## Icmaeric 0.0378754 -2.419595 0.72135 0.361302 -0.3736424 -2.092136 ## Cladcerv -0.9232858 -0.005233 -1.22058 0.305290 -0.8142627 0.414135 ## Claddefo 0.5190399 -0.496632 -0.15271 -0.695927 0.9042143 0.909191 ## Cladphyl -1.2836161 1.155872 -0.79912 -0.741170 -0.1608002 0.490526 ## ## ## Site scores (weighted averages of species scores) ## ## CA1 CA2 CA3 CA4 CA5 CA6 ## sit1 -0.108122 -0.53705 0.229574 0.24412 0.1405624 -0.14253 ## sit2 0.697118 -0.14441 -0.031788 -0.21743 -0.2738522 -0.08146 ## sit3 0.987603 0.15042 -1.348447 0.80472 0.3095168 0.46773 ## sit4 0.851765 0.49901 0.443559 0.12277 -0.4814871 0.07589 ## sit5 0.359881 -0.05608 0.145813 0.15087 0.2405263 -0.17770 ## sit6 0.003545 0.37017 0.027760 0.06168 -0.1158930 -0.03413 ## sit7 0.860732 -0.11504 0.110869 -1.02169 0.0772348 -0.60530 ## sit8 0.636936 -0.33250 0.001120 -0.79797 0.0130769 -0.54049 ## sit9 1.279352 0.81557 0.670053 0.23137 -0.8929976 0.41783 ## sit10 -0.195009 -0.80564 0.117686 -0.58286 -0.0007212 0.53071 ## sit11 0.528532 -0.70420 -0.517771 -0.86836 0.5713441 0.91671 ## sit12 0.382866 -0.18686 -0.004789 0.10156 0.0458125 0.21087 ## sit13 0.990715 0.11967 -1.110040 0.44929 0.1885902 -0.70694 ## sit14 -0.264704 -1.06013 0.334900 0.45973 -0.0326631 -0.19945 ## sit15 -0.428410 -1.20765 0.374344 0.74970 -0.2596294 -0.30467 ## sit16 -0.330534 -0.77498 0.130760 0.22391 0.0632686 0.09060 ## sit17 -0.899601 0.12075 -0.075742 0.03842 -0.1489585 -0.12031 ## sit18 -0.770294 -0.35351 -0.033779 -0.01795 -0.3007839 0.44303 ## sit19 -0.992193 0.50319 -0.157505 -0.07070 -0.1065172 -0.09928 ## sit20 -0.937173 0.78688 -0.258119 -0.19377 -0.0343535 -0.01259 ## sit21 -0.726413 0.49163 -0.157235 -0.08698 -0.0105774 -0.02801 ## sit22 -1.002083 0.71239 -0.236526 -0.18643 -0.0231666 -0.04928 ## sit23 -0.322647 -0.03871 -0.001297 0.09029 -0.1481448 0.06934 ## sit24 0.259527 0.80746 1.124258 0.36083 1.5437866 0.07051 varespec_eigenval &lt;- eigenvals(vare_cca, scaling = 1) prop_expl &lt;- varespec_eigenval / sum(varespec_eigenval) par(mfrow = c(1, 2)) plot(x = 1:length(varespec_eigenval), y = vare_cca$CA$eig, type = &quot;b&quot;, xlab = &quot;Rang de la valeur propre&quot;, ylab = &quot;Valeur propre&quot;) abline(h = mean(varespec_eigenval), col = &quot;red&quot;, lty = 2) plot(x = 1:length(varespec_eigenval), y = prop_expl, type = &quot;b&quot;, xlab = &quot;Rang de la valeur propre&quot;, ylab = &quot;Valeur propre&quot;) lines(x = 1:length(varespec_eigenval), y = broken_stick(length(varespec_eigenval)), col = &quot;red&quot;, lty = 2) Cr√©ons les biplots. par(mfrow = c(1, 2)) plot(vare_cca, scaling = 1, main = &quot;Biplot des esp√®ces&quot;) plot(vare_cca, scaling = 2, main = &quot;Biplot des sites&quot;) Le biplot des esp√®ces, √† gauche (scaling = 1), montre la distribution des sites selon les esp√®ces. Les emplacements des scores (en noir) montrent les contrastes entre sites selon les esp√®ces qui les recouvrent. Les sites 14 et 15, par exemple, contrastent les sites 19, 20, 21 et 22 selon le 2i√®me axe principal. Par ailleurs, les axes principaux sont form√© de plusieurs esp√®ces dont aucune ne domine clairement. Le biplot des sites, √† droite (scaling = 2), montre la distribution des recouvrements d‚Äôesp√®ces selon les sites. Par exemple, les esp√®ces Betupube (Betula pubescens) et Barbhatc (Barbilophozia hatcheri ) se recouvrent en particulier le site 24. Le site 1 est difficile √† identifier, car il est couvert par plusieurs noms d‚Äôesp√®ces, au bas au centre. Les sites 3 et 13 se confondent avec Dicrsp (une esp√®ce de Dicranum) qui le recouvre amplement. Pour les deux types de biplot, les sites o√π les esp√®ces situ√©s pr√®s de l‚Äôorigine, car ils peuvent √™tre soit pr√®s de la moyenne, soit distribu√©s uniform√©ment. Le nombre de composantes √† retenir peut √™tre √©valu√© par les approches Kaiser-Guttmann et broken-stick. scaling &lt;- 1 varespec_eigenval &lt;- eigenvals(vare_cca, scaling = scaling) # peut √™tre effectu√© sur les deux types de scaling prop_expl &lt;- varespec_eigenval / sum(varespec_eigenval) par(mfrow = c(1, 2)) plot(x = 1:length(varespec_eigenval), y = vare_cca$CA$eig, type = &quot;b&quot;, xlab = &quot;Rang de la valeur propre&quot;, ylab = &quot;Valeur propre&quot;, main = paste(&quot;Eigenvalue - Kaiser-Guttmann, scaling =&quot;, scaling)) abline(h = mean(varespec_eigenval), col = &quot;red&quot;, lty = 2) plot(x = 1:length(varespec_eigenval), y = prop_expl, type = &quot;b&quot;, xlab = &quot;Rang de la valeur propre&quot;, ylab = &quot;Valeur propre&quot;, main = paste(&quot;Proportion - broken stick, scaling =&quot;, scaling)) lines(x = 1:length(varespec_eigenval), y = broken_stick(length(varespec_eigenval)), col = &quot;red&quot;, lty = 2) Pour les deux scalings, l‚Äôapproche Kaiser-Guttmann propose 7 axes, tandis que l‚Äôapproche broken-stick en propose 5. Les repr√©sentations biplot d‚Äôanalyse de correspondance peuvent prendre la forme d‚Äôun boomerang, en particulier celles qui sont bas√©es sur des donn√©es d‚Äôoccurence. Le tableau suivant initialement de Chessel et al. (1987) et est distribu√© dans le module ade4. library(&quot;ade4&quot;) data(&quot;doubs&quot;) fish &lt;- doubs$fish doubs_cca &lt;- cca(fish %&gt;% filter(rowSums(.) &gt; 0)) plot(doubs_cca, scaling = 2) Les num√©ros de sites correspondent √† la position dans une rivi√®re, 1 √©tant en amont et 30 en aval. Le premier axe discrimine l‚Äôamont et l‚Äôaval, tandis que le deuxi√®me montre deux niches en amont. Bien que l‚Äôon observe une discontinuit√© dans le cours d‚Äôeau, il y a une continuit√© dans les abondances. Cet effet peut √™tre corrig√© en retirant la tendance de l‚Äôanalyse de correspondance par une detrended correspondance analysis. Pour cela, il faudra utiliser la fonction decorana, ce qui ne sera pas couvert ici. L‚Äôanalyse des correspondances multiples (ACM) est utile pour l‚Äôordination des donn√©es cat√©gorielles. Le module ade4 est en mesure d‚Äôeffectuer des AMC, mais n‚Äôest pas couvert dans ce manuel. Excercice. Effectuez et analysez une AC avec les donn√©es de recouvrement varespec. 9.4.1.3 Positionnement multidimensionnel (PoMd) Le positionnement multidimensionnel (PoMd), ou manifold analysis, se base sur les assiciations entre les objets (mode Q) ou les variables (mode R) pour en r√©duire les dimensions. Alors que l‚Äôanalyse en composantes principales conserve la distance euclidienne et que l‚Äôanalyse de correspondance conserve la distance du \\(\\chi^2\\), le PoMd conserve l‚Äôassociation que vous s√©lectionnerez √† votre convenance. Le PoMd vise √† repr√©senter en un nombre limit√© de dimensions (souvent 2) la distance (ou dissimilarit√©) qu‚Äôont les objets (ou des variables) les uns par rapport aux autres dans l‚Äôespace multidimensionnel. Il existe deux types d‚ÄôAEM. Le PoMd-m√©trique (metric multidimentional scaling MMDS, parfois le metric est retir√©, MDS, et parfois l‚Äôon parle de classic MDS) vise √† repr√©senter fid√®lement la distance entre les objets ou les variables. Le PoMd-m√©trique ne devrait √™tre utilis√©e que lorsque la m√©trique n‚Äôest ni euclidienne, ni de \\(\\chi^2\\) et que l‚Äôon d√©sire pr√©server les distances entre les objets. L‚ÄôPoMd-m√©trique aussi appel√©e analyse en coordonn√©es principales (ACoP ou de l‚Äôanglais PCoA) . Le PoMd-non-m√©trique (nonmetric multidimentional scaling, NMDS) vise quant √† lui √† repr√©senter l‚Äôordre des distances entre les objets ou les variables. C‚Äôest une approche par rang: le PoMd-non-m√©trique vise repr√©senter les objets sont plus proches ou plus √©loign√©es les uns des autres plut√¥t que de repr√©senter leur similarit√© dans l‚Äôespace multidimentionnelle. L‚ÄôIsoMap, pour isometric feature mapping, est une extension du PoMd qui recontruit les distances selon les points retrouv√©s dans le voisinage. Les isomaps sont en mesure d‚Äôapplatir des donn√©es ayant des formes complexes. Nous ne traitons pour l‚Äôinstant que de l‚ÄôPoMd-m√©trique (fonction vegan::cmdscale) et des PoMd-non-m√©trique (fonction vegan::metaMDS). 9.4.1.3.1 Application Utilisons les donn√©es d‚Äôabondance que nous avions au tout d√©but de ce chapitre. La matrice d‚Äôassociation de Bray-Curtis sera utilis√©e. assoc_mat &lt;- vegdist(abundance, method = &quot;bray&quot;) pheatmap(assoc_mat %&gt;% as.matrix(), cluster_rows = FALSE, cluster_cols = FALSE, display_numbers = round(assoc_mat %&gt;% as.matrix(), 2)) Les sites 2 et 3 devraient √™tre plus pr√®s l‚Äôun et l‚Äôautre, puis les sites 3 et 4. Les autres associations sont √©loign√©s d‚Äôenviron la m√™me distance. Lan√ßons le calcul de la PoMd-m√©trique. pcoa &lt;- cmdscale(assoc_mat, k = nrow(abundance)-1, eig = TRUE) spec_scores &lt;- wascores(pcoa$points, abundance) ordiplot(vegan::scores(pcoa), type = &#39;t&#39;, cex = 1.5) ## species scores not available text(spec_scores, row.names(spec_scores), col = &quot;red&quot;, cex = 0.75) On observe en effet que les sites 2 et 3 sont les plus pr√®s. Les sites 3 et 4sont plus √©loign√©s. Les sites 1, 2 et 4 font √† peu pr√®s un triangle √©quilat√©ral, ce qui correspond √† ce √† quoi on devrait s‚Äôattendre. Les wa-scores permettent de juxtaposer les esp√®ces sur les sites, pour r√©f√©rence. Le colibri n‚Äôest pr√©sent que sur le site 2. Le site 1 est popul√© par des jaseurs et des m√©sanges, et c‚Äôest le seul site o√π l‚Äôon a observ√© une citelle. On a observ√© des chardonnerets sur les sites 2 et 3. Sur le site 4, on n‚Äôa observ√© que des bruants, que l‚Äôon a aussi observ√© ailleurs, sauf au site 2. Le PoMd-non-m√©trique (non metric dimensional scaling, NMDS) fonctionne de la m√™me mani√®re que la PoMd-m√©trique, √† la diff√©rence que la distance est bas√©e sur les rangs. √Ä cet √©gard, le site 4 √† une distance de 0.76 du site 3, mais plut√¥t le deuxi√®me plus loin, apr√®s le site 2 et avant le site 1. Utilisons la fonction metaMDS. nmds &lt;- metaMDS(assoc_mat, k = nrow(abundance)-1, eig = TRUE) ## Run 0 stress 0 ## Run 1 stress 0 ## ... Procrustes: rmse 0.1840192 max resid 0.237726 ## Run 2 stress 0 ## ... Procrustes: rmse 0.1037867 max resid 0.1393279 ## Run 3 stress 0 ## ... Procrustes: rmse 0.1222791 max resid 0.155878 ## Run 4 stress 0 ## ... Procrustes: rmse 0.05430327 max resid 0.07882607 ## Run 5 stress 4.799773e-05 ## ... Procrustes: rmse 0.0724374 max resid 0.1057829 ## Run 6 stress 0 ## ... Procrustes: rmse 0.1693126 max resid 0.2344739 ## Run 7 stress 0 ## ... Procrustes: rmse 0.110785 max resid 0.1641954 ## Run 8 stress 0 ## ... Procrustes: rmse 0.09150849 max resid 0.1184164 ## Run 9 stress 0 ## ... Procrustes: rmse 0.1395212 max resid 0.1750131 ## Run 10 stress 8.061783e-05 ## ... Procrustes: rmse 0.08985349 max resid 0.1309147 ## Run 11 stress 0 ## ... Procrustes: rmse 0.1424352 max resid 0.1911444 ## Run 12 stress 0 ## ... Procrustes: rmse 0.1504805 max resid 0.199445 ## Run 13 stress 0 ## ... Procrustes: rmse 0.02144744 max resid 0.03110818 ## Run 14 stress 0 ## ... Procrustes: rmse 0.1068508 max resid 0.1444595 ## Run 15 stress 0 ## ... Procrustes: rmse 0.1115497 max resid 0.1475763 ## Run 16 stress 0 ## ... Procrustes: rmse 0.1098886 max resid 0.1577025 ## Run 17 stress 0 ## ... Procrustes: rmse 0.1132777 max resid 0.1349138 ## Run 18 stress 0 ## ... Procrustes: rmse 0.1441108 max resid 0.1649673 ## Run 19 stress 0 ## ... Procrustes: rmse 0.1179159 max resid 0.1707919 ## Run 20 stress 0 ## ... Procrustes: rmse 0.1442947 max resid 0.1927183 ## *** No convergence -- monoMDS stopping criteria: ## 20: stress &lt; smin ## Warning in metaMDS(assoc_mat, k = nrow(abundance) - 1, eig = TRUE): stress is ## (nearly) zero: you may have insufficient data spec_scores &lt;- wascores(nmds$points, abundance) ordiplot(vegan::scores(nmds), type = &#39;t&#39;, cex = 1.5) ## species scores not available text(spec_scores, row.names(spec_scores), col = &quot;red&quot;, cex = 0.75) Dans ce cas, entre PoMd-m√©trique et non-m√©trique, les r√©sultats peuvent √™tre interpr√©t√©s de mani√®re similaire. En ce qui a trait au dauphin, Pour plus de d√©tails, je vous invite √† vous r√©f√©rer √† Borcard et al. (2011)) ou de consulter l‚Äôexcellent site GUSTA ME. 9.4.1.4 Conclusion sur l‚Äôordination non contraignante Lorsque les donn√©es sont euclidiennes, l‚Äôanalyse en composantes principales (ACP) dervait √™tre utilis√©e. Lorsque la m√©trique est celle du \\(\\chi^2\\), on pr√©f√©rera l‚Äôanalyse de correspondance (AC). Si la m√©trique est autre, le positionnement multidimensionel (PoMd) est pr√©f√©rable. Dans ce dernier cas, si l‚Äôon recherche une repr√©sentation simplifi√©e de la distance entre les objets ou variables, on utilisera un PoMd-m√©trique. √Ä l‚Äôinverse, si l‚Äôon d√©sire une repr√©sentation plus fid√®le au rang des distances, on pr√©f√©rera l‚ÄôPoMd-non-m√©trique. 9.4.2 Ordination contraignante Alors que l‚Äôordination non contraignante vous permet de dresser un protrait de vos variables, l‚Äôordination contraignante (ou canonique) permet de tester statistiquement ainsi que de repr√©senter la relation entre plusieurs variables explicatives (par exemple, des conditions environnementales) et une ou plusieurs variables r√©ponses (par exemple, les esp√®ces observ√©es). L‚Äôanalyse discriminante n‚Äôa fondamentalement qu‚Äôune seulement variable r√©ponse, et celle-ci doit d√©crire l‚Äôappartenance √† une cat√©gorie. L‚Äôanalyse de redondance sera pr√©f√©r√©e lorsque le nombre de variable est plus restreint (variables ionomiques et indicateurs de performance des cultures). Les d√©tails, ainsi que les tenants et aboutissants de ces m√©thodes, sont pr√©sent√©s dans Numerical Ecology (Legendre et Legendre, 2012). L‚Äôanalyse canonique des corr√©lations sera pr√©f√©r√©e lorsque les variables sont parsem√©es (beaucoup de colonnes avec beaucoup de z√©ros, comme les variables d‚Äôabondance). 9.4.2.1 Analyse discriminante Alors que l‚Äôanalyse en composante principale vise √† pr√©senter la perspective (les axes) selon laquelle les points sont les plus √©clat√©es, l‚Äôanalyse discriminante, le plus souvent utilis√© dans sa forme lin√©aire (ADL) et quadratique (ADQ), vise √† pr√©senter la perspective selon laquelle les groupes sont les plus √©clat√©s, les groupes formant la variable contraignante. Ces groupes peuvent √™tre connus (e.g. cultivar, r√©gion g√©ographique) ou attribu√©s (exemple: par partitionnement). L‚ÄôADL est parfois nomm√©e analyse canonique de la variance. L‚ÄôAD vise √† repr√©senter des diff√©rences entre des groupes aux moyens de combinaisons lin√©aires (ADL) ou quadratique (ADQ) de variables mesur√©es. Sa repr√©sentation sous forme de biplot permet d‚Äôappr√©cier les diff√©rences entre les groupes d‚Äôidentifier les variables qui sont responsables de la discrimination. Biplot de distance de l‚Äôanalyse discriminante des ionomes d‚Äôesp√®ces de plantes √† fruits cultiv√©es sauvages et domestiqu√©es, Source: Parent et al. (2013) L‚ÄôADL a √©t√© d√©velopp√©e par Fisher (1936), qui √† titre d‚Äôexemple d‚Äôapplication a utilis√© un jeu de donn√©es de dimensions d‚Äôiris collect√©es par Edgar Anderson, du Jardin botanique du Missouri, sur 150 sp√©cimens d‚Äôiris collect√©s en Gasp√©sie (Est du Qu√©bec), ma r√©gion natale (suis-je assez chauvin?). Ce jeu de donn√©es est amplement utilis√© √† titre d‚Äôexemple en analyse multivari√©e. Williams (1983) a pr√©sent√© les tenants et aboutissants de l‚ÄôADL en √©cologie. Tout comme les donn√©es passant pas une ACP doivent suivre une distribution multinormale pour √™tre statistiquement valide, les distributions des groupes dans une ADL doivent √™tre multinormales et les variances des points par groupe doivent √™tre homog√®nes‚Ä¶ ce qui est rarement le cas en science. N√©anmoins: Heureusement, il y a des √©vidences dans la litt√©rature que certaines d‚Äôentre [ces r√®gles] peuvent √™tre transgress√©es mod√©r√©ment sans de grands changement dans les taux de classification. Cette conclusion d√©pends, toutefois, de la s√©v√©rit√© des transgressions, et de facteurs structueaux comme la position relative des moyennes des populations et de la nature des dispersions. - Williams (1983) L‚ÄôADL peut servir autant d‚Äôoutil d‚Äôinterpr√©tation que d‚Äôoutil de classification, c‚Äôest √† dire de pr√©dire une cat√©gorie selon les variables (chapitre 12). Dans les deux cas, lorsque le nombre de variables approchent le nombre d‚Äôobservation, les r√©sultats d‚Äôune ADL risque d‚Äô√™tre difficilement interpr√©tables. Le test appropri√© pour √©valuer l‚Äôhomod√©n√©it√© de la covariance est le M-test de Box. Ce test est peu document√© dans la litt√©rature, est rarement utilis√© mais a la r√©putation d‚Äô√™tre particuli√®rement s√©v√®re. Il est rare que des donn√©es √©cologiques aient des dispersions (covariances) homog√®nes. Contrairement √† l‚ÄôADL, l‚ÄôADQ ne demande pas √† ce que les dispersions (covariances) soient homog√®nes. N√©anmoins, l‚ÄôADQ ne g√©n√®re ni de scores, ni de loadings: il s‚Äôagit d‚Äôun outil pour pr√©dire des cat√©gories (classification), non pas d‚Äôun outil d‚Äôordination. 9.4.2.1.1 Application Utilisons les donn√©es d‚Äôiris. data(&quot;iris&quot;) Testons la multinormalit√© par groupe. Rappelons-nous que pour consid√©rer la distribution comme multinormale, la p-value de la distortion ainsi que la statistique de Kurtosis doivent √™tre √©gale ou plus √©lev√©e que 0.05. La fonction split s√©pare le tableau en listes et la fonction map applique la fonction sp√©cifi√©e √† chaque √©l√©ment de la liste. Cela permet d‚Äôeffectuer des tests de multinormalit√© sur chacune des esp√®ces d‚Äôiris. iris %&gt;% split(.$Species) %&gt;% map(~ mvn(.x %&gt;% select(-Species), mvnTest = &quot;mardia&quot;)$multivariateNormality) ## $setosa ## Test Statistic p value Result ## 1 Mardia Skewness 25.6643445196298 0.177185884467652 YES ## 2 Mardia Kurtosis 1.29499223711605 0.195322907441935 YES ## 3 MVN &lt;NA&gt; &lt;NA&gt; YES ## ## $versicolor ## Test Statistic p value Result ## 1 Mardia Skewness 25.1850115362466 0.194444483140265 YES ## 2 Mardia Kurtosis -0.57186635893429 0.567412516528727 YES ## 3 MVN &lt;NA&gt; &lt;NA&gt; YES ## ## $virginica ## Test Statistic p value Result ## 1 Mardia Skewness 26.2705981752915 0.157059707690356 YES ## 2 Mardia Kurtosis 0.152614173978342 0.878702546726567 YES ## 3 MVN &lt;NA&gt; &lt;NA&gt; YES Le test est pass√© pour toutes les esp√®ces. Voyons maintenant l‚Äôhomog√©n√©it√© de la covariance. Pour ce faire, nous aurons besoin de la fonction boxM, disponible avec le module biotools. Pour que les covariances soient consid√©r√©es comme √©gales, la p-vaule doit √™tre sup√©rieure √† 0.05. library(&quot;heplots&quot;) boxM(iris %&gt;% select(-Species), group = iris$Species) ## ## Box&#39;s M-test for Homogeneity of Covariance Matrices ## ## data: iris %&gt;% select(-Species) ## Chi-Sq (approx.) = 140.94, df = 20, p-value &lt; 2.2e-16 On est loin d‚Äôun cas o√π les distributions sont homog√®nes. Nous allons n√©anmoins proc√©der √† l‚Äôanalyse discriminante avec le module ade4. Nous aurons d‚Äôabord besoin d‚Äôeffectuer une ACP avec la fonction dudi.pca de ade4 (en sp√©cifiant une mise √† l‚Äô√©chelle), que nous projeterons en ADL avec discrimin. library(&quot;ade4&quot;) iris_pca &lt;- dudi.pca(df = iris %&gt;% select(-Species), scannf = FALSE, # ne pas g√©n√©rer de graphique scale = TRUE) iris_lda &lt;- discrimin(dudi = iris_pca, fac = iris$Species, scannf = FALSE) La visualisation peut √™tre effectu√©e directement sur l‚Äôobjet issu de la fonction discrimin. plot(iris_lda) Il s‚Äôagit toutefois d‚Äôune visualisation pour le diagnostic davantage que pour la publication. Si l‚Äôobjectif est la pubilcation, vous pourriez utiliser la fonction plotDA que j‚Äôai con√ßue √† cet effet. J‚Äôai aussi con√ßu une fonction similaire qui utilise le module graphique de base de R. source(&quot;https://raw.githubusercontent.com/essicolo/AgFun/master/plotDA_gg.R&quot;) plotDA(scores = iris_lda$li, loadings = iris_lda$fa, fac = iris$Species, level=0.95, facname = &quot;Species&quot;, propLoadings = 1) ## Loading required package: ellipse ## ## Attaching package: &#39;ellipse&#39; ## The following object is masked from &#39;package:car&#39;: ## ## ellipse ## The following object is masked from &#39;package:graphics&#39;: ## ## pairs ## Loading required package: grid ## Loading required package: plyr ## ------------------------------------------------------------------------------ ## You have loaded plyr after dplyr - this is likely to cause problems. ## If you need functions from both plyr and dplyr, please load plyr first, then dplyr: ## library(plyr); library(dplyr) ## ------------------------------------------------------------------------------ ## ## Attaching package: &#39;plyr&#39; ## The following objects are masked from &#39;package:dplyr&#39;: ## ## arrange, count, desc, failwith, id, mutate, rename, summarise, ## summarize ## The following object is masked from &#39;package:purrr&#39;: ## ## compact √Ä la diff√©rence de l‚ÄôACP, l‚ÄôADL maximise la s√©patation des groupes. Nous avions not√© avec l‚ÄôACP que les dimensions des p√©tales distingaient les groupes. Puisque nous avions justement des informations sur les groupes, nous aurions pu proc√©der directement √† un ADL pour obtenir des conclusions plus directes. Si la longueur des p√©tales permet de distinguer l‚Äôesp√®ce setosa des deux autres, la largeur des p√©tales permet de distinguer virginica et versicolor, bien que les nuages de points se superposent. De mani√®re bivari√©e, les r√©gions de confiance des moyennes des scores discriminants (petites ellipses) montrent des diff√©rence significatives au seuil 0.05. Excercice. Si l‚Äôon effectuait l‚ÄôADL sur notre dauphin, avec la colonne anatomy comme variable de regroupement, qu‚Äôobtiendrions-nous? Si l‚Äôon consi√®re la nageoire codale (queue) comme faisant partie du corps? Quelles sont les limitations? 9.4.2.2 Analyse de redondance (RDA) En anglais, on la nomme redundancy analysis, souvent abr√©g√©e RDA. Elle est utilis√©e pour r√©sumer les relations lin√©aires entre des variables r√©ponse et des variables explicatives. La ‚Äúredondance‚Äù se situe dans l‚Äôutilisation de deux tableaux de donn√©es contenant de l‚Äôinformation concordante. L‚Äôanalyse de redondance est une mani√®re √©l√©gante d‚Äôeffectuer une r√©gresssion lin√©aire multiple, o√π la matrice de valeurs pr√©dites par la r√©gression est assujettie √† une analyse en composantes principales. Il est ainsi possible de superposer les scores des variables explicatives √† ceux des variables r√©ponse. Plus pr√©cis√©ment, une RDA effectue les √©tapes suivantes (Borcard et al. (2011)) entre une matrice de variables ind√©pendantes (explicatives) \\(X\\) et une matrice de variables d√©pendantes (r√©ponse) \\(Y\\). 9.4.2.2.1 1. R√©gression entre \\(Y\\) et \\(X\\) Pour chacune des variables r√©ponse de \\(Y\\) (\\(y_1\\), \\(y_2\\), , \\(y_j\\)), effectuer une r√©gression lin√©aire sur les variables explicatives \\(X\\). \\[\\hat{y}_j = b_j + m_{1, j} \\times x_1 + m_{2, j} \\times x_2 + ... + m_{i, j} \\times x_i\\] \\[\\hat{y}_j = y_j + y_{res, j}\\] Pour chaque observation (\\(n\\)), nous obtenons une s√©rie de valeurs de \\(\\hat{y}_j\\) et de \\(y_{res, j}\\). Donc chaque cellule de la matrice \\(Y\\) a ses pendant \\(\\hat{y}\\) et \\(y_{res}\\). Nous obtenons ainsi une matrice de pr√©diction \\(\\hat{Y}\\) et une matrice des r√©sidus \\(Y_{res} = Y - \\hat{Y}\\). 9.4.2.2.2 2. Analyse en composantes principales Ensuite, on effectue une analyse en composantes principales (ACP) sur la matrice des pr√©dictions \\(\\hat{Y}\\). On obtient ainsi ses valeurs et vecteurs propres. Nommons \\(U\\) ses vecteurs propres. Les fonctions de RDA mettent souvent ces veceturs √† l‚Äô√©chelle avant de les retourner √† l‚Äôutilisateur. En ordination √©cologique, ces vecteurs mis √† l‚Äô√©chelle sont souvent appel√©s les scores des esp√®ces, bien qu‚Äôil ne s‚Äôagisse pas n√©cessairement d‚Äôesp√®ces, mais plus g√©n√©ralement des variables de la matrice d√©pendante \\(Y\\). Il est aussi possible d‚Äôeffectuer une ACP sur \\(Y_{res}\\). 9.4.2.2.3 3. Calculer les scores Les vecteurs propres \\(U\\) sont utilis√©s pour calculer les scores des sites, \\(Y \\times U\\), ainsi que les contraintes de site \\(\\hat{Y} \\times U\\). 9.4.2.2.4 Application Nous allons utiliser la fonction rda du module vegan. En ce qui a trait aux donn√©es, utilisons les donn√©es varespec (matrice Y) et varechem (matrice X). La fonction rda peut fonctionner avec l‚Äôinterface-formule de R, o√π √† gauche du ~ on retrouve le Y (la matrice de la communaut√© √©cologique, i.e. les abondances d‚Äôesp√®ces) contre le X (l), √† gauche, ce qui peut √™tre pratique pour l‚Äôanalyse d‚Äôint√©ractions. Mais pour comparer deux matrices, nous pouvons d√©finir X et Y. Ce qui est m√©langeant, c‚Äôest que vegan, contrairement aux conventions, d√©fini X comme √©tant la matrice r√©ponse et Y comme √©tant la matrice explicative. vare_rda &lt;- rda(X = varespec, Y = vareclr, scale = FALSE) par(mfrow = c(1, 2)) ordiplot(vare_rda, scaling = 1, type = &quot;text&quot;, main = &quot;Scaling 1: triplot de distance&quot;) ordiplot(vare_rda, scaling = 2, type = &quot;text&quot;, main = &quot;Scaling 2: triplot de corr√©lation&quot;) La fonction ordiplot permet de cr√©er un triplot de base. La repr√©sentation des wascores est r√©put√©e plus robuste (moins susceptible d‚Äô√™tre bruit√©e), mais leur interpr√©tation porte √† confusion (Borcard et al. (2011)). Triplot de distance (scaling 1). Les angles entre les variables explicatives repr√©sentent leur corr√©lation (non pas les variables r√©ponse). Triplot de corr√©lation (scaling 2). Les angles entre les variables repr√©sentent leurs corr√©lation, que les variables soient r√©ponse ou explicative, ou entre variables r√©ponses et variables explicatives. Les distances entre les objets sur le triplot ne sont pas des approximation de leur distance euclidienne. Les triplots montrent que les variables ont toutes un r√¥le important sur la dispersion des sites autours des axeds principaux. Le premier axe principal est compos√© de mani√®re plus marqu√©e par le clr de l‚ÄôAl et celui du Fe. Le deuxi√®me axe principal est compos√© de mani√®re plus marqu√©e par le clr du S, du P et du K. Le triplot de corr√©lation ne pr√©sente pas de tendance appr√©ciable pour la plupart des esp√®ces, qui ne poss√®dent pas de niche particuli√®re. Toutefois, l‚Äôesp√®ce Cladstel, pr√©sente surtout dans les sites 9 et 10, est li√©e √† de basses teneurs en N et √† de faibles valeurs de Baresoil (sol nu). L‚Äôesp√®ce Pleuschr est li√©e √† des sols o√π l‚Äôon retrouve une grande √©paisseur d‚Äôhumus, ainsi que des teneurs √©lev√©es en nutriment K, P, S, Ca, Mg et Zn. Elle semble appr√©cier les sols √† bas pH, mais √† faible teneur en Fe et Al. La teneur en N lui semble plus indiff√©rente (son vecteur √©tant presque perpendiculaire). On pourra personnaliser les graphiques en extrayant les scores. scaling &lt;- 2 sites &lt;- vegan::scores(vare_rda, display = &quot;wa&quot;, scaling = scaling) species &lt;- vegan::scores(vare_rda, display = &quot;species&quot;, scaling = scaling) env &lt;- vegan::scores(vare_rda, display = &quot;reg&quot;, scaling = scaling) plot(0, 0, type = &quot;n&quot;, xlim = c(-3, 5), ylim = c(-3, 4), asp = 1) abline(h=0, v = 0, col = &quot;grey80&quot;) text(sites/2, labels = rownames(sites), cex = 0.7, col = &quot;grey50&quot;) text(species/2, labels = rownames(species), col = &quot;green&quot;, cex = 0.7) segments(x0 = 0, y0 = 0, x = env[, 1], y = env[, 2], col = &quot;blue&quot;) text(env, labels = rownames(env), col = &quot;blue&quot;, cex = 1) On pourra effectuer une analyse de Kaiser-Guttmann ou de broken-stick de la m√™me mani√®re que pr√©c√©demment. √âtant une collection de r√©gressions, une RDA est en mesure d‚Äôeffectuer des tests statistiques sur les coefficients de la r√©gression en utilisant des permutations pour tester la signification des coefficients et des axes d‚Äôune RDA. On doit n√©anmoins obligatoirement effectuer la RDA avec l‚Äôinterface formule. L‚Äôa variable de gauche‚Äôobjet √† gauche du ~ peut √™tre une matrice ou un tableau, et celui de droite est d√©fini dans data. Le . dans l‚Äôinterface formule signifie ‚Äúune combinaison lin√©aire de toutes les variables, sans int√©raction‚Äù. vare_rda &lt;- rda(varespec ~ ., data = vareclr, scale = FALSE) perm_test_term &lt;- anova(vare_rda, by = &quot;term&quot;) #perm_test_axis &lt;- anova(vare_rda, by = &quot;axis&quot;) La signification des axes est difficile √† interpr√©ter. Toutefois, celui des variables pr√©sente un int√©r√™t. perm_test_term ## Permutation test for rda under reduced model ## Terms added sequentially (first to last) ## Permutation: free ## Number of permutations: 999 ## ## Model: rda(formula = varespec ~ N + P + K + Ca + Mg + S + Al + Fe + Mn + Zn + Mo + Fv + Baresoil + Humdepth + pH, data = vareclr, scale = FALSE) ## Df Variance F Pr(&gt;F) ## N 1 216.13 4.8470 0.010 ** ## P 1 272.71 6.1159 0.004 ** ## K 1 194.97 4.3724 0.014 * ## Ca 1 24.92 0.5589 0.664 ## Mg 1 52.61 1.1799 0.306 ## S 1 100.07 2.2441 0.112 ## Al 1 177.91 3.9900 0.021 * ## Fe 1 118.59 2.6595 0.059 . ## Mn 1 25.96 0.5822 0.638 ## Zn 1 35.81 0.8030 0.514 ## Mo 1 23.51 0.5273 0.698 ## Baresoil 1 98.64 2.2122 0.104 ## Humdepth 1 43.59 0.9777 0.405 ## pH 1 38.93 0.8730 0.435 ## Residual 9 401.31 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 La p-value est la probabilit√© que les pentes calcul√©es pour les variables √©mergent de distributions dont la moyenne est nulle. Au seuil 0.05, les variables significatives sont (les clr de) l‚Äôazote, le phosphore, le potassium et l‚Äôaluminium. Dans le cas des matrices d‚Äôabondance (ce n‚Äôest pas le cas de varespec, constitu√©e de donn√©es de recouvrement), il est pr√©f√©rable avec les RDA de les transformer pr√©alablement avec la transformation compositionnelle, de chord ou de Hellinger (chapitre 8). Une autre option est d‚Äôeffectuer une RDA sur des matrices d‚Äôassociation en passant par une analyse en coordonn√©es principales (Legendre et Anderson, 1999). Enfin, les donn√©es d‚Äôabondance √† l‚Äô√©tat brutes devraient plut√¥t passer utiliser une analyse canonique des corr√©lations. 9.4.2.3 Analyse canonique des correspondances (ACC) L‚Äôanalyse canonique des correspondances (Canonical correspondance analysis), ACC, a √©t√© √† l‚Äôorigine con√ßue pour √©tudier les liens entre des variables environnementales et l‚Äôabondance (d√©compte) ou l‚Äôoccurence (pr√©sence-absence) d‚Äôesp√®ces (ter Braak, 1986). L‚ÄôACC est √† la RDA ce que la CA est √† l‚ÄôACP. Alors que la RDA pr√©serve les distance euclidiennes entre variables d√©pendantes et indpendantes, l‚ÄôACC pr√©serve les distances du \\(\\chi^2\\). Tout comme l‚ÄôAC, elle h√©rite du coup une propri√©t√© importate de la distance du \\(\\chi^2\\): il y a davantage davantage d‚Äôimportance aux esp√®ces rares. L‚Äôanalyse des correspondances canoniques est souvent utilis√©e dans la litt√©rature, mais dans bien des cas une RDA sur des donn√©es d‚Äôabondance transform√©es donnera des r√©sultats davantage int√©rpr√©tables (Legendre et Gallagher, 2001). 9.4.2.3.1 Application Cet exemple d‚Äôapplication concerne des donn√©es d‚Äôabondance. Nous allons cons√©quemment utiliser une CCA avec la fonction cca, toujours avec le module vegan. Les tableaux doubs_fish et doubs_env comprennent respectivement des donn√©es d‚Äôabondance d‚Äôesp√®ces de poissons et dans diff√©rents environnements de la rivi√®re Doubs (Europe) publi√©es dans Verneaux. (1973) et export√©es du module ade4. data(&quot;doubs&quot;) doubs_fish &lt;- doubs$fish doubs_env &lt;- doubs$env Sur le site no 8, aucun poisson n‚Äôa pas √©t√© observ√©. Les observations ne comprenant que des z√©ro doivent √™tre pr√©alablement retir√©es. tot_spec &lt;- doubs_fish %&gt;% transmute(tot_spec = apply(., 1, sum)) doubs_fish &lt;- doubs_fish %&gt;% filter(tot_spec != 0) doubs_env &lt;- doubs_env %&gt;% filter(tot_spec != 0) De la m√™me mani√®re qu‚Äôavec la fonction rda de vegan, nous utilisons cca pour l‚ÄôACC. doubs_cca &lt;- cca(doubs_fish ~ ., data = doubs_env, scale = FALSE) Comparons les r√©sultats par(mfrow = c(1, 2)) ordiplot(doubs_cca, scaling = 1, type = &quot;text&quot;, main = &quot;CCA - Scaling 1 - Triplot de distance&quot;) ordiplot(doubs_cca, scaling = 2, type = &quot;text&quot;, main = &quot;CCA - Scaling 2 - Triplot de corr√©lation&quot;) Triplot de distance (scaling 1). La projection des variables r√©ponse √† angle droit sur les variables explicatives est une approximation de la r√©ponse sur l‚Äôexplication. (2) Un objet (site ou r√©ponse) situ√© pr√®s d‚Äôune variable explicative est plus susceptible d‚Äôavoir le d√©compte 1. (3) Les distances entre les variables (r√©ponse et explicatives) approximent la distance du \\(\\chi^2\\) (traduction adapt√©e de Borcard et al. (2011)). Triplot de corr√©lation (scaling 2). La valeur optmiale de l‚Äôesp√®ce sur une variable environnementale quantitative peut √™tre obtenue en projetant l‚Äôesp√®ce √† angle droit sur la variable. (2) Une esp√®ce se trouvant pr√®s d‚Äôune variable environnementale est susceptible de se trouver en plus grande abondance aux sites de statut 1 pour cette variable. (3) Les distances n‚Äôapproximent pas la distance du \\(\\chi^2\\) (traduction adapt√©e de Borcard et al. (2011)). "]
]
