[
["chapitre-biostats.html", "5 Biostatistiques 5.1 Populations et Ã©chantillons 5.2 Les variables 5.3 Les probabilitÃ©s 5.4 Les distributions 5.5 Statistiques descriptives 5.6 Tests dâ€™hypothÃ¨ses Ã  un et deux Ã©chantillons 5.7 Lâ€™analyse de variance 5.8 Les modÃ¨les statistiques", " 5 Biostatistiques ï¸Â Objectifs spÃ©cifiques: Ã€ la fin de ce chapitre, vous serez en mesure de dÃ©finir les concpets de base en statistique: population, Ã©chantillon, variable, probabilitÃ© et distribution serez en mesure de calculer des statistiques descriptives de base: moyenne et Ã©cart-type, quartiles, maximum et minimum comprendrez les notions de test dâ€™hypothÃ¨se, dâ€™effet et de p-value, ainsi quâ€™Ã©viter les erreurs communes dans leur interprÃ©tation saurez effectuer une modÃ©lisation statistique linÃ©aire simple, multiple et mixte, entre autre sur des catÃ©gories saurez effectuer une modÃ©lisation statistique non linÃ©aire simple, multiple et mixte Aux chapitres prÃ©cÃ©dents, nous avons vu comment visualiser, organiser et manipuler des tableaux de donnÃ©es. La statistique est une collection de disciplines liÃ©es Ã  la collecte, lâ€™organisation, lâ€™analyse, lâ€™interprÃ©tation et la prÃ©sentation de donnÃ©es. Les biostatistiques est lâ€™application de ces disciplines Ã  la biosphÃ¨re. Dans Principles and procedures of statistics: A biometrical approach, Steel, Torie et Dickey (1997) dÃ©finissent les statistiques ainsi: Les statistiques forment la science, pure et appliquÃ©e, de la crÃ©ation, du dÃ©veloppement, et de lâ€™application de techniques par lesquelles lâ€™incertitude de lâ€™induction infÃ©rentielle peut Ãªtre Ã©valuÃ©e. (ma traduction) Alors que lâ€™infÃ©rence consiste Ã  gÃ©nÃ©raliser des observations sur des Ã©chantillons Ã  lâ€™ensemble dâ€™une population, lâ€™induction est un type de raisonnement qui permet de gÃ©nÃ©raliser des observations en thÃ©ories. Les statistiques permettent dâ€™Ã©valuer lâ€™incertitude dÃ©coulant du processus qui permet dâ€™abord de passer de lâ€™Ã©chantillon Ã  la population reprÃ©sentÃ© par cet Ã©chantillon, puis de passer de cette reprÃ©sentation dâ€™une population en lois gÃ©nÃ©rales la concernant. La dÃ©finition de Whitlock et Schuluter (2015), dans The Analysis of Biological Data, est plus simple, insistant sur lâ€™infÃ©rence: La statistique est lâ€™Ã©tude des mÃ©thodes pour mesurer des aspects de populations Ã  partir dâ€™Ã©chantillons et pour quantifier lâ€™incertitude des mesures. (ma traduction) Les statistiques consistent Ã  faire du sens (anglicisme assumÃ©) avec des observations dans lâ€™objectif de rÃ©pondre Ã  une question que vous aurez formulÃ©e clairement, prÃ©alablement Ã  votre expÃ©rience. The more time I spend as The Statistician in the room, the more I think the best skill you can cultivate is the ability to remain calm and repeatedly ask â€œWhat question are you trying to answer?â€ â€” Bryan Howie (@bryan_howie) 13 dÃ©cembre 2018 Le flux de travail conventionnel consiste Ã  collecter des Ã©chantillons, transformer les donnÃ©es, effectuer des tests, analyser les rÃ©sultats, les interprÃ©ter et les visualiser. Bien que ces tÃ¢ches soient complexes, en particulier en ce qui a trait aux tests statistiques, la plupart des opÃ©rations statistiques peuvent Ãªtre effectuÃ©es sans lâ€™assistance de statisticien.ne.sâ€¦ Ã  condition de comprendre suffisamment les concepts utilisÃ©s. Ce chapitre Ã  lui seul est trop court pour permettre dâ€™intÃ©grer toutes les connaissances nÃ©cessaires Ã  une utilisation raisonnÃ©e des statistiques, mais fourni les bases pour aller plus loin. Notez que les erreurs dâ€™interprÃ©tation statistiques sont courantes et la consultation de spÃ©cialistes nâ€™est souvent pas un luxe. Dans ce chapitre, nous verrons comment rÃ©pondre correctement Ã  une question valide et adÃ©quate avec lâ€™aide dâ€™outils de calcul scientifique. Nous couvrirons les notions de bases des distributions et des variables alÃ©atoires qui nous permettront dâ€™effectuer des tests statistiques commun avec R. Nous couvrirons aussi les erreurs communÃ©ment commises en recherche acadÃ©mique et les moyens simples de les Ã©viter. Ce chapitre est une introduction aux statistiques avec R, et ne remplacera pas un bon cours de stats. En plus des modules de base de R nous utiliserons les modules de la tidyverse, le module de donnÃ©es agricoles agridat, ainsi que le module nlme spÃ©cialisÃ© pour la modÃ©lisation mixte. Avant de survoler les applications statistiques avec R, je vais dâ€™abord et rapidement prÃ©senter quelques notions importantes en statistiques: populations et Ã©chantillons, variables, probabilitÃ©s et distributions. Nous allons effectuer des tests dâ€™hypothÃ¨se univariÃ©s (notamment les tests de t et les analyses de variance) et dÃ©tailler la notion de p-value. Mais avant tout, je vais mâ€™attarder plus longuement aux modÃ¨les linÃ©aires gÃ©nÃ©ralisÃ©s, incluant en particulier des effets fixes et alÃ©atoires (modÃ¨les mixtes), qui fournissent une trousse dâ€™analyse polyvalente en analyse multivariÃ©e. Je terminerai avec les perspectives multivariÃ©s que sont les matrices de covariance et de corrÃ©lation. 5.1 Populations et Ã©chantillons Le principe dâ€™infÃ©rence consiste Ã  gÃ©nÃ©raliser des conclusions Ã  lâ€™Ã©chelle dâ€™une population Ã  partir dâ€™Ã©chantillons issus de cette population. Alors quâ€™une population contient tous les Ã©lÃ©ments Ã©tudiÃ©s, un Ã©chantillon dâ€™une population est une observation unique. Une expÃ©rience bien conÃ§ue fera en sorte que les Ã©chantillons sont reprÃ©sentatifs de la population qui, la plupart du temps, ne peut Ãªtre observÃ©e entiÃ¨rement pour des raisons pratiques. Les principes dâ€™expÃ©rimentation servant de base Ã  la conception dâ€™une bonne mÃ©thodologie sont prÃ©sentÃ©s dans le cours Dispositifs expÃ©rimentaux (BVG-7002). Ã‰galement, je recommande le livre Principes dâ€™expÃ©rimentation: planification des expÃ©riences et analyse de leurs rÃ©sultats de Pierre Dagnelie (2012), disponible en ligne en format PDF. Un bon aperÃ§u des dispositifs expÃ©rimentaux est aussi prÃ©sentÃ© dans Introductory Statistics with R, de Peter Dalgaard (2008). Une population est Ã©chantillonnÃ©e pour induire des paramÃ¨tres: un rendement typique dans des conditions mÃ©tÃ©orologiques, Ã©daphiques et managÃ©riales donnÃ©es, la masse typique des faucons pÃ¨lerins, mÃ¢les et femelles, le microbiome typique dâ€™un sol agricole ou forestier, etc. Une statistique est une estimation dâ€™un paramÃ¨tre calculÃ©e Ã  partir des donnÃ©es, par exemple une moyenne et un Ã©cart-type. Par exemple, la moyenne (\\(\\mu\\)) et lâ€™Ã©cart-type (\\(\\sigma\\)) dâ€™une population sont estimÃ©s par les moyennes (\\(\\bar{x}\\)) et Ã©carts-types (\\(s\\)) calculÃ©s sur les donnÃ©es issues de lâ€™Ã©chantillonnage. Chaque paramÃ¨tre est liÃ©e Ã  une perspective que lâ€™on dÃ©sire connaÃ®tre chez une population. Ces angles dâ€™observations sont les variables. 5.2 Les variables Nous avons abordÃ© au chapitre 4 la notion de variable par lâ€™intermÃ©diaire dâ€™une donnÃ©e. Une variable est lâ€™observation dâ€™une caractÃ©ristique dÃ©crivant un Ã©chantillon et qui est susceptible de varier dâ€™un Ã©chantillon Ã  un autre. Si les observations varient en effet dâ€™un Ã©chantillon Ã  un autre, on parlera de variable alÃ©atoire. MÃªme le hasard est rÃ©git par certaines loi: ce qui est alÃ©atoire dans une variable peut Ãªtre dÃ©crit par des lois de probabilitÃ©, que nous verrons plus bas. Mais restons aux variables pour lâ€™instant. Par convention, on peut attribuer aux variables un symbole mathÃ©matique. Par exemple, on peut donner Ã  la masse volumique dâ€™un sol (qui est le rÃ©sultat dâ€™une mÃ©thodologie prÃ©cise) le symbole \\(\\rho\\). Lorsque lâ€™on attribue une valeur Ã  \\(\\rho\\), on parle dâ€™une donnÃ©e. Chaque donnÃ©e dâ€™une observation a un indice qui lui est propre, que lâ€™on dÃ©signe souvent par \\(i\\), que lâ€™on place en indice \\(\\rho_i\\). Pour la premiÃ¨re donnÃ©e, on a \\(i=1\\), donc \\(\\rho_1\\). Pour un nombre \\(n\\) dâ€™Ã©chantillons, on aura \\(\\rho_1\\), \\(\\rho_2\\), \\(\\rho_3\\), â€¦, \\(\\rho_n\\), formant le vecteur \\(\\rho = \\left[\\rho_1, \\rho_2, \\rho_3, ..., \\rho_n \\right]\\). En R, une variable est associÃ©e Ã  un vecteur ou une colonne dâ€™un tableau. rho &lt;- c(1.34, 1.52, 1.26, 1.43, 1.39) # matrice 1D data &lt;- data.frame(rho = rho) # tableau data ## rho ## 1 1.34 ## 2 1.52 ## 3 1.26 ## 4 1.43 ## 5 1.39 Il existe plusieurs types de variables, qui se regroupe en deux grandes catÃ©gories: les variables quantitatives et les variables qualitatives. 5.2.1 Variables quantitatives Ces variables peuvent Ãªtre continuent dans un espace Ã©chantillonnal rÃ©el ou discrÃ¨tes dans un espace Ã©chantillonnal ne considÃ©rant que des valeurs fixes. Notons que la notion de nombre rÃ©el est toujours une approximation en sciences expÃ©rimentales comme en calcul numÃ©rique, Ã©tant donnÃ©e que lâ€™on est limitÃ© par la prÃ©cision des appareils comme par le nombre dâ€™octets Ã  utiliser. Bien que les valeurs fixes des distributions discrÃ¨tes ne soient pas toujours des valeurs entiÃ¨res, câ€™est bien souvent le cas en biostatistiques comme en dÃ©mographie, oÃ¹ les dÃ©comptes dâ€™individus sont souvent prÃ©sents (et oÃ¹ la notion de fraction dâ€™individus nâ€™est pas acceptÃ©e). 5.2.2 Variables qualitatives On exprime parfois quâ€™une variable qualitative est une variable impossible Ã  mesurer numÃ©riquement: une couleur, lâ€™appartenance Ã  espÃ¨ce ou Ã  une sÃ©rie de sol. Pourtant, dans bien des cas, les variables qualitatives peut Ãªtre encodÃ©es en variables quantitatives. Par exemple, on peut accoler des pourcentages de sable, limon et argile Ã  un loam sableux, qui autrement est dÃ©crit par la classe texturale dâ€™un sol. Pour une couleur, on peut lui associer des pourcentages de rouge, vert et bleu, ainsi quâ€™un ton. En ce qui a trait aux variables ordonnÃ©es, il est possible de supposer un Ã©talement. Par exemple, une variable dâ€™intensitÃ© faible-moyenne-forte peut Ãªtre transformÃ©e linÃ©airement en valeurs quantitatives -1, 0 et 1. Attention toutefois, lâ€™Ã©talement peut parfois Ãªtre quadratique ou logarithmique. Les sÃ©ries de sol peuvent Ãªtre encodÃ©es par la proportion de gleyfication (Parent et al., 2017). Quant aux catÃ©gories difficilement transformables en quantitÃ©s, on pourra passer par lâ€™encodage catÃ©goriel, souvent appelÃ© dummyfication, qui nous verrons plus loin. 5.3 Les probabilitÃ©s Â« Nous sommes si Ã©loignÃ©s de connaÃ®tre tous les agens de la nature, et leurs divers modes dâ€™action ; quâ€™il ne serait pas philosophique de nier les phÃ©nomÃ¨nes, uniquement parce quâ€™ils sont inexplicables dans lâ€™Ã©tat actuel de nos connaissances. Seulement, nous devons les examiner avec une attention dâ€™autant plus scrupuleuse, quâ€™il paraÃ®t plus difficile de les admettre ; et câ€™est ici que le calcul des probabilitÃ©s devient indispensable, pour dÃ©terminer jusquâ€™Ã  quel point il faut multiplier les observations ou les expÃ©riences, afin dâ€™obtenir en faveur des agens quâ€™elles indiquent, une probabilitÃ© supÃ©rieure aux raisons que lâ€™on peut avoir dâ€™ailleurs, de ne pas les admettre. Â» â€” Pierre-Simon de Laplace Une probabilitÃ© est la vraisemblance quâ€™un Ã©vÃ¨nements se rÃ©alise chez un Ã©chantillon. Les probabilitÃ©s forment le cadre des systÃ¨mes stochastiques, câ€™est-Ã -dire des systÃ¨mes trop complexes pour en connaÃ®tre exactement les aboutissants, auxquels ont attribue une part de hasard. Ces systÃ¨mes sont prÃ©dominants dans les processus vivants. On peut dÃ©gager deux perspectives sur les probabilitÃ©s: lâ€™une passe par une interprÃ©tation frÃ©quentielle, lâ€™autre bayÃ©sienne. Lâ€™interprÃ©tation frÃ©quentielle reprÃ©sente la frÃ©quence des occurrences aprÃ¨s un nombre infini dâ€™Ã©vÃ¨nements. Par exemple, si vous jouez Ã  pile ou face un grand nombre de fois, le nombre de pile sera Ã©gal Ã  la moitiÃ© du nombre de lancÃ©s. Il sâ€™agit de lâ€™interprÃ©tation communÃ©ment utilisÃ©e. Lâ€™interprÃ©tation bayÃ©sienne vise Ã  quantifier lâ€™incertitude des phÃ©nomÃ¨nes. Dans cette perspective, plus lâ€™information sâ€™accumule, plus lâ€™incertitude diminue. Cette approche gagne en notoriÃ©tÃ© notamment parce quâ€™elle permet de dÃ©crire des phÃ©nomÃ¨nes qui, intrinsÃ¨quement, ne peuvent Ãªtre rÃ©pÃ©tÃ©s infiniment (absence dâ€™asymptote), comme celles qui sont bien dÃ©finis dans le temps ou sur des populations limitÃ©s. Lâ€™approche frÃ©quentielle teste si les donnÃ©es concordent avec un modÃ¨le du rÃ©el, tandis que lâ€™approche bayÃ©sienne Ã©value la probabilitÃ© que le modÃ¨le soit rÃ©el. Une erreur courante consiste Ã  aborder des statistiques frÃ©quentielles comme des statistiques bayÃ©siennes. Par exemple, si lâ€™on dÃ©sire Ã©valuer la probabilitÃ© de lâ€™existence de vie sur Mars, on devra passer par le bayÃ©sien, car avec les stats frÃ©quentielles, lâ€™on devra plutÃ´t conclure si les donnÃ©es sont conformes ou non avec lâ€™hypothÃ¨se de la vie sur Mars (exemple tirÃ©e du blogue Dynamic Ecology). Des rivalitÃ©s factices sâ€™installent enter les tenants des diffÃ©rentes approches, dont chacune, en rÃ©alitÃ©, rÃ©pond Ã  des questions diffÃ©rentes dont il convient rÃ©flÃ©chir sur les limitations. Bien que les statistiques bayÃ©siennes soient de plus en plus utilisÃ©es, nous ne couvrirons dans ce chapitre que lâ€™approche frÃ©quentielle. Lâ€™approche bayÃ©sienne est nÃ©anmoins traitÃ©e dans le document complÃ©mentaire statistiques_bayes.ipynb (section en dÃ©veloppement). 5.4 Les distributions Une variable alÃ©atoire peut prendre des valeurs selon des modÃ¨les de distribution des probabilitÃ©s. Une distribution est une fonction mathÃ©matique dÃ©crivant la probabilitÃ© dâ€™observer une sÃ©rie dâ€™Ã©vÃ¨nements. Ces Ã©vÃ¨nements peuvent Ãªtre des valeurs continues, des nombres entiers, des catÃ©gories, des valeurs boolÃ©ennes (Vrai/Faux), etc. DÃ©pendamment du type de valeur et des observations obtenues, on peut associer des variables Ã  diffÃ©rentes lois de probabilitÃ©. Toujours, lâ€™aire sous la courbe dâ€™une distribution de probabilitÃ© est Ã©gale Ã  1. En statistiques infÃ©rentielle, les distributions sont les modÃ¨les, comprenant certains paramÃ¨tres comme la moyenne et la variance pour les distributions normales, Ã  partir desquelles les donnÃ©es sont gÃ©nÃ©rÃ©es. Il existe deux grandes familles de distribution: discrÃ¨tes et continues. Les distributions discrÃ¨tes sont contraintes Ã  des valeurs prÃ©dÃ©finies (finies ou infinies), alors que les distributions continues prennent nÃ©cessairement un nombre infinie de valeur, dont la probabilitÃ© ne peut pas Ãªtre Ã©valuÃ©e ponctuellement, mais sur un intervalle. Lâ€™espÃ©rance mathÃ©matique est une fonction de tendance centrale, souvent dÃ©crite par un paramÃ¨tre. Il sâ€™agit de la moyenne dâ€™une population pour une distribution normale. La variance, quant Ã  elle, dÃ©crit la variabilitÃ© dâ€™une population, i.e.Â son Ã©talement autour de lâ€™espÃ©rance. Pour une distribution normale, la variance dâ€™une population est aussi appelÃ©e variance, souvent prÃ©sentÃ©e par lâ€™Ã©cart-type. 5.4.1 Distribution binomiale En tant que scÃ©nario Ã  deux issues possibles, des tirages Ã  pile ou face suivent une loi binomiale, comme toute variable boolÃ©enne prenant une valeur vraie ou fausse. En biostatistiques, les cas communs sont la prÃ©sence/absence dâ€™une espÃ¨ce, dâ€™une maladie, dâ€™un trait phylogÃ©nÃ©tique, ainsi que les catÃ©gories encodÃ©es. Lorsque lâ€™opÃ©ration ne comprend quâ€™un seul Ã©chantillon (i.e.Â un seul tirage Ã  pile ou face), il sâ€™agit dâ€™un cas particulier dâ€™une loi binomiale que lâ€™on nomme une loi de Bernouilli. Pour 25 tirages Ã  pile ou face indÃ©pendants (i.e.Â dont lâ€™ordre des tirages ne compte pas), on peut dessiner une courbe de distribution dont la somme des probabilitÃ©s est de 1. La fonction dbinom est une fonctions de distribution de probabilitÃ©s. Les fonctions de distribution de probabilitÃ©s discrÃ¨tes sont appelÃ©es des fonctions de masse. library(&quot;tidyverse&quot;) ## â”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 1.2.1 â”€â”€ ## âœ” ggplot2 3.1.0 âœ” purrr 0.3.0 ## âœ” tibble 2.0.1 âœ” dplyr 0.7.8 ## âœ” tidyr 0.8.2 âœ” stringr 1.3.1 ## âœ” readr 1.3.1 âœ” forcats 0.3.0 ## â”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€ ## âœ– dplyr::filter() masks stats::filter() ## âœ– dplyr::lag() masks stats::lag() x &lt;- 0:25 y &lt;- dbinom(x = x, size = 25, prob = 0.5) print(paste(&#39;La somme des probabilitÃ©s est de&#39;, sum(y))) ## [1] &quot;La somme des probabilitÃ©s est de 1&quot; ggplot(data = tibble(x, y), mapping = aes(x, y)) + geom_segment(aes(x = x, xend = x, y = 0, yend = y), color = &quot;grey50&quot;) + geom_point() 5.4.2 Distribution de Poisson La loi de Poisson (avec un P majuscule, introduite par le mathÃ©maticien franÃ§ais SimÃ©on Denis Poisson et non pas lâ€™animal) dÃ©crit des distributions discrÃ¨tes de probabilitÃ© dâ€™un nombre dâ€™Ã©vÃ¨nements se produisant dans lâ€™espace ou dans le temps. Les distributions de Poisson dÃ©crive ce qui tient du dÃ©compte. Il peut sâ€™agir du nombre de grenouilles traversant une rue quotidiennement, du nombre de plants dâ€™asclÃ©piades se trouvant sur une terre cultivÃ©e, ou du nombre dâ€™Ã©vÃ¨nements de prÃ©cipitation au mois de juin, etc. La distribution de Poisson nâ€™a quâ€™un seul paramÃ¨tre, \\(\\lambda\\), qui dÃ©crit tant la moyenne des dÃ©comptes. Par exemple, en un mois de 30 jours, et une moyenne de 8 Ã©vÃ¨nements de prÃ©cipitation pour ce mois, on obtient la distribution suivante. x &lt;- 1:30 y &lt;- dpois(x, lambda = 8) print(paste(&#39;La somme des probabilitÃ©s est de&#39;, sum(y))) ## [1] &quot;La somme des probabilitÃ©s est de 0.999664536835124&quot; ggplot(data = data.frame(x, y), mapping = aes(x, y)) + geom_segment(aes(x = x, xend = x, y = 0, yend = y), color = &quot;grey50&quot;) + geom_point() 5.4.3 Distribution uniforme La distribution la plus simple est probablement la distribution uniforme. Si la variable est discrÃ¨te, chaque catÃ©gorie est associÃ© Ã  une probabilitÃ© Ã©gale. Si la variable est continue, la probabilitÃ© est directement proportionnelle Ã  la largeur de lâ€™intervalle. On utilise rarement la distribution uniforme en biostatistiques, sinon pour dÃ©crire des a priori vagues pour lâ€™analyse bayÃ©sienne (ce sujet est traitÃ© dans le chapitre 6). Nous utilisons la fonction dunif. Ã€ la diffÃ©rence des distributions discrÃ¨tes, les fonctions de distribution de probabilitÃ©s continues sont appelÃ©es des fonctions de densitÃ© dâ€™une loi de probabilitÃ© (probability density function). increment &lt;- 0.01 x &lt;- seq(-4, 4, by = increment) y1 &lt;- dunif(x, min = -3, max = 3) y2 &lt;- dunif(x, min = -2, max = 2) y3 &lt;- dunif(x, min = -1, max = 1) print(paste(&#39;La somme des probabilitÃ©s est de&#39;, sum(y3 * increment))) ## [1] &quot;La somme des probabilitÃ©s est de 1.005&quot; gg_unif &lt;- data.frame(x, y1, y2, y3) %&gt;% gather(variable, value, -x) ggplot(data = gg_unif, mapping = aes(x = x, y = value)) + geom_line(aes(colour = variable)) 5.4.4 Distribution normale La plus rÃ©pandue de ces lois est probablement la loi normale, parfois nommÃ©e loi gaussienne et plus rarement loi laplacienne. Il sâ€™agit de la distribution classique en forme de cloche. La loi normale est dÃ©crite par une moyenne, qui dÃ©signe la tendance centrale, et une variance, qui dÃ©signe lâ€™Ã©talement des probabilitÃ©s autours de la moyenne. La racine carrÃ©e de la variance est lâ€™Ã©cart-type. Les distributions de mesures exclusivement positives (comme le poids ou la taille) sont parfois avantageusement approximÃ©es par une loi log-normale, qui est une loi normale sur le logarithme des valeurs: la moyenne dâ€™une loi log-normale est la moyenne gÃ©omÃ©trique. increment &lt;- 0.01 x &lt;- seq(-10, 10, by = increment) y1 &lt;- dnorm(x, mean = 0, sd = 1) y2 &lt;- dnorm(x, mean = 0, sd = 2) y3 &lt;- dnorm(x, mean = 0, sd = 3) print(paste(&#39;La somme des probabilitÃ©s est de&#39;, sum(y3 * increment))) ## [1] &quot;La somme des probabilitÃ©s est de 0.999147010743368&quot; gg_norm &lt;- data.frame(x, y1, y2, y3) %&gt;% gather(variable, value, -x) ggplot(data = gg_norm, mapping = aes(x = x, y = value)) + geom_line(aes(colour = variable)) Quelle est la probabilitÃ© dâ€™obtenir le nombre 0 chez une observation continue distribuÃ©e normalement dont la moyenne est 0 et lâ€™Ã©cart-type est de 1? RÃ©ponse: 0. La loi normale Ã©tant une distribution continue, les probabilitÃ©s non-nulles ne peuvent Ãªtre calculÃ©s que sur des intervalles. Par exemple, la probabilitÃ© de retrouver une valeur dans lâ€™intervalle entre -1 et 2 est calculÃ©e en soustrayant la probabilitÃ© cumulÃ©e Ã  -1 de la probabilitÃ© cumulÃ©e Ã  2. increment &lt;- 0.01 x &lt;- seq(-5, 5, by = increment) y &lt;- dnorm(x, mean = 0, sd = 1) prob_between &lt;- c(-1, 2) gg_norm &lt;- data.frame(x, y) gg_auc &lt;- gg_norm %&gt;% filter(x &gt; prob_between[1], x &lt; prob_between[2]) %&gt;% rbind(c(prob_between[2], 0)) %&gt;% rbind(c(prob_between[1], 0)) ggplot(data.frame(x, y), aes(x, y)) + geom_polygon(data = gg_auc, fill = &#39;#71ad50&#39;) + # #71ad50 est un code de couleur format hexadÃ©cimal geom_line() prob_norm_between &lt;- pnorm(q = prob_between[2], mean = 0, sd = 1) - pnorm(q = prob_between[1], mean = 0, sd = 1) print(paste(&quot;La probabilitÃ© d&#39;obtenir un nombre entre&quot;, prob_between[1], &quot;et&quot;, prob_between[2], &quot;est d&#39;environ&quot;, round(prob_norm_between, 2) * 100, &quot;%&quot;)) ## [1] &quot;La probabilitÃ© d&#39;obtenir un nombre entre -1 et 2 est d&#39;environ 82 %&quot; La courbe normale peut Ãªtre utile pour Ã©valuer la distribution dâ€™une population. Par exemple, on peut calculer les limites de rÃ©gion sur la courbe normale qui contient 95% des valeurs possibles en tranchant 2.5% de part et dâ€™autre de la moyenne. Il sâ€™agit ainsi de lâ€™intervalle de confiance sur la dÃ©viation de la distribution. increment &lt;- 0.01 x &lt;- seq(-5, 5, by = increment) y &lt;- dnorm(x, mean = 0, sd = 1) alpha &lt;- 0.05 prob_between &lt;- c(qnorm(p = alpha/2, mean = 0, sd = 1), qnorm(p = 1 - alpha/2, mean = 0, sd = 1)) gg_norm &lt;- data.frame(x, y) gg_auc &lt;- gg_norm %&gt;% filter(x &gt; prob_between[1], x &lt; prob_between[2]) %&gt;% rbind(c(prob_between[2], 0)) %&gt;% rbind(c(prob_between[1], 0)) ggplot(data = data.frame(x, y), mapping = aes(x, y)) + geom_polygon(data = gg_auc, fill = &#39;#71ad50&#39;) + # #71ad50 est un code de couleur format hexadÃ©cimal geom_line() + geom_text(data = data.frame(x = prob_between, y = c(0, 0), labels = round(prob_between, 2)), mapping = aes(label = labels)) On pourrait aussi Ãªtre intÃ©ressÃ© Ã  lâ€™intervalle de confiance sur la moyenne. En effet, la moyenne suit aussi une distribution normale, dont la tendance centrale est la moyenne de la distribution, et dont lâ€™Ã©cart-type est notÃ© erreur standard. On calcule cette erreur en divisant la variance par le nombre dâ€™observation, ou en divisant lâ€™Ã©cart-type par la racine carrÃ©e du nombre dâ€™observations. Ainsi, pour 10 Ã©chantillons: increment &lt;- 0.01 x &lt;- seq(-5, 5, by = increment) y &lt;- dnorm(x, mean = 0, sd = 1) alpha &lt;- 0.05 prob_between &lt;- c(qnorm(p = alpha/2, mean = 0, sd = 1) / sqrt(10), qnorm(p = 1 - alpha/2, mean = 0, sd = 1) / sqrt(10)) gg_norm &lt;- data.frame(x, y) gg_auc &lt;- gg_norm %&gt;% filter(x &gt; prob_between[1], x &lt; prob_between[2]) %&gt;% rbind(c(prob_between[2], 0)) %&gt;% rbind(c(prob_between[1], 0)) ggplot(data = data.frame(x, y), mapping = aes(x, y)) + geom_polygon(data = gg_auc, fill = &#39;#71ad50&#39;) + # #71ad50 est un code de couleur format hexadÃ©cimal geom_line() + geom_text(data = data.frame(x = prob_between, y = c(0, 0), labels = round(prob_between, 2)), mapping = aes(label = labels)) 5.5 Statistiques descriptives On a vu comment gÃ©nÃ©rer des statistiques sommaires en R avec la fonction summary(). Reprenons les donnÃ©es dâ€™iris. data(&quot;iris&quot;) summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## Pour prÃ©cisÃ©ment effectuer une moyenne et un Ã©cart-type sur un vecteur, passons par les fonctions mean() et sd(). mean(iris$Sepal.Length) ## [1] 5.843333 sd(iris$Sepal.Length) ## [1] 0.8280661 Pour effectuer un sommaire de tableau pilotÃ© par une fonction, nous passons par la gamme de fonctions summarise(), de dplyr. Dans ce cas, avec group_by(), nous fragmentons le tableau par espÃ¨ce pour effectuer un sommaire sur toutes les variables. iris %&gt;% group_by(Species) %&gt;% summarise_all(mean) ## # A tibble: 3 x 5 ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.01 3.43 1.46 0.246 ## 2 versicolor 5.94 2.77 4.26 1.33 ## 3 virginica 6.59 2.97 5.55 2.03 Vous pourriez Ãªtre intÃ©ressÃ© par les quartiles Ã  25, 50 et 75%. Mais la fonction summarise() nâ€™autorise que les fonctions dont la sortie est dâ€™un seul objet, alors faisons sorte que lâ€™objet soit une liste - lorsque lâ€™on imbrique une fonction funs, le tableau Ã  insÃ©rer dans la fonction est indiquÃ© par un .. iris %&gt;% group_by(Species) %&gt;% summarise_all(funs(list(quantile(.)))) ## # A tibble: 3 x 5 ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;fct&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 setosa &lt;dbl [5]&gt; &lt;dbl [5]&gt; &lt;dbl [5]&gt; &lt;dbl [5]&gt; ## 2 versicolor &lt;dbl [5]&gt; &lt;dbl [5]&gt; &lt;dbl [5]&gt; &lt;dbl [5]&gt; ## 3 virginica &lt;dbl [5]&gt; &lt;dbl [5]&gt; &lt;dbl [5]&gt; &lt;dbl [5]&gt; En mode programmation classique de R, on pourra gÃ©nÃ©rer les quartiles Ã  la piÃ¨ce. quantile(iris$Sepal.Length[iris$Species == &#39;setosa&#39;]) ## 0% 25% 50% 75% 100% ## 4.3 4.8 5.0 5.2 5.8 quantile(iris$Sepal.Length[iris$Species == &#39;versicolor&#39;]) ## 0% 25% 50% 75% 100% ## 4.9 5.6 5.9 6.3 7.0 quantile(iris$Sepal.Length[iris$Species == &#39;virginica&#39;]) ## 0% 25% 50% 75% 100% ## 4.900 6.225 6.500 6.900 7.900 La fonction table() permettra dâ€™obtenir des dÃ©comptes par catÃ©gorie, ici par plages de longueurs de sÃ©pales. Pour obtenir les proportions du nombre total, il sâ€™agit dâ€™encapsuler le tableau croisÃ© dans la fonction prop.table(). tableau_croise &lt;- table(iris$Species, cut(iris$Sepal.Length, breaks = quantile(iris$Sepal.Length))) tableau_croise ## ## (4.3,5.1] (5.1,5.8] (5.8,6.4] (6.4,7.9] ## setosa 35 14 0 0 ## versicolor 4 20 17 9 ## virginica 1 5 18 26 prop.table(tableau_croise) ## ## (4.3,5.1] (5.1,5.8] (5.8,6.4] (6.4,7.9] ## setosa 0.234899329 0.093959732 0.000000000 0.000000000 ## versicolor 0.026845638 0.134228188 0.114093960 0.060402685 ## virginica 0.006711409 0.033557047 0.120805369 0.174496644 5.6 Tests dâ€™hypothÃ¨ses Ã  un et deux Ã©chantillons Un test dâ€™hypothÃ¨se permet de dÃ©cider si une hypothÃ¨se est confirmÃ©e ou rejetÃ©e Ã  un seuil de probabilitÃ© prÃ©dÃ©terminÃ©. Cette section est inspirÃ©e du chapitre 5 de Dalgaard, 2008. 5.6.0.1 Information: lâ€™hypothÃ¨se nulle Les tests dâ€™hypothÃ¨se Ã©value des effets statistiques (qui ne sont pas nÃ©cessairement des effets de causalitÃ©). Lâ€™effet Ã  Ã©valuer peut Ãªtre celui dâ€™un traitement, dâ€™indicateurs mÃ©tÃ©orologiques (e.g.Â prÃ©cipitations totales, degrÃ©-jour, etc.), de techniques de gestion des paysages, etc. Une recherche est menÃ©e pour Ã©valuer lâ€™hypothÃ¨se que lâ€™on retrouve des diffÃ©rences entre des unitÃ©s expÃ©rimentales. Par convention, lâ€™hypothÃ¨se nulle (Ã©crite \\(H_0\\)) est lâ€™hypothÃ¨se quâ€™il nâ€™y ait pas dâ€™effet (câ€™est lâ€™hypothÃ¨se de lâ€™avocat du diable ğŸ˜ˆ) Ã  lâ€™Ã©chelle de la population (et non pas Ã  lâ€™Ã©chelle de lâ€™Ã©chantillon). Ã€ lâ€™inverse, lâ€™hypothÃ¨se alternative (Ã©crite \\(H_1\\)) est lâ€™hypothÃ¨se quâ€™il y ait un effet Ã  lâ€™Ã©chelle de la population. Ã€ titre dâ€™exercice en stats, on dÃ©bute souvent par en testant si deux vecteurs de valeurs continues proviennent de populations Ã  moyennes diffÃ©rentes ou si un vecteur de valeurs a Ã©tÃ© gÃ©nÃ©rÃ© Ã  partir dâ€™une population ayant une moyenne donner. Dans cette section, nous utiliserons la fonction t.test() pour les tests de t et la fonction wilcox.test() pour les tests de Wilcoxon (aussi appelÃ© de Mann-Whitney). 5.6.1 Test de t Ã  un seul Ã©chantillon Nous devons assumer, pour ce test, que lâ€™Ã©chantillon est recueillit dâ€™une population dont la distribution est normale, \\(\\mathcal{N} \\sim \\left( \\mu, \\sigma^2 \\right)\\), et que chaque Ã©chantillon est indÃ©pendant lâ€™un de lâ€™autre. Lâ€™hypothÃ¨se nulle est souvent celle de lâ€™avocat du diable, que la moyenne soit Ã©gale Ã  une valeur donnÃ©e (donc la diffÃ©rence entre la moyenne de la population et une moyenne donnÃ©e est de zÃ©ro): ici, que \\(\\mu = \\bar{x}\\). Lâ€™erreur standard sur la moyenne (ESM) de lâ€™Ã©chantillon, \\(\\bar{x}\\) est calculÃ©e comme suit. \\[ESM = \\frac{s}{\\sqrt{n}}\\] oÃ¹ \\(s\\) est lâ€™Ã©cart-type de lâ€™Ã©chantillon et \\(n\\) est le nombre dâ€™Ã©chantillons. Pour tester lâ€™intervalle de confiance de lâ€™Ã©chantillon, on multiplie lâ€™ESM par lâ€™aire sous la courbe de densitÃ© couvrant une certaine proportion de part et dâ€™autre de lâ€™Ã©chantillon. Pour un niveau de confiance de 95%, on retranche 2.5% de part et dâ€™autre. set.seed(33746) x &lt;- rnorm(20, 16, 4) level &lt;- 0.95 alpha &lt;- 1-level x_bar &lt;- mean(x) s &lt;- sd(x) n &lt;- length(x) error &lt;- qnorm(1 - alpha/2) * s / sqrt(n) error ## [1] 1.483253 intervalle de confiance est lâ€™erreur de par et dâ€™autre de la moyenne. c(x_bar - error, x_bar + error) ## [1] 14.35630 17.32281 Si la moyenne de la population est de 16, un nombre qui se situe dans lâ€™intervalle de confiance on accepte lâ€™hypothÃ¨se nulle au seuil 0.05. Si le nombre dâ€™Ã©chantillon est rÃ©duit (gÃ©nÃ©ralement &lt; 30), on passera plutÃ´t par une distribution de t, avec \\(n-1\\) degrÃ©s de libertÃ©. error &lt;- qt(1 - alpha/2, n-1) * s / sqrt(n) c(x_bar - error, x_bar + error) ## [1] 14.25561 17.42351 Plus simplement, on pourra utiliser la fonction t.test() en spÃ©cifiant la moyenne de la population. Nous avons gÃ©nÃ©rÃ© 20 donnÃ©es avec une moyenne de 16 et un Ã©cart-type de 4. Nous savons donc que la vraie moyenne de lâ€™Ã©chantillon est de 16. Mais disons que nous testons lâ€™hypothÃ¨se que ces donnÃ©es sont tirÃ©es dâ€™une population dont la moyenne est 18 (et implicitement que sont Ã©cart-type est de 4). t.test(x, mu = 18) ## ## One Sample t-test ## ## data: x ## t = -2.8548, df = 19, p-value = 0.01014 ## alternative hypothesis: true mean is not equal to 18 ## 95 percent confidence interval: ## 14.25561 17.42351 ## sample estimates: ## mean of x ## 15.83956 La fonction retourne la valeur de t (t-value), le nombre de degrÃ©s de libertÃ© (\\(n-1 = 19\\)), une description de lâ€™hypothÃ¨se alternative (alternative hypothesis: true mean is not equal to 18), ainsi que lâ€™intervalle de confiance au niveau de 95%. Le test contient aussi la p-value. Bien que la p-value soit largement utilisÃ©e en science 5.6.1.1 Information: la p-value La p-value, ou valeur-p ou p-valeur, est utilisÃ©e pour trancher si, oui ou non, un rÃ©sultat est significatif (en langage scientifique, le mot significatif ne devrait Ãªtre utilisÃ© que lorsque lâ€™on rÃ©fÃ¨re Ã  un test dâ€™hypothÃ¨se statistique). Vous retrouverez des p-value partout en stats. Les p-values indiquent la confiance que lâ€™hypothÃ¨se nulle soit vraie, selon les donnÃ©es et le modÃ¨le statistique utilisÃ©es. La p-value est la probabilitÃ© que les donnÃ©es aient Ã©tÃ© gÃ©nÃ©rÃ©es pour obtenir un effet Ã©quivalent ou plus prononcÃ© si lâ€™hypothÃ¨se nulle est vraie. Une p-value Ã©levÃ©e indique que le modÃ¨le appliquÃ© Ã  vos donnÃ©es concordent avec la conclusion que lâ€™hypothÃ¨se nulle est vraie, et inversement si la p-value est faible. Le seuil arbitraire utilisÃ©e en Ã©cologie et en agriculture, comme dans plusieurs domaines, est 0.05. Les six principes de lâ€™American Statistical Association guident lâ€™interprÃ©tation des p-values. [ma traduction] Les p-values indique lâ€™ampleur de lâ€™incompatibilitÃ© des donnÃ©es avec le modÃ¨le statistique Les p-values ne mesurent pas la probabilitÃ© que lâ€™hypothÃ¨se Ã©tudiÃ©e soit vraie, ni la probabilitÃ© que les donnÃ©es ont Ã©tÃ© gÃ©nÃ©rÃ©es uniquement par la chance. Les conclusions scientifiques et dÃ©cisions dâ€™affaire ou politiques ne devraient pas Ãªtre basÃ©es sur si une p-value atteint un seuil spÃ©cifique. Une infÃ©rence appropriÃ©e demande un rapport complet et transparent. Une p-value, ou une signification statistique, ne mesure pas lâ€™ampleur dâ€™un effet ou lâ€™importance dâ€™un rÃ©sultat. En tant que tel, une p-value nâ€™offre pas une bonne mesure des Ã©vidences dâ€™un modÃ¨le ou dâ€™une hypothÃ¨se. Cet encadrÃ© est inspirÃ© dâ€™un billet de blogue de Jim Frost et dâ€™un rapport de lâ€™American Statistical Association. Dans le cas prÃ©cÃ©dent, la p-value Ã©tait de 0.01014. Pour aider notre interprÃ©tation, prenons lâ€™hypothÃ¨se alternative: true mean is not equal to 18. Lâ€™hypothÃ¨se nulle Ã©tait bien que la vraie moyenne est Ã©gale Ã  18. InsÃ©rons la p-value dans la dÃ©finition: la probabilitÃ© que les donnÃ©es aient Ã©tÃ© gÃ©nÃ©rÃ©es pour obtenir un effet Ã©quivalent ou plus prononcÃ© si lâ€™hypothÃ¨se nulle est vraie est de 0.01014. Il est donc trÃ¨s peu probable que les donnÃ©es soient tirÃ©es dâ€™un Ã©chantillon dont la moyenne est de 18. Au seuil de signification de 0.05, on rejette lâ€™hypothÃ¨se nulle et lâ€™on conclu quâ€™Ã  ce seuil de confiance, lâ€™Ã©chantillon ne provient pas dâ€™une population ayant une moyenne de 18. 5.6.2 Attention: mauvaises interprÃ©tations des p-values â€œLa p-value nâ€™a jamais Ã©tÃ© conÃ§ue comme substitut au raisonnement scientifiqueâ€ Ron Wasserstein, directeur de lâ€™American Statistical Association [ma traduction]. Un rÃ©sultat montrant une p-value plus Ã©levÃ©e que 0.05 est-il pertinent? Lors dâ€™une confÃ©rence, Dr Evil ne prÃ©sentent que les rÃ©sultats significatifs de ses essais au seuil de 0.05. Certains essais ne sont pas significatifs, mais bon, ceux-ci ne sont pas importantsâ€¦ En Ã©cartant ces rÃ©sultats, Dr Evil commet 3 erreurs: La p-value nâ€™est pas un bon indicateur de lâ€™importance dâ€™un test statistique. Lâ€™importance dâ€™une variable dans un modÃ¨le devrait Ãªtre Ã©valuÃ©e par la valeur de son coefficient. Son incertitude devrait Ãªtre Ã©valuÃ©e par sa variance. Une maniÃ¨re dâ€™Ã©valuer plus intuitive la variance est lâ€™Ã©cart-type ou lâ€™intervalle de confiance. Ã€ un certain seuil dâ€™intervalle de confiance, la p-value traduira la probabilitÃ© quâ€™un coefficient soit rÃ©ellement nul ait pu gÃ©nÃ©rer des donnÃ©es dÃ©montrant un coefficient Ã©gal ou supÃ©rieur. Il est tout aussi important de savoir que le traitement fonctionne que de savoir quâ€™il ne fonctionne pas. Les rÃ©sultats dÃ©montrant des effets sont malheureusement davantage soumis aux journaux et davantage publiÃ©s que ceux ne dÃ©montrant pas dâ€™effets (Decullier et al., 2005). Le seuil de 0.05 est arbitraire. 5.6.2.1 Attention au p-hacking Le p-hacking (ou data dredging) consiste Ã  manipuler les donnÃ©es et les modÃ¨les pour faire en sorte dâ€™obtenir des p-values favorables Ã  lâ€™hypothÃ¨se testÃ©e et, Ã©ventuellement, aux conclusions recherchÃ©es. Ã€ Ã©viter dans tous les cas. Toujours. Toujours. Toujours. VidÃ©o suggÃ©rÃ©e (en anglais). 5.6.3 Test de Wilcoxon Ã  un seul Ã©chantillon Le test de t suppose que la distribution des donnÃ©es est normaleâ€¦ ce qui est rarement le cas, surtout lorsque les Ã©chantillons sont peu nombreux. Le test de Wilcoxon ne demande aucune supposition sur la distribution: câ€™est un test non-paramÃ©trique basÃ© sur le tri des valeurs. wilcox.test(x, mu = 18) ## ## Wilcoxon signed rank test ## ## data: x ## V = 39, p-value = 0.01208 ## alternative hypothesis: true location is not equal to 18 Le V est la somme des rangs positifs. Dans ce cas, la p-value est semblable Ã  celle du test de t, et les mÃªmes conclusions sâ€™appliquent. 5.6.4 Tests de t Ã  deux Ã©chantillons Les tests Ã  un Ã©chantillon servent plutÃ´t Ã  sâ€™exercer: rarement en aura-t-on besoin en recherche, oÃ¹ plus souvent, on voudra comparer les moyennes de deux unitÃ©s expÃ©rimentales. Lâ€™expÃ©rience comprend donc deux sÃ©ries de donnÃ©es continues, \\(x_1\\) et \\(x_2\\), issus de lois de distribution normale \\(\\mathcal{N} \\left( \\mu_1, \\sigma_1^2 \\right)\\) et \\(\\mathcal{N} \\left( \\mu_2, \\sigma_2^2 \\right)\\), et nous testons lâ€™hypothÃ¨se nulle que \\(\\mu_1 = \\mu_2\\). La statistique t est calculÃ©e comme suit. \\[t = \\frac{\\bar{x_1} - \\bar{x_2}}{ESDM}\\] Lâ€™ESDM est lâ€™erreur standard de la diffÃ©rence des moyennes: \\[ESDM = \\sqrt{ESM_1^2 + ESM_2^2}\\] Si vous supposez que les variances sont identiques, lâ€™erreur standard (s) est calculÃ©e pour les Ã©chantillons des deux groupes, puis insÃ©rÃ©e dans le calcul des ESM. La statistique t sera alors Ã©valuÃ©e Ã  \\(n_1 + n_2 - 2\\) degrÃ©s de libertÃ©. Si vous supposez que la variance est diffÃ©rente (procÃ©dure de Welch), vous calculez les ESM avec les erreurs standards respectives, et la statistique t devient une approximation de la distribution de t avec un nombre de degrÃ©s de libertÃ© calculÃ© Ã  partir des erreurs standards et du nombre dâ€™Ã©chantillon dans les groupes: cette procÃ©dure est considÃ©rÃ©e comme plus prudente (Dalgaard, 2008, page 101). Prenons les donnÃ©es dâ€™iris pour lâ€™exemple en excluant lâ€™iris setosa Ã©tant donnÃ©e que les tests de t se restreignent Ã  deux groupes. Nous allons tester la longueur des pÃ©tales. iris_pl &lt;- iris %&gt;% filter(Species != &quot;setosa&quot;) %&gt;% select(Species, Petal.Length) sample_n(iris_pl, 5) ## Species Petal.Length ## 98 virginica 5.2 ## 55 virginica 5.8 ## 92 virginica 5.1 ## 13 versicolor 4.0 ## 46 versicolor 4.2 Dans la prochaine cellule, nous introduisons lâ€™interface-formule de R, oÃ¹ lâ€™on retrouve typiquement le ~, entre les variables de sortie Ã  gauche et les variables dâ€™entrÃ©e Ã  droite. Dans notre cas, la variable de sortie est la variable testÃ©e, Petal.Length, qui varie en fonction du groupe Species, qui est la variable dâ€™entrÃ©e (variable explicative) - nous verrons les types de variables plus en dÃ©tails dans la section Les modÃ¨les statistiques, plus bas. t.test(formula = Petal.Length ~ Species, data = iris_pl, var.equal = FALSE) ## ## Welch Two Sample t-test ## ## data: Petal.Length by Species ## t = -12.604, df = 95.57, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.49549 -1.08851 ## sample estimates: ## mean in group versicolor mean in group virginica ## 4.260 5.552 Nous obtenons une sortie similaire aux prÃ©cÃ©dentes. Lâ€™intervalle de confiance Ã  95% exclu le zÃ©ro, ce qui est cohÃ©rent avec la p-value trÃ¨s faible, qui nous indique le rejet de lâ€™hypothÃ¨se nulle au seuil 0.05. Les groupes ont donc des moyennes de longueurs de pÃ©tale significativement diffÃ©rentes. 5.6.4.1 Enregistrer les rÃ©sultats dâ€™un test Il est possible dâ€™enregistrer un test dans un objet. tt_pl &lt;- t.test(formula = Petal.Length ~ Species, data = iris_pl, var.equal = FALSE) summary(tt_pl) ## Length Class Mode ## statistic 1 -none- numeric ## parameter 1 -none- numeric ## p.value 1 -none- numeric ## conf.int 2 -none- numeric ## estimate 2 -none- numeric ## null.value 1 -none- numeric ## alternative 1 -none- character ## method 1 -none- character ## data.name 1 -none- character str(tt_pl) ## List of 9 ## $ statistic : Named num -12.6 ## ..- attr(*, &quot;names&quot;)= chr &quot;t&quot; ## $ parameter : Named num 95.6 ## ..- attr(*, &quot;names&quot;)= chr &quot;df&quot; ## $ p.value : num 4.9e-22 ## $ conf.int : num [1:2] -1.5 -1.09 ## ..- attr(*, &quot;conf.level&quot;)= num 0.95 ## $ estimate : Named num [1:2] 4.26 5.55 ## ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;mean in group versicolor&quot; &quot;mean in group virginica&quot; ## $ null.value : Named num 0 ## ..- attr(*, &quot;names&quot;)= chr &quot;difference in means&quot; ## $ alternative: chr &quot;two.sided&quot; ## $ method : chr &quot;Welch Two Sample t-test&quot; ## $ data.name : chr &quot;Petal.Length by Species&quot; ## - attr(*, &quot;class&quot;)= chr &quot;htest&quot; 5.6.5 Comparaison des variances Pour comparer les variances, on a recours au test de F (F pour Fisher). var.test(formula = Petal.Length ~ Species, data = iris_pl) ## ## F test to compare two variances ## ## data: Petal.Length by Species ## F = 0.72497, num df = 49, denom df = 49, p-value = 0.2637 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.411402 1.277530 ## sample estimates: ## ratio of variances ## 0.7249678 Il semble que lâ€™on pourrait relancer le test de t sans la procÃ©dure Welch, avec var.equal = TRUE. 5.6.6 Tests de Wilcoxon Ã  deux Ã©chantillons Cela ressemble au test de t! wilcox.test(formula = Petal.Length ~ Species, data = iris_pl, var.equal = TRUE) ## ## Wilcoxon rank sum test with continuity correction ## ## data: Petal.Length by Species ## W = 44.5, p-value &lt; 2.2e-16 ## alternative hypothesis: true location shift is not equal to 0 5.6.7 Les tests pairÃ©s Les tests pairÃ©s sont utilisÃ©s lorsque deux Ã©chantillons proviennent dâ€™une mÃªme unitÃ© expÃ©rimentale: il sâ€™agit en fait de tests sur la diffÃ©rences entre deux observations. set.seed(2555) n &lt;- 20 avant &lt;- rnorm(n, 16, 4) apres &lt;- rnorm(n, 18, 3) Il est important de spÃ©cifier que le test est pairÃ©, la valeur par dÃ©faut de paired Ã©tant FALSE. t.test(avant, apres, paired = TRUE) ## ## Paired t-test ## ## data: avant and apres ## t = -1.5168, df = 19, p-value = 0.1458 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -4.5804586 0.7311427 ## sample estimates: ## mean of the differences ## -1.924658 Lâ€™hypothÃ¨se nulle quâ€™il nâ€™y ait pas de diffÃ©rence entre lâ€™avant et lâ€™aprÃ¨s traitement est acceptÃ©e au seuil 0.05. Exercice. Effectuer un test de Wilcoxon pairÃ©. 5.7 Lâ€™analyse de variance Lâ€™analyse de variance consiste Ã  comparer des moyennes de plusieurs groupe distribuÃ©s normalement et de mÃªme variance. Cette section sera Ã©laborÃ©e prochainement plus en profondeur. ConsidÃ©rons-la pour le moment comme une rÃ©gression sur une variable catÃ©gorielle. pl_aov &lt;- aov(Petal.Length ~ Species, iris) summary(pl_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 2 437.1 218.55 1180 &lt;2e-16 *** ## Residuals 147 27.2 0.19 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 La prochaine section, justement, est vouÃ©e aux modÃ¨les statistiques explicatifs, qui incluent la rÃ©gression. 5.8 Les modÃ¨les statistiques La modÃ©lisation statistique consiste Ã  lier de maniÃ¨re explicite des variables de sortie \\(y\\) (ou variables-rÃ©ponse ou variables dÃ©pendantes) Ã  des variables explicatives \\(x\\) (ou variables prÃ©dictives / indÃ©pendantes / covariables). Les variables-rÃ©ponse sont modÃ©lisÃ©es par une fonction des variables explicatives ou prÃ©dictives. Pourquoi garder les termes explicatives et prÃ©dictives? Parce que les modÃ¨les statistiques (basÃ©s sur des donnÃ©es et non pas sur des mÃ©canismes) sont de deux ordres. Dâ€™abord, les modÃ¨les prÃ©dictifs sont conÃ§us pour prÃ©dire de maniÃ¨re fiable une ou plusieurs variables-rÃ©ponse Ã  partir des informations contenues dans les variables qui sont, dans ce cas, prÃ©dictives. Ces modÃ¨les sont couverts dans le chapitre 11 de ce manuel (en dÃ©veloppement). Lorsque lâ€™on dÃ©sire tester des hypothÃ¨ses pour Ã©valuer quelles variables expliquent la rÃ©ponse, on parlera de modÃ©lisation (et de variables) explicatives. En infÃ©rence statistique, on Ã©valuera les corrÃ©lations entre les variables explicatives et les variables-rÃ©ponse. Un lien de corrÃ©lation nâ€™est pas un lien de causalitÃ©. Lâ€™infÃ©rence causale peut en revanche Ãªtre Ã©valuÃ©e par des modÃ¨les dâ€™Ã©quations structurelles, sujet qui fera Ã©ventuellement partie de ce cours. Cette section couvre la modÃ©lisation explicative. Les variables qui contribuent Ã  crÃ©er les modÃ¨les peuvent Ãªtre de diffÃ©rentes natures et distribuÃ©es selon diffÃ©rentes lois de probabilitÃ©. Alors que les modÃ¨les linÃ©aires simples (lm) impliquent une variable-rÃ©ponse distribuÃ©e de maniÃ¨re continue, les modÃ¨les linÃ©aires gÃ©nÃ©ralisÃ©s peuvent aussi expliquer des variables de sorties discrÃ¨tes. Dans les deux cas, on distinguera les variables fixes et les variables alÃ©atoires. Les variables fixes sont des les variables testÃ©es lors de lâ€™expÃ©rience: dose du traitement, espÃ¨ce/cultivar, mÃ©tÃ©o, etc. Les variables alÃ©atoires sont les sources de variation qui gÃ©nÃ¨rent du bruit dans le modÃ¨le: les unitÃ©s expÃ©rimentales ou le temps lors de mesures rÃ©pÃ©tÃ©es. Les modÃ¨les incluant des effets fixes seulement sont des modÃ¨les Ã  effets fixes. GÃ©nÃ©ralement, les modÃ¨les incluant des variables alÃ©atoires incluent aussi des variables fixes: on parlera alors de modÃ¨les mixtes. Nous couvrirons ces deux types de modÃ¨le. 5.8.1 ModÃ¨les Ã  effets fixes Les tests de t et de Wilcoxon, explorÃ©s prÃ©cÃ©demment, sont des modÃ¨les statistiques Ã  une seule variable. Nous avons vu dans lâ€™interface-formule quâ€™une variable-rÃ©ponse peut Ãªtre liÃ©e Ã  une variable explicative avec le tilde ~. En particulier, le test de t est rÃ©gression linÃ©aire univariÃ©e (Ã  une seule variable explicative) dont la variable explicative comprend deux catÃ©gories. De mÃªme, lâ€™anova est une rÃ©gression linÃ©aire univariÃ©e dont la variable explicative comprend plusieurs catÃ©gories. Or lâ€™interface-formule peut Ãªtre utilisÃ© dans plusieurs circonstance, notamment pour ajouter plusieurs variables de diffÃ©rents types: on parlera de rÃ©gression multivariÃ©e. La plupart des modÃ¨les statistiques peuvent Ãªtre approximÃ©s comme une combinaison linÃ©aire de variables: ce sont des modÃ¨les linÃ©aires. Les modÃ¨les non-linÃ©aires impliquent des stratÃ©gies computationnelles complexes qui rendent leur utilisation plus difficile Ã  manÅ“uvrer. Un modÃ¨le linÃ©aire univariÃ© prendra la forme \\(y = \\beta_0 + \\beta_1 x + \\epsilon\\), oÃ¹ \\(\\beta_0\\) est lâ€™intercept et \\(\\beta_1\\) est la pente et \\(\\epsilon\\) est lâ€™erreur. Vous verrez parfois la notation \\(\\hat{y} = \\beta_0 + \\beta_1 x\\). La notation avec le chapeau \\(\\hat{y}\\) exprime quâ€™il sâ€™agit des valeurs gÃ©nÃ©rÃ©es par le modÃ¨le. En fait, \\(y = \\hat{y} - \\epsilon\\). 5.8.1.1 ModÃ¨le linÃ©aire univariÃ© avec variable continue Prenons les donnÃ©es lasrosas.corn incluses dans le module agridat, oÃ¹ lâ€™on retrouve le rendement dâ€™une production de maÃ¯s Ã  dose dâ€™azote variable, en Argentine. library(&quot;agridat&quot;) data(&quot;lasrosas.corn&quot;) sample_n(lasrosas.corn, 10) ## year lat long yield nitro topo bv rep nf ## 2172 2001 -33.05109 -63.84451 94.15 50.6 E 173.54 R1 N2 ## 3066 2001 -33.04908 -63.84830 90.19 99.8 W 91.74 R3 N4 ## 949 1999 -33.05165 -63.84257 73.60 0.0 LO 173.23 R2 N0 ## 1909 2001 -33.05175 -63.84320 108.95 75.4 LO 163.38 R1 N3 ## 2173 2001 -33.05111 -63.84444 101.67 50.6 E 174.00 R1 N2 ## 2770 2001 -33.05139 -63.84244 98.74 50.6 LO 167.86 R2 N2 ## 1737 1999 -33.05121 -63.84224 81.70 29.0 LO 166.59 R3 N1 ## 2261 2001 -33.05088 -63.84491 50.59 39.0 E 179.56 R1 N1 ## 1501 1999 -33.05103 -63.84404 61.44 0.0 E 166.84 R3 N0 ## 2149 2001 -33.05052 -63.84613 46.06 50.6 HT 178.54 R1 N2 Ces donnÃ©es comprennent plusieurs variables. Prenons le rendement (yield) comme variable de sortie et, pour le moment, ne retenons que la dose dâ€™azote (nitro) comme variable explicative: il sâ€™agit dâ€™une rÃ©gression univariÃ©e. Les deux variables sont continuent. Explorons dâ€™abord le nuage de points de lâ€™une et lâ€™autre. ggplot(data = lasrosas.corn, mapping = aes(x = nitro, y = yield)) + geom_point() Lâ€™hypothÃ¨se nulle est que la dose dâ€™azote nâ€™affecte pas le rendement, câ€™est Ã  dire que le coefficient de pente et nul. Une autre hypothÃ¨se est que lâ€™intercept est nul: donc quâ€™Ã  dose de 0, rendement de 0. Un modÃ¨le linÃ©aire Ã  variable de sortie continue est crÃ©Ã© avec la fonction lm(), pour linear model. modlin_1 &lt;- lm(yield ~ nitro, data = lasrosas.corn) summary(modlin_1) ## ## Call: ## lm(formula = yield ~ nitro, data = lasrosas.corn) ## ## Residuals: ## Min 1Q Median 3Q Max ## -53.183 -15.341 -3.079 13.725 45.897 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 65.843213 0.608573 108.193 &lt; 2e-16 *** ## nitro 0.061717 0.007868 7.845 5.75e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 19.66 on 3441 degrees of freedom ## Multiple R-squared: 0.01757, Adjusted R-squared: 0.01728 ## F-statistic: 61.54 on 1 and 3441 DF, p-value: 5.754e-15 Le diagnostic du modÃ¨le comprend plusieurs informations. Dâ€™abord la formule utilisÃ©e, affichÃ©e pour la traÃ§abilitÃ©. Viens ensuite un aperÃ§u de la distribution des rÃ©sidus. La mÃ©diane devrait sâ€™approcher de la moyenne des rÃ©sidus (qui est toujours de 0). Bien que le -3.079 peut sembler important, il faut prendre en considÃ©ration de lâ€™Ã©chelle de y, et ce -3.079 est exprimÃ© en terme de rendement, ici en quintaux (i.e.Â 100 kg) par hectare. La distribution des rÃ©sidus mÃ©rite dâ€™Ãªtre davantage investiguÃ©e. Nous verrons cela un peu plus tard. Les coefficients apparaissent ensuite. Les estimÃ©s sont les valeurs des effets. R fournit aussi lâ€™erreur standard associÃ©e, la valeur de t ainsi que la p-value (la probabilitÃ© dâ€™obtenir cet effet ou un effet plus extrÃªme si en rÃ©alitÃ© il y avait absence dâ€™effet). Lâ€™intercept est bien sÃ»r plus Ã©levÃ© que 0 (Ã  dose nulle, on obtient 65.8 quintaux par hectare en moyenne). La pente de la variable nitro est de ~0.06: pour chaque augmentation dâ€™un kg/ha de dose, on a obtenu ~0.06 quintaux/ha de plus de maÃ¯s. Donc pour 100 kg/ha de N, on a obtenu un rendement moyen de 6 quintaux de plus que lâ€™intercept. Soulignons que lâ€™ampleur du coefficient est trÃ¨s important pour guider la fertilisation: ne rapporter que la p-value, ou ne rapporter que le fait quâ€™elle est infÃ©rieure Ã  0.05 (ce qui arrive souvent dans la littÃ©rature), serait trÃ¨s insuffisant pour lâ€™interprÃ©tation des statistiques. La p-value nous indique nÃ©anmoins quâ€™il serait trÃ¨s improbable quâ€™une telle pente ait Ã©tÃ© gÃ©nÃ©rÃ©e alors que celle-ci est nulle en rÃ©alitÃ©. Les Ã©toiles Ã  cÃ´tÃ© des p-values indiquent lâ€™ampleur selon lâ€™Ã©chelle Signif. codes indiquÃ©e en-dessous du tableau des coefficients. Sous ce tableau, R offre dâ€™autres statistiques. En outre, les RÂ² et RÂ² ajustÃ©s indiquent si la rÃ©gression passe effectivement par les points. Le RÂ² prend un maximum de 1 lorsque la droite passe exactement sur les points. Enfin, le test de F gÃ©nÃ¨re une p-value indiquant la probabilitÃ© que les coefficients de pente ait Ã©tÃ© gÃ©nÃ©rÃ©s si les vrais coefficients Ã©taient nuls. Dans le cas dâ€™une rÃ©gression univariÃ©e, cela rÃ©pÃ¨te lâ€™information sur lâ€™unique coefficient. On pourra Ã©galement obtenir les intervalles de confiance avec la fonction confint(). confint(modlin_1, level = 0.95) ## 2.5 % 97.5 % ## (Intercept) 64.65001137 67.03641474 ## nitro 0.04629164 0.07714271 Ou soutirer lâ€™information de diffÃ©rentes maniÃ¨res, comme avec la fonction coefficients(). coefficients(modlin_1) ## (Intercept) nitro ## 65.84321305 0.06171718 Ã‰galement, on pourra exÃ©cuter le modÃ¨le sur les donnÃ©es qui ont servi Ã  le gÃ©nÃ©rer: predict(modlin_1)[1:5] ## 1 2 3 4 5 ## 73.95902 73.95902 73.95902 73.95902 73.95902 Ou sur des donnÃ©es externes. nouvelles_donnees &lt;- data.frame(nitro = seq(from = 0, to = 100, by = 5)) predict(modlin_1, newdata = nouvelles_donnees)[1:5] ## 1 2 3 4 5 ## 65.84321 66.15180 66.46038 66.76897 67.07756 5.8.1.2 Analyse des rÃ©sidus Les rÃ©sidus sont les erreurs du modÃ¨le. Câ€™est le vecteur \\(\\epsilon\\), qui est un dÃ©calage entre les donnÃ©es et le modÃ¨le. Le RÂ² est un indicateur de lâ€™ampleur du dÃ©calage, mais une rÃ©gression linÃ©aire explicative en bonne et due forme devrait Ãªtre accompagnÃ©e dâ€™une analyse des rÃ©sidus. On peut les calculÃ©s par \\(\\epsilon = y - \\hat{y}\\), mais aussi bien utiliser la fonction residuals(). res_df &lt;- data.frame(nitro = lasrosas.corn$nitro, residus_lm = residuals(modlin_1), residus_calcul = lasrosas.corn$yield - predict(modlin_1)) sample_n(res_df, 10) ## nitro residus_lm residus_calcul ## 124 66.0 -18.5165468 -18.5165468 ## 691 66.0 -7.5965468 -7.5965468 ## 459 53.0 -2.6342234 -2.6342234 ## 2783 39.0 26.5998170 26.5998170 ## 2519 99.8 -20.7825873 -20.7825873 ## 954 0.0 -0.1132131 -0.1132131 ## 2039 0.0 -16.9032131 -16.9032131 ## 651 131.5 2.6109781 2.6109781 ## 2856 39.0 36.5598170 36.5598170 ## 128 66.0 -20.2665468 -20.2665468 Dans une bonne rÃ©gression linÃ©aire, on ne retrouvera pas de structure identifiable dans les rÃ©sidus, câ€™est-Ã -dire que les rÃ©sidus sont bien distribuÃ©s de part et dâ€™autre du modÃ¨le de rÃ©gression. ggplot(res_df, aes(x = nitro, y = residus_lm)) + geom_point() + labs(x = &quot;Dose N&quot;, y = &quot;RÃ©sidus&quot;) + geom_hline(yintercept = 0, col = &quot;red&quot;, size = 1) Bien que le jugement soit subjectif, on peut dire avec confiance quâ€™il nâ€™y a pas structure particuliÃ¨re. En revanche, on pourrait gÃ©nÃ©rer un \\(y\\) qui varie de maniÃ¨re quadratique avec \\(x\\), un modÃ¨le linÃ©aire montrera une structure Ã©vidente. set.seed(36164) x &lt;- 0:100 y &lt;- 10 + x*1 + x^2 * 0.05 + rnorm(length(x), 0, 50) modlin_2 &lt;- lm(y ~ x) ggplot(data.frame(x, residus = residuals(modlin_2)), aes(x = x, y = residus)) + geom_point() + labs(x = &quot;x&quot;, y = &quot;RÃ©sidus&quot;) + geom_hline(yintercept = 0, col = &quot;red&quot;, size = 1) De mÃªme, les rÃ©sidus ne devraient pas croÃ®tre avec \\(x\\). set.seed(3984) x &lt;- 0:100 y &lt;- 10 + x + x * rnorm(length(x), 0, 2) modlin_3 &lt;- lm(y ~ x) ggplot(data.frame(x, residus = residuals(modlin_3)), aes(x = x, y = residus)) + geom_point() + labs(x = &quot;x&quot;, y = &quot;RÃ©sidus&quot;) + geom_hline(yintercept = 0, col = &quot;red&quot;, size = 1) On pourra aussi inspecter les rÃ©sidus avec un graphique de leur distribution. Reprenons notre modÃ¨le de rendement du maÃ¯s. ggplot(res_df, aes(x = residus_lm)) + geom_histogram(binwidth = 2, color = &quot;white&quot;) + labs(x = &quot;Residual&quot;) Lâ€™histogramme devrait prÃ©senter une distribution normale. Les tests de normalitÃ© comme le test de Shapiro-Wilk peuvent aider, mais ils sont gÃ©nÃ©ralement trÃ¨s sÃ©vÃ¨res. shapiro.test(res_df$residus_lm) ## ## Shapiro-Wilk normality test ## ## data: res_df$residus_lm ## W = 0.94868, p-value &lt; 2.2e-16 Lâ€™hypothÃ¨se nulle que la distribution est normale est rejetÃ©e au seuil 0.05. Dans notre cas, il est Ã©vident que la sÃ©vÃ©ritÃ© du test nâ€™est pas en cause, car les rÃ©sidus semble gÃ©nÃ©rer trois ensembles. Ceci indique que les variables explicatives sont insuffisantes pour expliquer la variabilitÃ© de la variable-rÃ©ponse. 5.8.1.3 RÃ©gression multiple Comme câ€™est le cas pour bien des phÃ©nomÃ¨nes en Ã©cologie, le rendement dâ€™une culture nâ€™est certainement pas expliquÃ© seulement par la dose dâ€™azote. Lorsque lâ€™on combine plusieurs variables explicatives, on crÃ©e un modÃ¨le de rÃ©gression multivariÃ©e, ou une rÃ©gression multiple. Bien que les tendances puissent sembler non-linÃ©aires, lâ€™ajout de variables et le calcul des coefficients associÃ©s reste un problÃ¨me dâ€™algÃ¨bre linÃ©aire. On pourra en effet gÃ©nÃ©raliser les modÃ¨les linÃ©aires, univariÃ©s et multivariÃ©s, de la maniÃ¨re suivante. \\[ y = X \\beta + \\epsilon \\] oÃ¹: \\(X\\) est la matrice du modÃ¨le Ã  \\(n\\) observations et \\(p\\) variables. \\[ X = \\left( \\begin{matrix} 1 &amp; x_{11} &amp; \\cdots &amp; x_{1p} \\\\ 1 &amp; x_{21} &amp; \\cdots &amp; x_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; \\cdots &amp; x_{np} \\end{matrix} \\right) \\] \\(\\beta\\) est la matrice des \\(p\\) coefficients, \\(\\beta_0\\) Ã©tant lâ€™intercept qui multiplie la premiÃ¨re colonne de la matrice \\(X\\). \\[ \\beta = \\left( \\begin{matrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{matrix} \\right) \\] \\(\\epsilon\\) est lâ€™erreur de chaque observation. \\[ \\epsilon = \\left( \\begin{matrix} \\epsilon_0 \\\\ \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n \\end{matrix} \\right) \\] 5.8.1.4 ModÃ¨les linÃ©aires univariÃ©s avec variable catÃ©gorielle nominale Une variable catÃ©gorielle nominale (non ordonnÃ©e) utilisÃ©e Ã  elle seule dans un modÃ¨le comme variable explicative, est un cas particulier de rÃ©gression multiple. En effet, lâ€™encodage catÃ©goriel (ou dummyfication) transforme une variable catÃ©gorielle nominale en une matrice de modÃ¨le comprenant une colonne dÃ©signant lâ€™intercept (une sÃ©rie de 1) dÃ©signant la catÃ©gorie de rÃ©fÃ©rence, ainsi que des colonnes pour chacune des autres catÃ©gories dÃ©signant lâ€™appartenance (1) ou la non appartenance (0) de la catÃ©gorie dÃ©signÃ©e par la colonne. 5.8.1.4.1 Lâ€™encodage catÃ©goriel Une variable Ã  \\(C\\) catÃ©gories pourra Ãªtre dÃ©clinÃ©e en \\(C\\) variables dont chaque colonne dÃ©signe par un 1 lâ€™appartenance au groupe de la colonne et par un 0 la non-appartenance. Pour lâ€™exemple, crÃ©ons un vecteur dÃ©signant le cultivar de pomme de terre. data &lt;- data.frame(cultivar = c(&#39;Superior&#39;, &#39;Superior&#39;, &#39;Superior&#39;, &#39;Russet&#39;, &#39;Kenebec&#39;, &#39;Russet&#39;)) model.matrix(~cultivar, data) ## (Intercept) cultivarRusset cultivarSuperior ## 1 1 0 1 ## 2 1 0 1 ## 3 1 0 1 ## 4 1 1 0 ## 5 1 0 0 ## 6 1 1 0 ## attr(,&quot;assign&quot;) ## [1] 0 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$cultivar ## [1] &quot;contr.treatment&quot; Nous avons trois catÃ©gories, encodÃ©es en trois colonnes. La premiÃ¨re colonne est un intercept et les deux autres dÃ©crivent lâ€™absence (0) ou la prÃ©sence (1) des cultivars Russet et Superior. Le cultivar Kenebec est absent du tableau. En effet, en partant du principe que lâ€™appartenance Ã  une catÃ©gorie est mutuellement exclusive, câ€™est-Ã -dire quâ€™un Ã©chantillon ne peut Ãªtre assignÃ© quâ€™Ã  une seule catÃ©gorie, on peut dÃ©duire une catÃ©gorie Ã  partir de lâ€™information sur toutes les autres. Par exemple, si cultivar_Russet et cultivar_Superior sont toutes deux Ã©gales Ã  \\(0\\), on conclura que cultivar_Kenebec est nÃ©cessairement Ã©gal Ã  \\(1\\). Et si lâ€™un dâ€™entre cultivar_Russet et cultivar_Superior est Ã©gal Ã  \\(1\\), cultivar_Kenebec est nÃ©cessairement Ã©gal Ã  \\(0\\). Lâ€™information contenue dans un nombre \\(C\\) de catÃ©gorie peut Ãªtre encodÃ©e dans un nombre \\(C-1\\) de colonnes. Câ€™est pourquoi, dans une analyse statistique, on dÃ©signera une catÃ©gorie comme une rÃ©fÃ©rence, que lâ€™on dÃ©tecte lorsque toutes les autres catÃ©gories sont encodÃ©es avec des \\(0\\): cette rÃ©fÃ©rence sera incluse dans lâ€™intercept. La catÃ©gorie de rÃ©fÃ©rence par dÃ©faut en R est celle la premiÃ¨re catÃ©gorie dans lâ€™ordre alphabÃ©tique. On pourra modifier cette rÃ©fÃ©rence avec la fonction relevel(). data$cultivar &lt;- relevel(data$cultivar, ref = &quot;Superior&quot;) model.matrix(~cultivar, data) ## (Intercept) cultivarKenebec cultivarRusset ## 1 1 0 0 ## 2 1 0 0 ## 3 1 0 0 ## 4 1 0 1 ## 5 1 1 0 ## 6 1 0 1 ## attr(,&quot;assign&quot;) ## [1] 0 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$cultivar ## [1] &quot;contr.treatment&quot; Pour certains modÃ¨les, vous devrez vous assurer vous-mÃªme de lâ€™encodage catÃ©goriel. Pour dâ€™autre, en particulier avec lâ€™interface par formule de R, ce sera fait automatiquement. 5.8.1.4.2 Exemple dâ€™application Prenons la topographie du terrain, qui peut prendre plusieurs niveaux. levels(lasrosas.corn$topo) ## [1] &quot;E&quot; &quot;HT&quot; &quot;LO&quot; &quot;W&quot; Explorons le rendement selon la topographie. ggplot(lasrosas.corn, aes(x = topo, y = yield)) + geom_boxplot() Les diffÃ©rences sont Ã©videntes, et la modÃ©lisation devrait montrer des effets significatifs. Lâ€™encodage catÃ©goriel peut Ãªtre visualisÃ© en gÃ©nÃ©rant la matrice de modÃ¨le avec la fonction model.matrix() et lâ€™interface-formule - sans la variable-rÃ©ponse. model.matrix(~ topo, data = lasrosas.corn) %&gt;% tbl_df() %&gt;% # tbl_df pour transformer la matrice en tableau sample_n(10) ## # A tibble: 10 x 4 ## `(Intercept)` topoHT topoLO topoW ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 1 0 ## 2 1 0 0 1 ## 3 1 0 0 0 ## 4 1 0 0 1 ## 5 1 0 0 1 ## 6 1 0 0 1 ## 7 1 0 0 0 ## 8 1 0 0 0 ## 9 1 1 0 0 ## 10 1 0 0 1 Dans le cas dâ€™un modÃ¨le avec une variable catÃ©gorielle nominale seule, lâ€™intercept reprÃ©sente la catÃ©gorie de rÃ©fÃ©rence, ici E. Les autres colonnes spÃ©cifient lâ€™appartenance (1) ou la non-appartenance (0) de la catÃ©gorie pour chaque observation. Cette matrice de modÃ¨le utilisÃ©e pour la rÃ©gression donnera un intercept, qui indiquera lâ€™effet de la catÃ©gorie de rÃ©fÃ©rence, puis les diffÃ©rences entre les catÃ©gories subsÃ©quentes et la catÃ©gorie de rÃ©fÃ©rence. modlin_4 &lt;- lm(yield ~ topo, data = lasrosas.corn) summary(modlin_4) ## ## Call: ## lm(formula = yield ~ topo, data = lasrosas.corn) ## ## Residuals: ## Min 1Q Median 3Q Max ## -47.371 -11.933 -1.593 11.080 44.119 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 78.6653 0.5399 145.707 &lt;2e-16 *** ## topoHT -30.0526 0.7500 -40.069 &lt;2e-16 *** ## topoLO 6.2832 0.7293 8.615 &lt;2e-16 *** ## topoW -11.8841 0.7039 -16.883 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.59 on 3439 degrees of freedom ## Multiple R-squared: 0.4596, Adjusted R-squared: 0.4591 ## F-statistic: 975 on 3 and 3439 DF, p-value: &lt; 2.2e-16 Le modÃ¨le linÃ©aire est Ã©quivalent Ã  lâ€™anova, mais les rÃ©sultats de lm sont plus Ã©laborÃ©s. summary(aov(yield ~ topo, data = lasrosas.corn)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## topo 3 622351 207450 975 &lt;2e-16 *** ## Residuals 3439 731746 213 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Lâ€™analyse de rÃ©sidus peut Ãªtre effectuÃ©e de la mÃªme maniÃ¨re. 5.8.1.5 ModÃ¨les linÃ©aires univariÃ©s avec variable catÃ©gorielle ordinale Bien que jâ€™introduise la rÃ©gression sur variable catÃ©gorielle ordinale Ã  la suite de la section sur les variables nominales, nous revenons dans ce cas Ã  une rÃ©gression simple, univariÃ©e. Voyons un cas Ã  5 niveaux. statut &lt;- c(&quot;Totalement en dÃ©saccord&quot;, &quot;En dÃ©saccord&quot;, &quot;Ni en accord, ni en dÃ©saccord&quot;, &quot;En accord&quot;, &quot;Totalement en accord&quot;) statut_o &lt;- factor(statut, levels = statut, ordered=TRUE) model.matrix(~statut_o) # ou bien, sans passer par model.matrix, contr.poly(5) oÃ¹ 5 est le nombre de niveaux ## (Intercept) statut_o.L statut_o.Q statut_o.C statut_o^4 ## 1 1 -0.6324555 0.5345225 -3.162278e-01 0.1195229 ## 2 1 -0.3162278 -0.2672612 6.324555e-01 -0.4780914 ## 3 1 0.0000000 -0.5345225 -4.095972e-16 0.7171372 ## 4 1 0.3162278 -0.2672612 -6.324555e-01 -0.4780914 ## 5 1 0.6324555 0.5345225 3.162278e-01 0.1195229 ## attr(,&quot;assign&quot;) ## [1] 0 1 1 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$statut_o ## [1] &quot;contr.poly&quot; La matrice de modÃ¨le a 5 colonnes, soit le nombre de niveaux: un intercept, puis 4 autres dÃ©signant diffÃ©rentes valeurs que peuvent prendre les niveaux. Ces niveaux croient-ils linÃ©airement? De maniÃ¨re quadratique, cubique ou plus loin dans des distributions polynomiales? modmat_tidy &lt;- data.frame(statut, model.matrix(~statut_o)[, -1]) %&gt;% gather(variable, valeur, -statut) modmat_tidy$statut &lt;- factor(modmat_tidy$statut, levels = statut, ordered=TRUE) ggplot(data = modmat_tidy, mapping = aes(x = statut, y = valeur)) + facet_wrap(. ~ variable) + geom_point() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) RÃ¨gle gÃ©nÃ©rale, pour les variables ordinales, on prÃ©fÃ©rera une distribution linÃ©aire, et câ€™est lâ€™option par dÃ©faut de la fonction lm(). Lâ€™utilisation dâ€™une autre distribution peut Ãªtre effectuÃ©e Ã  la mitaine en utilisant dans le modÃ¨le la colonne dÃ©sirÃ©e de la sortie de la fonction model.matrix(). 5.8.1.6 RÃ©gression multiple Ã  plusieurs variables Reprenons le tableau de donnÃ©es du rendement de maÃ¯s. head(lasrosas.corn) ## year lat long yield nitro topo bv rep nf ## 1 1999 -33.05113 -63.84886 72.14 131.5 W 162.60 R1 N5 ## 2 1999 -33.05115 -63.84879 73.79 131.5 W 170.49 R1 N5 ## 3 1999 -33.05116 -63.84872 77.25 131.5 W 168.39 R1 N5 ## 4 1999 -33.05117 -63.84865 76.35 131.5 W 176.68 R1 N5 ## 5 1999 -33.05118 -63.84858 75.55 131.5 W 171.46 R1 N5 ## 6 1999 -33.05120 -63.84851 70.24 131.5 W 170.56 R1 N5 Pour ajouter des variables au modÃ¨le dans lâ€™interface-formule, on additionne les noms de colonne. La variable lat dÃ©signe la latitude, la variable long dÃ©signe la latitude et la variable bv (brightness value) dÃ©signe la teneur en matiÃ¨re organique du sol (plus bv est Ã©levÃ©e, plus faible est la teneur en matiÃ¨re organique). modlin_5 &lt;- lm(yield ~ lat + long + nitro + topo + bv, data = lasrosas.corn) summary(modlin_5) ## ## Call: ## lm(formula = yield ~ lat + long + nitro + topo + bv, data = lasrosas.corn) ## ## Residuals: ## Min 1Q Median 3Q Max ## -48.405 -11.071 -1.251 10.592 40.078 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.946e+05 3.309e+04 5.882 4.45e-09 *** ## lat 5.541e+03 4.555e+02 12.163 &lt; 2e-16 *** ## long 1.776e+02 4.491e+02 0.395 0.693 ## nitro 6.867e-02 5.451e-03 12.597 &lt; 2e-16 *** ## topoHT -2.665e+01 1.087e+00 -24.520 &lt; 2e-16 *** ## topoLO 5.565e+00 1.035e+00 5.378 8.03e-08 *** ## topoW -1.465e+01 1.655e+00 -8.849 &lt; 2e-16 *** ## bv -5.089e-01 3.069e-02 -16.578 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.47 on 3435 degrees of freedom ## Multiple R-squared: 0.5397, Adjusted R-squared: 0.5387 ## F-statistic: 575.3 on 7 and 3435 DF, p-value: &lt; 2.2e-16 Lâ€™ampleur des coefficients est relatif Ã  lâ€™Ã©chelle de la variable. En effet, un coefficient de 5541 sur la variable lat nâ€™est pas comparable au coefficient de la variable bv, de -0.5089, Ã©tant donnÃ© que les variables ne sont pas exprimÃ©es avec la mÃªme Ã©chelle. Pour les comparer sur une mÃªme base, on peut centrer (soustraire la moyenne) et rÃ©duire (diviser par lâ€™Ã©cart-type). scale_vec &lt;- function(x) as.vector(scale(x)) # la fonction scale gÃ©nÃ¨re une matrice: nous dÃ©sirons un vecteur lasrosas.corn_sc &lt;- lasrosas.corn %&gt;% mutate_at(c(&quot;lat&quot;, &quot;long&quot;, &quot;nitro&quot;, &quot;bv&quot;), scale_vec) modlin_5_sc &lt;- lm(yield ~ lat + long + nitro + topo + bv, data = lasrosas.corn_sc) summary(modlin_5_sc) ## ## Call: ## lm(formula = yield ~ lat + long + nitro + topo + bv, data = lasrosas.corn_sc) ## ## Residuals: ## Min 1Q Median 3Q Max ## -48.405 -11.071 -1.251 10.592 40.078 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 78.9114 0.6666 118.376 &lt; 2e-16 *** ## lat 3.9201 0.3223 12.163 &lt; 2e-16 *** ## long 0.3479 0.8796 0.395 0.693 ## nitro 2.9252 0.2322 12.597 &lt; 2e-16 *** ## topoHT -26.6487 1.0868 -24.520 &lt; 2e-16 *** ## topoLO 5.5647 1.0347 5.378 8.03e-08 *** ## topoW -14.6487 1.6555 -8.849 &lt; 2e-16 *** ## bv -4.9253 0.2971 -16.578 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.47 on 3435 degrees of freedom ## Multiple R-squared: 0.5397, Adjusted R-squared: 0.5387 ## F-statistic: 575.3 on 7 and 3435 DF, p-value: &lt; 2.2e-16 Typiquement, les variables catÃ©gorielles, qui ne sont pas mises Ã  lâ€™Ã©chelle, donneront des coefficients plus Ã©levÃ©es, et devrons Ãªtre Ã©valuÃ©es entre elles et non comparativement aux variables mises Ã  lâ€™Ã©chelle. Une maniÃ¨re conviviale de reprÃ©senter des coefficients consiste Ã  crÃ©er un tableau (fonction tibble()) incluant les coefficients ainsi que leurs intervalles de confiance, puis Ã  les porter graphiquement. intervals &lt;- tibble(Estimate = coefficients(modlin_5_sc)[-1], # [-1] enlever l&#39;intercept LL = confint(modlin_5_sc)[-1, 1], # [-1, ] enlever la premiÃ¨re ligne, celle de l&#39;intercept UL = confint(modlin_5_sc)[-1, 2], variable = names(coefficients(modlin_5_sc)[-1])) intervals ## # A tibble: 7 x 4 ## Estimate LL UL variable ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 3.92 3.29 4.55 lat ## 2 0.348 -1.38 2.07 long ## 3 2.93 2.47 3.38 nitro ## 4 -26.6 -28.8 -24.5 topoHT ## 5 5.56 3.54 7.59 topoLO ## 6 -14.6 -17.9 -11.4 topoW ## 7 -4.93 -5.51 -4.34 bv ggplot(data = intervals, mapping = aes(x = Estimate, y = variable)) + geom_vline(xintercept = 0, lty = 2) + geom_segment(mapping = aes(x = LL, xend = UL, y = variable, yend = variable)) + geom_point() + labs(x = &quot;Coefficient standardisÃ©&quot;, y = &quot;&quot;) On y voit quâ€™Ã  lâ€™exception de la variable long, tous les coefficients sont diffÃ©rents de 0. Le coefficient bv est nÃ©gatif, indiquant que plus la valeur de bv est Ã©levÃ© (donc plus le sol est pauvre en matiÃ¨re organique), plus le rendement est faible. Plus la latitude est Ã©levÃ©e (plus on se dirige vers le Nord de lâ€™Argentine), plus le rendement est Ã©levÃ©. La dose dâ€™azote a aussi un effet statistique positif sur le rendement. Quant aux catÃ©gories topographiques, elles sont toutes diffÃ©rentes de la catÃ©gorie E, ne croisant pas le zÃ©ro. De plus, les intervalles de confiance ne se chevauchant pas, on peut conclure en une diffÃ©rence significative dâ€™une Ã  lâ€™autre. Bien sÃ»r, tout cela au seuil de confiance de 0.05. On pourra retrouver des cas oÃ¹ lâ€™effet combinÃ© de plusieurs variables diffÃ¨re de lâ€™effet des deux variables prises sÃ©parÃ©ment. Par exemple, on pourrait Ã©valuer lâ€™effet de lâ€™azote et celui de la topographie dans un mÃªme modÃ¨le, puis y ajouter une interaction entre lâ€™azote et la topographie, qui dÃ©finira des effets supplÃ©mentaires de lâ€™azote selon chaque catÃ©gorie topographique. Câ€™est ce que lâ€™on appelle une interaction. Dans lâ€™interface-formule, lâ€™interaction entre lâ€™azote et la topographie est notÃ©e nitro:topo. Pour ajouter cette interaction, la formule deviendra yield ~ nitro + topo + nitro:topo. Une approche Ã©quivalente est dâ€™utiliser le raccourci yield ~ nitro*topo. modlin_5_sc &lt;- lm(yield ~ nitro*topo, data = lasrosas.corn_sc) summary(modlin_5_sc) ## ## Call: ## lm(formula = yield ~ nitro * topo, data = lasrosas.corn_sc) ## ## Residuals: ## Min 1Q Median 3Q Max ## -47.984 -11.985 -1.388 10.339 40.636 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 78.6999 0.5322 147.870 &lt; 2e-16 *** ## nitro 1.8131 0.5351 3.388 0.000711 *** ## topoHT -30.0052 0.7394 -40.578 &lt; 2e-16 *** ## topoLO 6.2026 0.7190 8.627 &lt; 2e-16 *** ## topoW -11.9628 0.6939 -17.240 &lt; 2e-16 *** ## nitro:topoHT 1.2553 0.7461 1.682 0.092565 . ## nitro:topoLO 0.5695 0.7186 0.792 0.428141 ## nitro:topoW 0.7702 0.6944 1.109 0.267460 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.38 on 3435 degrees of freedom ## Multiple R-squared: 0.4756, Adjusted R-squared: 0.4746 ## F-statistic: 445.1 on 7 and 3435 DF, p-value: &lt; 2.2e-16 Les rÃ©sultats montre des effets de lâ€™azote et des catÃ©gories topographiques, mais il y a davantage dâ€™incertitude sur les interactions, indiquant que lâ€™effet statistique de lâ€™azote est sensiblement le mÃªme indÃ©pendamment des niveaux topographiques. 5.8.1.7 Attention Ã  ne pas surcharger le modÃ¨le Il est possible dâ€™ajouter des interactions doubles, triples, quadruples, etc. Mais plus il y a dâ€™interactions, plus votre modÃ¨le comprendra de variables et vos tests dâ€™hypothÃ¨se perdront en puissance statistique. 5.8.1.8 Les modÃ¨les linÃ©aires gÃ©nÃ©ralisÃ©s Dans un modÃ¨le linÃ©aire ordinaire, un changement constant dans les variables explicatives rÃ©sulte en un changement constant de la variable-rÃ©ponse. Cette supposition ne serait pas adÃ©quate si la variable-rÃ©ponse Ã©tait un dÃ©compte, si elle est boolÃ©enne ou si, de maniÃ¨re gÃ©nÃ©rale, la variable-rÃ©ponse ne suivait pas une distribution continue. Ou, de maniÃ¨re plus spÃ©cifique, il nâ€™y a pas moyen de retrouver une distribution normale des rÃ©sidus? On pourra bien sÃ»r transformer les variables (sujet du chapitre 6, en dÃ©veloppement). Mais il pourrait sâ€™avÃ©rer impossible, ou tout simplement non souhaitable de transformer les variables. Le modÃ¨le linÃ©aire gÃ©nÃ©ralisÃ© (MLG, ou generalized linear model - GLM) est une gÃ©nÃ©ralisation du modÃ¨le linÃ©aire ordinaire chez qui la variable-rÃ©ponse peut Ãªtre caractÃ©risÃ© par une distribution de Poisson, de Bernouilli, etc. Prenons dâ€™abord cas dâ€™un dÃ©compte de vers fil-de-fer (worms) retrouvÃ©s dans des parcelles sous diffÃ©rents traitements (trt). Les dÃ©comptes sont typiquement distribuÃ© selon une loi de Poisson. cochran.wireworms %&gt;% ggplot(aes(x = worms)) + geom_histogram(bins = 10) Explorons les dÃ©comptes selon les traitements. cochran.wireworms %&gt;% ggplot(aes(x = trt, y = worms)) + geom_boxplot() Les traitements semble Ã  premiÃ¨re vue avoir un effet comparativement au contrÃ´le. LanÃ§ons un MLG avec la fonction glm(), et spÃ©cifions que la sortie est une distribution de Poisson. modglm_1 &lt;- glm(worms ~ trt, cochran.wireworms, family = &quot;poisson&quot;) summary(modglm_1) ## ## Call: ## glm(formula = worms ~ trt, family = &quot;poisson&quot;, data = cochran.wireworms) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.8279 -0.9455 -0.2862 0.6916 3.1888 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.1823 0.4082 0.447 0.655160 ## trtM 1.6422 0.4460 3.682 0.000231 *** ## trtN 1.7636 0.4418 3.991 6.57e-05 *** ## trtO 1.5755 0.4485 3.513 0.000443 *** ## trtP 1.3437 0.4584 2.931 0.003375 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 64.555 on 24 degrees of freedom ## Residual deviance: 38.026 on 20 degrees of freedom ## AIC: 125.64 ## ## Number of Fisher Scoring iterations: 5 Lâ€™interprÃ©tation spÃ©cifique des coefficients dâ€™une rÃ©gression de Poisson sort du cadre du cours. Il est nÃ©amoins trÃ¨s probable (p-value de ~0.66) quâ€™un intercept (traitement K) de 0.18 ayant une erreur standard de 0.4082 ait Ã©tÃ© gÃ©nÃ©rÃ© depuis une population dont lâ€™intercept est nul. Quant aux autres traitements, leurs effets sont tous significatifs au seuil 0.05, mais peuvent-ils Ãªtre considÃ©rÃ©s comme Ã©quivalents? intervals &lt;- tibble(Estimate = coefficients(modglm_1), # [-1] enlever l&#39;intercept LL = confint(modglm_1)[, 1], # [-1, ] enlever la premiÃ¨re ligne, celle de l&#39;intercept UL = confint(modglm_1)[, 2], variable = names(coefficients(modglm_1))) ## Waiting for profiling to be done... ## Waiting for profiling to be done... intervals ## # A tibble: 5 x 4 ## Estimate LL UL variable ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.182 -0.740 0.888 (Intercept) ## 2 1.64 0.840 2.62 trtM ## 3 1.76 0.972 2.74 trtN ## 4 1.58 0.766 2.56 trtO ## 5 1.34 0.509 2.34 trtP ggplot(data = intervals, mapping = aes(x = Estimate, y = variable)) + geom_vline(xintercept = 0, lty = 2) + geom_segment(mapping = aes(x = LL, xend = UL, y = variable, yend = variable)) + geom_point() + labs(x = &quot;Coefficient&quot;, y = &quot;&quot;) Les intervalles de confiance se superposant, on ne peut pas conclure quâ€™un traitement est liÃ© Ã  une rÃ©duction plus importante de vers quâ€™un autre, au seuil 0.05. Maintenant, Ã  dÃ©faut de trouver un tableau de donnÃ©es plus appropriÃ©, prenons le tableau mtcars, qui rassemble des donnÃ©es sur des modÃ¨les de voitures. La colonne vs, pour v-shaped, inscrit 0 si les pistons sont droit et 1 sâ€™ils sont placÃ©s en V dans le moteur. Peut-on expliquer la forme des pistons selon le poids du vÃ©hicule (wt)? mtcars %&gt;% sample_n(6) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 mtcars %&gt;% ggplot(aes(x = wt, y = vs)) + geom_point() Il semble y avoir une tendance: les vÃ©hicules plus lourds ont plutÃ´t des pistons droits (vs = 0). VÃ©rifions cela. modglm_2 &lt;- glm(vs ~ wt, data = mtcars, family = binomial) summary(modglm_2) ## ## Call: ## glm(formula = vs ~ wt, family = binomial, data = mtcars) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.9003 -0.7641 -0.1559 0.7223 1.5736 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 5.7147 2.3014 2.483 0.01302 * ## wt -1.9105 0.7279 -2.625 0.00867 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.860 on 31 degrees of freedom ## Residual deviance: 31.367 on 30 degrees of freedom ## AIC: 35.367 ## ## Number of Fisher Scoring iterations: 5 Exercice. Analyser les rÃ©sultats. 5.8.1.9 Les modÃ¨les non-linÃ©aires La hauteur dâ€™un arbre en fonction du temps nâ€™est typiquement pas linÃ©aire. Elle tend Ã  croÃ®tre de plus en plus lentement jusquâ€™Ã  un plateau. De mÃªme, le rendement dâ€™une culture traitÃ© avec des doses croissantes de fertilisants tend Ã  atteindre un maximum, puis Ã  se stabiliser. Ces phÃ©nomÃ¨nes ne peuvent pas Ãªtre approximÃ©s par des modÃ¨les linÃ©aires. Examinons les donnÃ©es du tableau engelstad.nitro. engelstad.nitro %&gt;% sample_n(10) ## loc year nitro yield ## 38 Knoxville 1963 67 73.2 ## 40 Knoxville 1963 201 91.2 ## 21 Jackson 1965 134 60.5 ## 35 Knoxville 1962 268 78.4 ## 22 Jackson 1965 201 70.2 ## 12 Jackson 1963 335 87.0 ## 24 Jackson 1965 335 73.0 ## 20 Jackson 1965 67 47.6 ## 15 Jackson 1964 134 55.2 ## 48 Knoxville 1964 335 84.5 engelstad.nitro %&gt;% ggplot(aes(x = nitro, y = yield)) + facet_grid(year ~ loc) + geom_line() + geom_point() Le modÃ¨le de Mitscherlich pourrait Ãªtre utilisÃ©. \\[ y = A \\left( 1 - e^{-R \\left( E + x \\right)} \\right) \\] oÃ¹ \\(y\\) est le rendement, \\(x\\) est la dose, \\(A\\) est lâ€™asymptote vers laquelle la courbe converge Ã  dose croissante, \\(E\\) est lâ€™Ã©quivalent de dose fourni par lâ€™environnement et \\(R\\) est le taux de rÃ©ponse. Explorons la fonction. mitscherlich_f &lt;- function(x, A, E, R) { A * (1 - exp(-R*(E + x))) } x &lt;- seq(0, 350, by = 5) y &lt;- mitscherlich_f(x, A = 75, E = 30, R = 0.02) ggplot(tibble(x, y), aes(x, y)) + geom_point(data = engelstad.nitro, aes(x = nitro, y = yield)) + geom_line() + ylim(c(0, 100)) Exercice. Changez les paramÃ¨tres pour visualiser comment la courbe rÃ©agit. Nous pouvons dÃ©crire le modÃ¨le grÃ¢ce Ã  lâ€™interface formule dans la fonction nls(). Notez que les modÃ¨les non-linÃ©aires demandent des stratÃ©gies de calcul diffÃ©rentes de celles des modÃ¨les linÃ©aires. En tout temps, nous devons identifier des valeurs de dÃ©part raisonnables pour les paramÃ¨tres dans lâ€™argument start. Vous rÃ©ussirez rarement Ã  obtenir une convergence du premier coup avec vos paramÃ¨tres de dÃ©part. Le dÃ©fi est dâ€™en trouver qui permettront au modÃ¨le de converger. Parfois, le modÃ¨le ne convergera jamais. Dâ€™autres fois, il convergera vers des solutions diffÃ©rentes selon les variables de dÃ©part choisies. &lt; #modnl_1 &lt;- nls(yield ~ A * (1 - exp(-R*(E + nitro))), # data = engelstad.nitro, # start = list(A = 50, E = 10, R = 0.2)) Le modÃ¨le ne converge pas. Essayons les valeurs prises plus haut, lors de la crÃ©ation du graphique, qui semblent bien sâ€™ajuster. modnl_1 &lt;- nls(yield ~ A * (1 - exp(-R*(E + nitro))), data = engelstad.nitro, start = list(A = 75, E = 30, R = 0.02)) Bingo! Voyons maintenant le sommaire. summary(modnl_1) ## ## Formula: yield ~ A * (1 - exp(-R * (E + nitro))) ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## A 75.023427 3.331860 22.517 &lt;2e-16 *** ## E 66.164111 27.251592 2.428 0.0184 * ## R 0.012565 0.004881 2.574 0.0127 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12.34 on 57 degrees of freedom ## ## Number of iterations to convergence: 5 ## Achieved convergence tolerance: 8.063e-06 Les paramÃ¨tres sont significativement diffÃ©rents de zÃ©ro au seuil 0.05, et donnent la courbe suivante. x &lt;- seq(0, 350, by = 5) y &lt;- mitscherlich_f(x, A = coefficients(modnl_1)[1], E = coefficients(modnl_1)[2], R = coefficients(modnl_1)[3]) ggplot(tibble(x, y), aes(x, y)) + geom_point(data = engelstad.nitro, aes(x = nitro, y = yield)) + geom_line() + ylim(c(0, 100)) Et les rÃ©sidusâ€¦ tibble(res = residuals(modnl_1)) %&gt;% ggplot(aes(x = res)) + geom_histogram(bins = 20) tibble(nitro = engelstad.nitro$nitro, res = residuals(modnl_1)) %&gt;% ggplot(aes(x = nitro, y = res)) + geom_point() + geom_hline(yintercept = 0, colour = &quot;red&quot;) Les rÃ©sidus ne sont pas distribuÃ©s normalement, mais semble bien partagÃ©s de part et dâ€™autre de la courbe. 5.8.2 ModÃ¨les Ã  effets mixtes Lorsque lâ€™on combine des variables fixes (testÃ©es lors de lâ€™expÃ©rience) et des variables alÃ©atoire (variation des unitÃ©s expÃ©rimentales), on obtient un modÃ¨le mixte. Les modÃ¨les mixtes peuvent Ãªtre univariÃ©s, multivariÃ©s, linÃ©aires ordinaires ou gÃ©nÃ©ralisÃ©s ou non linÃ©aires. Ã€ la diffÃ©rence dâ€™un effet fixe, un effet alÃ©atoire sera toujours distribuÃ© normalement avec une moyenne de 0 et une certaine variance. Dans un modÃ¨le linÃ©aire oÃ¹ lâ€™effet alÃ©atoire est un dÃ©calage dâ€™intercept, cet effet sâ€™additionne aux effets fixes: \\[ y = X \\beta + Z b + \\epsilon \\] oÃ¹: \\(Z\\) est la matrice du modÃ¨le Ã  \\(n\\) observations et \\(p\\) variables alÃ©atoires. Les variables alÃ©atoires sont souvent des variables nominales qui subissent un encodage catÃ©goriel. \\[ Z = \\left( \\begin{matrix} z_{11} &amp; \\cdots &amp; z_{1p} \\\\ z_{21} &amp; \\cdots &amp; z_{2p} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ z_{n1} &amp; \\cdots &amp; z_{np} \\end{matrix} \\right) \\] \\(b\\) est la matrice des \\(p\\) coefficients alÃ©atoires. \\[ b = \\left( \\begin{matrix} b_0 \\\\ b_1 \\\\ \\vdots \\\\ b_p \\end{matrix} \\right) \\] Le tableau lasrosas.corn, utilisÃ© prÃ©cÃ©demment, contenait trois rÃ©pÃ©titions effectuÃ©s au cours de deux annÃ©es, 1999 et 2001. Ã‰tant donnÃ© que la rÃ©pÃ©tition R1 de 1999 nâ€™a rien Ã  voir avec la rÃ©pÃ©tition R1 de 2001, on dit quâ€™elle est emboÃ®tÃ©e dans lâ€™annÃ©e. Le module nlme nous aidera Ã  monter notre modÃ¨le mixte. library(&quot;nlme&quot;) ## ## Attaching package: &#39;nlme&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## collapse mmodlin_1 &lt;- lme(fixed = yield ~ lat + long + nitro + topo + bv, random = ~ 1|year/rep, data = lasrosas.corn) Ã€ ce stade vous devriez commencer Ã  Ãªtre familier avec lâ€™interface formule et vous deviez saisir lâ€™argument fixed, qui dÃ©signe lâ€™effet fixe. Lâ€™effet alÃ©atoire, random, suit un tilde ~. Ã€ gauche de la barre verticale |, on place les variables dÃ©signant les effets alÃ©atoire sur la pente. Nous nâ€™avons pas couvert cet aspect, alors nous le laissons Ã  1. Ã€ droite, on retrouve un structure dâ€™emboÃ®tement dÃ©signant lâ€™effet alÃ©atoire: le premier niveau est lâ€™annÃ©e, dans laquelle est emboÃ®tÃ©e la rÃ©pÃ©tition. summary(mmodlin_1) ## Linear mixed-effects model fit by REML ## Data: lasrosas.corn ## AIC BIC logLik ## 26535.37 26602.93 -13256.69 ## ## Random effects: ## Formula: ~1 | year ## (Intercept) ## StdDev: 20.35425 ## ## Formula: ~1 | rep %in% year ## (Intercept) Residual ## StdDev: 11.17447 11.35617 ## ## Fixed effects: yield ~ lat + long + nitro + topo + bv ## Value Std.Error DF t-value p-value ## (Intercept) -1379436.9 55894.55 3430 -24.679273 0.000 ## lat -25453.0 1016.53 3430 -25.039084 0.000 ## long -8432.3 466.05 3430 -18.092988 0.000 ## nitro 0.0 0.00 3430 1.739757 0.082 ## topoHT -27.7 0.92 3430 -30.122438 0.000 ## topoLO 6.8 0.88 3430 7.804733 0.000 ## topoW -16.7 1.40 3430 -11.944793 0.000 ## bv -0.5 0.03 3430 -19.242424 0.000 ## Correlation: ## (Intr) lat long nitro topoHT topoLO topoW ## lat 0.897 ## long 0.866 0.555 ## nitro 0.366 0.391 0.247 ## topoHT 0.300 -0.017 0.582 0.024 ## topoLO -0.334 -0.006 -0.621 -0.038 -0.358 ## topoW 0.403 -0.004 0.762 0.027 0.802 -0.545 ## bv -0.121 -0.012 -0.214 -0.023 -0.467 0.346 -0.266 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -4.32360269 -0.66781575 -0.07450856 0.61587533 3.96434001 ## ## Number of Observations: 3443 ## Number of Groups: ## year rep %in% year ## 2 6 La sortie est semblable Ã  celle de la fonction lm(). 5.8.2.1 ModÃ¨les mixtes non-linÃ©aires Le modÃ¨le non linÃ©aire crÃ©Ã© plus haut liait le rendement Ã  la dose dâ€™azote. Toutefois, les unitÃ©s expÃ©rimentales (le site loc et lâ€™annÃ©e year) nâ€™Ã©taient pas pris en considÃ©ration. Nous allons maintenant les considÃ©rer. Nous devons dÃ©cider la structure de lâ€™effet alÃ©atoire, et sur quelles variables il doit Ãªtre appliquÃ© - la dÃ©cision appartient Ã  lâ€™analyste. Il me semble plus convenable de supposer que le site et lâ€™annÃ©e affectera le rendement maximum plutÃ´t que lâ€™environnement et le taux: les effets alÃ©atoires seront donc affectÃ©s Ã  la variable A. Les effets alÃ©atoires nâ€™ont pas de structure dâ€™emboÃ®tement. Lâ€™effet de lâ€™annÃ©e sur A sera celui dâ€™une pente et lâ€™effet de site sera celui de lâ€™intercept. La fonction que nous utiliserons est nlme(). mm &lt;- nlme(yield ~ A * (1 - exp(-R*(E + nitro))), data = engelstad.nitro, start = c(A = 75, E = 30, R = 0.02), fixed = list(A ~ 1, E ~ 1, R ~ 1), random = A ~ year | loc) summary(mm) ## Nonlinear mixed-effects model fit by maximum likelihood ## Model: yield ~ A * (1 - exp(-R * (E + nitro))) ## Data: engelstad.nitro ## AIC BIC logLik ## 477.2285 491.8889 -231.6143 ## ## Random effects: ## Formula: A ~ year | loc ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## A.(Intercept) 2.602881636 A.(In) ## A.year 0.003066058 -0.556 ## Residual 11.152760033 ## ## Fixed effects: list(A ~ 1, E ~ 1, R ~ 1) ## Value Std.Error DF t-value p-value ## A.(Intercept) 74.58277 4.722806 56 15.792046 0.0000 ## E 65.57006 25.534747 56 2.567876 0.0129 ## R 0.01308 0.004807 56 2.720245 0.0087 ## Correlation: ## A.(In) E ## E 0.379 ## R -0.483 -0.934 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -1.83376801 -0.89290179 0.07419027 0.68354461 1.82431474 ## ## Number of Observations: 60 ## Number of Groups: 2 Et sur graphique: engelstad.nitro %&gt;% ggplot(aes(x = nitro, y = yield)) + facet_grid(year ~ loc) + geom_line(data = tibble(nitro = engelstad.nitro$nitro, yield = predict(mm, level = 0)), colour = &quot;grey35&quot;) + geom_point() + ylim(c(0, 95)) Les modÃ¨les mixtes non linÃ©aires peuvent devenir trÃ¨s complexes lorsque les paramÃ¨tres, par exemple A, E et R, sont eux-mÃªme affectÃ©s linÃ©airement par des variables (par exemple A ~ topo). Pour aller plus loin, consultez Parent et al. (2017) ainsi que les calculs associÃ©s Ã  lâ€™article. Ou Ã©crivez-moi un courriel pour en discuter! Note. Lâ€™interprÃ©tation de p-values sur les modÃ¨les mixtes est controversÃ©e. Ã€ ce sujet, Douglas Bates a Ã©crit une longue lettre Ã  la communautÃ© de dÃ©veloppement du module lme4, une alternative Ã  nlme, qui remet en cause lâ€™utilisation des p-values, ici. De plus en plus, pour les modÃ¨les mixtes, on se tourne vers les statistiques bayÃ©siennes, couvertes dans le chapitre 6 avec le module greta. Mais en ce qui a trait aux modÃ¨les mixtes, le module brms automatise bien des aspects de lâ€™approche bayÃ©sienne. 5.8.3 Aller plus loin 5.8.3.1 Statistiques gÃ©nÃ©rales: The analysis of biological data 5.8.3.2 Statistiques avec R Disponibles en version Ã©lectronique Ã  la bibliothÃ¨que de lâ€™UniversitÃ© Laval: Introduction aux statistiques avec R: Introductory statistics with R Approfondir les statistiques avec R: The R Book, Second edition Approfondir les modÃ¨les Ã  effets mixtes avec R: Mixed Effects Models and Extensions in Ecology with R ModernDive, un livre en ligne offrant une approche moderne avec le package moderndive. "]
]
