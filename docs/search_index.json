[
["chapitre-biostats.html", "6 Biostatistiques 6.1 Populations et √©chantillons 6.2 Les variables 6.3 Les probabilit√©s 6.4 Les distributions 6.5 Statistiques descriptives 6.6 Tests d‚Äôhypoth√®ses √† un et deux √©chantillons 6.7 L‚Äôanalyse de variance 6.8 Les mod√®les statistiques", " 6 Biostatistiques Ô∏è Objectifs sp√©cifiques: √Ä la fin de ce chapitre, vous serez en mesure de d√©finir les concepts de base en statistique: population, √©chantillon, variable, probabilit√© et distribution serez en mesure de calculer des statistiques descriptives de base: moyenne et √©cart-type, quartiles, maximum et minimum comprendrez les notions de test d‚Äôhypoth√®se, d‚Äôeffet et de p-value, ainsi qu‚Äô√©viter les erreurs communes dans leur interpr√©tation saurez effectuer une mod√©lisation statistique lin√©aire simple, multiple et mixte, entre autre sur des cat√©gories saurez effectuer une mod√©lisation statistique non lin√©aire simple, multiple et mixte Aux chapitres pr√©c√©dents, nous avons vu comment visualiser, organiser et manipuler des tableaux de donn√©es. La statistique est une collection de disciplines li√©es √† la collecte, l‚Äôorganisation, l‚Äôanalyse, l‚Äôinterpr√©tation et la pr√©sentation de donn√©es. Les biostatistiques sont des applications de ces disciplines √† la biosph√®re. Dans Principles and procedures of statistics: A biometrical approach, Steel, Torie et Dickey (1997) d√©finissent les statistiques ainsi: Les statistiques forment la science, pure et appliqu√©e, de la cr√©ation, du d√©veloppement, et de l‚Äôapplication de techniques par lesquelles l‚Äôincertitude de l‚Äôinduction inf√©rentielle peut √™tre √©valu√©e. (ma traduction) Alors que l‚Äôinf√©rence consiste √† g√©n√©raliser des √©chantillons √† l‚Äôensemble d‚Äôune population, l‚Äôinduction est un type de raisonnement qui permet de g√©n√©raliser des observations sous forme de th√©ories. En d‚Äôautres mots, les statistiques permettent d‚Äô√©valuer l‚Äôincertitude sur des processus, de passer par infrence de l‚Äô√©chantillon √† la population, puis par induction de passer de cette repr√©sentation d‚Äôune population en lois g√©n√©rales la concernant. La d√©finition de Whitlock et Schuluter (2015), dans The Analysis of Biological Data, est plus simple et n‚Äôinsiste que sur l‚Äôinf√©rence: La statistique est l‚Äô√©tude des m√©thodes pour mesurer des aspects de populations √† partir d‚Äô√©chantillons et pour quantifier l‚Äôincertitude des mesures. (ma traduction) Les statistiques consistent √† faire du sens (anglicisme assum√©) avec des observations dans l‚Äôobjectif de r√©pondre √† une question que vous aurez formul√©e clairement, pr√©alablement √† votre exp√©rience. The more time I spend as The Statistician in the room, the more I think the best skill you can cultivate is the ability to remain calm and repeatedly ask ‚ÄúWhat question are you trying to answer?‚Äù ‚Äî Bryan Howie (@bryan_howie) 13 d√©cembre 2018 Le flux de travail conventionnel en statistiques consiste √† collecter des √©chantillons, transformer (pr√©traiter) les donn√©es, effectuer des tests, analyser les r√©sultats, les interpr√©ter et les visualiser. \\[Collecte \\rightarrow Pr√©traitement \\rightarrow Tests~statistiques \\rightarrow Analyse \\rightarrow Interpr√©tation \\rightarrow Visualisation \\] Ce chapitre √† lui seul est trop court pour permettre d‚Äôint√©grer toutes les connaissances n√©cessaires √† une utilisation raisonn√©e des statistiques, mais fourni les bases pour aller plus loin. Notez que les erreurs d‚Äôinterpr√©tation statistiques sont courantes et la consultation de sp√©cialistes n‚Äôest souvent pas un luxe. Mais bien que les statistiques soient complexes, la plupart des op√©rations statistiques peuvent √™tre effectu√©es sans l‚Äôassistance de statisticien.ne.s‚Ä¶ √† condition de comprendre suffisamment les concepts utilis√©s. Dans ce chapitre, nous verrons comment r√©pondre correctement √† une question valide et ad√©quate avec l‚Äôaide d‚Äôoutils de calcul scientifique. Nous couvrirons les notions de bases des distributions et des variables al√©atoires qui nous permettront d‚Äôeffectuer des tests statistiques communs avec R. Nous couvrirons aussi les erreurs commun√©ment commises en recherche acad√©mique et les moyens simples de les √©viter. En plus des modules de base de R nous utiliserons les modules de la tidyverse, le module de donn√©es agricoles agridat, ainsi que le module nlme sp√©cialis√© pour la mod√©lisation mixte. Avant de survoler les applications statistiques avec R, je vais rapidement pr√©senter quelques notions importantes en statistiques : populations et √©chantillons, variables, probabilit√©s et distributions. Puis nous allons effectuer des tests d‚Äôhypoth√®se univari√©s (notamment les tests de t et les analyses de variance) et d√©tailler la notion controvers√©e de p-value. Je vais m‚Äôattarder plus longuement aux mod√®les lin√©aires g√©n√©ralis√©s, incluant en particulier des effets fixes et al√©atoires (mod√®les mixtes), qui fournissent une trousse d‚Äôanalyse polyvalente en analyse multivari√©e. Je terminerai avec les perspectives multivari√©es que sont les matrices de covariance et de corr√©lation. 6.1 Populations et √©chantillons Le principe d‚Äôinf√©rence consiste √† g√©n√©raliser des conclusions √† l‚Äô√©chelle d‚Äôune population √† partir d‚Äô√©chantillons issus de cette population. Alors qu‚Äôune population contient tous les √©l√©ments √©tudi√©s, un √©chantillon d‚Äôune population est une observation unique. Une exp√©rience bien con√ßue fera en sorte que les √©chantillons soient repr√©sentatifs de la population qui, la plupart du temps, ne peut √™tre observ√©e enti√®rement pour des raisons pratiques. Les principes d‚Äôexp√©rimentation servant de base √† la conception d‚Äôune bonne m√©thodologie sont pr√©sent√©s dans le cours Dispositifs exp√©rimentaux (BVG-7002). √âgalement, je recommande le livre Principes d‚Äôexp√©rimentation: planification des exp√©riences et analyse de leurs r√©sultats de Pierre Dagnelie (2012), disponible en ligne en format PDF. Un bon aper√ßu des dispositifs exp√©rimentaux est aussi pr√©sent√© dans Introductory Statistics with R, de Peter Dalgaard (2008), que vous pouvez t√©l√©charger du site de la biblioth√®que de l‚ÄôUniversit√© Laval vous avez un identifiant autoris√©. Une population est √©chantillonn√©e pour induire des param√®tres: un rendement typique dans des conditions m√©t√©orologiques, √©daphiques et manag√©riales donn√©es, la masse typique des faucons p√®lerins, m√¢les et femelles, le microbiome typique d‚Äôun sol agricole ou forestier, etc. Une statistique est une estimation d‚Äôun param√®tre calcul√©e √† partir des donn√©es, par exemple une moyenne et un √©cart-type, ou un intercept et une pente. Par exemple, la moyenne (\\(\\mu\\)) et l‚Äô√©cart-type (\\(\\sigma\\)) d‚Äôune population sont estim√©s par les moyennes (\\(\\bar{x}\\)) et √©carts-types (\\(s\\)) calcul√©s sur les donn√©es issues de l‚Äô√©chantillonnage. Chaque param√®tre est li√©e √† une perspective que l‚Äôon d√©sire conna√Ætre chez une population. Ces angles d‚Äôobservations sont les variables. 6.2 Les variables Nous avons abord√© au chapitre 3 la notion de variable par l‚Äôinterm√©diaire d‚Äôune donn√©e. Une variable est l‚Äôobservation d‚Äôune caract√©ristique d√©crivant un √©chantillon. Si la charact√©ristique varie d‚Äôun √©chantillon √† un autre sans que vous en expliquiez la raison (i.e. si identifier la source de la variabilit√© ne fait pas partie de votre exp√©rience), on parlera de variable al√©atoire. M√™me le hasard est r√©gi par certaines lois: ce qui est al√©atoire dans une variable peut √™tre d√©crit par des lois de probabilit√©, que nous verrons plus bas. Mais restons aux variables pour l‚Äôinstant. Par convention, on peut attribuer aux variables un symbole math√©matique. Par exemple, on peut donner √† la masse volumique d‚Äôun sol (qui est le r√©sultat d‚Äôune m√©thodologie pr√©cise) le symbole \\(\\rho\\). Lorsque l‚Äôon attribue une valeur √† \\(\\rho\\), on parle d‚Äôune donn√©e. Chaque donn√©e d‚Äôune observation a un indice qui lui est propre, que l‚Äôon d√©signe souvent par \\(i\\), que l‚Äôon place en indice \\(\\rho_i\\). Pour la premi√®re donn√©e, on a \\(i=1\\), donc \\(\\rho_1\\). Pour un nombre \\(n\\) d‚Äô√©chantillons, on aura \\(\\rho_1\\), \\(\\rho_2\\), \\(\\rho_3\\), ‚Ä¶, \\(\\rho_n\\), formant le vecteur \\(\\rho = \\left[\\rho_1, \\rho_2, \\rho_3, ..., \\rho_n \\right]\\). En R, une variable est associ√©e √† un vecteur ou une colonne d‚Äôun tableau. rho &lt;- c(1.34, 1.52, 1.26, 1.43, 1.39) # matrice 1D data &lt;- data.frame(rho = rho) # tableau data ## rho ## 1 1.34 ## 2 1.52 ## 3 1.26 ## 4 1.43 ## 5 1.39 Il existe plusieurs types de variables, qui se regroupe en deux grandes cat√©gories: les variables quantitatives et les variables qualitatives. 6.2.1 Variables quantitatives Ces variables peuvent √™tre continues dans un espace √©chantillonnal r√©el ou discr√®tes dans un espace √©chantillonnal ne consid√©rant que des valeurs fixes. Notons que la notion de nombre r√©el est toujours une approximation en sciences exp√©rimentales comme en calcul num√©rique, √©tant donn√©e que l‚Äôon est limit√© par la pr√©cision des appareils comme par le nombre d‚Äôoctets √† utiliser. Bien que les valeurs fixes des distributions discr√®tes ne soient pas toujours des valeurs enti√®res, c‚Äôest bien souvent le cas en biostatistiques comme en d√©mographie, o√π les d√©comptes d‚Äôindividus sont souvent pr√©sents (et o√π la notion de fraction d‚Äôindividus n‚Äôest pas accept√©e). 6.2.2 Variables qualitatives On exprime parfois qu‚Äôune variable qualitative est une variable impossible √† mesurer num√©riquement: une couleur, l‚Äôappartenance √† esp√®ce ou √† une s√©rie de sol. Pourtant, dans bien des cas, les variables qualitatives peut √™tre encod√©es en variables quantitatives. Par exemple, on peut accoler des pourcentages de sable, limon et argile √† un loam sableux, qui autrement est d√©crit par la classe texturale d‚Äôun sol. Pour une couleur, on peut lui associer une longueur d‚Äôonde ou des pourcentages de rouge, vert et bleu, ainsi qu‚Äôun ton. En ce qui a trait aux variables ordonn√©es, il est possible de supposer un √©talement. Par exemple, une variable d‚Äôintensit√© faible-moyenne-forte peut √™tre transform√©e lin√©airement en valeurs quantitatives -1, 0 et 1. Attention toutefois, l‚Äô√©talement peut parfois √™tre quadratique ou logarithmique. Les s√©ries de sol peuvent √™tre encod√©es par la proportion de gleyfication (Parent et al., 2017). Quant aux cat√©gories difficilement transformables en quantit√©s, on pourra passer par l‚Äôencodage cat√©goriel, souvent appel√© dummyfication, qui nous verrons plus loin. L‚Äôanalyse qualitative consiste en l‚Äôanalyse de verbatims, essentiellement utile en sciences sociales: nous n‚Äôen n‚Äôaurons pas besoin ici. Nous consid√©rerons les variables qualitatives comme des variables quantitatives qui n‚Äôont pas subi de pr√©traitement. 6.3 Les probabilit√©s ¬´ Nous sommes si √©loign√©s de conna√Ætre tous les agens de la nature, et leurs divers modes d‚Äôaction ; qu‚Äôil ne serait pas philosophique de nier les ph√©nom√®nes, uniquement parce qu‚Äôils sont inexplicables dans l‚Äô√©tat actuel de nos connaissances. Seulement, nous devons les examiner avec une attention d‚Äôautant plus scrupuleuse, qu‚Äôil para√Æt plus difficile de les admettre ; et c‚Äôest ici que le calcul des probabilit√©s devient indispensable, pour d√©terminer jusqu‚Äô√† quel point il faut multiplier les observations ou les exp√©riences, afin d‚Äôobtenir en faveur des agens qu‚Äôelles indiquent, une probabilit√© sup√©rieure aux raisons que l‚Äôon peut avoir d‚Äôailleurs, de ne pas les admettre. ¬ª ‚Äî Pierre-Simon de Laplace Une probabilit√© est la vraisemblance qu‚Äôun √©v√®nement se r√©alise chez un √©chantillon. Les probabilit√©s forment le cadre des syst√®mes stochastiques, c‚Äôest-√†-dire des syst√®mes trop complexes pour en conna√Ætre exactement les aboutissants, auxquels on attribue une part de hasard. Ces syst√®mes sont pr√©dominants dans les processus vivants. On peut d√©gager deux perspectives sur les probabilit√©s: l‚Äôune passe par une interpr√©tation fr√©quentielle, l‚Äôautre bay√©sienne. L‚Äôinterpr√©tation fr√©quentielle repr√©sente la fr√©quence des occurrences apr√®s un nombre infini d‚Äô√©v√®nements. Par exemple, si vous jouez √† pile ou face un grand nombre de fois, le nombre de pile sera √©gal √† la moiti√© du nombre de lanc√©s. L‚Äôapproche fr√©quentielle teste si les donn√©es concordent avec un mod√®le du r√©el. Il s‚Äôagit de l‚Äôinterpr√©tation commun√©ment utilis√©e. L‚Äôinterpr√©tation bay√©sienne vise √† quantifier l‚Äôincertitude des ph√©nom√®nes. Dans cette perspective, plus l‚Äôinformation s‚Äôaccumule, plus l‚Äôincertitude diminue. Cette approche gagne en notori√©t√© notamment parce qu‚Äôelle permet de d√©crire des ph√©nom√®nes qui, intrins√®quement, ne peuvent √™tre r√©p√©t√©s infiniment (absence d‚Äôasymptote), comme celles qui sont bien d√©finis dans le temps ou sur des populations limit√©s. L‚Äôapproche bay√©sienne √©value la probabilit√© que le mod√®le soit r√©el. Une erreur courante consiste √† aborder des statistiques fr√©quentielles comme des statistiques bay√©siennes. Par exemple, si l‚Äôon d√©sire √©valuer la probabilit√© de l‚Äôexistence de vie sur Mars, on devra passer par le bay√©sien, car avec les stats fr√©quentielles, l‚Äôon devra plut√¥t conclure si les donn√©es sont conformes ou non avec l‚Äôhypoth√®se de la vie sur Mars (exemple tir√©e du blogue Dynamic Ecology). Des rivalit√©s factices s‚Äôinstallent enter les tenants des diff√©rentes approches, dont chacune, en r√©alit√©, r√©pond √† des questions diff√©rentes dont il convient r√©fl√©chir sur les limitations. Bien que les statistiques bay√©siennes soient de plus en plus utilis√©es, nous ne couvrirons dans ce chapitre que l‚Äôapproche fr√©quentielle. L‚Äôapproche bay√©sienne est n√©anmoins trait√©e dans le chapitre 7, qui est facultatif au cours. 6.4 Les distributions Une variable al√©atoire peut prendre des valeurs selon des mod√®les de distribution des probabilit√©s. Une distribution est une fonction math√©matique d√©crivant la probabilit√© d‚Äôobserver une s√©rie d‚Äô√©v√®nements. Ces √©v√®nements peuvent √™tre des valeurs continues, des nombres entiers, des cat√©gories, des valeurs bool√©ennes (Vrai/Faux), etc. D√©pendemment du type de valeur et des observations obtenues, on peut associer des variables √† diff√©rentes lois de probabilit√©. Toujours, l‚Äôaire sous la courbe d‚Äôune distribution de probabilit√© est √©gale √† 1. En statistiques inf√©rentielles, les distributions sont les mod√®les, comprenant certains param√®tres comme la moyenne et la variance pour les distributions normales, √† partir desquelles les donn√©es sont g√©n√©r√©es. Il existe deux grandes familles de distribution: discr√®tes et continues. Les distributions discr√®tes sont contraintes √† des valeurs pr√©d√©finies (finies ou infinies), alors que les distributions continues prennent n√©cessairement un nombre infini de valeur, dont la probabilit√© ne peut pas √™tre √©valu√©e ponctuellement, mais sur un intervalle. L‚Äôesp√©rance math√©matique est une fonction de tendance centrale, souvent d√©crite par un param√®tre. Il s‚Äôagit de la moyenne d‚Äôune population pour une distribution normale. La variance, quant √† elle, d√©crit la variabilit√© d‚Äôune population, i.e. son √©talement autour de l‚Äôesp√©rance. Pour une distribution normale, la variance d‚Äôune population est aussi appel√©e variance, souvent pr√©sent√©e par l‚Äô√©cart-type (√©gal √† la racine carr√©e de la variance). 6.4.1 Distribution binomiale En tant que sc√©nario √† deux issues possibles, des tirages √† pile ou face suivent une loi binomiale, comme toute variable bool√©enne prenant une valeur vraie ou fausse. En biostatistiques, les cas communs sont la pr√©sence/absence d‚Äôune esp√®ce, d‚Äôune maladie, d‚Äôun trait phylog√©n√©tique, ainsi que les cat√©gories encod√©es. Lorsque l‚Äôop√©ration ne comprend qu‚Äôun seul √©chantillon (i.e. un seul tirage √† pile ou face), il s‚Äôagit d‚Äôun cas particulier d‚Äôune loi binomiale que l‚Äôon nomme une loi de Bernouilli. Pour 25 tirages √† pile ou face ind√©pendants (i.e. dont l‚Äôordre des tirages ne compte pas), on peut dessiner une courbe de distribution dont la somme des probabilit√©s est de 1. La fonction dbinom est une fonction de distribution de probabilit√©s. Les fonctions de distribution de probabilit√©s discr√®tes sont appel√©es des fonctions de masse. library(&quot;tidyverse&quot;) ## -- Attaching packages ----------------------------------- tidyverse 1.3.0 -- ## v ggplot2 3.3.0 v purrr 0.3.4 ## v tibble 3.0.1 v dplyr 0.8.5 ## v tidyr 1.1.0 v stringr 1.4.0 ## v readr 1.3.1 v forcats 0.5.0 ## -- Conflicts -------------------------------------- tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() x &lt;- 0:25 y &lt;- dbinom(x = x, size = 25, prob = 0.5) print(paste(&#39;La somme des probabilit√©s est de&#39;, sum(y))) ## [1] &quot;La somme des probabilit√©s est de 1&quot; ggplot(data = tibble(x, y), mapping = aes(x, y)) + geom_segment(aes(x = x, xend = x, y = 0, yend = y), color = &quot;grey50&quot;) + geom_point() 6.4.2 Distribution de Poisson La loi de Poisson (avec un P majuscule, introduite par le math√©maticien fran√ßais Sim√©on Denis Poisson et non pas l‚Äôanimal) d√©crit des distributions discr√®tes de probabilit√© d‚Äôun nombre d‚Äô√©v√®nements se produisant dans l‚Äôespace ou dans le temps. Les distributions de Poisson d√©crivent ce qui tient du d√©compte. Il peut s‚Äôagir du nombre de grenouilles traversant une rue quotidiennement, du nombre de plants d‚Äôascl√©piades se trouvant sur une terre cultiv√©e, ou du nombre d‚Äô√©v√®nements de pr√©cipitation au mois de juin, etc. La distribution de Poisson n‚Äôa qu‚Äôun seul param√®tre, \\(\\lambda\\), qui d√©crit tant la moyenne des d√©comptes. Par exemple, en un mois de 30 jours, et une moyenne de 8 √©v√®nements de pr√©cipitation pour ce mois, on obtient la distribution suivante. x &lt;- 1:30 y &lt;- dpois(x, lambda = 8) print(paste(&#39;La somme des probabilit√©s est de&#39;, sum(y))) ## [1] &quot;La somme des probabilit√©s est de 0.999664536835124&quot; ggplot(data = data.frame(x, y), mapping = aes(x, y)) + geom_segment(aes(x = x, xend = x, y = 0, yend = y), color = &quot;grey50&quot;) + geom_point() 6.4.3 Distribution uniforme La distribution la plus simple est probablement la distribution uniforme. Si la variable est discr√®te, chaque cat√©gorie est associ√©e √† une probabilit√© √©gale. Si la variable est continue, la probabilit√© est directement proportionnelle √† la largeur de l‚Äôintervalle. On utilise rarement la distribution uniforme en biostatistiques, sinon pour d√©crire des a priori vagues pour l‚Äôanalyse bay√©sienne (ce sujet est trait√© dans le chapitre 7). Nous utilisons la fonction dunif. √Ä la diff√©rence des distributions discr√®tes, les fonctions de distribution de probabilit√©s continues sont appel√©es des fonctions de densit√© d‚Äôune loi de probabilit√© (probability density function). increment &lt;- 0.01 x &lt;- seq(-4, 4, by = increment) y1 &lt;- dunif(x, min = -3, max = 3) y2 &lt;- dunif(x, min = -2, max = 2) y3 &lt;- dunif(x, min = -1, max = 1) print(paste(&#39;La somme des probabilit√©s est de&#39;, sum(y3 * increment))) ## [1] &quot;La somme des probabilit√©s est de 1.005&quot; gg_unif &lt;- data.frame(x, y1, y2, y3) %&gt;% gather(variable, value, -x) ggplot(data = gg_unif, mapping = aes(x = x, y = value)) + geom_line(aes(colour = variable)) 6.4.4 Distribution normale La plus r√©pandue de ces lois est probablement la loi normale, parfois nomm√©e loi gaussienne et plus rarement loi laplacienne. Il s‚Äôagit de la distribution classique en forme de cloche. La loi normale est d√©crite par une moyenne, qui d√©signe la tendance centrale, et une variance, qui d√©signe l‚Äô√©talement des probabilit√©s autour de la moyenne. La racine carr√©e de la variance est l‚Äô√©cart-type. Les distributions de mesures exclusivement positives (comme le poids ou la taille) sont parfois avantageusement approxim√©es par une loi log-normale, qui est une loi normale sur le logarithme des valeurs: la moyenne d‚Äôune loi log-normale est la moyenne g√©om√©trique. increment &lt;- 0.01 x &lt;- seq(-10, 10, by = increment) y1 &lt;- dnorm(x, mean = 0, sd = 1) y2 &lt;- dnorm(x, mean = 0, sd = 2) y3 &lt;- dnorm(x, mean = 0, sd = 3) print(paste(&#39;La somme des probabilit√©s est de&#39;, sum(y3 * increment))) ## [1] &quot;La somme des probabilit√©s est de 0.999147010743368&quot; gg_norm &lt;- data.frame(x, y1, y2, y3) %&gt;% gather(variable, value, -x) ggplot(data = gg_norm, mapping = aes(x = x, y = value)) + geom_line(aes(colour = variable)) Quelle est la probabilit√© d‚Äôobtenir le nombre 0 chez une observation continue distribu√©e normalement dont la moyenne est 0 et l‚Äô√©cart-type est de 1? R√©ponse: 0. La loi normale √©tant une distribution continue, les probabilit√©s non-nulles ne peuvent √™tre calcul√©s que sur des intervalles. Par exemple, la probabilit√© de retrouver une valeur dans l‚Äôintervalle entre -1 et 2 est calcul√©e en soustrayant la probabilit√© cumul√©e √† -1 de la probabilit√© cumul√©e √† 2. increment &lt;- 0.01 x &lt;- seq(-5, 5, by = increment) y &lt;- dnorm(x, mean = 0, sd = 1) prob_between &lt;- c(-1, 2) gg_norm &lt;- data.frame(x, y) gg_auc &lt;- gg_norm %&gt;% filter(x &gt; prob_between[1], x &lt; prob_between[2]) %&gt;% rbind(c(prob_between[2], 0)) %&gt;% rbind(c(prob_between[1], 0)) ggplot(data.frame(x, y), aes(x, y)) + geom_polygon(data = gg_auc, fill = &#39;#71ad50&#39;) + # #71ad50 est un code de couleur format hexad√©cimal geom_line() prob_norm_between &lt;- pnorm(q = prob_between[2], mean = 0, sd = 1) - pnorm(q = prob_between[1], mean = 0, sd = 1) print(paste(&quot;La probabilit√© d&#39;obtenir un nombre entre&quot;, prob_between[1], &quot;et&quot;, prob_between[2], &quot;est d&#39;environ&quot;, round(prob_norm_between, 2) * 100, &quot;%&quot;)) ## [1] &quot;La probabilit√© d&#39;obtenir un nombre entre -1 et 2 est d&#39;environ 82 %&quot; La courbe normale peut √™tre utile pour √©valuer la distribution d‚Äôune population. Par exemple, on peut calculer les limites de r√©gion sur la courbe normale qui contient 95% des valeurs possibles en tranchant 2.5% de part et d‚Äôautre de la moyenne. Il s‚Äôagit ainsi de l‚Äôintervalle de confiance sur la d√©viation de la distribution. increment &lt;- 0.01 x &lt;- seq(-5, 5, by = increment) y &lt;- dnorm(x, mean = 0, sd = 1) alpha &lt;- 0.05 prob_between &lt;- c(qnorm(p = alpha/2, mean = 0, sd = 1), qnorm(p = 1 - alpha/2, mean = 0, sd = 1)) gg_norm &lt;- data.frame(x, y) gg_auc &lt;- gg_norm %&gt;% filter(x &gt; prob_between[1], x &lt; prob_between[2]) %&gt;% rbind(c(prob_between[2], 0)) %&gt;% rbind(c(prob_between[1], 0)) ggplot(data = data.frame(x, y), mapping = aes(x, y)) + geom_polygon(data = gg_auc, fill = &#39;#71ad50&#39;) + # #71ad50 est un code de couleur format hexad√©cimal geom_line() + geom_text(data = data.frame(x = prob_between, y = c(0, 0), labels = round(prob_between, 2)), mapping = aes(label = labels)) On pourrait aussi √™tre int√©ress√© √† l‚Äôintervalle de confiance sur la moyenne. En effet, la moyenne suit aussi une distribution normale, dont la tendance centrale est la moyenne de la distribution, et dont l‚Äô√©cart-type est not√© erreur standard. On calcule cette erreur en divisant la variance par le nombre d‚Äôobservation, ou en divisant l‚Äô√©cart-type par la racine carr√©e du nombre d‚Äôobservations. Ainsi, pour 10 √©chantillons: increment &lt;- 0.01 x &lt;- seq(-5, 5, by = increment) y &lt;- dnorm(x, mean = 0, sd = 1) alpha &lt;- 0.05 prob_between &lt;- c(qnorm(p = alpha/2, mean = 0, sd = 1) / sqrt(10), qnorm(p = 1 - alpha/2, mean = 0, sd = 1) / sqrt(10)) gg_norm &lt;- data.frame(x, y) gg_auc &lt;- gg_norm %&gt;% filter(x &gt; prob_between[1], x &lt; prob_between[2]) %&gt;% rbind(c(prob_between[2], 0)) %&gt;% rbind(c(prob_between[1], 0)) ggplot(data = data.frame(x, y), mapping = aes(x, y)) + geom_polygon(data = gg_auc, fill = &#39;#71ad50&#39;) + # #71ad50 est un code de couleur format hexad√©cimal geom_line() + geom_text(data = data.frame(x = prob_between, y = c(0, 0), labels = round(prob_between, 2)), mapping = aes(label = labels)) 6.5 Statistiques descriptives On a vu comment g√©n√©rer des statistiques sommaires en R avec la fonction summary(). Reprenons les donn√©es d‚Äôiris. data(&quot;iris&quot;) summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## Pour pr√©cis√©ment effectuer une moyenne et un √©cart-type sur un vecteur, passons par les fonctions mean() et sd(). mean(iris$Sepal.Length) ## [1] 5.843333 sd(iris$Sepal.Length) ## [1] 0.8280661 Pour effectuer un sommaire de tableau pilot√© par une fonction, nous passons par la gamme de fonctions summarise(), de dplyr. Dans ce cas, avec group_by(), nous fragmentons le tableau par esp√®ce pour effectuer un sommaire sur toutes les variables. iris %&gt;% group_by(Species) %&gt;% summarise_all(mean) ## # A tibble: 3 x 5 ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.01 3.43 1.46 0.246 ## 2 versicolor 5.94 2.77 4.26 1.33 ## 3 virginica 6.59 2.97 5.55 2.03 Vous pourriez √™tre int√©ress√© par les quartiles √† 25, 50 et 75%. Mais la fonction summarise() n‚Äôautorise que les fonctions dont la sortie est d‚Äôun seul objet, alors faisons sorte que l‚Äôobjet soit une liste - lorsque l‚Äôon imbrique une fonction funs, le tableau √† ins√©rer dans la fonction est indiqu√© par un .. iris %&gt;% group_by(Species) %&gt;% summarise_all(list(q25 = ~ quantile(., probs = 0.25), q50 = ~ quantile(., probs = 0.50), q75 = ~ quantile(., probs = 0.75))) ## # A tibble: 3 x 13 ## Species Sepal.Length_q25 Sepal.Width_q25 Petal.Length_q25 Petal.Width_q25 ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 4.8 3.2 1.4 0.2 ## 2 versic~ 5.6 2.52 4 1.2 ## 3 virgin~ 6.22 2.8 5.1 1.8 ## # ... with 8 more variables: Sepal.Length_q50 &lt;dbl&gt;, Sepal.Width_q50 &lt;dbl&gt;, ## # Petal.Length_q50 &lt;dbl&gt;, Petal.Width_q50 &lt;dbl&gt;, Sepal.Length_q75 &lt;dbl&gt;, ## # Sepal.Width_q75 &lt;dbl&gt;, Petal.Length_q75 &lt;dbl&gt;, Petal.Width_q75 &lt;dbl&gt; En mode programmation classique de R, on pourra g√©n√©rer les quartiles √† la pi√®ce. quantile(iris$Sepal.Length[iris$Species == &#39;setosa&#39;]) ## 0% 25% 50% 75% 100% ## 4.3 4.8 5.0 5.2 5.8 quantile(iris$Sepal.Length[iris$Species == &#39;versicolor&#39;]) ## 0% 25% 50% 75% 100% ## 4.9 5.6 5.9 6.3 7.0 quantile(iris$Sepal.Length[iris$Species == &#39;virginica&#39;]) ## 0% 25% 50% 75% 100% ## 4.900 6.225 6.500 6.900 7.900 La fonction table() permettra d‚Äôobtenir des d√©comptes par cat√©gorie, ici par plages de longueurs de s√©pales. Pour obtenir les proportions du nombre total, il s‚Äôagit d‚Äôencapsuler le tableau crois√© dans la fonction prop.table(). tableau_croise &lt;- table(iris$Species, cut(iris$Sepal.Length, breaks = quantile(iris$Sepal.Length))) tableau_croise ## ## (4.3,5.1] (5.1,5.8] (5.8,6.4] (6.4,7.9] ## setosa 35 14 0 0 ## versicolor 4 20 17 9 ## virginica 1 5 18 26 prop.table(tableau_croise) ## ## (4.3,5.1] (5.1,5.8] (5.8,6.4] (6.4,7.9] ## setosa 0.234899329 0.093959732 0.000000000 0.000000000 ## versicolor 0.026845638 0.134228188 0.114093960 0.060402685 ## virginica 0.006711409 0.033557047 0.120805369 0.174496644 6.6 Tests d‚Äôhypoth√®ses √† un et deux √©chantillons Un test d‚Äôhypoth√®se permet de d√©cider si une hypoth√®se est confirm√©e ou rejet√©e √† un seuil de probabilit√© pr√©d√©termin√©. Cette section est inspir√©e du chapitre 5 de Dalgaard, 2008. Information: l‚Äôhypoth√®se nulle. Les tests d‚Äôhypoth√®se √©valuent des effets statistiques (qui ne sont pas n√©cessairement des effets de causalit√©). L‚Äôeffet √† √©valuer peut √™tre celui d‚Äôun traitement, d‚Äôindicateurs m√©t√©orologiques (e.g. pr√©cipitations totales, degr√©-jour, etc.), de techniques de gestion des paysages, etc. Une recherche est men√©e pour √©valuer l‚Äôhypoth√®se que l‚Äôon retrouve des diff√©rences entre des unit√©s exp√©rimentales. Par convention, l‚Äôhypoth√®se nulle (√©crite \\(H_0\\)) est l‚Äôhypoth√®se qu‚Äôil n‚Äôy ait pas d‚Äôeffet (c‚Äôest l‚Äôhypoth√®se de l‚Äôavocat du diable üòà) √† l‚Äô√©chelle de la population (et non pas √† l‚Äô√©chelle de l‚Äô√©chantillon). √Ä l‚Äôinverse, l‚Äôhypoth√®se alternative (√©crite \\(H_1\\)) est l‚Äôhypoth√®se qu‚Äôil y ait un effet √† l‚Äô√©chelle de la population. √Ä titre d‚Äôexercice en stats, on d√©bute souvent par en testant si deux vecteurs de valeurs continues proviennent de populations √† moyennes diff√©rentes ou si un vecteur de valeurs a √©t√© g√©n√©r√© √† partir d‚Äôune population ayant une moyenne donner. Dans cette section, nous utiliserons la fonction t.test() pour les tests de t et la fonction wilcox.test() pour les tests de Wilcoxon (aussi appel√© de Mann-Whitney). 6.6.1 Test de t √† un seul √©chantillon Nous devons assumer, pour ce test, que l‚Äô√©chantillon est recueillit d‚Äôune population dont la distribution est normale, \\(\\mathcal{N} \\sim \\left( \\mu, \\sigma^2 \\right)\\), et que chaque √©chantillon est ind√©pendant l‚Äôun de l‚Äôautre. L‚Äôhypoth√®se nulle est souvent celle de l‚Äôavocat du diable, que la moyenne soit √©gale √† une valeur donn√©e (donc la diff√©rence entre la moyenne de la population et une moyenne donn√©e est de z√©ro): ici, que \\(\\mu = \\bar{x}\\). L‚Äôerreur standard sur la moyenne (ESM) de l‚Äô√©chantillon, \\(\\bar{x}\\) est calcul√©e comme suit. \\[ESM = \\frac{s}{\\sqrt{n}}\\] o√π \\(s\\) est l‚Äô√©cart-type de l‚Äô√©chantillon et \\(n\\) est le nombre d‚Äô√©chantillons. Pour tester l‚Äôintervalle de confiance de l‚Äô√©chantillon, on multiplie l‚ÄôESM par l‚Äôaire sous la courbe de densit√© couvrant une certaine proportion de part et d‚Äôautre de l‚Äô√©chantillon. Pour un niveau de confiance de 95%, on retranche 2.5% de part et d‚Äôautre. set.seed(33746) x &lt;- rnorm(20, 16, 4) level &lt;- 0.95 alpha &lt;- 1-level x_bar &lt;- mean(x) s &lt;- sd(x) n &lt;- length(x) error &lt;- qnorm(1 - alpha/2) * s / sqrt(n) error ## [1] 1.483253 intervalle de confiance est l‚Äôerreur de par et d‚Äôautre de la moyenne. c(x_bar - error, x_bar + error) ## [1] 14.35630 17.32281 Si la moyenne de la population est de 16, un nombre qui se situe dans l‚Äôintervalle de confiance on accepte l‚Äôhypoth√®se nulle au seuil 0.05. Si le nombre d‚Äô√©chantillon est r√©duit (g√©n√©ralement &lt; 30), on passera plut√¥t par une distribution de t, avec \\(n-1\\) degr√©s de libert√©. error &lt;- qt(1 - alpha/2, n-1) * s / sqrt(n) c(x_bar - error, x_bar + error) ## [1] 14.25561 17.42351 Plus simplement, on pourra utiliser la fonction t.test() en sp√©cifiant la moyenne de la population. Nous avons g√©n√©r√© 20 donn√©es avec une moyenne de 16 et un √©cart-type de 4. Nous savons donc que la vraie moyenne de l‚Äô√©chantillon est de 16. Mais disons que nous testons l‚Äôhypoth√®se que ces donn√©es sont tir√©es d‚Äôune population dont la moyenne est 18 (et implicitement que sont √©cart-type est de 4). t.test(x, mu = 18) ## ## One Sample t-test ## ## data: x ## t = -2.8548, df = 19, p-value = 0.01014 ## alternative hypothesis: true mean is not equal to 18 ## 95 percent confidence interval: ## 14.25561 17.42351 ## sample estimates: ## mean of x ## 15.83956 La fonction retourne la valeur de t (t-value), le nombre de degr√©s de libert√© (\\(n-1 = 19\\)), une description de l‚Äôhypoth√®se alternative (alternative hypothesis: true mean is not equal to 18), ainsi que l‚Äôintervalle de confiance au niveau de 95%. Le test contient aussi la p-value. Bien que la p-value soit largement utilis√©e en science 6.6.1.1 Information: la p-value La p-value, ou valeur-p ou p-valeur, est utilis√©e pour trancher si, oui ou non, un r√©sultat est significatif. En langage scientifique, le mot significatif ne devrait √™tre utilis√© que lorsque l‚Äôon r√©f√®re √† un test d‚Äôhypoth√®se statistique. Vous retrouverez des p-values partout en stats. Les p-values indiquent la probabilit√© que les donn√©es ait √©t√© √©chantillonn√©es d‚Äôune population o√π un effet est observable selon le mod√®le statistique utilis√©. La p-value est la probabilit√© que les donn√©es aient √©t√© g√©n√©r√©es pour obtenir un effet √©quivalent ou plus prononc√© si l‚Äôhypoth√®se nulle est vraie. Une p-value √©lev√©e indique que le mod√®le appliqu√© √† vos donn√©es concorde avec la conclusion que l‚Äôhypoth√®se nulle est vraie, et inversement si la p-value est faible. Le seuil arbitraire utilis√©e en √©cologie et en agriculture, comme dans plusieurs domaines, est de 0.05. L‚Äôutilisation d‚Äôun seuil est toutefois contest√©e √† raison. Une enqu√™te men√©e dans la litt√©rature scientifiques a r√©v√©l√© que 49% des 791 articles √©tudi√©s interpr√©taient un effet non significatif comme un effet nul (Amrhein et al., 2019). En effet, une cat√©gorisation de la p-value avec un seuil de significativit√© brouille le jugement sur l‚Äôimportance des effets et de leur incertitude. Les six principes de l‚ÄôAmerican Statistical Association guident l‚Äôinterpr√©tation des p-values. [ma traduction] Les p-values indique l‚Äôampleur de l‚Äôincompatibilit√© des donn√©es avec le mod√®le statistique Les p-values ne mesurent pas la probabilit√© que l‚Äôhypoth√®se √©tudi√©e soit vraie, ni la probabilit√© que les donn√©es ont √©t√© g√©n√©r√©es uniquement par la chance. Les conclusions scientifiques et d√©cisions d‚Äôaffaire ou politiques ne devraient pas √™tre bas√©es sur l‚Äôatteinte d‚Äôune p-value √† un seuil sp√©cifique. Une inf√©rence appropri√©e demande un rapport complet et transparent. Une p-value, ou une signification statistique, ne mesure pas l‚Äôampleur d‚Äôun effet ou l‚Äôimportance d‚Äôun r√©sultat. En tant que tel, une p-value n‚Äôoffre pas une bonne mesure des √©vidences d‚Äôun mod√®le ou d‚Äôune hypoth√®se. Dans le cas pr√©c√©dent, la p-value √©tait de 0.01014. Pour aider notre interpr√©tation, prenons l‚Äôhypoth√®se alternative: true mean is not equal to 18. L‚Äôhypoth√®se nulle √©tait bien que la vraie moyenne est √©gale √† 18. Ins√©rons la p-value dans la d√©finition: la probabilit√© que les donn√©es aient √©t√© g√©n√©r√©es pour obtenir un effet √©quivalent ou plus prononc√© si l‚Äôhypoth√®se nulle est vraie est de 0.01014. Il est donc tr√®s peu probable que les donn√©es soient tir√©es d‚Äôun √©chantillon dont la moyenne est de 18. Au seuil de signification de 0.05, on rejette l‚Äôhypoth√®se nulle et l‚Äôon conclut qu‚Äô√† ce seuil de confiance, l‚Äô√©chantillon ne provient pas d‚Äôune population ayant une moyenne de 18. 6.6.2 Attention: mauvaises interpr√©tations des p-values ‚ÄúLa p-value n‚Äôa jamais √©t√© con√ßue comme substitut au raisonnement scientifique‚Äù Ron Wasserstein, directeur de l‚ÄôAmerican Statistical Association [ma traduction]. Un r√©sultat montrant une p-value plus √©lev√©e que 0.05 est-il pertinent? Lors d‚Äôune conf√©rence, Dr Evil ne pr√©sentent que les r√©sultats significatifs de ses essais au seuil de 0.05. Certains essais ne sont pas significatifs, mais bon, ceux-ci ne sont pas importants‚Ä¶ En √©cartant ces r√©sultats, Dr Evil commet 3 erreurs: La p-value n‚Äôest pas un bon indicateur de l‚Äôimportance d‚Äôun test statistique. L‚Äôimportance d‚Äôune variable dans un mod√®le devrait √™tre √©valu√©e par la valeur de son coefficient. Son incertitude devrait √™tre √©valu√©e par sa variance. Une mani√®re d‚Äô√©valuer plus intuitive la variance est l‚Äô√©cart-type ou l‚Äôintervalle de confiance. √Ä un certain seuil d‚Äôintervalle de confiance, la p-value traduira la probabilit√© qu‚Äôun coefficient soit r√©ellement nul ait pu g√©n√©rer des donn√©es d√©montrant un coefficient √©gal ou sup√©rieur. Il est tout aussi important de savoir que le traitement fonctionne que de savoir qu‚Äôil ne fonctionne pas. Les r√©sultats d√©montrant des effets sont malheureusement davantage soumis aux journaux et davantage publi√©s que ceux ne d√©montrant pas d‚Äôeffets (Decullier et al., 2005). Le seuil de 0.05 est arbitraire. 6.6.2.1 Attention au p-hacking Le p-hacking (ou data dredging) consiste √† manipuler les donn√©es et les mod√®les pour faire en sorte d‚Äôobtenir des p-values favorables √† l‚Äôhypoth√®se test√©e et, √©ventuellement, aux conclusions recherch√©es. √Ä √©viter dans tous les cas. Toujours. Toujours. Toujours. Figure 6.1: Un sketch humoristique de John Oliver sur le p-hacking, Last week tonight, 2016 (en anglais) 6.6.3 Test de Wilcoxon √† un seul √©chantillon Le test de t suppose que la distribution des donn√©es est normale‚Ä¶ ce qui est rarement le cas, surtout lorsque les √©chantillons sont peu nombreux. Le test de Wilcoxon ne demande aucune supposition sur la distribution: c‚Äôest un test non-param√©trique bas√© sur le tri des valeurs. wilcox.test(x, mu = 18) ## ## Wilcoxon signed rank exact test ## ## data: x ## V = 39, p-value = 0.01208 ## alternative hypothesis: true location is not equal to 18 Le V est la somme des rangs positifs. Dans ce cas, la p-value est semblable √† celle du test de t, et les m√™mes conclusions s‚Äôappliquent. 6.6.4 Tests de t √† deux √©chantillons Les tests √† un √©chantillon servent plut√¥t √† s‚Äôexercer: rarement en aura-t-on besoin en recherche, o√π plus souvent, on voudra comparer les moyennes de deux unit√©s exp√©rimentales. L‚Äôexp√©rience comprend donc deux s√©ries de donn√©es continues, \\(x_1\\) et \\(x_2\\), issus de lois de distribution normale \\(\\mathcal{N} \\left( \\mu_1, \\sigma_1^2 \\right)\\) et \\(\\mathcal{N} \\left( \\mu_2, \\sigma_2^2 \\right)\\), et nous testons l‚Äôhypoth√®se nulle que \\(\\mu_1 = \\mu_2\\). La statistique t est calcul√©e comme suit. \\[t = \\frac{\\bar{x_1} - \\bar{x_2}}{ESDM}\\] L‚ÄôESDM est l‚Äôerreur standard de la diff√©rence des moyennes: \\[ESDM = \\sqrt{ESM_1^2 + ESM_2^2}\\] Si vous supposez que les variances sont identiques, l‚Äôerreur standard (s) est calcul√©e pour les √©chantillons des deux groupes, puis ins√©r√©e dans le calcul des ESM. La statistique t sera alors √©valu√©e √† \\(n_1 + n_2 - 2\\) degr√©s de libert√©. Si vous supposez que la variance est diff√©rente (proc√©dure de Welch), vous calculez les ESM avec les erreurs standards respectives, et la statistique t devient une approximation de la distribution de t avec un nombre de degr√©s de libert√© calcul√© √† partir des erreurs standards et du nombre d‚Äô√©chantillon dans les groupes: cette proc√©dure est consid√©r√©e comme plus prudente (Dalgaard, 2008, page 101). Prenons les donn√©es d‚Äôiris pour l‚Äôexemple en excluant l‚Äôiris setosa √©tant donn√©e que les tests de t se restreignent √† deux groupes. Nous allons tester la longueur des p√©tales. iris_pl &lt;- iris %&gt;% filter(Species != &quot;setosa&quot;) %&gt;% select(Species, Petal.Length) sample_n(iris_pl, 5) ## Species Petal.Length ## 1 virginica 5.1 ## 2 versicolor 4.0 ## 3 virginica 5.0 ## 4 versicolor 4.6 ## 5 versicolor 4.1 Dans la prochaine cellule, nous introduisons l‚Äôinterface-formule de R, o√π l‚Äôon retrouve typiquement le ~, entre les variables de sortie √† gauche et les variables d‚Äôentr√©e √† droite. Dans notre cas, la variable de sortie est la variable test√©e, Petal.Length, qui varie en fonction du groupe Species, qui est la variable d‚Äôentr√©e (variable explicative) - nous verrons les types de variables plus en d√©tails dans la section Les mod√®les statistiques, plus bas. t.test(formula = Petal.Length ~ Species, data = iris_pl, var.equal = FALSE) ## ## Welch Two Sample t-test ## ## data: Petal.Length by Species ## t = -12.604, df = 95.57, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.49549 -1.08851 ## sample estimates: ## mean in group versicolor mean in group virginica ## 4.260 5.552 Nous obtenons une sortie similaire aux pr√©c√©dentes. L‚Äôintervalle de confiance √† 95% exclu le z√©ro, ce qui est coh√©rent avec la p-value tr√®s faible, qui nous indique le rejet de l‚Äôhypoth√®se nulle au seuil 0.05. Les donn√©es montrent que les groupes ont des moyennes de longueurs de p√©tale diff√©rentes. 6.6.4.1 Enregistrer les r√©sultats d‚Äôun test Il est possible d‚Äôenregistrer un test dans un objet. tt_pl &lt;- t.test(formula = Petal.Length ~ Species, data = iris_pl, var.equal = FALSE) summary(tt_pl) ## Length Class Mode ## statistic 1 -none- numeric ## parameter 1 -none- numeric ## p.value 1 -none- numeric ## conf.int 2 -none- numeric ## estimate 2 -none- numeric ## null.value 1 -none- numeric ## stderr 1 -none- numeric ## alternative 1 -none- character ## method 1 -none- character ## data.name 1 -none- character str(tt_pl) ## List of 10 ## $ statistic : Named num -12.6 ## ..- attr(*, &quot;names&quot;)= chr &quot;t&quot; ## $ parameter : Named num 95.6 ## ..- attr(*, &quot;names&quot;)= chr &quot;df&quot; ## $ p.value : num 4.9e-22 ## $ conf.int : num [1:2] -1.5 -1.09 ## ..- attr(*, &quot;conf.level&quot;)= num 0.95 ## $ estimate : Named num [1:2] 4.26 5.55 ## ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;mean in group versicolor&quot; &quot;mean in group virginica&quot; ## $ null.value : Named num 0 ## ..- attr(*, &quot;names&quot;)= chr &quot;difference in means&quot; ## $ stderr : num 0.103 ## $ alternative: chr &quot;two.sided&quot; ## $ method : chr &quot;Welch Two Sample t-test&quot; ## $ data.name : chr &quot;Petal.Length by Species&quot; ## - attr(*, &quot;class&quot;)= chr &quot;htest&quot; 6.6.5 Comparaison des variances Pour comparer les variances, on a recours au test de F (F pour Fisher). var.test(formula = Petal.Length ~ Species, data = iris_pl) ## ## F test to compare two variances ## ## data: Petal.Length by Species ## F = 0.72497, num df = 49, denom df = 49, p-value = 0.2637 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.411402 1.277530 ## sample estimates: ## ratio of variances ## 0.7249678 Il semble que l‚Äôon pourrait relancer le test de t sans la proc√©dure Welch, avec var.equal = TRUE. 6.6.6 Tests de Wilcoxon √† deux √©chantillons Cela ressemble au test de t! wilcox.test(formula = Petal.Length ~ Species, data = iris_pl, var.equal = TRUE) ## ## Wilcoxon rank sum test with continuity correction ## ## data: Petal.Length by Species ## W = 44.5, p-value &lt; 2.2e-16 ## alternative hypothesis: true location shift is not equal to 0 6.6.7 Les tests pair√©s Les tests pair√©s sont utilis√©s lorsque deux √©chantillons proviennent d‚Äôune m√™me unit√© exp√©rimentale: il s‚Äôagit en fait de tests sur la diff√©rence entre deux observations. set.seed(2555) n &lt;- 20 avant &lt;- rnorm(n, 16, 4) apres &lt;- rnorm(n, 18, 3) Il est important de sp√©cifier que le test est pair√©, la valeur par d√©faut de paired √©tant FALSE. t.test(avant, apres, paired = TRUE) ## ## Paired t-test ## ## data: avant and apres ## t = -1.5168, df = 19, p-value = 0.1458 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -4.5804586 0.7311427 ## sample estimates: ## mean of the differences ## -1.924658 L‚Äôhypoth√®se nulle qu‚Äôil n‚Äôy ait pas de diff√©rence entre l‚Äôavant et l‚Äôapr√®s traitement est accept√©e au seuil 0.05. Exercice. Effectuer un test de Wilcoxon pair√©. 6.7 L‚Äôanalyse de variance L‚Äôanalyse de variance consiste √† comparer des moyennes de plusieurs groupe distribu√©s normalement et de m√™me variance. Cette section sera √©labor√©e prochainement plus en profondeur. Consid√©rons-la pour le moment comme une r√©gression sur une variable cat√©gorielle. pl_aov &lt;- aov(Petal.Length ~ Species, iris) summary(pl_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 2 437.1 218.55 1180 &lt;2e-16 *** ## Residuals 147 27.2 0.19 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 La prochaine section, justement, est vou√©e aux mod√®les statistiques explicatifs, qui incluent la r√©gression. 6.8 Les mod√®les statistiques La mod√©lisation statistique consiste √† lier de mani√®re explicite des variables de sortie \\(y\\) (ou variables-r√©ponse ou variables d√©pendantes) √† des variables explicatives \\(x\\) (ou variables pr√©dictives / ind√©pendantes / covariables). Les variables-r√©ponse sont mod√©lis√©es par une fonction des variables explicatives ou pr√©dictives. Pourquoi garder les termes explicatives et pr√©dictives? Parce que les mod√®les statistiques (bas√©s sur des donn√©es et non pas sur des m√©canismes) sont de deux ordres. D‚Äôabord, les mod√®les pr√©dictifs sont con√ßus pour pr√©dire de mani√®re fiable une ou plusieurs variables-r√©ponse √† partir des informations contenues dans les variables qui sont, dans ce cas, pr√©dictives. Ces mod√®les sont couverts dans le chapitre 11 de ce manuel (en d√©veloppement). Lorsque l‚Äôon d√©sire tester des hypoth√®ses pour √©valuer quelles variables expliquent la r√©ponse, on parlera de mod√©lisation (et de variables) explicatives. En inf√©rence statistique, on √©valuera les corr√©lations entre les variables explicatives et les variables-r√©ponse. Un lien de corr√©lation n‚Äôest pas un lien de causalit√©. L‚Äôinf√©rence causale peut en revanche √™tre √©valu√©e par des mod√®les d‚Äô√©quations structurelles, sujet qui fera √©ventuellement partie de ce cours. Cette section couvre la mod√©lisation explicative. Les variables qui contribuent √† cr√©er les mod√®les peuvent √™tre de diff√©rentes natures et distribu√©es selon diff√©rentes lois de probabilit√©. Alors que les mod√®les lin√©aires simples (lm) impliquent une variable-r√©ponse distribu√©e de mani√®re continue, les mod√®les lin√©aires g√©n√©ralis√©s peuvent aussi expliquer des variables de sorties discr√®tes. Dans les deux cas, on distinguera les variables fixes et les variables al√©atoires. Les variables fixes sont les variables test√©es lors de l‚Äôexp√©rience: dose du traitement, esp√®ce/cultivar, m√©t√©o, etc. Les variables al√©atoires sont les sources de variation qui g√©n√®rent du bruit dans le mod√®le: les unit√©s exp√©rimentales ou le temps lors de mesures r√©p√©t√©es. Les mod√®les incluant des effets fixes seulement sont des mod√®les √† effets fixes. G√©n√©ralement, les mod√®les incluant des variables al√©atoires incluent aussi des variables fixes: on parlera alors de mod√®les mixtes. Nous couvrirons ces deux types de mod√®le. 6.8.1 Mod√®les √† effets fixes Les tests de t et de Wilcoxon, explor√©s pr√©c√©demment, sont des mod√®les statistiques √† une seule variable. Nous avons vu dans l‚Äôinterface-formule qu‚Äôune variable-r√©ponse peut √™tre li√©e √† une variable explicative avec le tilde ~. En particulier, le test de t est r√©gression lin√©aire univari√©e (√† une seule variable explicative) dont la variable explicative comprend deux cat√©gories. De m√™me, l‚Äôanova est une r√©gression lin√©aire univari√©e dont la variable explicative comprend plusieurs cat√©gories. Or l‚Äôinterface-formule peut √™tre utilis√© dans plusieurs circonstances, notamment pour ajouter plusieurs variables de diff√©rents types: on parlera de r√©gression multivari√©e. La plupart des mod√®les statistiques peuvent √™tre approxim√©s comme une combinaison lin√©aire de variables: ce sont des mod√®les lin√©aires. Les mod√®les non-lin√©aires impliquent des strat√©gies computationnelles complexes qui rendent leur utilisation plus difficile √† man≈ìuvrer. Un mod√®le lin√©aire univari√© prendra la forme \\(y = \\beta_0 + \\beta_1 x + \\epsilon\\), o√π \\(\\beta_0\\) est l‚Äôintercept et \\(\\beta_1\\) est la pente et \\(\\epsilon\\) est l‚Äôerreur. Vous verrez parfois la notation \\(\\hat{y} = \\beta_0 + \\beta_1 x\\). La notation avec le chapeau \\(\\hat{y}\\) exprime qu‚Äôil s‚Äôagit des valeurs g√©n√©r√©es par le mod√®le. En fait, \\(y = \\hat{y} - \\epsilon\\). 6.8.1.1 Mod√®le lin√©aire univari√© avec variable continue Prenons les donn√©es lasrosas.corn incluses dans le module agridat, o√π l‚Äôon retrouve le rendement d‚Äôune production de ma√Øs √† dose d‚Äôazote variable, en Argentine. library(&quot;agridat&quot;) data(&quot;lasrosas.corn&quot;) sample_n(lasrosas.corn, 10) ## year lat long yield nitro topo bv rep nf ## 1 1999 -33.05207 -63.84230 69.57 0.0 LO 185.67 R1 N0 ## 2 1999 -33.05137 -63.84383 67.41 53.0 E 175.12 R2 N2 ## 3 1999 -33.05104 -63.84323 68.33 29.0 LO 168.70 R3 N1 ## 4 1999 -33.05162 -63.84456 68.06 53.0 E 171.71 R1 N2 ## 5 1999 -33.05180 -63.84386 63.99 0.0 LO 172.46 R1 N0 ## 6 2001 -33.05065 -63.84578 35.85 50.6 HT 194.85 R1 N2 ## 7 1999 -33.05170 -63.84553 58.89 131.5 HT 187.98 R1 N5 ## 8 2001 -33.05077 -63.84502 50.95 124.6 HT 184.66 R2 N5 ## 9 1999 -33.05181 -63.84202 78.75 106.0 LO 169.25 R2 N4 ## 10 1999 -33.05154 -63.84468 68.58 29.0 E 169.35 R1 N1 Ces donn√©es comprennent plusieurs variables. Prenons le rendement (yield) comme variable de sortie et, pour le moment, ne retenons que la dose d‚Äôazote (nitro) comme variable explicative: il s‚Äôagit d‚Äôune r√©gression univari√©e. Les deux variables sont continues. Explorons d‚Äôabord le nuage de points de l‚Äôune et l‚Äôautre. ggplot(data = lasrosas.corn, mapping = aes(x = nitro, y = yield)) + geom_point() L‚Äôhypoth√®se nulle est que la dose d‚Äôazote n‚Äôaffecte pas le rendement, c‚Äôest √† dire que le coefficient de pente et nul. Une autre hypoth√®se est que l‚Äôintercept est nul: donc qu‚Äô√† dose de 0, rendement de 0. Un mod√®le lin√©aire √† variable de sortie continue est cr√©√© avec la fonction lm(), pour linear model. modlin_1 &lt;- lm(yield ~ nitro, data = lasrosas.corn) summary(modlin_1) ## ## Call: ## lm(formula = yield ~ nitro, data = lasrosas.corn) ## ## Residuals: ## Min 1Q Median 3Q Max ## -53.183 -15.341 -3.079 13.725 45.897 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 65.843213 0.608573 108.193 &lt; 2e-16 *** ## nitro 0.061717 0.007868 7.845 5.75e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 19.66 on 3441 degrees of freedom ## Multiple R-squared: 0.01757, Adjusted R-squared: 0.01728 ## F-statistic: 61.54 on 1 and 3441 DF, p-value: 5.754e-15 Le diagnostic du mod√®le comprend plusieurs informations. D‚Äôabord la formule utilis√©e, affich√©e pour la tra√ßabilit√©. Viens ensuite un aper√ßu de la distribution des r√©sidus. La m√©diane devrait s‚Äôapprocher de la moyenne des r√©sidus (qui est toujours de 0). Bien que le -3.079 peut sembler important, il faut prendre en consid√©ration de l‚Äô√©chelle de y, et ce -3.079 est exprim√© en terme de rendement, ici en quintaux (i.e. 100 kg) par hectare. La distribution des r√©sidus m√©rite d‚Äô√™tre davantage investigu√©e. Nous verrons cela un peu plus tard. Les coefficients apparaissent ensuite. Les estim√©s sont les valeurs des effets. R fournit aussi l‚Äôerreur standard associ√©e, la valeur de t ainsi que la p-value (la probabilit√© d‚Äôobtenir cet effet ou un effet plus extr√™me si en r√©alit√© il y avait absence d‚Äôeffet). L‚Äôintercept est bien s√ªr plus √©lev√© que 0 (√† dose nulle, on obtient 65.8 quintaux par hectare en moyenne). La pente de la variable nitro est de ~0.06: pour chaque augmentation d‚Äôun kg/ha de dose, on a obtenu ~0.06 quintaux/ha de plus de ma√Øs. Donc pour 100 kg/ha de N, on a obtenu un rendement moyen de 6 quintaux de plus que l‚Äôintercept. Soulignons que l‚Äôampleur du coefficient est tr√®s important pour guider la fertilisation: ne rapporter que la p-value, ou ne rapporter que le fait qu‚Äôelle est inf√©rieure √† 0.05 (ce qui arrive souvent dans la litt√©rature), serait tr√®s insuffisant pour l‚Äôinterpr√©tation des statistiques. La p-value nous indique n√©anmoins qu‚Äôil serait tr√®s improbable qu‚Äôune telle pente ait √©t√© g√©n√©r√©e alors que celle-ci est nulle en r√©alit√©. Les √©toiles √† c√¥t√© des p-values indiquent l‚Äôampleur selon l‚Äô√©chelle Signif. codes indiqu√©e en-dessous du tableau des coefficients. Sous ce tableau, R offre d‚Äôautres statistiques. En outre, les R¬≤ et R¬≤ ajust√©s indiquent si la r√©gression passe effectivement par les points. Le R¬≤ prend un maximum de 1 lorsque la droite passe exactement sur les points. Enfin, le test de F g√©n√®re une p-value indiquant la probabilit√© que les coefficients de pente ait √©t√© g√©n√©r√©s si les vrais coefficients √©taient nuls. Dans le cas d‚Äôune r√©gression univari√©e, cela r√©p√®te l‚Äôinformation sur l‚Äôunique coefficient. On pourra √©galement obtenir les intervalles de confiance avec la fonction confint(). confint(modlin_1, level = 0.95) ## 2.5 % 97.5 % ## (Intercept) 64.65001137 67.03641474 ## nitro 0.04629164 0.07714271 Ou soutirer l‚Äôinformation de diff√©rentes mani√®res, comme avec la fonction coefficients(). coefficients(modlin_1) ## (Intercept) nitro ## 65.84321305 0.06171718 √âgalement, on pourra ex√©cuter le mod√®le sur les donn√©es qui ont servi √† le g√©n√©rer: predict(modlin_1)[1:5] ## 1 2 3 4 5 ## 73.95902 73.95902 73.95902 73.95902 73.95902 Ou sur des donn√©es externes. nouvelles_donnees &lt;- data.frame(nitro = seq(from = 0, to = 100, by = 5)) predict(modlin_1, newdata = nouvelles_donnees)[1:5] ## 1 2 3 4 5 ## 65.84321 66.15180 66.46038 66.76897 67.07756 6.8.1.2 Analyse des r√©sidus Les r√©sidus sont les erreurs du mod√®le. C‚Äôest le vecteur \\(\\epsilon\\), qui est un d√©calage entre les donn√©es et le mod√®le. Le R¬≤ est un indicateur de l‚Äôampleur du d√©calage, mais une r√©gression lin√©aire explicative en bonne et due forme devrait √™tre accompagn√©e d‚Äôune analyse des r√©sidus. On peut les calculer par \\(\\epsilon = y - \\hat{y}\\), mais aussi bien utiliser la fonction residuals(). res_df &lt;- data.frame(nitro = lasrosas.corn$nitro, residus_lm = residuals(modlin_1), residus_calcul = lasrosas.corn$yield - predict(modlin_1)) sample_n(res_df, 10) ## nitro residus_lm residus_calcul ## 1 124.6 24.666827 24.666827 ## 2 124.6 11.126827 11.126827 ## 3 99.8 25.417413 25.417413 ## 4 66.0 -11.636547 -11.636547 ## 5 131.5 11.460978 11.460978 ## 6 75.4 -18.686688 -18.686688 ## 7 29.0 -1.763011 -1.763011 ## 8 131.5 -11.289022 -11.289022 ## 9 131.5 -5.639022 -5.639022 ## 10 131.5 -13.129022 -13.129022 Dans une bonne r√©gression lin√©aire, on ne retrouvera pas de structure identifiable dans les r√©sidus, c‚Äôest-√†-dire que les r√©sidus sont bien distribu√©s de part et d‚Äôautre du mod√®le de r√©gression. ggplot(res_df, aes(x = nitro, y = residus_lm)) + geom_point() + labs(x = &quot;Dose N&quot;, y = &quot;R√©sidus&quot;) + geom_hline(yintercept = 0, col = &quot;red&quot;, size = 1) Bien que le jugement soit subjectif, on peut dire avec confiance qu‚Äôil n‚Äôy a pas structure particuli√®re. En revanche, on pourrait g√©n√©rer un \\(y\\) qui varie de mani√®re quadratique avec \\(x\\), un mod√®le lin√©aire montrera une structure √©vidente. set.seed(36164) x &lt;- 0:100 y &lt;- 10 + x*1 + x^2 * 0.05 + rnorm(length(x), 0, 50) modlin_2 &lt;- lm(y ~ x) ggplot(data.frame(y, residus = residuals(modlin_2)), aes(x = x, y = residus)) + geom_point() + labs(x = &quot;x&quot;, y = &quot;R√©sidus&quot;) + geom_hline(yintercept = 0, col = &quot;red&quot;, size = 1) De m√™me, les r√©sidus ne devraient pas cro√Ætre avec \\(x\\). set.seed(3984) x &lt;- 0:100 y &lt;- 10 + x + x * rnorm(length(x), 0, 2) modlin_3 &lt;- lm(y ~ x) ggplot(data.frame(x, residus = residuals(modlin_3)), aes(x = x, y = residus)) + geom_point() + labs(x = &quot;x&quot;, y = &quot;R√©sidus&quot;) + geom_hline(yintercept = 0, col = &quot;red&quot;, size = 1) On pourra aussi inspecter les r√©sidus avec un graphique de leur distribution. Reprenons notre mod√®le de rendement du ma√Øs. ggplot(res_df, aes(x = residus_lm)) + geom_histogram(binwidth = 2, color = &quot;white&quot;) + labs(x = &quot;Residual&quot;) L‚Äôhistogramme devrait pr√©senter une distribution normale. Les tests de normalit√© comme le test de Shapiro-Wilk peuvent aider, mais ils sont g√©n√©ralement tr√®s s√©v√®res. shapiro.test(res_df$residus_lm) ## ## Shapiro-Wilk normality test ## ## data: res_df$residus_lm ## W = 0.94868, p-value &lt; 2.2e-16 L‚Äôhypoth√®se nulle que la distribution est normale est rejet√©e au seuil 0.05. Dans notre cas, il est √©vident que la s√©v√©rit√© du test n‚Äôest pas en cause, car les r√©sidus semble g√©n√©rer trois ensembles. Ceci indique que les variables explicatives sont insuffisantes pour expliquer la variabilit√© de la variable-r√©ponse. 6.8.1.3 R√©gression multiple Comme c‚Äôest le cas pour bien des ph√©nom√®nes en √©cologie, le rendement d‚Äôune culture n‚Äôest certainement pas expliqu√© seulement par la dose d‚Äôazote. Lorsque l‚Äôon combine plusieurs variables explicatives, on cr√©e un mod√®le de r√©gression multivari√©e, ou une r√©gression multiple. Bien que les tendances puissent sembler non-lin√©aires, l‚Äôajout de variables et le calcul des coefficients associ√©s reste un probl√®me d‚Äôalg√®bre lin√©aire. On pourra en effet g√©n√©raliser les mod√®les lin√©aires, univari√©s et multivari√©s, de la mani√®re suivante. \\[ y = X \\beta + \\epsilon \\] o√π: \\(X\\) est la matrice du mod√®le √† \\(n\\) observations et \\(p\\) variables. \\[ X = \\left( \\begin{matrix} 1 &amp; x_{11} &amp; \\cdots &amp; x_{1p} \\\\ 1 &amp; x_{21} &amp; \\cdots &amp; x_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; \\cdots &amp; x_{np} \\end{matrix} \\right) \\] \\(\\beta\\) est la matrice des \\(p\\) coefficients, \\(\\beta_0\\) √©tant l‚Äôintercept qui multiplie la premi√®re colonne de la matrice \\(X\\). \\[ \\beta = \\left( \\begin{matrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{matrix} \\right) \\] \\(\\epsilon\\) est l‚Äôerreur de chaque observation. \\[ \\epsilon = \\left( \\begin{matrix} \\epsilon_0 \\\\ \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n \\end{matrix} \\right) \\] 6.8.1.4 Mod√®les lin√©aires univari√©s avec variable cat√©gorielle nominale Une variable cat√©gorielle nominale (non ordonn√©e) utilis√©e √† elle seule dans un mod√®le comme variable explicative, est un cas particulier de r√©gression multiple. En effet, l‚Äôencodage cat√©goriel (ou dummyfication) transforme une variable cat√©gorielle nominale en une matrice de mod√®le comprenant une colonne d√©signant l‚Äôintercept (une s√©rie de 1) d√©signant la cat√©gorie de r√©f√©rence, ainsi que des colonnes pour chacune des autres cat√©gories d√©signant l‚Äôappartenance (1) ou la non appartenance (0) de la cat√©gorie d√©sign√©e par la colonne. 6.8.1.4.1 L‚Äôencodage cat√©goriel Une variable √† \\(C\\) cat√©gories pourra √™tre d√©clin√©e en \\(C\\) variables dont chaque colonne d√©signe par un 1 l‚Äôappartenance au groupe de la colonne et par un 0 la non-appartenance. Pour l‚Äôexemple, cr√©ons un vecteur d√©signant le cultivar de pomme de terre. data &lt;- data.frame(cultivar = factor(c(&#39;Superior&#39;, &#39;Superior&#39;, &#39;Superior&#39;, &#39;Russet&#39;, &#39;Kenebec&#39;, &#39;Russet&#39;))) model.matrix(~cultivar, data) ## (Intercept) cultivarRusset cultivarSuperior ## 1 1 0 1 ## 2 1 0 1 ## 3 1 0 1 ## 4 1 1 0 ## 5 1 0 0 ## 6 1 1 0 ## attr(,&quot;assign&quot;) ## [1] 0 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$cultivar ## [1] &quot;contr.treatment&quot; Nous avons trois cat√©gories, encod√©es en trois colonnes. La premi√®re colonne est un intercept et les deux autres d√©crivent l‚Äôabsence (0) ou la pr√©sence (1) des cultivars Russet et Superior. Le cultivar Kenebec est absent du tableau. En effet, en partant du principe que l‚Äôappartenance √† une cat√©gorie est mutuellement exclusive, c‚Äôest-√†-dire qu‚Äôun √©chantillon ne peut √™tre assign√© qu‚Äô√† une seule cat√©gorie, on peut d√©duire une cat√©gorie √† partir de l‚Äôinformation sur toutes les autres. Par exemple, si cultivar_Russet et cultivar_Superior sont toutes deux √©gales √† \\(0\\), on conclura que cultivar_Kenebec est n√©cessairement √©gal √† \\(1\\). Et si l‚Äôun d‚Äôentre cultivar_Russet et cultivar_Superior est √©gal √† \\(1\\), cultivar_Kenebec est n√©cessairement √©gal √† \\(0\\). L‚Äôinformation contenue dans un nombre \\(C\\) de cat√©gorie peut √™tre encod√©e dans un nombre \\(C-1\\) de colonnes. C‚Äôest pourquoi, dans une analyse statistique, on d√©signera une cat√©gorie comme une r√©f√©rence, que l‚Äôon d√©tecte lorsque toutes les autres cat√©gories sont encod√©es avec des \\(0\\): cette r√©f√©rence sera incluse dans l‚Äôintercept. La cat√©gorie de r√©f√©rence par d√©faut en R est celle la premi√®re cat√©gorie dans l‚Äôordre alphab√©tique. On pourra modifier cette r√©f√©rence avec la fonction relevel(). data$cultivar &lt;- relevel(data$cultivar, ref = &quot;Superior&quot;) model.matrix(~cultivar, data) ## (Intercept) cultivarKenebec cultivarRusset ## 1 1 0 0 ## 2 1 0 0 ## 3 1 0 0 ## 4 1 0 1 ## 5 1 1 0 ## 6 1 0 1 ## attr(,&quot;assign&quot;) ## [1] 0 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$cultivar ## [1] &quot;contr.treatment&quot; Pour certains mod√®les, vous devrez vous assurer vous-m√™me de l‚Äôencodage cat√©goriel. Pour d‚Äôautre, en particulier avec l‚Äôinterface par formule de R, ce sera fait automatiquement. 6.8.1.4.2 Exemple d‚Äôapplication Prenons la topographie du terrain, qui peut prendre plusieurs niveaux. levels(lasrosas.corn$topo) ## [1] &quot;E&quot; &quot;HT&quot; &quot;LO&quot; &quot;W&quot; Explorons le rendement selon la topographie. ggplot(lasrosas.corn, aes(x = topo, y = yield)) + geom_boxplot() Les diff√©rences sont √©videntes, et la mod√©lisation devrait montrer des effets diff√©rents. L‚Äôencodage cat√©goriel peut √™tre visualis√© en g√©n√©rant la matrice de mod√®le avec la fonction model.matrix() et l‚Äôinterface-formule - sans la variable-r√©ponse. model.matrix(~ topo, data = lasrosas.corn) %&gt;% tbl_df() %&gt;% # tbl_df pour transformer la matrice en tableau sample_n(10) ## # A tibble: 10 x 4 ## `(Intercept)` topoHT topoLO topoW ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 0 0 ## 2 1 0 1 0 ## 3 1 0 0 1 ## 4 1 0 0 1 ## 5 1 0 1 0 ## 6 1 0 0 1 ## 7 1 1 0 0 ## 8 1 0 1 0 ## 9 1 0 0 1 ## 10 1 0 0 0 Dans le cas d‚Äôun mod√®le avec une variable cat√©gorielle nominale seule, l‚Äôintercept repr√©sente la cat√©gorie de r√©f√©rence, ici E. Les autres colonnes sp√©cifient l‚Äôappartenance (1) ou la non-appartenance (0) de la cat√©gorie pour chaque observation. Cette matrice de mod√®le utilis√©e pour la r√©gression donnera un intercept, qui indiquera l‚Äôeffet de la cat√©gorie de r√©f√©rence, puis les diff√©rences entre les cat√©gories subs√©quentes et la cat√©gorie de r√©f√©rence. modlin_4 &lt;- lm(yield ~ topo, data = lasrosas.corn) summary(modlin_4) ## ## Call: ## lm(formula = yield ~ topo, data = lasrosas.corn) ## ## Residuals: ## Min 1Q Median 3Q Max ## -47.371 -11.933 -1.593 11.080 44.119 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 78.6653 0.5399 145.707 &lt;2e-16 *** ## topoHT -30.0526 0.7500 -40.069 &lt;2e-16 *** ## topoLO 6.2832 0.7293 8.615 &lt;2e-16 *** ## topoW -11.8841 0.7039 -16.883 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.59 on 3439 degrees of freedom ## Multiple R-squared: 0.4596, Adjusted R-squared: 0.4591 ## F-statistic: 975 on 3 and 3439 DF, p-value: &lt; 2.2e-16 Le mod√®le lin√©aire est √©quivalent √† l‚Äôanova, mais les r√©sultats de lm sont plus √©labor√©s. summary(aov(yield ~ topo, data = lasrosas.corn)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## topo 3 622351 207450 975 &lt;2e-16 *** ## Residuals 3439 731746 213 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 L‚Äôanalyse de r√©sidus peut √™tre effectu√©e de la m√™me mani√®re. 6.8.1.5 Mod√®les lin√©aires univari√©s avec variable cat√©gorielle ordinale Bien que j‚Äôintroduise la r√©gression sur variable cat√©gorielle ordinale √† la suite de la section sur les variables nominales, nous revenons dans ce cas √† une r√©gression simple, univari√©e. Voyons un cas √† 5 niveaux. statut &lt;- c(&quot;Totalement en d√©saccord&quot;, &quot;En d√©saccord&quot;, &quot;Ni en accord, ni en d√©saccord&quot;, &quot;En accord&quot;, &quot;Totalement en accord&quot;) statut_o &lt;- factor(statut, levels = statut, ordered=TRUE) model.matrix(~statut_o) # ou bien, sans passer par model.matrix, contr.poly(5) o√π 5 est le nombre de niveaux ## (Intercept) statut_o.L statut_o.Q statut_o.C statut_o^4 ## 1 1 -0.6324555 0.5345225 -3.162278e-01 0.1195229 ## 2 1 -0.3162278 -0.2672612 6.324555e-01 -0.4780914 ## 3 1 0.0000000 -0.5345225 -4.095972e-16 0.7171372 ## 4 1 0.3162278 -0.2672612 -6.324555e-01 -0.4780914 ## 5 1 0.6324555 0.5345225 3.162278e-01 0.1195229 ## attr(,&quot;assign&quot;) ## [1] 0 1 1 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$statut_o ## [1] &quot;contr.poly&quot; La matrice de mod√®le a 5 colonnes, soit le nombre de niveaux: un intercept, puis 4 autres d√©signant diff√©rentes valeurs que peuvent prendre les niveaux. Ces niveaux croient-ils lin√©airement? De mani√®re quadratique, cubique ou plus loin dans des distributions polynomiales? modmat_tidy &lt;- data.frame(statut, model.matrix(~statut_o)[, -1]) %&gt;% gather(variable, valeur, -statut) modmat_tidy$statut &lt;- factor(modmat_tidy$statut, levels = statut, ordered=TRUE) ggplot(data = modmat_tidy, mapping = aes(x = statut, y = valeur)) + facet_wrap(. ~ variable) + geom_point() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) R√®gle g√©n√©rale, pour les variables ordinales, on pr√©f√©rera une distribution lin√©aire, et c‚Äôest l‚Äôoption par d√©faut de la fonction lm(). L‚Äôutilisation d‚Äôune autre distribution peut √™tre effectu√©e √† la mitaine en utilisant dans le mod√®le la colonne d√©sir√©e de la sortie de la fonction model.matrix(). 6.8.1.6 R√©gression multiple √† plusieurs variables Reprenons le tableau de donn√©es du rendement de ma√Øs. head(lasrosas.corn) ## year lat long yield nitro topo bv rep nf ## 1 1999 -33.05113 -63.84886 72.14 131.5 W 162.60 R1 N5 ## 2 1999 -33.05115 -63.84879 73.79 131.5 W 170.49 R1 N5 ## 3 1999 -33.05116 -63.84872 77.25 131.5 W 168.39 R1 N5 ## 4 1999 -33.05117 -63.84865 76.35 131.5 W 176.68 R1 N5 ## 5 1999 -33.05118 -63.84858 75.55 131.5 W 171.46 R1 N5 ## 6 1999 -33.05120 -63.84851 70.24 131.5 W 170.56 R1 N5 Pour ajouter des variables au mod√®le dans l‚Äôinterface-formule, on additionne les noms de colonne. La variable lat d√©signe la latitude, la variable long d√©signe la latitude et la variable bv (brightness value) d√©signe la teneur en mati√®re organique du sol (plus bv est √©lev√©e, plus faible est la teneur en mati√®re organique). modlin_5 &lt;- lm(yield ~ lat + long + nitro + topo + bv, data = lasrosas.corn) summary(modlin_5) ## ## Call: ## lm(formula = yield ~ lat + long + nitro + topo + bv, data = lasrosas.corn) ## ## Residuals: ## Min 1Q Median 3Q Max ## -48.405 -11.071 -1.251 10.592 40.078 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.946e+05 3.309e+04 5.882 4.45e-09 *** ## lat 5.541e+03 4.555e+02 12.163 &lt; 2e-16 *** ## long 1.776e+02 4.491e+02 0.395 0.693 ## nitro 6.867e-02 5.451e-03 12.597 &lt; 2e-16 *** ## topoHT -2.665e+01 1.087e+00 -24.520 &lt; 2e-16 *** ## topoLO 5.565e+00 1.035e+00 5.378 8.03e-08 *** ## topoW -1.465e+01 1.655e+00 -8.849 &lt; 2e-16 *** ## bv -5.089e-01 3.069e-02 -16.578 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.47 on 3435 degrees of freedom ## Multiple R-squared: 0.5397, Adjusted R-squared: 0.5387 ## F-statistic: 575.3 on 7 and 3435 DF, p-value: &lt; 2.2e-16 L‚Äôampleur des coefficients est relatif √† l‚Äô√©chelle de la variable. En effet, un coefficient de 5541 sur la variable lat n‚Äôest pas comparable au coefficient de la variable bv, de -0.5089, √©tant donn√© que les variables ne sont pas exprim√©es avec la m√™me √©chelle. Pour les comparer sur une m√™me base, on peut centrer (soustraire la moyenne) et r√©duire (diviser par l‚Äô√©cart-type). lasrosas.corn_sc &lt;- lasrosas.corn %&gt;% mutate_at(c(&quot;lat&quot;, &quot;long&quot;, &quot;nitro&quot;, &quot;bv&quot;), scale) modlin_5_sc &lt;- lm(yield ~ lat + long + nitro + topo + bv, data = lasrosas.corn_sc) summary(modlin_5_sc) ## ## Call: ## lm(formula = yield ~ lat + long + nitro + topo + bv, data = lasrosas.corn_sc) ## ## Residuals: ## Min 1Q Median 3Q Max ## -48.405 -11.071 -1.251 10.592 40.078 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 78.9114 0.6666 118.376 &lt; 2e-16 *** ## lat 3.9201 0.3223 12.163 &lt; 2e-16 *** ## long 0.3479 0.8796 0.395 0.693 ## nitro 2.9252 0.2322 12.597 &lt; 2e-16 *** ## topoHT -26.6487 1.0868 -24.520 &lt; 2e-16 *** ## topoLO 5.5647 1.0347 5.378 8.03e-08 *** ## topoW -14.6487 1.6555 -8.849 &lt; 2e-16 *** ## bv -4.9253 0.2971 -16.578 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.47 on 3435 degrees of freedom ## Multiple R-squared: 0.5397, Adjusted R-squared: 0.5387 ## F-statistic: 575.3 on 7 and 3435 DF, p-value: &lt; 2.2e-16 Typiquement, les variables cat√©gorielles, qui ne sont pas mises √† l‚Äô√©chelle, donneront des coefficients plus √©lev√©es, et devrons √™tre √©valu√©es entre elles et non comparativement aux variables mises √† l‚Äô√©chelle. Une mani√®re conviviale de repr√©senter des coefficients consiste √† utiliser la fonction tidy du module broom, qui g√©n√®re un tableau contennt les coefficients ainsi que leurs intervalles de confiance, que nous pourrons ensuite porter graphiquement. library(&quot;broom&quot;) # ou bien charger le m√©ta-module tidymodels intervals &lt;- tidy(modlin_5_sc, conf.int = TRUE, conf.level = 0.95) intervals ## # A tibble: 8 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 78.9 0.667 118. 0. 77.6 80.2 ## 2 lat 3.92 0.322 12.2 2.34e- 33 3.29 4.55 ## 3 long 0.348 0.880 0.395 6.93e- 1 -1.38 2.07 ## 4 nitro 2.93 0.232 12.6 1.33e- 35 2.47 3.38 ## 5 topoHT -26.6 1.09 -24.5 1.74e-122 -28.8 -24.5 ## 6 topoLO 5.56 1.03 5.38 8.03e- 8 3.54 7.59 ## 7 topoW -14.6 1.66 -8.85 1.39e- 18 -17.9 -11.4 ## 8 bv -4.93 0.297 -16.6 1.92e- 59 -5.51 -4.34 La valeur par d√©faut de l‚Äôargument conf.level est de 0.95, mais je vous sugg√®re de toujours l‚Äô√©crire de mani√®re explicite, ne serait-ce que pour rappeler √† vous-m√™me ainsi qu‚Äô√† vos coll√®gues, que cette valeur est arbitraire: il s‚Äôagit d‚Äôune d√©cision d‚Äôanalyse, non pas d‚Äôune valeur √† utiliser par convention. Pour le graphique, on aura avantage √† s√©parer les effets cat√©goriels aux effets num√©riques pour mieux interpr√©ter leurs effets entre eux. J‚Äôutilise la fonction dplyr::case_when() pour cr√©er une nouvelle colonne qui cat√©gorisera les termes de l‚Äô√©quation. Cette cat√©gorie me permettra d‚Äôeffectuer un facet_wrap(). intervals %&gt;% mutate(type = case_when( term %in% c(&quot;topoHT&quot;, &quot;topoLO&quot;, &quot;topoW&quot;) ~ &quot;Cat√©gorie&quot;, # condition ~ r√©sultat term == &quot;(Intercept)&quot; ~ &quot;Intercept&quot;, # condition ~ r√©sultat TRUE ~ &quot;num√©rique&quot; # pour toute autre condition (TRUE) ~ r√©sultat )) %&gt;% ggplot(mapping = aes(x = estimate, y = term)) + geom_vline(xintercept = 0, lty = 2) + geom_segment(mapping = aes(x = conf.low, xend = conf.high, yend = term)) + geom_point() + labs(x = &quot;Coefficient standardis√©&quot;, y = &quot;&quot;) + facet_wrap(~type, scales = &quot;free&quot;, ncol = 1, strip.position = &quot;right&quot;) On y voit qu‚Äô√† l‚Äôexception de la variable long, tous les coefficients sont √©loign√©s de 0. Le coefficient bv est n√©gatif, indiquant que plus la valeur de bv est √©lev√© (donc plus le sol est pauvre en mati√®re organique), plus le rendement est faible. Plus la latitude est √©lev√©e (plus on se dirige vers le Nord de l‚ÄôArgentine), plus le rendement est √©lev√©. La dose d‚Äôazote a aussi un effet statistique positif sur le rendement. Quant aux cat√©gories topographiques, elles sont toutes √©loign√©es de la cat√©gorie E, plac√©e √† z√©ro. De plus, les intervalles de confiance √† 0.95 ne se chevauchant pas, on peut conclure que la variabilit√© du ph√©nom√®ne √©chantillonn√© n‚Äôest pas suffisante pour expliquer les diff√©rences importantes d‚Äôune √† l‚Äôautre. On pourra retrouver des cas o√π l‚Äôeffet combin√© de plusieurs variables diff√®re de l‚Äôeffet des deux variables prises s√©par√©ment. Par exemple, on pourrait √©valuer l‚Äôeffet de l‚Äôazote et celui de la topographie dans un m√™me mod√®le, puis y ajouter une interaction entre l‚Äôazote et la topographie, qui d√©finira des effets suppl√©mentaires de l‚Äôazote selon chaque cat√©gorie topographique. C‚Äôest ce que l‚Äôon appelle une interaction. Dans l‚Äôinterface-formule, l‚Äôinteraction entre l‚Äôazote et la topographie est not√©e nitro:topo. Pour ajouter cette interaction, la formule deviendra yield ~ nitro + topo + nitro:topo. Une approche √©quivalente est d‚Äôutiliser le raccourci yield ~ nitro * topo. modlin_5_sc &lt;- lm(yield ~ nitro * topo, data = lasrosas.corn_sc) summary(modlin_5_sc) ## ## Call: ## lm(formula = yield ~ nitro * topo, data = lasrosas.corn_sc) ## ## Residuals: ## Min 1Q Median 3Q Max ## -47.984 -11.985 -1.388 10.339 40.636 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 78.6999 0.5322 147.870 &lt; 2e-16 *** ## nitro 1.8131 0.5351 3.388 0.000711 *** ## topoHT -30.0052 0.7394 -40.578 &lt; 2e-16 *** ## topoLO 6.2026 0.7190 8.627 &lt; 2e-16 *** ## topoW -11.9628 0.6939 -17.240 &lt; 2e-16 *** ## nitro:topoHT 1.2553 0.7461 1.682 0.092565 . ## nitro:topoLO 0.5695 0.7186 0.792 0.428141 ## nitro:topoW 0.7702 0.6944 1.109 0.267460 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.38 on 3435 degrees of freedom ## Multiple R-squared: 0.4756, Adjusted R-squared: 0.4746 ## F-statistic: 445.1 on 7 and 3435 DF, p-value: &lt; 2.2e-16 Les r√©sultats montre des effets de l‚Äôazote et des cat√©gories topographiques, mais il y a davantage d‚Äôincertitude sur les interactions, indiquant que l‚Äôeffet statistique de l‚Äôazote est sensiblement le m√™me ind√©pendamment des niveaux topographiques. Dans le cas des r√©gressions multiples, les r√©sidus ne peuvent pas √™tre pr√©sent√©s selon une variable explicative \\(x\\), puisqu‚Äôil y en a plusieurs. On fera l‚Äôanalyse des r√©sidus selon la variable r√©ponse \\(y\\). tibble( y = lasrosas.corn_sc$yield, residus = residuals(modlin_5_sc) ) %&gt;% ggplot(aes(x = y, y = residus)) + geom_point() + labs(x = &quot;y&quot;, y = &quot;R√©sidus&quot;) + geom_hline(yintercept = 0, col = &quot;red&quot;, size = 1) Dans ce mod√®le, il y a clairement une structure qui nous √©chappe! L‚Äôajout de d‚Äôautres variables nous permettrait √©ventuellement d‚Äôobtenir une distribution qui s‚Äôapproche d‚Äôun bruit. 6.8.1.7 Les interactions Une interaction est un effet suppl√©mentaire qui est investigu√© pour des combinaisons de variables. L‚Äôinteraction entre l‚Äôazote et la topographie est une nouvelle variable cr√©√©e par la multiplication de l‚Äôazote, une variable num√©rique, et de la topographie, qui ici est une variable cat√©gorielle. model.matrix(~ nitro * topo, data = lasrosas.corn_sc) %&gt;% head() ## (Intercept) nitro topoHT topoLO topoW nitro:topoHT nitro:topoLO ## 1 1 1.571194 0 0 1 0 0 ## 2 1 1.571194 0 0 1 0 0 ## 3 1 1.571194 0 0 1 0 0 ## 4 1 1.571194 0 0 1 0 0 ## 5 1 1.571194 0 0 1 0 0 ## 6 1 1.571194 0 0 1 0 0 ## nitro:topoW ## 1 1.571194 ## 2 1.571194 ## 3 1.571194 ## 4 1.571194 ## 5 1.571194 ## 6 1.571194 L‚Äôent√™te de la matrice mod√®le montre que l‚Äôinteraction est l‚Äôaddition de trois variables, qui sont nulles si la cat√©gorie topographique est absente, mais qui prend la dose d‚Äôazote pour la cat√©gorie pr√©sente seulement. L‚Äôinterpr√©tation d‚Äôune interaction est sp√©cifique au mod√®le utilis√©. Une mani√®re de l‚Äôinterpr√©ter est de se demander dans quelles unit√©s elle est exprim√©e. Dans notre exemple, il s‚Äôagit de kg/ha standardis√©s. Prenons un autre exemple, cette fois-ci avec des donn√©es fictives. Une enqu√™te a √©t√© men√©e, o√π des personnes √©valuait le karma (√©chelle 0 √† 10) de pieds nus, en bas (chaussettes) et/ou en sandales. karma_df &lt;- read_csv(&quot;data/karma_df.csv&quot;) ## Parsed with column specification: ## cols( ## ID = col_double(), ## sandales = col_double(), ## bas = col_double(), ## karma = col_double() ## ) Nous d√©sirons savoir quelle est l‚Äôeffet des bas et des sandales sur le karma, donc üíñ ~ üë° + üß¶. tidy(lm(karma ~ sandales + bas, karma_df)) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 3.67 0.147 25.0 1.11e-94 ## 2 sandales 2.29 0.166 13.8 7.99e-38 ## 3 bas 1.77 0.168 10.5 6.31e-24 √Ä partir du sc√©nario √† pieds nus d‚Äôun karma de 3.67, les sandales ajoutent 2.29 de points de karma, alors que les bas en ajoutent 1.8. Mais ce mod√®le est incomplet, cas on n‚Äô√©value pas l‚Äôeffet des bas ET des sandales, donc üíñ ~ üë° * üß¶. tidy(lm(karma ~ sandales * bas, karma_df)) ## # A tibble: 4 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 2.68 0.134 20.0 2.22e-68 ## 2 sandales 3.68 0.159 23.2 4.07e-85 ## 3 bas 3.20 0.162 19.8 1.40e-67 ## 4 sandales:bas -5.25 0.309 -17.0 3.79e-53 Le mod√®les est plus clair. Sans interaction, les effets sur le karma des bas et des sandales √©taient n√©gativement affect√©s par l‚Äôeffet d‚Äôinteraction sandales:bas, le karma √©tant pouss√© √† la baisse par le bas blanc dans vos sandales. Il est possible d‚Äôajouter des interactions doubles, triples, quadruples, etc. Mais plus il y a d‚Äôinteractions, plus votre mod√®le comprendra de variables et vos tests d‚Äôhypoth√®se perdront en puissance statistique. 6.8.1.8 Les mod√®les lin√©aires g√©n√©ralis√©s Dans un mod√®le lin√©aire ordinaire, un changement constant dans les variables explicatives r√©sulte en un changement constant de la variable-r√©ponse. Cette supposition ne serait pas ad√©quate si la variable-r√©ponse √©tait un d√©compte, si elle est bool√©enne ou si, de mani√®re g√©n√©rale, la variable-r√©ponse ne suivait pas une distribution continue. Ou, de mani√®re plus sp√©cifique, il n‚Äôy a pas moyen de retrouver une distribution normale des r√©sidus? On pourra bien s√ªr transformer les variables (sujet du chapitre 6, en d√©veloppement). Mais il pourrait s‚Äôav√©rer impossible, ou tout simplement non souhaitable de transformer les variables. Le mod√®le lin√©aire g√©n√©ralis√© (MLG, ou generalized linear model - GLM) est une g√©n√©ralisation du mod√®le lin√©aire ordinaire chez qui la variable-r√©ponse peut √™tre caract√©ris√© par une distribution de Poisson, de Bernouilli, etc. Prenons d‚Äôabord cas d‚Äôun d√©compte de vers fil-de-fer (worms) retrouv√©s dans des parcelles sous diff√©rents traitements (trt). Les d√©comptes sont typiquement distribu√© selon une loi de Poisson. cochran.wireworms %&gt;% ggplot(aes(x = worms)) + geom_histogram(bins = 10) Explorons les d√©comptes selon les traitements. cochran.wireworms %&gt;% ggplot(aes(x = trt, y = worms)) + geom_boxplot() Les traitements semble √† premi√®re vue avoir un effet comparativement au contr√¥le. Lan√ßons un MLG avec la fonction glm(), et sp√©cifions que la sortie est une distribution de Poisson. Bien que la fonction de lien (link = \"log\") soit explictement impos√©e, le log est la valeur par d√©faut pour les distributions de Poisson. Ainsi, les coefficients du mod√®les devront √™tre interpr√©t√©s selon un mod√®le \\(log \\left(worms \\right) = intercept + pente \\times coefficient\\). modglm_1 &lt;- glm(worms ~ trt, cochran.wireworms, family = stats::poisson(link=&quot;log&quot;)) summary(modglm_1) ## ## Call: ## glm(formula = worms ~ trt, family = stats::poisson(link = &quot;log&quot;), ## data = cochran.wireworms) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.8279 -0.9455 -0.2862 0.6916 3.1888 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.1823 0.4082 0.447 0.655160 ## trtM 1.6422 0.4460 3.682 0.000231 *** ## trtN 1.7636 0.4418 3.991 6.57e-05 *** ## trtO 1.5755 0.4485 3.513 0.000443 *** ## trtP 1.3437 0.4584 2.931 0.003375 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 64.555 on 24 degrees of freedom ## Residual deviance: 38.026 on 20 degrees of freedom ## AIC: 125.64 ## ## Number of Fisher Scoring iterations: 5 L‚Äôinterpr√©tation sp√©cifique des coefficients d‚Äôune r√©gression de Poisson doit passer par la fonction de lien \\(log \\left(worms \\right) = intercept + pente \\times coefficient\\). Le traitement de r√©f√©rence (K), qui correspond √† l‚Äôintercept, sera accompagn√© d‚Äôun nombre de vers de \\(exp \\left(0.1823\\right) = 1.20\\) vers, et le traitement M, √† \\(exp \\left(1.6422\\right) = 5.17\\) vers. Cela correspond √† ce que l‚Äôon observe sur les boxplots plus haut. Il est tr√®s probable (p-value de ~0.66) qu‚Äôun intercept (traitement K) de 0.18 ayant une erreur standard de 0.4082 ait √©t√© g√©n√©r√© depuis une population dont l‚Äôintercept est nul. Quant aux autres traitements, leurs effets sont tous significatifs au seuil 0.05, mais peuvent-ils √™tre consid√©r√©s comme √©quivalents? intervals &lt;- tibble(Estimate = coefficients(modglm_1), # [-1] enlever l&#39;intercept LL = confint(modglm_1)[, 1], # [-1, ] enlever la premi√®re ligne, celle de l&#39;intercept UL = confint(modglm_1)[, 2], variable = names(coefficients(modglm_1))) ## Waiting for profiling to be done... ## Waiting for profiling to be done... intervals ## # A tibble: 5 x 4 ## Estimate LL UL variable ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.182 -0.740 0.888 (Intercept) ## 2 1.64 0.840 2.62 trtM ## 3 1.76 0.972 2.74 trtN ## 4 1.58 0.766 2.56 trtO ## 5 1.34 0.509 2.34 trtP ggplot(data = intervals, mapping = aes(x = Estimate, y = variable)) + geom_vline(xintercept = 0, lty = 2) + geom_segment(mapping = aes(x = LL, xend = UL, y = variable, yend = variable)) + geom_point() + labs(x = &quot;Coefficient&quot;, y = &quot;&quot;) Les intervalles de confiance se superposant, on ne peut pas conclure qu‚Äôun traitement est li√© √† une r√©duction plus importante de vers qu‚Äôun autre, au seuil 0.05. Maintenant, √† d√©faut de trouver un tableau de donn√©es plus appropri√©, prenons le tableau mtcars, qui rassemble des donn√©es sur des mod√®les de voitures. La colonne vs, pour v-shaped, inscrit 0 si les pistons sont droit et 1 s‚Äôils sont plac√©s en V dans le moteur. Peut-on expliquer la forme des pistons selon le poids du v√©hicule (wt)? mtcars %&gt;% sample_n(6) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## 2 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 ## 3 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## 4 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## 5 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## 6 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 mtcars %&gt;% ggplot(aes(x = wt, y = vs)) + geom_point() Il semble y avoir une tendance: les v√©hicules plus lourds ont plut√¥t des pistons droits (vs = 0). V√©rifions cela. modglm_2 &lt;- glm(vs ~ wt, data = mtcars, family = stats::binomial()) summary(modglm_2) ## ## Call: ## glm(formula = vs ~ wt, family = stats::binomial(), data = mtcars) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.9003 -0.7641 -0.1559 0.7223 1.5736 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 5.7147 2.3014 2.483 0.01302 * ## wt -1.9105 0.7279 -2.625 0.00867 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.860 on 31 degrees of freedom ## Residual deviance: 31.367 on 30 degrees of freedom ## AIC: 35.367 ## ## Number of Fisher Scoring iterations: 5 Exercice. Analyser les r√©sultats. 6.8.1.9 Les mod√®les non-lin√©aires La hauteur d‚Äôun arbre en fonction du temps n‚Äôest typiquement pas lin√©aire. Elle tend √† cro√Ætre de plus en plus lentement jusqu‚Äô√† un plateau. De m√™me, le rendement d‚Äôune culture trait√© avec des doses croissantes de fertilisants tend √† atteindre un maximum, puis √† se stabiliser. Ces ph√©nom√®nes ne peuvent pas √™tre approxim√©s par des mod√®les lin√©aires. Examinons les donn√©es du tableau engelstad.nitro. engelstad.nitro %&gt;% sample_n(10) ## loc year nitro yield ## 1 Knoxville 1966 0 63.0 ## 2 Knoxville 1965 335 61.2 ## 3 Jackson 1965 335 73.0 ## 4 Jackson 1966 201 61.3 ## 5 Jackson 1966 335 59.8 ## 6 Knoxville 1964 0 60.9 ## 7 Knoxville 1964 67 75.9 ## 8 Jackson 1966 67 45.2 ## 9 Jackson 1962 201 73.1 ## 10 Jackson 1964 335 67.8 engelstad.nitro %&gt;% ggplot(aes(x = nitro, y = yield)) + facet_grid(year ~ loc) + geom_line() + geom_point() Le mod√®le de Mitscherlich pourrait √™tre utilis√©. \\[ y = A \\left( 1 - e^{-R \\left( E + x \\right)} \\right) \\] o√π \\(y\\) est le rendement, \\(x\\) est la dose, \\(A\\) est l‚Äôasymptote vers laquelle la courbe converge √† dose croissante, \\(E\\) est l‚Äô√©quivalent de dose fourni par l‚Äôenvironnement et \\(R\\) est le taux de r√©ponse. Explorons la fonction. mitscherlich_f &lt;- function(x, A, E, R) { A * (1 - exp(-R*(E + x))) } x &lt;- seq(0, 350, by = 5) y &lt;- mitscherlich_f(x, A = 75, E = 30, R = 0.02) ggplot(tibble(x, y), aes(x, y)) + geom_point(data = engelstad.nitro, aes(x = nitro, y = yield)) + geom_line() + ylim(c(0, 100)) Exercice. Changez les param√®tres pour visualiser comment la courbe r√©agit. Nous pouvons d√©crire le mod√®le gr√¢ce √† l‚Äôinterface formule dans la fonction nls(). Notez que les mod√®les non-lin√©aires demandent des strat√©gies de calcul diff√©rentes de celles des mod√®les lin√©aires. En tout temps, nous devons identifier des valeurs de d√©part raisonnables pour les param√®tres dans l‚Äôargument start. Vous r√©ussirez rarement √† obtenir une convergence du premier coup avec vos param√®tres de d√©part. Le d√©fi est d‚Äôen trouver qui permettront au mod√®le de converger. Parfois, le mod√®le ne convergera jamais. D‚Äôautres fois, il convergera vers des solutions diff√©rentes selon les variables de d√©part choisies. modnl_1 &lt;- nls(yield ~ A * (1 - exp(-R*(E + nitro))), data = engelstad.nitro, start = list(A = 50, E = 10, R = 0.2)) Le mod√®le ne converge pas (le bloc de calcul est d√©sactiv√©). Essayons les valeurs prises plus haut, lors de la cr√©ation du graphique, qui semblent bien s‚Äôajuster. modnl_1 &lt;- nls(yield ~ A * (1 - exp(-R*(E + nitro))), data = engelstad.nitro, start = list(A = 75, E = 30, R = 0.02)) Bingo! Voyons maintenant le sommaire. summary(modnl_1) ## ## Formula: yield ~ A * (1 - exp(-R * (E + nitro))) ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## A 75.023427 3.331860 22.517 &lt;2e-16 *** ## E 66.164110 27.251591 2.428 0.0184 * ## R 0.012565 0.004881 2.574 0.0127 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12.34 on 57 degrees of freedom ## ## Number of iterations to convergence: 5 ## Achieved convergence tolerance: 8.067e-06 Les param√®tres sont diff√©rents de z√©ro, et donnent la courbe suivante. x &lt;- seq(0, 350, by = 5) y &lt;- mitscherlich_f(x, A = coefficients(modnl_1)[1], E = coefficients(modnl_1)[2], R = coefficients(modnl_1)[3]) ggplot(tibble(x, y), aes(x, y)) + geom_point(data = engelstad.nitro, aes(x = nitro, y = yield)) + geom_line() + ylim(c(0, 100)) Et les r√©sidus‚Ä¶ tibble(res = residuals(modnl_1)) %&gt;% ggplot(aes(x = res)) + geom_histogram(bins = 20) tibble(nitro = engelstad.nitro$nitro, res = residuals(modnl_1)) %&gt;% ggplot(aes(x = nitro, y = res)) + geom_point() + geom_hline(yintercept = 0, colour = &quot;red&quot;) Les r√©sidus ne sont pas distribu√©s normalement, mais semble bien partag√©s de part et d‚Äôautre de la courbe. 6.8.2 Mod√®les √† effets mixtes Lorsque l‚Äôon combine des variables fixes (test√©es lors de l‚Äôexp√©rience) et des variables al√©atoire (variation des unit√©s exp√©rimentales), on obtient un mod√®le mixte. Les mod√®les mixtes peuvent √™tre univari√©s, multivari√©s, lin√©aires ordinaires ou g√©n√©ralis√©s ou non lin√©aires. √Ä la diff√©rence d‚Äôun effet fixe, un effet al√©atoire sera toujours distribu√© normalement avec une moyenne de 0 et une certaine variance. Dans un mod√®le lin√©aire o√π l‚Äôeffet al√©atoire est un d√©calage d‚Äôintercept, cet effet s‚Äôadditionne aux effets fixes: \\[ y = X \\beta + Z b + \\epsilon \\] o√π: \\(Z\\) est la matrice du mod√®le √† \\(n\\) observations et \\(p\\) variables al√©atoires. Les variables al√©atoires sont souvent des variables nominales qui subissent un encodage cat√©goriel. \\[ Z = \\left( \\begin{matrix} z_{11} &amp; \\cdots &amp; z_{1p} \\\\ z_{21} &amp; \\cdots &amp; z_{2p} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ z_{n1} &amp; \\cdots &amp; z_{np} \\end{matrix} \\right) \\] \\(b\\) est la matrice des \\(p\\) coefficients al√©atoires. \\[ b = \\left( \\begin{matrix} b_0 \\\\ b_1 \\\\ \\vdots \\\\ b_p \\end{matrix} \\right) \\] Le tableau lasrosas.corn, utilis√© pr√©c√©demment, contenait trois r√©p√©titions effectu√©s au cours de deux ann√©es, 1999 et 2001. √âtant donn√© que la r√©p√©tition R1 de 1999 n‚Äôa rien √† voir avec la r√©p√©tition R1 de 2001, on dit qu‚Äôelle est embo√Æt√©e dans l‚Äôann√©e. Le module nlme nous aidera √† monter notre mod√®le mixte. library(&quot;nlme&quot;) ## ## Attaching package: &#39;nlme&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## collapse mmodlin_1 &lt;- lme(fixed = yield ~ lat + long + nitro + topo + bv, random = ~ 1|year/rep, data = lasrosas.corn) √Ä ce stade vous devriez commencer √† √™tre familier avec l‚Äôinterface formule et vous deviez saisir l‚Äôargument fixed, qui d√©signe l‚Äôeffet fixe. L‚Äôeffet al√©atoire, random, suit un tilde ~. √Ä gauche de la barre verticale |, on place les variables d√©signant les effets al√©atoire sur la pente. Nous n‚Äôavons pas couvert cet aspect, alors nous le laissons √† 1. √Ä droite, on retrouve un structure d‚Äôembo√Ætement d√©signant l‚Äôeffet al√©atoire: le premier niveau est l‚Äôann√©e, dans laquelle est embo√Æt√©e la r√©p√©tition. {r biostats-multivariate-nlme-model}-summ summary(mmodlin_1) La sortie est semblable √† celle de la fonction lm(). 6.8.2.1 Mod√®les mixtes non-lin√©aires Le mod√®le non lin√©aire cr√©√© plus haut liait le rendement √† la dose d‚Äôazote. Toutefois, les unit√©s exp√©rimentales (le site loc et l‚Äôann√©e year) n‚Äô√©taient pas pris en consid√©ration. Nous allons maintenant les consid√©rer. Nous devons d√©cider la structure de l‚Äôeffet al√©atoire, et sur quelles variables il doit √™tre appliqu√© - la d√©cision appartient √† l‚Äôanalyste. Il me semble plus convenable de supposer que le site et l‚Äôann√©e affectera le rendement maximum plut√¥t que l‚Äôenvironnement et le taux: les effets al√©atoires seront donc affect√©s √† la variable A. Les effets al√©atoires n‚Äôont pas de structure d‚Äôembo√Ætement. L‚Äôeffet de l‚Äôann√©e sur A sera celui d‚Äôune pente et l‚Äôeffet de site sera celui de l‚Äôintercept. La fonction que nous utiliserons est nlme(). mm &lt;- nlme(yield ~ A * (1 - exp(-R*(E + nitro))), data = engelstad.nitro, start = c(A = 75, E = 30, R = 0.02), fixed = list(A ~ 1, E ~ 1, R ~ 1), random = A ~ year | loc) summary(mm) ## Nonlinear mixed-effects model fit by maximum likelihood ## Model: yield ~ A * (1 - exp(-R * (E + nitro))) ## Data: engelstad.nitro ## AIC BIC logLik ## 477.2286 491.889 -231.6143 ## ## Random effects: ## Formula: A ~ year | loc ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## A.(Intercept) 2.611534836 A.(In) ## A.year 0.003066832 -0.556 ## Residual 11.152757999 ## ## Fixed effects: list(A ~ 1, E ~ 1, R ~ 1) ## Value Std.Error DF t-value p-value ## A.(Intercept) 74.58222 4.722715 56 15.792234 0.0000 ## E 65.56721 25.533993 56 2.567840 0.0129 ## R 0.01308 0.004808 56 2.720215 0.0087 ## Correlation: ## A.(In) E ## E 0.379 ## R -0.483 -0.934 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -1.83373132 -0.89293033 0.07418165 0.68353577 1.82434347 ## ## Number of Observations: 60 ## Number of Groups: 2 Et sur graphique: engelstad.nitro %&gt;% ggplot(aes(x = nitro, y = yield)) + facet_grid(year ~ loc) + geom_line(data = tibble(nitro = engelstad.nitro$nitro, yield = predict(mm, level = 0)), colour = &quot;grey35&quot;) + geom_point() + ylim(c(0, 95)) Les mod√®les mixtes non lin√©aires peuvent devenir tr√®s complexes lorsque les param√®tres, par exemple A, E et R, sont eux-m√™me affect√©s lin√©airement par des variables (par exemple A ~ topo). Pour aller plus loin, consultez Parent et al. (2017) ainsi que les calculs associ√©s √† l‚Äôarticle. Ou √©crivez-moi un courriel pour en discuter! Note. L‚Äôinterpr√©tation de p-values sur les mod√®les mixtes est controvers√©e. √Ä ce sujet, Douglas Bates a √©crit une longue lettre √† la communaut√© de d√©veloppement du module lme4, une alternative √† nlme, qui remet en cause l‚Äôutilisation des p-values, ici. De plus en plus, pour les mod√®les mixtes, on se tourne vers les statistiques bay√©siennes, couvertes dans le chapitre 7 avec le module greta. Mais en ce qui a trait aux mod√®les mixtes, le module brms automatise bien des aspects de l‚Äôapproche bay√©sienne. 6.8.3 Aller plus loin 6.8.3.1 Statistiques g√©n√©rales: The analysis of biological data 6.8.3.2 Statistiques avec R Disponibles en version √©lectronique √† la biblioth√®que de l‚ÄôUniversit√© Laval: Introduction aux statistiques avec R: Introductory statistics with R Approfondir les statistiques avec R: The R Book, Second edition Approfondir les mod√®les √† effets mixtes avec R: Mixed Effects Models and Extensions in Ecology with R ModernDive, un livre en ligne offrant une approche moderne avec le package moderndive. "]
]
