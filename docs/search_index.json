[
["index.html", "Analyse et mod√©lisation d‚Äôagro√©cosyst√®mes 1 Introduction 1.1 D√©finitions 1.2 √Ä qui s‚Äôadresse ce manuel? 1.3 Les logiciels libres 1.4 Langage de programmation 1.5 Contenu du manuel 1.6 Objectifs g√©n√©raux 1.7 Lectures compl√©mentaires 1.8 Besoin d‚Äôaide? 1.9 √Ä propos de l‚Äôauteur 1.10 Un cours compl√©mentaire √† d‚Äôautres cours 1.11 Contribuer au manuel", " Analyse et mod√©lisation d‚Äôagro√©cosyst√®mes Serge-√âtienne Parent 2020-02-11 1 Introduction En d√©veloppant son jeu de la vie (game of life) en 1970, John Horton Connway a pr√©sent√© un exemple percutant que des r√®gles simples peuvent mener √† des r√©sultats inattendus. Le jeu consiste √† placer des jetons sur les cases d‚Äôun plateau de jeu consistant en une simple grille orthogonale. Le jeu √©volue en fonction du nombre de jetons pr√©sents parmi les huit cases du voisinage des jetons ou des cases vides. Les jetons ayant 0 ou 1 voisin sont retir√©s. Les jetons ayant 2 ou 3 voisins restent intacts Les jetons ayant plus de 3 voisins sont retir√©s Un jeton est pos√© sur les cases ayant exactement 3 voisins C‚Äôest tout. Selon la mani√®re dont les jetons sont plac√©s au d√©part, il se peut que la grille se vide de ses jetons, ou que les jetons y prennent beaucoup de place. Il arrive aussi que des cycles r√©guliers se d√©gagent ou que l‚Äôon se retrouve avec des formes r√©guli√®res. Vous aurez peut-√™tre compris √† ce stade pourquoi le jeu est appel√© ‚Äújeu de la vie‚Äù. La premi√®re r√®gle est une situation localis√©e de sous-population, condition dans laquelle la reproduction est difficile. La deuxi√®me r√®gle est une situation localis√©e stable. La troisi√®me est une situation de surpopulation, o√π des individus meurent dans un environnement rendu inad√©quat par une insuffisance de ressource ou une toxicit√© excessive. Enfin, la quatri√®me r√®gle indique une situation favorable √† la reproduction. Une grille vid√©e correspond √† une extinction et une grille remplie correspond √† une explosion de population. Une oscillation est un ‚Äúclimax‚Äù, un √©tat stable en √©cologie. Un l√©ger changement dans la disposition initiale des jetons peut mener √† des solutions diff√©rentes. Le jeu est une application de la technique des automates cellulaires. Il se complexifie √† mesure que le nombre de jetons grandit. Un humain passera des heures √† calculer une seule ronde √† 50 jetons, commettra probablement quelques erreurs et prendra quelques caf√©s. Un processeur pourra g√©rer des centaines de rondes sur des grilles de centaines de jetons en quelques secondes. En √©tablissant des r√®gles correspondant aux m√©canismes de l‚Äôobjet √©tudi√©, il devient possible de mod√©liser l‚Äô√©volution des syst√®mes vivants, comme l‚Äô√©mergence ou le d√©clin d‚Äôesp√®ces. La figure 1.1 pr√©sente un cas simple d‚Äôautomates cellularies g√©n√©r√© dans le langage de programmation R. Figure 1.1: Simulation avec automates cellulaires g√©n√©r√©s en R. 1.1 D√©finitions Les math√©matiques conf√®rent aux humains une capacit√© d‚Äôabstraction suffisamment complexe pour leur permettre de toucher les √©toiles et les atomes, de comprendre le pass√© et de pr√©dire le futur, de toucher l‚Äôinfini et de go√ªter √† l‚Äô√©ternit√©. √Ä partir des maths, on a pu cr√©er des outils de calcul qui permettent de projeter des images de l‚Äôunivers, bien au-del√† de la Voie lact√©e. Mais appr√©hender le vivant, tout pr√®s de nous, demeure une t√¢che complexe. Figure 1.2: Domaines scientifiques de l‚Äô√©cologie math√©matique. L‚Äô√©cologie math√©matique couvre un large spectre de domaines (figure 1.2), mais peut √™tre divis√©e en deux branches: l‚Äô√©cologie th√©orique et l‚Äô√©cologie quantitative (Legendre et Legendre, 2012). Alors que l‚Äô√©cologie th√©orique s‚Äôint√©resse √† l‚Äôexpression math√©matique des m√©canismes √©cologiques, l‚Äô√©cologie quantitative, plus empirique, en √©tudie principalement les ph√©nom√®nes. La mod√©lisation √©cologique vise √† pr√©voir une situation selon des conditions donn√©es. Faisant partie √† la fois de l‚Äô√©cologie th√©orique et de l‚Äô√©cologie quantitative, elle superpose souvent des m√©canismes de l‚Äô√©cologie th√©orique et des ph√©nom√®nes empiriques de l‚Äô√©cologie quantitative. L‚Äô√©cologie num√©rique comprend la branche descriptive de l‚Äô√©cologie quantitative, c‚Äôest-√†-dire qu‚Äôelle s‚Äôint√©resse √† √©valuer des effets √† partir de donn√©es empiriques. L‚Äôexploration des donn√©es dans le but d‚Äôy d√©couvrir des structures passe souvent par des techniques multivari√©es comme la classification hi√©rarchique ou la r√©duction d‚Äôaxe (par exemple, l‚Äôanalyse en composantes principales), qui sont davantage heuristiques (dans notre cas, bioheuristique) que statistiques. Les tests d‚Äôhypoth√®ses et l‚Äôanalyse des probabilit√©s, quant √† eux, rel√®vent de la biostatistique. Le g√©nie √©cologique, une discipline intimement li√©e √† l‚Äô√©cologie math√©matique, est vou√© √† l‚Äôanalyse, la mod√©lisation, la conception et la construction de syst√®mes vivants dans le but de r√©soudre de mani√®re efficace des probl√®mes li√©s √† l‚Äô√©cologie et √† une panoplie de domaines qui lui sont raccord√©s. L‚Äôagriculture est l‚Äôun de ces domaines. C‚Äôest d‚Äôembl√©e la discipline qui sera pris√©e dans ce manuel. N√©anmoins, les principes qui seront discut√©s sont transf√©rables √† l‚Äô√©cologie g√©n√©rale. 1.2 √Ä qui s‚Äôadresse ce manuel? Le cours vise √† introduire des √©tudiant.e.s gradu√©.e.s en agronomie, biologie, √©cologie, sols, g√©nie agroenvironnemental, g√©nie civil et g√©nie √©cologique √† l‚Äôanalyse et la mod√©lisation dans leur domaine, tant pour les appuyer pour leurs travaux de recherche que pour leur fournir une trousse d‚Äôoutil √©mancipatrice pour leur cheminement professionnel. Plus sp√©cifiquement, vous serez accompagn√© √† d√©couvrir diff√©rents outils num√©riques qui vous permettront d‚Äôappr√©hender vos donn√©es, d‚Äôen faire √©merger l‚Äôinformation et de construire des mod√®les. L‚Äôobjectif de ce cours n‚Äôest pas de vous former en math√©matique, mais de vous aider √† les utiliser. En ce sens, c‚Äôest un cours de pilotage, pas un cours de m√©canique. Vous ferez tout de m√™me un peu de m√©canique pour mieux comprendre les r√©actions de notre machine. Bien que des connaissances en programmation et en statistiques aideront grandement les √©tudiant.e.s √† appr√©hender ce document, une litt√©ratie informatique n‚Äôest pas requise. Dans tous les cas, quiconque voudra tirer profit de ce manuel devra faire preuve d‚Äôautonomie. Vous serez guid√©s vers des ressources et des r√©f√©rences, mais je vous sugg√®re vivement de d√©velopper votre propre biblioth√®que adapt√©e √† vos besoins et √† votre mani√®re de comprendre. 1.3 Les logiciels libres Tous les outils num√©riques qui sont propos√©s dans ce cours sont des logiciels libres: ¬´ Logiciel libre ¬ª [free software] d√©signe des logiciels qui respectent la libert√© des utilisateurs. En gros, cela veut dire que les utilisateurs ont la libert√© d‚Äôex√©cuter, copier, distribuer, √©tudier, modifier et am√©liorer ces logiciels. Ainsi, ¬´ logiciel libre ¬ª fait r√©f√©rence √† la libert√©, pas au prix1 (pour comprendre ce concept, vous devez penser √† ¬´ libert√© d‚Äôexpression ¬ª, pas √† ¬´ entr√©e libre ¬ª). - Projet GNU Donc: codes sources ouverts, d√©veloppement souvent communautaire, gratuit√©. Plusieurs raisons √©thiques, principalement li√©es au contr√¥le de l‚Äôenvironnement virtuel par les utilisateurs et les communaut√©s, peuvent justifier l‚Äôutilisation de logiciels libres. Plusieurs raisons pratiques justifient aussi cette orientation. Les logiciels libres vous permettent de transporter vos outils avec vous, d‚Äôune entreprise √† l‚Äôautre, au bureau, ou √† la maison, et ce, sans vous soucier d‚Äôacheter de co√ªteuses licences. Il existe tout de m√™me des risques li√©s aux possibles erreurs dans les codes des logiciels communautaires. Ces risques sont d‚Äôailleurs les m√™mes que ceux li√©s aux logiciels propri√©taires. Pour les scientifiques, une erreur peut mener √† une √©tude retir√©e de la litt√©rature et m√™me, potentiellement, des politiques publiques mal avis√©es. Pour les ing√©nieurs, les cons√©quences pourraient √™tre dramatiques. Mais retenez qu‚Äôen toute circonstance, comme professionnel.le, vous √™tes responsable des outils que vous utilisez: vous devez vous assurer de la bonne qualit√© d‚Äôun logiciel, qu‚Äôil soit propri√©taire ou communautaire. Alors que la qualit√© des logiciels propri√©taires est g√©n√©ralement suivie par audits, celle des logiciels libres est plut√¥t soumise √† la vigilance communautaire. Chaque approche a ses avantages et inconv√©nients, mais elles ne sont pas exclusives. Ainsi les logiciels libres peuvent √™tre audit√©s √† l‚Äôexterne par quiconque d√©cide de le faire. Diff√©rentes entreprises, souvent concurrentes, participent tant √† cette vigilance qu‚Äôau d√©veloppement des logiciels libres: elles en sont m√™me souvent les instigatrices (comme RStudio, Anaconda et Enthought). Par ailleurs, ce manuel est distribu√© librement sous licence Creative commons, selon les termes suivants. Analyse et mod√©lisation d‚Äôagro√©cosyst√®mes de Serge-√âtienne Parent est mis √† disposition selon les termes de la licence Creative Commons Attribution - Pas d‚ÄôUtilisation Commerciale - Partage dans les M√™mes Conditions 4.0 International.Fond√©(e) sur une ≈ìuvre √† https://github.com/essicolo/ecologie-mathematique-R. 1.4 Langage de programmation 1.4.1 R Ce cours est bas√© sur le langage R. En plus d‚Äô√™tre libre, R est un langage de programmation dynamique largement utilis√© dans le monde universitaire, et dont l‚Äôutilisation s‚Äô√©tend de mani√®re soutenue hors des tours d‚Äôivoire. R is also the name of a popular programming language used by a growing number of data analysts inside corporations and academia. It is becoming their lingua franca partly because data mining has entered a golden age, whether being used to set ad prices, find new drugs more quickly or fine-tune financial models. New York Times, janvier 2019 Son d√©veloppement est support√© par la R Foundation for Statistical Computing, bas√©e √† l‚ÄôUniversit√© de Vienne. √âgalement, l‚Äô√©quipe de RStudio contribue largement au d√©veloppement de modules g√©n√©riques. R est principalement utilis√© pour le calcul statistique, mais les r√©cents d√©veloppements le rendent un outil de choix pour tout ce qui entoure la science des donn√©es, de l‚Äôinteraction avec les bases de donn√©es au d√©ploiement d‚Äôoutils d‚Äôintelligence artificielle en passant par la visualisation. Une fois impl√©ment√© avec des modules de calcul scientifique sp√©cialis√©s en biologie, en √©cologie et en agronomie (que nous couvrirons au long du cours), R devient un outil de calcul convivial, rapide et fiable. 1.4.2 Pourquoi pas Python? La premi√®re mouture de ce cours se fondait sur le langage Python. Tout comme R, Python est un langage de programmation dynamique pris√© pour le calcul scientifique. Python est un langage g√©n√©rique appr√©ci√© pour sa polyvalence et sa simplicit√©. Python est utilis√© autant pour cr√©er des logiciels ou des sites web que pour le calcul scientifique. Ainsi, Python peut √™tre utilis√© en interop√©rabilit√© avec une panoplie de logiciels libres, comme QGIS pour la cartographie et FreeCAD pour le dessin technique. Il est particuli√®rement appr√©ci√© en ing√©nierie pour ses modules de calcul par √©l√©ments finis (e.g. FeNICS) et en bioinformatique pour ses outils li√©s au s√©quen√ßage (scikit-bio), mais ses lacunes en analyse statistique, en particulier en statistiques multivari√©es m‚Äôont amen√© √† favoriser R. Bien que leurs possibilit√©s se superposent largement, ce serait une erreur d‚Äôaborder R et Python comme des langages rivaux. Les deux langages s‚Äôexpriment de mani√®re similaire et s‚Äôinspirent mutuellement: apprendre √† travailler avec l‚Äôun revient √† apprendre l‚Äôautre. Les sp√©cialistes en calcul scientifique tendent √† apprendre √† travailler avec plus d‚Äôun langage de programmation. Par ailleurs, il existe de plus en plus des moyens de travailler en R et en Python dans un m√™me flux de travail. L‚Äôinterface de calcul RStudio, que nous utiliserons pendant le cours, permet d‚Äôinclure des blocs de code en Python. 1.4.3 Pourquoi pas Matlab? Parce qu‚Äôon est en 2020. 1.4.4 Et‚Ä¶ SAS? Parce qu‚Äôon est √† l‚Äôuniversit√©. 1.4.5 Mais pourquoi pas ______ ? D‚Äôautres langages, comme Julia, Scala, Javascript et m√™me Ruby sont utilis√©s en calcul scientifique. Ils sont n√©anmoins moins garnis et moins document√©s que R. Des langages de plus bas niveau, comme Fortran et C++, viennent souvent appuyer les fonctions des autres langages: ces langages sont plus ardus √† utiliser au jour le jour, mais leur rapidit√© de calcul est imbattable. 1.5 Contenu du manuel Le pire angle avec lequel je pourrais aborder le sujet, c‚Äôest avec du code et des formules math√©matiques. √Ä travers chacun des chapitres, je tenterai de vous amener √† r√©soudre des probl√®mes de la mani√®re la plus intuitive possible. Nous aborderons l‚Äôanalyse et la mod√©lisation inf√©rentielle, pr√©dictive et d√©terministe appliqu√©e aux agro√©cosyst√®mes. Chapitre 2 - Introduction au langage de programmation R. Qu‚Äôest-ce que R? Comment l‚Äôaborder? Quelles sont les fonctionnalit√©s de base et comment tirer profit de tout l‚Äô√©cosyst√®me de programmation? Chapitre 3 - Organisation des donn√©es et op√©rations sur des tableaux. Les tableaux permettent d‚Äôench√¢sser l‚Äôinformation dans un format pr√™t-√†-porter pour R. Comment les importer, les exporter, les filtrer, et en faire des sommaires? Chapitre 4 - Visualisation. Comment pr√©senter l‚Äôinformation contenue dans un long tableau en un seul coup d‚Äôoeil? Chapitre 5 - Le travail collaboratif, le suivi de version et la science ouverte. Ce chapitre offre une introduction √† l‚Äôutilisation des outils de calcul collaboratif, ainsi qu‚Äôun aper√ßu du syst√®me de suivi de version git et de son utilisation sur GitHub. Chapitre 6 - Biostatistiques. Il est audacieux de ne consacrer qu‚Äôun seul chapitre sur ce vaste sujet. Nous irons √† l‚Äôessentiel‚Ä¶ pour vous donner les outils qui permettront d‚Äôapprofondir le sujet. Chapitre 7 - Biostatistiques bay√©siennes. Une tr√®s br√®ve introduction pour qui est int√©ress√© √† l‚Äôanalyse bay√©sienne. Chapitre 8 - Explorer R. La science des donn√©es √©volue rapidement. Vous gagnerez √† vous tenir au courrant de son √©volution, et immanquablement vous vous buterez sur des op√©rations qui vous sembleront insolubles. Ce chapitre vous accompagnera √† rester √† jour sur le d√©veloppement de R, √† poser de bonnes questions et proposera des modules int√©ressants en √©cologie math√©matique. Chapitre 9 - Association, partitionnement et ordination. Les √©cosyst√®mes diff√®rent, mais en quoi sont-ils semblables, et en quoi dff√®rent-ils? Ces questions importantes peuvent √™tre abord√©s par l‚Äô√©cologie num√©rique, domaine d‚Äô√©tude au sein duquel l‚Äôassociation, le partitionnement et l‚Äôordination sont des outils pr√©dominants. Chapitre 10 - D√©tection de valeurs aberrantes et imputation. Une donn√©e aberrante sortira du lot, pour une raison ou pour une autre. Comment les d√©tecter de mani√®re syst√©matique? D‚Äôautre part, que faire lorsqu‚Äôune donn√©e est manquante? Peut-on l‚Äôimputer? Comment? Chapitre 11 - Les s√©ries temporelles. Les capteurs modernes permettent de g√©n√©rer des donn√©es en fonction du temps. Que ce soit des donn√©es m√©t√©orologiques enregistr√©es quotidiennement ou des donn√©es de teneur en eau enregistr√©es au 5 secondes, les donn√©es en fonction du temps forment un signal. Comment analyser ces signaux? Chapitre 12 - L‚Äôautoapprentissage. Les applications de l‚Äôintelligence artificielle ne sont limit√©es que par votre imagination. Encore faut-il l‚Äôutiliser intelligemment. Chapitre 13 - Les donn√©es spatiales. Non, nous n‚Äôaborderons pas les g√©ostatistiques. Ce chapitre porte plut√¥t sur l‚Äôutilisation de R comme syst√®me d‚Äôinformation g√©ographique de base. Nous utiliserons aussi l‚Äôautoapprentissage comme outil d‚Äôinterpolation spatial. Chapitre 14 - La mod√©lisation d√©terministe. Les mod√®les sont des maquettes simplifi√©es. Comment utiliser les √©quations diff√©rentielles ordinaires pour cr√©er ces maquettes? Si les chapitres 3 √† 5 peuvent √™tre consid√©r√©s comme fondamentaux pour bien ma√Ætriser R, les autres peuvent √™tre feuillet√©s √† la pi√®ce, bien qu‚Äôils forment une suite logique. Chaque chapitre de ce manuel est r√©dig√© en format R notebook, dans un environnement RStudio. Pour ex√©cuter les commandes, les vous pourrez soit copier-coller les commandes dans R (ou RStudio), soit t√©l√©charger les fichiers-sources et ex√©cuter les blocs de code. 1.6 Objectifs g√©n√©raux √Ä la fin du cours, l‚Äô√©tudiant.e sera en mesure: de programmer en langage R d‚Äôimporter, de manipuler (s√©lection des colonnes, filtres, sommaires statistiques) et d‚Äôexporter des tableaux de g√©n√©rer des graphiques d‚Äôutilisation commune d‚Äôappr√©hender des donn√©es √©cologiques et agronomiques √† l‚Äôaide de tests statistiques fr√©quentiels d‚Äôexplorer par lui.elle-m√™me les possibilit√©s offertes par la communaut√© de d√©veloppement de modules R d‚Äôexplorer les donn√©es √† l‚Äôaide des outils de l‚Äô√©cologie num√©rique (association, partitionnement et ordination) d‚Äôimputer des donn√©es manquantes dans un tableau et de d√©tecter des valeurs aberrantes d‚Äôeffectuer une analyse de s√©rie temporelle de s‚Äôassurer que ses calculs soit auditables et reproductibles dans une perspective de science ouverte de cr√©er un mod√®le d‚Äôautoapprentissage d‚Äôintrapoler des donn√©es spatiales de mod√©liser des √©quations diff√©rentielles ordinaires 1.7 Lectures compl√©mentaires 1.7.1 √âcologie math√©matique How to be a quantitative ecologist. Jason Mathipoulos vous prend par la main pour d√©couvrir les notions de math√©matiques fondamentales en √©cologie, appliqu√©es avec le langage R. Numerical ecology. L‚Äôouvrage hautement d√©taill√© des fr√®res Legendre est non seulement fondamental, mais aussi fondateur d‚Äôune science qui √©volue encore aujourd‚Äôhui: l‚Äôanalyse des donn√©es √©cologiques. A practical guide to ecological modelling. Soetaert et Herman portent une attention particuli√®re √† la pr√©sentation des principes de mod√©lisation dans un langage accessible - ce qui est rarement le cas dans le domaine de la mod√©lisation. Les mod√®les pr√©sent√©s concernent principalement les bilans de masse, en termes de syst√®mes de r√©actions chimiques et de relations biologiques. Mod√©lisation math√©matique en √©cologie. Rare livre en mod√©lisation √©cologique publi√© en fran√ßais, la premi√®re partie s‚Äôattarde aux concepts math√©matiques, alors que la deuxi√®me planche √† les appliquer. Si le haut niveau d‚Äôabstraction de la premi√®re partie vous rebute, n‚Äôh√©sitez pas d√©buter par la seconde partie et de vous r√©f√©rer √† la premi√®re au besoin. A new ecology: systems perspective. Principalement gr√¢ce au soleil, la Terre forme un ensemble de gradients d‚Äô√©nergie qui se d√©clinent en des syst√®mes d‚Äôune √©tonnante complexit√©. C‚Äôest ainsi que le regrett√© Sven Erik J√∏rgensen (1934-2016, figure 1.3) et ses collaborateurs d√©crivent les √©cosyst√®mes dans cet ouvrage qui fait suite aux travaux fondateurs de Howard Thomas Odum. Ecological engineering. Principle and Practice. Ecological processes handbook. Modeling complex ecological dynamics Figure 1.3: Sven Erik J√∏rgensen, Source: Elsevier. 1.7.2 Programmation R for data science. L‚Äôanalyse de donn√©es est une branche importante de l‚Äô√©cologie math√©matique. Ce manuel traite des matrices et la manipulation de donn√©es chapitre 3), de la visualisation (chapitre 4) ainsi que de l‚Äôapprentissage automatique (chapitre 11). R for data science repasse ces sujets plus en profondeur. En particulier, l‚Äôouvrage de Garrett Grolemund et Hadley Wickham offre une introduction au module graphique ggplot2. Numerical ecology with R. Daniel Borcard enseigne l‚Äô√©cologie num√©rique √† l‚ÄôUniversit√© de Montr√©al. Son cours est condens√© dans ce livre recettes vou√© √† l‚Äôapplication des principes lourdement d√©crits dans Numerical ecology. 1.7.3 Divers The truthful art. Dans cet ouvrage, Alberto Cairo s‚Äôint√©resse √† l‚Äôutilisation des donn√©es et de leurs pr√©sentations pour fournir une information ad√©quate √† diff√©rents publics. 1.8 Besoin d‚Äôaide? Les ouvrages de r√©f√©rence reconnus vous offrent des bases solides sur lesquelles vous pouvez vous appuyer dans vos travaux. Mais au-del√† des principes, au jour le jour, vous vous buterez immanquablement √† toutes sortes de petits probl√®mes. Quel module utiliser pour cette t√¢che pr√©cise? Que veut dire ce message d‚Äôerreur? Comment interpr√©ter ce r√©sultat? Pour tous les petits accrocs du quotidien en calcul scientifique, internet offre de nombreuses ressources qui sont tr√®s h√©t√©rog√®nes en qualit√©. Vous apprendrez √† reconna√Ætre les ressources fiables √† celles qui sont douteuses. Les plateformes bas√©es sur Stack Exchange, comme Stack Overflow et Cross Validated, m‚Äôont souvent √©t√© d‚Äôune aide pr√©cieuse. Vous aurez avantage √† vous construire une petite banque d‚Äôinformation (Turtl, Notion, Evernote, Google Keep, One Note, etc.) en collectant des liens, en prenant en notes certaines recettes et en suivant des sites d‚Äôint√©r√™t avec des flux RSS. 1.9 √Ä propos de l‚Äôauteur Je m‚Äôappelle Serge-√âtienne Parent. Je suis ing√©nieur √©cologue et professeur adjoint au D√©partement des sols et de g√©nie agroalimentaire de l‚ÄôUniversit√© Laval, Qu√©bec, Canada. Je crois que la science est le meilleur moyen d‚Äôappr√©hender le monde pour prendre des d√©cisions avis√©es. 1.10 Un cours compl√©mentaire √† d‚Äôautres cours Ce cours a √©t√© d√©velopp√© pour ouvrir des perspectives math√©matiques en √©cologie et en agronomie √† la FSAA de l‚ÄôUniversit√© Laval. Il est compl√©mentaire √† certains cours offerts dans d‚Äôautres institutions acad√©miques au Qu√©bec, dont ceux-ci. BIO2041. Biostatistiques 1, Universit√© de Montr√©al BIO2042. Biostatistiques 2, Universit√© de Montr√©al BIO109. Introduction √† la programmation scientifique, Universit√© de Sherbrooke BIO500. M√©thodes en √©cologie computationnelle, Universit√© de Sherbrooke. 1.11 Contribuer au manuel Je suis ouvert aux commentaires et suggestions. Pour contribuer directement, dirigez-vous sur le d√©p√¥t du manuel sur GitHub, puis ouvrez une Issue pour en discuter. Cr√©ez une nouvelle branche (fork), effectuez les modifications, puis lancer une requ√™te de fusion (pull resquest). "],
["chapitre-intro-a-R.html", "2 La science des donn√©es avec R 2.1 Statistiques ou science des donn√©es? 2.2 D√©buter en R 2.3 Pr√©parer son flux de travail 2.4 Premiers pas avec R 2.5 Enfin‚Ä¶ 2.6 Petit truc! 2.7 Extra: Utiliser R avec Jupyter", " 2 La science des donn√©es avec R Ô∏è¬†Objectifs sp√©cifiques: √Ä la fin de ce chapitre, vous saurez contextualiser la science des donn√©es par rapport aux statistiques, serez en mesure de vous lancer dans un environnement de programmation R, serez en mesure d‚Äôeffectuer des op√©rations de base en R, saurez diff√©rencier les grands types d‚Äôobjets de R et saurez installer et charger des modules compl√©mentaire. Un projet en science des donn√©es comprend trois grandes √©tapes. D‚Äôabord, vous devez collecter des donn√©es et vous les compilez ad√©quatement. Cela peut consister √† t√©l√©charger des donn√©es existantes, ex√©cuter un dispositif exp√©rimental ou effectuer une recensement (√©tude observationnelle). Compiler les donn√©es dans un format qui puisse √™tre import√© est une t√¢che souvent longue et fastidieuse. Puis, vous investiguez les donn√©es collect√©es, c‚Äôest-√†-dire vous les visualisez, vous appliquez des mod√®les et testez des hypoth√®ses. Enfin, la communication des r√©sultats consiste √† pr√©senter les connaissances qui √©mergent de votre analyse sous forme visuelle et narrative, avec un langage adapt√© √† la personne qui vous √©coute, qu‚Äôelle soit experte ou novice, r√©viseure de revue savante ou gestionnaire Grolemund et Wickham (2018) propose la structure d‚Äôanalyse de la figure 2.1, avec de l√©g√®res modifications de ma part. Figure 2.1: Flux des donn√©es en sciences des donn√©es. Le grand cadre sp√©cifie Programmer. Oui, vous aurez besoin d‚Äô√©crire du code. Mais comme je l‚Äôai indiqu√© dans le premier chapitre, ceci n‚Äôest pas un cours de programmation et je pr√©f√©rerai les approches intuitives. 2.1 Statistiques ou science des donn√©es? Selon Whitlock et Schluter (2015), la statistique est l‚Äô√©tude des m√©thodes pour d√©crire et mesurer des aspects de la nature √† partir d‚Äô√©chantillon. Pour Grolemund et Wickham (2018), la science des donn√©es est une discipline excitante permettant de transformer des donn√©es brutes en compr√©hension, perspectives et connaissances. Oui, excitante! La diff√©rence entre les deux champs d‚Äôexpertise est subtile, et certaines personnes n‚Äôy voient qu‚Äôune diff√©rence de ton. Data Science is statistics on a Mac. ‚Äî Big Data Borat (@BigDataBorat) 27 ao√ªt 2013 Confin√©es √† ses applications traditionnelles, les statistiques sont davantage vou√©es √† la d√©finition de dispositifs exp√©rimentaux et √† l‚Äôex√©cution de tests d‚Äôhypoth√®ses, alors que la science des donn√©es est moins lin√©aire, en particulier dans sa phase d‚Äôanalyse, o√π de nouvelles questions (donc de nouvelles hypoth√®ses) peuvent √™tre pos√©es au fur et √† mesure de l‚Äôanalyse. Cela arrive g√©n√©ralement davantage lorsque l‚Äôon fait face √† de nombreuses observations sur lesquelles de nombreux param√®tres sont mesur√©s. La quantit√© de donn√©es et de mesures auxquelles nous avons aujourd‚Äôhui acc√®s gr√¢ce aux technologies de mesure et de stockage relativement peu dispendieux rend la science des donn√©es une discipline particuli√®rement attrayante, pour ne pas dire sexy. 2.2 D√©buter en R R est un langage de programmation d√©riv√© du langage S, qui fut initialement lanc√© en 1976. Figure 2.2: Logo officiel du language R. R figure parmi les langages de programmation les plus utilis√©s au monde. Bien qu‚Äôil soit bas√© sur les langages statiques C et Fortran, R est un langage dynamique, c‚Äôest-√†-dire que le code peut √™tre ex√©cut√© ligne par ligne ou bloc par bloc: un avantage majeur pour des activit√©s qui n√©cessitent des interactions fr√©quentes. Bien que R soit surtout utilis√© pour le calcul statistique, il s‚Äôimpose de plus en plus comme outil privil√©gi√© en sciences des donn√©es en raison des r√©cents d√©veloppements de modules d‚Äôanalyse, de mod√©lisation et de visualisation, dont plusieurs seront utilis√©s dans ce manuel. Un langage de programmation s‚Äôapprend un peu comme une langue. Au d√©but, un code R peut sembler incompr√©hensible. Et face √† son clavier, on ne sait pas trop comment exprimer ce que l‚Äôon d√©sire. Au fur et √† mesure de l‚Äôapprentissage, les symboles, les fonctions et le style deviennent de plus en plus familiers et on apprend tranquillement √† traduire en code ce que l‚Äôon d√©sire effectuer. Comme une langue s‚Äôapprend en la parlant dans la vie de tous les jours, un language de programmation s‚Äôapprend avantageusement en solutionnant vos propres probl√®mes (figure 2.3). Figure 2.3: R avant et maintenant, Illustration de Allison Horst 2.3 Pr√©parer son flux de travail Il existe de nombreuses mani√®res d‚Äôutiliser R. Parmi celles-ci, j‚Äôen couvrirai 3: Installation classique (installation sugg√©r√©e) Installation avec Anaconda Utilisation infonuagique 2.3.1 Installation classique Installation sugg√©r√©e. Sur Windows ou Mac, dirigez-vous ici, t√©l√©chargez et installez. Sur Linux, ouvrez votre gestionnaire d‚Äôapplication, chercher r-base (Ubuntu, Debian), R-base (openSuse) ou R-core (Fedora) et installez-le (assurez-vous que les librairies suivantes sont aussi install√©es: gcc, gcc-fortran, gcc-c++ et make), vous aurez peut-√™tre besoin d‚Äôinstaller des librairies suppl√©mentaires pour faire fonctionner certains modules. Note. Les modules pr√©sent√©s dans ce cours devraient √™tre disponibles sur Linux, Windows et Mac. Ce n‚Äôest pas le cas pour tous les modules R. La plupart fonctionnent n√©anmoins sur Linux, dont les syst√®mes d‚Äôop√©ration (je recommande Ubuntu ou l‚Äôune de ses d√©riv√©es comme elementary OS) sont de bonnes options pour le calcul scientifique. √Ä cette √©tape, R devrait fonctionner dans un interpr√©teur de commande . Si vous lancez R dans un terminal (chercher cmd dans le menu si vous √™tes sur Windows), vous obtiendrez quelque chose comme ceci. Figure 2.4: R dans le terminal. Le symbole &gt; indique que R attend que vos instructions. Vous voil√† dans un √©tat m√©ditatif devant l‚Äôind√©chiffrable vide du terminal üòµ. Ne vous en faites pas: nous commencerons bient√¥t √† jaser avec R. Avant cela, installons-nous au salon. Afin de travailler dans un environnement de travail plus confortable, je recommande l‚Äôinstallation de l‚Äôinterface RStudio, gratuite et open source: t√©l√©chargez l‚Äôinstallateur et suivez les instructions. RStudio ressemble √† ceci. Figure 2.5: Fen√™tre de RStudio. En haut √† droite se trouve un menu Project (None). Il s‚Äôagit d‚Äôun menu de vos projets. Je recommande d‚Äôutiliser ces projets avec RStudio, qui vous permettront de mieux g√©rer vos sessions de travail, en particulier en lien avec les chemins vers de vos donn√©es, graphiques, etc., que vous pouvez g√©rer relativement √† l‚Äôemplacement de votre dossier de projet plut√¥t qu‚Äô√† l‚Äôemplacement des fichiers sur votre machine: nous verrons plus en d√©tails au chapitre 5. En haut √† gauche, vous avez vos feuilles de calcul, qui appara√Ætront en tant qu‚Äôonglets. Une feuille de calcul est une s√©rie de commandes que vous lancez en s√©quence. Il peut aussi s‚Äôagir d‚Äôun livre de calcul (notebook) si vous choisissez de travailler en format R markdown. Ce format vous permettra de d‚Äô√©crire du texte en format Markdown entre des blocs de code. Il est question du format R markdown au chapitre (chapitre-git). En bas √† gauche appara√Æt la Console, o√π vous voyez les commandes envoy√©es √† R ainsi que ses sorties. En haut √† droite, les diff√©rents onglets indiquent o√π vous en √™tes dans vos calculs. En particulier, la liste sous Environment indique les objets qui ont √©t√© g√©n√©r√©s ou charg√©s jusqu‚Äôalors. En bas √† droite, on retrouve des onglets de nature vari√©s. Files contient les sous-dossiers et fichiers du dossier de projets. Plots est l‚Äôendroit o√π appara√Ætront vos graphiques. Packages contient la liste des modules d√©j√† install√©s, ainsi qu‚Äôun outil de gestion des modules pour leur installation, leur d√©sinstallation et leur mise √† jour. Help affiche les fiches d‚Äôaide des fonctions (pour obtenir de l‚Äôaide sur une fonction dans RStudio, surlignez la fonction dans votre feuille de calcul, puis appuyez sur F1). Enfin, l‚Äôonglet Viewer affichera les sorties HTML, en particulier les graphiques interactifs que vous g√©n√©rerez par exemple avec le module plotly. Si votre environnement de travail √©tait un avion, R serait le moteur et RStudio serait le cockpit! Figure 2.6: Sc√®ne de Fifi Brindacier (Astrid Lindgren, 1945). 2.3.2 Installation avec Anaconda Si vous cherchez une trousse compl√®te d‚Äôanalyse de donn√©es, comprenant R et Python, vous pourrez pr√©f√©rer Anaconda. Une fois install√©e, vous pourrez isoler un environnement de travail sur R, ou m√™me isoler des environnements de travail particuliers pour vos projets. Une mani√®re conviviale de cr√©er des environnements de travail est de passer par l‚Äôinterface Anaconda navigator, que vous lancerez soit dans le menu Windows, soit en ligne de commande anaconda-navigator sous Mac et Linux, puis d‚Äôinstaller r-essentials, rstudio et jupyterlab dans l‚Äôonglet Environment. Vous pourrez aussi installer RStudio et Jupyter lab via l‚Äôonglet Home de Anaconda navigator. Dans l‚Äôenvironnement de base, installez le package nb_conda_kernels pour vous assurer que tous les noyaux (R, Python, etc.) install√©s dans les environnements de travail soient automatiquement accessibles dans Jupyter. Si vous d√©sirez utiliser dans Jupyter la version de R install√©e avec l‚Äôinstallation classique, r√©f√©rez-vous au guide pr√©sent√© en extra au bas de la page. Figure 2.7: Anaconda navigator. Jupyter lab est une interface notebook semblable √† R markdown - les format Jupyter (*.ipynb) et R markdown (*.Rmd) sont par ailleurs convertibles gr√¢ce au module jupytext. L‚Äôutilisation de R en Anaconda n‚Äôest pas tout √† fait au point, et pourrait poser probl√®me pour l‚Äôinstallation de certains modules. Si vous optez pour cette option, pr√©parez-vous √† avoir √† bidouiller un peu. Plusieurs pr√©f√®rent Jupyter √† RStudio (ce n‚Äôest pas mon cas). 2.3.3 Utilisation infonuagique Pas besoin d‚Äôavoir une machine super puissante pour travailler en R. Il existe une multitude de services infonuagiques (dans le cloud) vous permettant de lancer vos calculs sur des serveurs plut√¥t que sur votre Chromebook ou votre vieux laptop d√©glingu√©. Certains services sont gratuits, et d‚Äôautres souvent plus √©labor√©s sont payants. Un service gratuit qui fonctionne bien en R est Azure Notebooks, offert par Microsoft. Vous y aurez acc√®s avec un compte Microsoft ou un compte Exchange (par exemeple avec un IDUL de l‚ÄôUniversit√© Laval). Azure notebooks offre des dossiers comportant des notebooks de type Jupyter ainsi que des fichiers de donn√©es. Les dossiers peuvent √™tre rendus publics et cl√¥n√©s≈ù par des coll√®gues. Le travail collaboratif n‚Äôest pas disponible. Pour collaborer en temps r√©el dans un document, vous pourrez utiliser Google Colaboratory, offert gratuitement par Google. Bien que les noyaux de calcul soient seulement offerts de mani√®re explicite pour Python, vous pouvez cl√¥ner ou importer un notebook con√ßu pour fonctionner en R, et un noyau R sera charg√©. Il vous faudra bidouiller un peu pour charger vos donn√©es depuis Google drive ou votre ordinateur. Un avantage de Google Colaboratory est que vous pouvez sp√©cifier que vous d√©sirez que vos calculs tournent sur un processeur graphique, utile pour les calculs lourds et parall√©lisables comme les r√©seaux neuronnaux. Si je viens de vous perdre, pas de probl√®me, c‚Äôest de l‚Äôextra. Si vous d√©sirer autant que possible rester ind√©pendant des g√©ants du web, CoCalc est une option avec un volet gratuit et un autre payant. 2.4 Premiers pas avec R R ne fonctionne pas avec des menus, en faisant danser une souris sous une musique de clics. Vous devrez donc entrer des commandes avec votre clavier, que vous apprendrez par c≈ìur au fur et √† mesure, ou que vous retrouverez en lan√ßant des recherches sur internet. Par exp√©rience personnelle, lorsque je travaille avec R, j‚Äôai toujours un navigateur ouvert pr√™t √† recevoir une question. Les √©tapes qui suivent sont des premiers pas. Elles ne feront pas de vous des ceintures noires del√† programmation. La plupart des utilisateurs de R ont appris R en se pratiquant sur leurs donn√©es, en frappant des murs, en apprenant comment les escalader ou les contourner‚Ä¶ Pour l‚Äôinstant, ouvrez seulement un interpr√©teur de commande, et lancez R. Voyons si R est aussi libre qu‚Äôon le pr√©tend. ‚ÄúLa libert√©, c‚Äôest la libert√© de dire que deux et deux font quatre. Si cela est accord√©, tout le reste suit.‚Äù - George Orwell, 1984 2 + 2 ## [1] 4 Et voil√†. Les op√©rations math√©matiques sont effectu√©es telles que l‚Äôon devrait s‚Äôattendre. 67.1 - 43.3 ## [1] 23.8 2 * 4 ## [1] 8 1 / 2 ## [1] 0.5 L‚Äôexposant peut √™tre not√© ^, comme c‚Äôest le cas dans Excel, ou ** comme c‚Äôest le cas en Python. 2^4 ## [1] 16 2**4 ## [1] 16 1 / 2 # utilisez des espaces de part et d&#39;autre des op√©rateurs (sauf pour l&#39;exposant) pour √©claircir le code ## [1] 0.5 R ne lit pas ce qui suit le caract√®re #. Cela vous laisse l‚Äôopportunit√© de commenter un code comprenant une s√©quence de plusieurs lignes. Remarquez √©galement que la derni√®re op√©ration comporte des espaces entre les nombres et l‚Äôop√©rateur /. Dans ce cas (ce n‚Äôest pas toujours le cas), les espaces ne signifient rien: ils aident seulement √† √©claircir le code. Il existe des guides pour l‚Äô√©criture de code en R. Je recommande le guide de style de Hadley Wickahm. Assigner des objets √† des variables est fondamental en programmation. En R, on assigne traditionnellement avec la fl√®che &lt;-, mais vous verrez parfois le =, qui est davantage utilis√© comme standard dans d‚Äôautres langages de programmation. Par exemple. a &lt;- 3 Techniquement, a pointe vers le nombre entier 3. Cons√©quemment, on peut effectuer des op√©rations sur a. a * 6 ## [1] 18 # A + 2 Le message d‚Äôerreur nous dit que A n‚Äôest pas d√©fini. Sa version minuscule, a, l‚Äôest pourtant. La raison est que R consid√®re la case dans la d√©finition des objets. Utiliser la mauvaise case m√®ne donc √† des erreurs. Note. Les messages d‚Äôerreur ne sont pas toujours clairs, mais vous apprendrez √† les comprendre. Dans tous les cas, ils sont fait pour vous aider. Lisez-les attentivement! En g√©n√©ral, le nom d‚Äôune variable doit toujours commencer par une lettre, et ne doit pas contenir de caract√®res r√©serv√©s (espaces, +, *). Dans la d√©finition des variables, plusieurs utilisent des symboles . pour d√©limiter les mots, mais la barre de soulignement _ est √† pr√©f√©rer. En effet, dans d‚Äôautres langages de programmation comme Python, le . a une autre signification: son utilisation est √† √©viter autant que possible. Note. √Ä ce stade, vous serez probablement plus √† l‚Äôaise de copier-coller ces commandes dans votre terminal. rendement_arbre &lt;- 50 # pomme/arbre nombre_arbre &lt;- 300 # arbre nombre_pomme &lt;- rendement_arbre * nombre_arbre nombre_pomme ## [1] 15000 Comme chez la plupart des langages de programmation, R respecte les conventions des priorit√©s des op√©rations math√©atiques. 10 - 9^0.5 * 2 ## [1] 4 2.4.1 Types de donn√©es Jusqu‚Äô√† maintenant, nous n‚Äôavons utilis√© que des nombres entiers (integer ou int) et des nombres r√©els (numeric ou float64). R inclut d‚Äôautres types. La cha√Æne de caract√®re (string ou character) contient un ou plusieurs symboles. Elle est d√©finie entre des doubles guillemets &quot; &quot; ou des apostrophes ' '. Il n‚Äôexiste pas de standard sur l‚Äôutilisation de l‚Äôun ou de l‚Äôautre, mais en r√®gle g√©n√©rale, on utilise les apostrophes pour les expressions courtes, contenant un simple mot ou s√©quence de lettres, et les guillemets pour les phrases. Une raison pour cela: les guillemets sont utiles pour ins√©rer des apostrophes dans une cha√Æne de caract√®re. a &lt;- &quot;L&#39;ours&quot; b &lt;- &quot;polaire&quot; paste(a, b) ## [1] &quot;L&#39;ours polaire&quot; On colle a et b avec la fonction paste. Notez que l‚Äôobjet a a √©t√© d√©fini pr√©c√©demment. Il est possible en R de r√©assigner une variable, mais cela peut porter √† confusion, jusqu‚Äô√† g√©n√©rer des erreurs de calcul si une variable n‚Äôest pas assign√©e √† l‚Äôobjet auquel on voulait r√©f√©rer. Combien de caract√®res contient la cha√Æne &quot;L'ours polaire&quot;? R sait compter. Demandons-lui. c &lt;- paste(a, b) nchar(c) ## [1] 14 Quatorze, c‚Äôest bien cela (comptez ‚ÄúL‚Äôours polaire‚Äù, en incluant l‚Äôespace). Comme paste, nchar est une fonction incluse par d√©faut dans l‚Äôenvironnement de travail de R: plus pr√©cis√©ment, ces fonctions sont incluses dans le module base, inclut par d√©faut lorsque R est lanc√©. La fonction est appel√©e en √©crivant nchar(). Mais une fonction de quoi? Des arguments, qui se trouvent entre les parenth√®ses. Dans ce cas, il y a un seul argument: c. En calcul scientifique, il est courant de lancer des requ√™tes sur si un r√©sultat est vrai ou faux. a &lt;- 17 a &lt; 10 ## [1] FALSE a &gt; 10 ## [1] TRUE a == 10 ## [1] FALSE a != 10 ## [1] TRUE a == 17 ## [1] TRUE !(a == 17) ## [1] FALSE Je viens d‚Äôintroduire un nouveau type de donn√©e: les donn√©es bool√©ennes (boolean, ou logical), qui ne peuvent prendre que deux √©tats - TRUE ou FALSE. En m√™me temps, j‚Äôai utilis√© la fonction print parce que dans mon carnet, seule la derni√®re op√©ration permet d‚Äôafficher le r√©sultat. Si l‚Äôon veut forcer une sortie, on utilise print. Puis, on a vu plus haut que le symbole = est r√©serv√© pour assigner des objets: pour les tests d‚Äô√©galit√©, on utilise le double √©gal, ==, ou != pour la non-√©galit√©. Enfin, pour inverser une donn√©e de type bool√©enne, on utilise le point d‚Äôexclamation !. 2.4.2 Les collections de donn√©es Les exercices pr√©c√©dents ont permis de pr√©senter les types de donn√©es offerts par d√©faut sur R qui sont les plus importants pour le calcul scientifique: int (integer, ou nombre entier), numeric (nombre r√©el), character (string, ou cha√Æne de caract√®re) et logical (bool√©en). D‚Äôautres s‚Äôajouteront tout au long du cours, comme les cat√©gories (factor) et les unit√©s de temps (date-heure). Lorsque l‚Äôon proc√®de √† des op√©rations de calcul en science, nous utilisons rarement des valeurs uniques. Nous pr√©f√©rons les organiser et les traiter en collections. Par d√©faut, R offre quatre types importants de collections: les vecteurs, les matrices, les listes et les tableaux. 2.4.2.1 Vecteurs D‚Äôabord, les vecteurs sont une s√©rie de variables de m√™me type. Un vecteur est d√©limit√© par la fonction c( ) (c pour concat√©nation). Les √©l√©ments de la liste sont s√©par√©s par des virgules. espece &lt;- c(&quot;Petromyzon marinus&quot;, &quot;Lepisosteus osseus&quot;, &quot;Amia calva&quot;, &quot;Hiodon tergisus&quot;) espece ## [1] &quot;Petromyzon marinus&quot; &quot;Lepisosteus osseus&quot; &quot;Amia calva&quot; &quot;Hiodon tergisus&quot; Pour acc√©der aux √©l√©ments d‚Äôune liste, appelle la liste suivie de la position de l‚Äôobjet d√©sir√© entre crochets. espece[1] ## [1] &quot;Petromyzon marinus&quot; espece[2] ## [1] &quot;Lepisosteus osseus&quot; espece[1:3] ## [1] &quot;Petromyzon marinus&quot; &quot;Lepisosteus osseus&quot; &quot;Amia calva&quot; espece[c(1, 3)] ## [1] &quot;Petromyzon marinus&quot; &quot;Amia calva&quot; On peut noter que le premier √©l√©ment de la liste est not√© 1, et non 0 comme c‚Äôest le cas de la plupart de langages. Le raccourcis 1:3 cr√©e une liste de nombres entiers de 1 √† 3 inclusivement, c‚Äôest-√†-dire l‚Äô√©quivalent de c(1, 2, 3). En effet, on cr√©e une liste d‚Äôindices pour soutirer des √©l√©ments d‚Äôune liste. On peut utiliser le symbole de soustraction pour retirer un ou plusieurs √©l√©ments d‚Äôun vecteur. espece[-c(1, 3)] ## [1] &quot;Lepisosteus osseus&quot; &quot;Hiodon tergisus&quot; Pour ajouter un √©l√©ment √† notre liste, on peut utiliser la fonction c( ). espece &lt;- c(espece, &quot;Cyprinus carpio&quot;) espece ## [1] &quot;Petromyzon marinus&quot; &quot;Lepisosteus osseus&quot; &quot;Amia calva&quot; &quot;Hiodon tergisus&quot; &quot;Cyprinus carpio&quot; Notez que l‚Äôon efface l‚Äôobjet espece par une concat√©nation de l‚Äôobjet espece, pr√©c√©demment d√©finie, et d‚Äôun autre √©l√©ment. En lan√ßant espece[3] &lt;- &quot;Lepomis gibbosus&quot;, il est possible de changer un √©l√©ment de la liste. espece[3] &lt;- &quot;Lepomis gibbosus&quot; espece ## [1] &quot;Petromyzon marinus&quot; &quot;Lepisosteus osseus&quot; &quot;Lepomis gibbosus&quot; &quot;Hiodon tergisus&quot; &quot;Cyprinus carpio&quot; 2.4.2.2 Matrices Une matrice est un vecteur de dimension plus √©lev√©e que 1. En √©cologie, on d√©passe rarement la deuxi√®me dimension, quoi que les matrices en N dimensions soient courantes en mod√©lisation math√©matique. Je ne consid√©rerai pour le moment que des matrices 2D. Comme c‚Äôest la cas des vecteurs, les matrices contiennent des valeurs de m√™me type. En R, on peut attribuer aux matrices 2D des noms de ligne et de colonne. mat &lt;- matrix(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12), ncol = 3) mat ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 colnames(mat) &lt;- c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) rownames(mat) &lt;- c(&quot;site_1&quot;, &quot;site_2&quot;, &quot;site_3&quot;, &quot;site_4&quot;) mat ## A B C ## site_1 1 5 9 ## site_2 2 6 10 ## site_3 3 7 11 ## site_4 4 8 12 On peut soutirer les noms de colonne et les noms de ligne. Le r√©sultat est un vecteur. colnames(mat) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; rownames(mat) ## [1] &quot;site_1&quot; &quot;site_2&quot; &quot;site_3&quot; &quot;site_4&quot; 2.4.2.3 Listes Les listes sont des collections h√©t√©rog√®nes dans lesquelles on peut placer les objets d√©sir√©s, sans distinction: elles peuvent m√™me inclure d‚Äôautres listes. Chacun des √©l√©ments de la liste peut √™tre identifi√© par une cl√©. ma_liste &lt;- list( especes = c( &quot;Petromyzon marinus&quot;, &quot;Lepisosteus osseus&quot;, &quot;Amia calva&quot;, &quot;Hiodon tergisus&quot; ), site = &quot;A101&quot;, stations_meteos = c(&quot;746583&quot;, &quot;783786&quot;, &quot;856363&quot;) ) ma_liste ## $especes ## [1] &quot;Petromyzon marinus&quot; &quot;Lepisosteus osseus&quot; &quot;Amia calva&quot; &quot;Hiodon tergisus&quot; ## ## $site ## [1] &quot;A101&quot; ## ## $stations_meteos ## [1] &quot;746583&quot; &quot;783786&quot; &quot;856363&quot; Les √©l√©ments de la liste peuvent √™tre soutir√©s par le nom de la cl√© ou par l‚Äôindice, de cette mani√®re. ma_liste$especes ## [1] &quot;Petromyzon marinus&quot; &quot;Lepisosteus osseus&quot; &quot;Amia calva&quot; &quot;Hiodon tergisus&quot; ma_liste[[1]] ## [1] &quot;Petromyzon marinus&quot; &quot;Lepisosteus osseus&quot; &quot;Amia calva&quot; &quot;Hiodon tergisus&quot; Exercice. Acc√©der au deuxi√®me √©l√©ment du vecteur d‚Äôesp√®ces dans la liste ma_liste. 2.4.2.4 Tableaux Enfin, le type de collection de donn√©es le plus important est le tableau, ou data.frame. Techniquement, il s‚Äôagit d‚Äôune liste compos√©e de vecteurs de m√™me longueur. Chaque colonne peut ainsi prendre un type de donn√©e ind√©pendamment des autres colonnes. tableau &lt;- data.frame( espece = c( &quot;Petromyzon marinus&quot;, &quot;Lepisosteus osseus&quot;, &quot;Amia calva&quot;, &quot;Hiodon tergisus&quot; ), poids = c(10, 13, 21, 4), longueur = c(35, 44, 50, 8) ) tableau ## espece poids longueur ## 1 Petromyzon marinus 10 35 ## 2 Lepisosteus osseus 13 44 ## 3 Amia calva 21 50 ## 4 Hiodon tergisus 4 8 En programmation classique en R (nous verrons plus loin la m√©thode tidyverse), les √©l√©ments d‚Äôun tableau se manipulent comme ceux d‚Äôune matrice et les colonnes peuvent √™tre appel√©s comme les √©l√©ments d‚Äôune liste. tableau[, 2:3] ## poids longueur ## 1 10 35 ## 2 13 44 ## 3 21 50 ## 4 4 8 tableau$poids ## [1] 10 13 21 4 Vous verrez aussi, quoi que rarement, ce format, qui √† la diff√©rence du format $ g√©n√®re un tableau. tableau[&quot;poids&quot;] ## poids ## 1 10 ## 2 13 ## 3 21 ## 4 4 Le tableau est le format de collection √† privil√©gier pour manipuler des donn√©es. R√©cemment, le format de tableau tibble a √©t√© cr√©√© par l‚Äô√©quipe de RStudio pour offrir un format plus moderne. 2.4.3 Les fonctions Lorsque vous √©crivez une commande suivit de parenth√®ses, comme data.frame(especes = ...), vous demandez √† R de passer √† l‚Äôaction en appelant une fonction. De mani√®re tr√®s g√©n√©rale, une fonction transforme quelque chose en quelque chose d‚Äôautre (figure 2.8). Figure 2.8: Sch√©ma simplifi√© d‚Äôune fonction. Par exemple, la fonction mean() prend une collection de nombre comme entr√©e, puis en sort vous devinez quoi. mean(tableau$poids) ## [1] 12 Les entr√©es sont appel√©s les arguments de la fonction. Leur d√©finition est toujours disponible dans la documentation. Exercice. Familiarisez-vous avec la documentation de R en lan√ßant ?mean. Truc: si vous avez pris de l‚Äôavance et que vous travaillez d√©j√† en RStudio, mettez le terme en surbrillance, puis appuyez sur F1. Vous verrez dans la documentation que la fonction mean() demande trois arguments, x, trim et na.rm. Or nous avons seulement plac√© un vecteur, sans sp√©cifier d‚Äôargument! En effet. En l‚Äôabsence d‚Äôune d√©finition des arguments, R supposera que les arguments dans la parenth√®se, s√©par√©s par une virgule, sont pr√©sent√©s dans le m√™me ordre que celui sp√©cifi√© dans la d√©finition de la fonction (celle qui est pr√©sent√©e dans le fichier d‚Äôaide). Dans le cas qui nous int√©resse, mean(tableau$poids) est √©quivalent √† mean(x = tableau$poids). Maintenant, selon la fiche d‚Äôaire l‚Äôargument na.rm est un valeur logique sp√©cifiant si oui (TRUE) ou non (FALSE) les valeurs manquantes doivent √™tre consid√©r√©es (une moyenne d‚Äôun vecteur comprenant au moins un NA sera de NA). En ne sp√©cifiant rien, R prend la valeur par d√©faut, telle que sp√©cifi√©e dans la documentation. Il en va de m√™me que l‚Äôargument trim, qui permet d‚Äô√©laguer des valeurs extr√™mes. Dans la fiche d‚Äôaide, mean(x, trim = 0, na.rm = FALSE, ...) signifie que par d√©faut, l‚Äôargument x est vide (il doit donc √™tre sp√©cifi√©), l‚Äôargument trim est de 0 et l‚Äôargument na.rm est FALSE. mean(c(6, 1, 7, 4, 9, NA, 1)) ## [1] NA mean(c(6, 1, 7, 4, 9, NA, 1), na.rm = TRUE) ## [1] 4.666667 Vous n‚Äô√™tes pas emprisonn√© par les fonctions offertes par R. Vous pouvez installer des modules qui compl√®tent les fonctions de base de R: on le verra un peu plus loin dans ce chapitre. Mais pour l‚Äôinstant, voyons comment vous pouvez cr√©er vos propres fonctions. Disons que vous voulez cr√©er une fonction qui calcule la sortie de \\(x^3-2y+a\\). Pour obtenir la r√©ponse on a besoin des arguments x, y et a. La sortie de la fonction est ici triviale: la r√©ponse de l‚Äô√©quation. L‚Äôop√©ration function permet de prendre √ßa en charge. operation_f &lt;- function(x, y, a = 10) { return(x^3 - 2 * y + a) } Notez que a a une valeur par d√©faut. La sortie de la fonction est ce qui se trouve entre les parenth√®ses de return. Vous pouvez maintenant utiliser la fonction operation_nl au besoin. operation_f(x = 2, y = 3, a = 1) ## [1] 3 Une telle fonction est peu utile. Mais l‚Äôutilisation de fonctions personnalis√©es vous permettra d‚Äô√©viter de r√©p√©ter la m√™me op√©ration plusieurs fois dans un flux de travail, en √©vitant de g√©n√©rer trop de code, donc aussi de potentielles erreurs. Personnellement, j‚Äôutilise les fonctions surtout pour g√©n√©rer des graphiques personnalis√©s. Exercice. Afin d‚Äôacqu√©rir de l‚Äôautonomie, vous devrez √™tre en mesure de trouver le nom des commandes dont vous avez besoin pour effectuer la t√¢che que vous d√©sirer effectuer. Cela peut causer des frustrations, mais vous vous sentirez toujours plus √† l‚Äôaise avec R jour apr√®s jour. L‚Äôexercice ici est de trouver par vous-m√™me la commande qui vous permettra mesurer la longueur d‚Äôun vecteur. 2.4.4 Les boucles Les boucles permettent d‚Äôeffectuer une m√™me suite d‚Äôop√©rations sur plusieurs objets. Pour faire suite √† notre exemple, nous d√©sirons obtenir le r√©sultat de l‚Äôop√©ration f pour des param√®tres que nous enregistrons dans ce tableau. params &lt;- data.frame( x = c(2, 4, 1, 5, 6), y = c(3, 4, 8, 1, 0), a = c(6, 1, 8, 2, 5) ) params ## x y a ## 1 2 3 6 ## 2 4 4 1 ## 3 1 8 8 ## 4 5 1 2 ## 5 6 0 5 Nous cr√©ons un vecteur vide, puis nous it√©rons ligne par ligne en remplissant le vecteur. operation_res &lt;- c() for (i in 1:nrow(params)) { operation_res[i] &lt;- operation_f(x = params[i, 1], y = params[i, 2], a = params[i, 3]) } operation_res ## [1] 8 57 -7 125 221 En faisant varier i sur des valeurs du vecteur donn√© par la s√©quence de nombre entiers de 1 au nombre de ligne du tableau de param√®tres, nous demandons √† R d‚Äôeffectuer la suite d‚Äôop√©ration entre les accolades {}. √Ä chaque boucle, i prend une valeur de la s√©quence. i est utilis√© ici comme indice de la ligne √† soutirer du tableau params, qui correspond √† l‚Äôindice dans le vecteur operation_res. Ainsi, chaque r√©sultat est calcul√© dans l‚Äôordre des lignes du tableau de param√®tres et l‚Äôon pourra tr√®s bien y coller nos r√©sultats: params$resultats &lt;- operation_res params ## x y a resultats ## 1 2 3 6 8 ## 2 4 4 1 57 ## 3 1 8 8 -7 ## 4 5 1 2 125 ## 5 6 0 5 221 Notez que puisque la colonne resultat n‚Äôexiste pas dans le tableau params, R cr√©e automatiquement une nouvelle colonne. Les boucles for vous permettront par exemple de g√©n√©rer en peu de temps 10, 100, 1000 graphiques (autant que vous voulez), chacun issu de simulations obtenues √† partir de conditions initiales diff√©rentes, et de les enregistrer dans un r√©pertoire sur votre ordinateur. Un travail qui pourrait prendre des semaines sur Excel peut √™tre effectu√© en R en quelques secondes. Un second outil est disponible pour les it√©rations: les boucles while. Elles effectuent une op√©ration tant qu‚Äôun crit√®re n‚Äôest pas atteint. Elles sont utiles pour les op√©rations o√π l‚Äôon cherche une convergence. Je les couvre rapidement puisqu‚Äôelles sont rarement utilis√©es dans les flux de travail courants. En voici un petit exemple. x &lt;- 100 while (x &gt; 1.1) { x &lt;- sqrt(x) print(x) } ## [1] 10 ## [1] 3.162278 ## [1] 1.778279 ## [1] 1.333521 ## [1] 1.154782 ## [1] 1.074608 Nous avons initi√© x √† une valeur de 100. Puis, tant que (while) le test x &gt; 1.1 est vrai, attribuer √† x la nouvelle valeur calcul√©e en extrayant la racine de la valeur pr√©c√©dente de x. Enfin, indiquer la valeur avec print. 2.4.5 Conditions: if, else if, else Si la condition 1 est remplie, effectuer une suite d‚Äôinstruction 1. Si la condition 1 n‚Äôest pas remplie, et si la condition 2 est remplie, effectuer la suite d‚Äôinstruction 2. Sinon, effectuer la suite d‚Äôinstruction 3. Voil√† comment on exprime une suite de conditions. Prenons l‚Äôexemple simple d‚Äôune discr√©tisation d‚Äôune valeur continue. Si \\(x&lt;10\\), il est class√© comme faible. Si \\(10 \\leq x &lt;20\\), il est class√© comme moyen. Si \\(x \\geq 20\\), il est class√© comme √©lev√©. Pla√ßons cette classification dans une fonction. classification &lt;- function(x, lim1 = 10, lim2 = 20) { if (x &lt; lim1) { categorie &lt;- &quot;faible&quot; } else if (x &lt; lim2) { categorie &lt;- &quot;moyen&quot; } else { categorie &lt;- &quot;√©lev√©&quot; } return(categorie) } classification(-10) ## [1] &quot;faible&quot; classification(15.4) ## [1] &quot;moyen&quot; classification(1000) ## [1] &quot;√©lev√©&quot; Une condition est d√©finie avec le if, suivi du test √† vrai ou faux entre parenth√®ses. Si le test retourne un vrai (TRUE), l‚Äôinstruction entre accolades est ex√©cut√©e. Si elle est fausse, on passe au suivant. Exercice. Explorer les commandes ifelse et cut et r√©fl√©chissez √† la mani√®re qu‚Äôelles pourraient √™tre utilis√©es pour effectuer une discr√©tisation plus efficacement qu‚Äôavec les if et les else. 2.4.6 Installer et charger un module La plupart des op√©rations d‚Äôordre g√©n√©ral (comme les racines carr√©es, les tests statistiques, la gestion de matrices et de tableau, les graphiques, etc.) sont accessibles gr√¢ce aux modules de base de R, qui sont install√©s et charg√©s par d√©faut lors du d√©marrage de R. Des √©quipes de travail ont n√©anmoins d√©velopp√© plusieurs modules pour r√©pondre √† leurs besoins sp√©cialis√©s, et les ont laiss√©es disponibles au grand public dans des modules que vous pouvez installer d‚Äôun d√©p√¥t CRAN (le AppStore de R), d‚Äôun d√©p√¥t Anaconda (le AppStore de Anaconda, si vous utilisez cette plate-forme), d‚Äôun d√©p√¥t Github (d√©p√¥ts d√©centralis√©s), etc. RStudio poss√®de un pratique bouton Install qui vous permet d‚Äôy inscrire une liste de modules. Le navigateur anaconda offre aussi une interface d‚Äôinstallation. La commande R pour installer un module est install.packages(&quot;ggplot2&quot;), si par exemple vous d√©sirez installer ggplot2, le module graphique par excellence en R. C‚Äôest la commande que RStudio lancera tout seul si vous lui demandez d‚Äôinstaller ggplot2. Les modules sont l‚Äô√©quivalent des applications sp√©cialis√©es que vous installez sur un t√©l√©phone mobile. Pour les utiliser, il faut les ouvrir. G√©n√©ralement, j‚Äôouvre toutes les applications n√©cessaires √† mon flux de travail au tout d√©but de ma feuille de calcul (la prochaine cellule retournera un message d‚Äôerreur si les packages ne sont pas install√©s). library(&quot;tidyverse&quot;) # m√©ta-package qui charge entre autres dplyr et ggplot2 ## ‚îÄ‚îÄ Attaching packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 1.3.0 ‚îÄ‚îÄ ## ‚úì ggplot2 3.2.1 ‚úì purrr 0.3.3 ## ‚úì tibble 2.1.3 ‚úì dplyr 0.8.4 ## ‚úì tidyr 1.0.2 ‚úì stringr 1.4.0 ## ‚úì readr 1.3.1 ‚úì forcats 0.4.0 ## ‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(&quot;vegan&quot;) ## Loading required package: permute ## Loading required package: lattice ## This is vegan 2.5-6 library(&quot;nlme&quot;) ## ## Attaching package: &#39;nlme&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## collapse Les modules sont install√©s sur votre ordinateur √† un endroit que vous pourrez retrouver avec la commande .libPaths() Exercice. √Ä partir d‚Äôici jusqu‚Äô√† la fin du cours, nous utiliserons RStudio. Ouvrez-le et familiarisez-vous avec l‚Äôinterface! Quelques petits trucs: pour lancer une ligne, placez votre curseur sur la ligne, puis appuyez sur Ctrl+Enter pour lancer une partie de code pr√©cise, mettez le en surbrillance, puis Ctrl+Enter utilisez toujours le gestionnaire de projets, en haut √† droite! installez le module tidyverse lancez data(&quot;iris&quot;) pour obtenir un tableau d‚Äôexercice, puis cliquez sur l‚Äôobjet dans la fen√™tre environnement 2.5 Enfin‚Ä¶ Comme une langue, on n‚Äôapprend √† s‚Äôexprimer en un langage informatique qu‚Äôen se mettant √† l‚Äô√©preuve, ce que vous ferez tout au long de ce cours. Pour vous encourager, voici quelques trucs pour apprendre √† coder en R. R n‚Äôaime pas l‚Äôambigu√Øt√©. Une simple virgule mal plac√©e et il ne sait plus quoi faire. Cela peut √™tre frustrant au d√©but, mais cette rigidit√© est n√©cessaire pour effectuer du calcul scientifique. Le copier-coller est votre ami. En gardant √† l‚Äôesprit que vous √™tre responsable de votre code et que vous respectez les droits d‚Äôauteur, n‚Äôayez pas peur de copier-coller des lignes de code et de personnaliser par la suite. L‚Äôerreur que vous obtenez: d‚Äôautres l‚Äôont obtenue avant vous. Le site de question-r√©ponse stackoverflow est une ressource inestimable o√π des gens ayant post√© des questions ont re√ßu des r√©ponses d‚Äôexperts (les meilleures r√©ponses et les meilleures questions apparaissent en premier). Apprenez √† chercher intelligemment des r√©ponses en formulant pr√©cis√©ment vos questions! √âtudiez et pratiquez. Les messages d‚Äôerreur en R sont courants, m√™me chez les personnes exp√©riment√©es. La meilleure mani√®re d‚Äôapprendre une langue est de la parler, d‚Äô√©tudier ses susceptibilit√©s, de les tester dans une conversation, etc. 2.6 Petit truc! RStudio peut √™tre impl√©ment√© avec des extensions. L‚Äôune d‚Äôelle permet d‚Äôajuster votre style de code. Par exemple, vous voulez vous assurer que toutes les allocations sont bien effectu√©es avec des &lt;- et non pas des =, ‚Äôil y a bien des espaces de part et d‚Äôautre de &lt;-, que les retours de lignes sont bien plac√©s, etc. Installez le module stryler, et des options appara√Ætront dans le menu Addins comme √† la figure 4.3. Figure 2.9: L‚Äôextension styler permet de formater votre code dans un style particulier 2.7 Extra: Utiliser R avec Jupyter Pour utiliser R dans Jupyter notebook ou Jupyter lab, vous devez installer le module IRkernel dans la version de R que vous d√©sirez utiliser avec Jupyter, puis de lancer la commande IRkernel::installspec(). La prochaine fois que vous ouvrirez Jupyter, le noyau de R devrait appara√Ætre. Je n‚Äôai aucune exp√©rience sur Mac, mais semble-t-il cela fonctionne comme en Linux. Ouvrez R √† partir d‚Äôun terminal (R + Enter), puis lancez IRkernel::installspec() apr√®s avoir install√© IRkernel. Si vous travaillez en Windows, il vous faudra lancer R par son chemin complet dans l‚Äôinvite de commande de Anaconda (Anaconda Powershell Prompt). Par exemple, ouvrir Anaconda Powershell Prompt, puis, si votre installation de R se trouve dans C:\\Program Files\\R\\R-3.6.2, (base) PS C:\\Users\\fifi&gt; cd &quot;C:\\Program Files\\R\\R-3.6.2\\bin&quot; (base) PS C:\\Program Files\\R\\R-3.6.2\\bin&gt; .\\R.exe R version 3.6.2 (2019-12-12) -- &quot;Dark and Stormy Night&quot; Copyright (C) 2019 The R Foundation for Statistical Computing Platform: x86_64-w64-mingw32/x64 (64-bit) R est un logiciel libre livr√© sans AUCUNE GARANTIE. Vous pouvez le redistribuer sous certaines conditions. Tapez &#39;license()&#39; ou &#39;licence()&#39; pour plus de d√©tails. R est un projet collaboratif avec de nombreux contributeurs. Tapez &#39;contributors()&#39; pour plus d&#39;information et &#39;citation()&#39; pour la fa√ßon de le citer dans les publications. Tapez &#39;demo()&#39; pour des d√©monstrations, &#39;help()&#39; pour l&#39;aide en ligne ou &#39;help.start()&#39; pour obtenir l&#39;aide au format HTML. Tapez &#39;q()&#39; pour quitter R. &gt; install.packages(&quot;IRkernel&quot;) Installation du package dans &#39;C:/Users/fifi/Documents/R/win-library/3.6&#39; (car &#39;lib&#39; n&#39;est pas sp√©cifi√©) --- SVP s√©lectionner un miroir CRAN pour cette session --- essai de l&#39;URL &#39;https://cloud.r-project.org/bin/windows/contrib/3.6/IRkernel_1.1.zip&#39; Content type &#39;application/zip&#39; length 138696 bytes (135 KB) downloaded 135 KB le package &#39;IRkernel&#39; a √©t√© d√©compress√© et les sommes MD5 ont √©t√© v√©rifi√©es avec succ√©s Les packages binaires t√©l√©charg√©s sont dans C:\\Users\\fifi\\AppData\\Local\\Temp\\Rtmp6xJtB3\\downloaded_packages &gt; IRkernel::installspec() [InstallKernelSpec] Installed kernelspec ir in C:\\Users\\fifi\\AppData\\Roaming\\jupyter\\kernels\\ir &gt; qui() "],
["chapitre-tableaux.html", "3 Organisation des donn√©es et op√©rations sur des tableaux 3.1 Les collections de donn√©es 3.2 Organiser un tableau de donn√©es 3.3 Formats de tableau 3.4 Entreposer ses donn√©es 3.5 Manipuler des donn√©es en mode tidyverse 3.6 R√©f√©rences", " 3 Organisation des donn√©es et op√©rations sur des tableaux ¬†Objectifs sp√©cifiques: √Ä la fin de ce chapitre, vous comprendrez les r√®gles guidant la cr√©ation et la gestion des tableaux, saurez importer et exporter des donn√©es et saurez effectuer des op√©rations en cascade avec le module tidyverse, dont des filtres sur les lignes, des s√©lections de colonnes, des sommaires statistiques et des jointures entre tableaux. Les donn√©es sont utilis√©es √† chaque √©tape dans les flux de travail en sciences. Elles alimentent l‚Äôanalyse et la mod√©lisation. Les r√©sultats qui en d√©coulent sont aussi des donn√©es qui peuvent alimenter les travaux subs√©quents. Une bonne organisation des donn√©es facilitera le flux de travail. Dicton. Proportions de temps vou√© au calcul scientifique¬†: 80 % de nettoyage de donn√©es mal organis√©es, 20 % de calcul. Qu‚Äôest-ce qu‚Äôune donn√©e ? De mani√®re abstraite, il s‚Äôagit d‚Äôune valeur associ√©e √† une variable. Une variable peut √™tre une dimension, une date, une couleur, le r√©sultat d‚Äôun test statistique, √† laquelle on attribue la valeur quantitative ou qualitative d‚Äôun chiffre, d‚Äôune cha√Æne de caract√®re, d‚Äôun symbole conventionn√©, etc. Par exemple, lorsque vous commandez un caf√© latte v√©gane, au latte est la valeur que vous attribuez √† la variable type de caf√©, et v√©gane est la valeur de la variable type de lait. L‚Äôexemple est peut-√™tre horrible. J‚Äôai besoin d‚Äôun caf√©‚Ä¶ Ce chapitre traite de l‚Äôimportation, l‚Äôutilisation et l‚Äôexportation de donn√©es structur√©es, en R, sous forme de vecteurs, matrices, tableaux et ensemble de tableaux (bases de donn√©es). Bien qu‚Äôil soit toujours pr√©f√©rable d‚Äôorganiser les structures qui accueilleront les donn√©es d‚Äôune exp√©rience avant-m√™me de proc√©der √† la collecte de donn√©es, l‚Äôanalyste doit s‚Äôattendre √† r√©organiser ses donn√©es en cours de route. Or, des donn√©es bien organis√©es au d√©part faciliteront aussi leur r√©organisation. Ce chapitre d√©bute avec quelques d√©finitions : les donn√©es, les matrices, les tableaux et les bases de donn√©es, ainsi que leur signification en R. Puis nous verrons comment organiser un tableau selon quelques r√®gles simples, mais importantes pour √©viter les erreurs et les op√©rations fastidieuses pour reconstruire un tableau mal con√ßu. Ensuite, nous traiterons des formats de tableau courant, pour enfin passer √† l‚Äôutilisation de dplyr, le module du tidyverse pour effectuer des op√©rations sur les tableaux. 3.1 Les collections de donn√©es Dans le chapitre 2, nous avons survol√© diff√©rents types d‚Äôobjets : r√©els, entiers, cha√Ænes de caract√®res et bool√©ens. Les donn√©es peuvent appartenir √† d‚Äôautres types : dates, cat√©gories ordinales (ordonn√©es : faible, moyen, √©lev√©) et nominales (non ordonn√©es : esp√®ces, cultivars, couleurs, unit√© p√©dologique, etc.). Comme mentionn√© en d√©but de chapitre, une donn√©e est une valeur associ√©e √† une variable. Les donn√©es peuvent √™tre organis√©es en collections. Nous avons aussi vu au chapitre 2 que la mani√®re privil√©gi√©e d‚Äôorganiser des donn√©es √©tait sous forme de tableaux. De mani√®re g√©n√©rale, un tableau de donn√©es est une organisation de donn√©es en deux dimensions, comportant des lignes et des colonnes. Il est pr√©f√©rable de respecter la convention selon laquelle les lignes sont des observations et les colonnes sont des variables. Ainsi, un tableau est une liste de vecteurs de m√™me longueur, chaque vecteur repr√©sentant une variable. Chaque variable est libre de prendre le type de donn√©es appropri√©. La position d‚Äôune donn√©e dans le vecteur correspond √† une observation. Lorsque les vecteurs sont pos√©s les uns √† c√¥t√© des autres, la position dans le vecteur devient une ligne qui d√©finit les valeurs des variables d‚Äôune observation. Imaginez que vous consignez des donn√©es m√©t√©orologiques comme les pr√©cipitations totales ou la temp√©rature moyenne pour chaque jour, pendant une semaine sur les sites A, B et C. Chaque site poss√®de ses propres caract√©ristiques, comme la position en longitude-latitude. Il est redondant de r√©p√©ter la position du site pour chaque jour de la semaine. Vous pr√©f√©rerez cr√©er deux tableaux : un pour d√©crire vos observations, et un autre pour d√©crire les sites. De cette mani√®re, vous cr√©ez une collection de tableaux interreli√©s : une base de donn√©es. Nous couvrirons cette notion un peu plus loin. R peut soutirer des donn√©es des bases de donn√©es gr√¢ce au module DBI, qui n‚Äôest pas couvert √† ce stade de d√©veloppement du cours. Dans R, les donn√©es structur√©es en tableaux, ainsi que les op√©rations sur les tableaux, peuvent √™tre g√©r√©s gr√¢ce aux modules readr, dplyr et tidyr, tous des modules faisant partie du m√©ta-module tidyverse. Mais avant de se lancer dans l‚Äôutilisation de ces modules, voyons quelques r√®gles √† suivre pour bien structurer ses donn√©es en format tidy, un jargon du tidyverse qui signifie proprement organis√©. 3.2 Organiser un tableau de donn√©es Afin de rep√©rer chaque cellule d‚Äôun tableau, on attribue √† chaque ligne et √† chaque colonne un identifiant unique, que l‚Äôon nomme indice pour les lignes et ent√™te pour les colonnes. R√®gle no 1. Une variable par colonne, une observation par ligne, une valeur par cellule. Les unit√©s exp√©rimentales sont d√©crits par une ou plusieurs variables par des chiffres ou des lettres. Chaque variable devrait √™tre pr√©sente en une seule colonne, et chaque ligne devrait correspondre √† une unit√© exp√©rimentale o√π ces variables ont √©t√© mesur√©es. La r√®gle parait simple, mais elle est rarement respect√©e. Prenez par exemple le tableau suivant. Table 3.1: Rendements obtenus sur les sites exp√©rimentaux selon les traitements. Site Traitement A Traitement B Traitement C Sainte-Souris 4.1 8.2 6.8 Sainte-Fourmi 5.8 5.9 NA Saint-Ours 2.9 3.4 4.6 Qu‚Äôest-ce qui cloche avec ce tableau? Chaque ligne est une observation, mais contient plusieurs observations d‚Äôune m√™me variable, le rendement, qui devient √©tal√© sur plusieurs colonnes. √Ä bien y penser, le type de traitement est une variable et le rendement en est une autre: Table 3.2: Rendements obtenus sur les sites exp√©rimentaux selon les traitements. Site Traitement Rendement Sainte-Souris Traitement A 4.1 Sainte-Souris Traitement B 8.2 Sainte-Souris Traitement C 6.8 Sainte-Fourmi Traitement A 5.8 Sainte-Fourmi Traitement B 5.9 Sainte-Fourmi Traitement C NA Saint-Ours Traitement A 2.9 Saint-Ours Traitement B 3.4 Saint-Ours Traitement C 4.6 Plus pr√©cis√©ment, l‚Äôexpression √† bien y penser sugg√®re une r√©flexion sur la signification des donn√©es. Certaines variables peuvent parfois √™tre int√©gr√©es dans une m√™me colonne, parfois pas. Par exemple, les concentrations en cuivre, zinc et plomb dans un sol contamin√© peuvent √™tre plac√©s dans la m√™me colonne ‚ÄúConcentration‚Äù ou d√©clin√©es en plusieurs colonnes Cu, Zn et Pb. La premi√®re version trouvera son utilit√© pour des cr√©er des graphiques (chapitre 3), alors que la deuxi√®me favorise le traitement statistique (chapitre 5). Il est possible de passer d‚Äôun format √† l‚Äôautre gr√¢ce √† la fonction pivot_longer() et pivot_wider() du module tidyr. R√®gle no 2. Un tableau par unit√© observationnelle: ne pas r√©p√©ter les informations. Reprenons la m√™me exp√©rience. Supposons que vous mesurez la pr√©cipitation √† l‚Äô√©chelle du site. Table 3.3: Rendements et pr√©cipitations obtenus sur les sites exp√©rimentaux selon les traitements. Site Traitement Rendement Pr√©cipitations Sainte-Souris Traitement A 4.1 813 Sainte-Souris Traitement B 8.2 813 Sainte-Souris Traitement C 6.8 813 Sainte-Fourmi Traitement A 5.8 642 Sainte-Fourmi Traitement B 5.9 642 Sainte-Fourmi Traitement C NA 642 Saint-Ours Traitement A 2.9 1028 Saint-Ours Traitement B 3.4 1028 Saint-Ours Traitement C 4.6 1028 Segmenter l‚Äôinformation en deux tableaux serait pr√©f√©rable. Table 3.4: Pr√©cipitations sur les sites exp√©rimentaux. Site Pr√©cipitations Sainte-Souris 813 Sainte-Fourmi 642 Saint-Ours 1028 Les tableaux 3.2 et 3.4, ensemble, forment une base de donn√©es (collection organis√©e de tableaux). Les op√©rations de fusion entre les tableaux peuvent √™tre effectu√©es gr√¢ce aux fonctions de jointure (left_join(), par exemple) du module tidyr. Une jointure de 3.4 vers 3.2 donnera le tableau 3.3. R√®gle no 3. Ne pas bousiller les donn√©es. Par exemple. Ajouter des commentaires dans des cellules. Si une cellule m√©rite d‚Äô√™tre comment√©e, il est pr√©f√©rable de placer les commentaires soit dans un fichier d√©crivant le tableau de donn√©es, soit dans une colonne de commentaire juxtapos√©e √† la colonne de la variable √† commenter. Par exemple, si vous n‚Äôavez pas mesure le pH pour une observation, n‚Äô√©crivez pas ‚Äú√©chantillon contamin√©‚Äù dans la cellule, mais annoter dans un fichier d‚Äôexplication que l‚Äô√©chantillon no X a √©t√© contamin√©. Si les commentaires sont syst√©matiques, il peut √™tre pratique de les inscrire dans une colonne commentaire_pH. Inscription non syst√©matique. Il arrive souvent que des cat√©gories d‚Äôune variable ou que des valeurs manquantes soient annot√©es diff√©remment. Il arrive m√™me que le s√©parateur d√©cimal soit non syst√©matique, parfois not√© par un point, parfois par une virgule. Par exemple, une fois import√©s dans votre session, les cat√©gories St-Ours et Saint-Ours seront trait√©es comme deux cat√©gories distinctes. De m√™me, les cellules correspondant √† des valeurs manquantes ne devraient pas √™tre inscrite parfois avec une cellule vide, parfois avec un point, parfois avec un tiret ou avec la mention NA. Le plus simple est de laisser syst√©matiquement ces cellules vides. Inclure des notes dans un tableau. La r√®gle ‚Äúune colonne, une variable‚Äù n‚Äôest pas respect√©e si on ajoute des notes un peu n‚Äôimporte o√π sous ou √† c√¥t√© du tableau. Ajouter des sommaires. Si vous ajoutez une ligne sous un tableau comprenant la moyenne de chaque colonne, qu‚Äôest-ce qui arrivera lorsque vous importerez votre tableau dans votre session de travail? La ligne sera consid√©r√©e comme une observation suppl√©mentaire. Inclure une hi√©rarchie dans les ent√™tes. Afin de consigner des donn√©es de texture du sol, comprenant la proportion de sable, de limon et d‚Äôargile, vous organisez votre ent√™te en plusieurs lignes. Une ligne pour la cat√©gorie de donn√©e, Texture, fusionn√©e sur trois colonnes, puis trois colonnes intitul√©es Sable, Limon et Argile. Votre tableau est joli, mais il ne pourra pas √™tre import√© conform√©ment dans un votre session de calcul : on recherche une ent√™te unique par colonne. Votre tableau de donn√©es devrait plut√¥t porter les ent√™tes Texture sable, Texture limon et Texture argile. Un conseil : r√©server le travail esth√©tique √† la toute fin d‚Äôun flux de travail. 3.3 Formats de tableau Plusieurs outils sont √† votre disposition pour cr√©er des tableaux. Je vous pr√©sente ici les plus communs. 3.3.1 xls ou xlsx Microsoft Excel est un logiciel de type tableur, ou chiffrier √©lectronique. L‚Äôancien format xls a √©t√© remplac√© par le format xlsx avec l‚Äôarriv√©e de Microsoft Office 2010. Il s‚Äôagit d‚Äôun format propri√©taire, dont l‚Äôalternative libre la plus connue est le format ods, popularis√© par la suite bureautique LibreOffice. Les formats xls, xlsx ou ods sont davantage utilis√©s comme outils de calcul que d‚Äôentreposage de donn√©es. Ils contiennent des formules, des graphiques, du formatage de cellule, etc. Je ne les recommande pas pour stocker des donn√©es. 3.3.2 csv Le format csv, pour comma separated values, est un fichier texte, que vous pouvez ouvrir avec n‚Äôimporte quel √©diteur de texte brut (Bloc note, Atom, Notepad++, etc.). Chaque colonne doit √™tre d√©limit√©e par un caract√®re coh√©rent (conventionnellement une virgule, mais en fran√ßais un point-virgule ou une tabulation pour √©viter la confusion avec le s√©parateur d√©cimal) et chaque ligne du tableau est un retour de ligne. Il est possible d‚Äôouvrir et d‚Äô√©diter les fichiers csv dans un √©diteur texte, mais il est plus pratique de les ouvrir avec des tableurs (LibreOffice Calc, Microsoft Excel, Google Sheets, etc.). Encodage des fichiers texte. Puisque le format csv est un fichier texte, un souci particulier doit √™tre port√© sur la mani√®re dont le texte est encod√©. Les caract√®res accentu√©s pourrait √™tre import√© incorrectement si vous importez votre tableau en sp√©cifiant le mauvais encodage. Pour les fichiers en langues occidentales, l‚Äôencodage UTF-8 devrait √™tre utilis√©. Toutefois, par d√©faut, Excel utilise un encodage de Microsoft. Si le csv a √©t√© g√©n√©r√© par Excel, il est pr√©f√©rable de l‚Äôouvrir avec votre √©diteur texte et de l‚Äôenregistrer dans l‚Äôencodage UTF-8. 3.3.3 json Comme le format csv, le format json indique un fichier en texte clair. En permettant des structures de tableaux embo√Æt√©s et en ne demandant pas que chaque colonne ait la m√™me longueur, le format json permet plus de souplesse que le format csv, mais il est plus compliqu√© √† consult√© et prend davantage d‚Äôespace sur le disque que le csv. Il est utilis√© davantage pour le partage de donn√©es des applications web, mais en ce qui concerne la mati√®re du cours, ce format est surtout utilis√© pour les donn√©es g√©or√©f√©renc√©es. L‚Äôencodage est g√©r√© de la m√™me mani√®re qu‚Äôun fichier csv. 3.3.4 SQLite SQLite est une application pour les bases de donn√©es relationnelles de type SQL qui n‚Äôa pas besoin de serveur pour fonctionner. Les bases de donn√©es SQLite sont encod√©s dans des fichiers portant l‚Äôextension db, qui peuvent √™tre facilement partag√©s. 3.3.5 Suggestion En csv pour les petits tableaux, en sqlite pour les bases de donn√©es plus complexes. Ce cours se concentre toutefois sur les donn√©es de type csv. 3.4 Entreposer ses donn√©es La mani√®re la plus s√©curis√©e pour entreposer ses donn√©es est de les confiner dans une base de donn√©es s√©curis√©e sur un serveur s√©curis√© dans un environnement s√©curis√© et d‚Äôencrypter les communications. C‚Äôest aussi‚Ä¶ la mani√®re la moins accessible. Des espaces de stockage nuagiques, comme Dropbox ou d‚Äôautres options similaires, peuvent √™tre pratiques pour les backups et le partage des donn√©es avec une √©quipe de travail (qui risque en retour de bousiller vos donn√©es). Le suivi de version est possible chez certains fournisseurs d‚Äôespace de stockage. Mais pour un suivi de version plus rigoureux, les espaces de d√©veloppement (comme GitHub et GitLab) sont plus appropri√©s (couverts au chapitre 5). Dans tous les cas, il est important de garder (1) des copies anciennes pour y revenir en cas d‚Äôerreurs et (2) un petit fichier d√©crivant les changements effectu√©s sur les donn√©es. 3.5 Manipuler des donn√©es en mode tidyverse Le m√©ta-module tidyverse regroupe une collection de pr√©cieux modules pour l‚Äôanalyse de donn√©es en R. Il permet d‚Äôimporter des donn√©es dans votre session de travail avec readr, de les explorer avec le module de visualisation ggplot2, de les transformer avec tidyr et dplyr et de les exporter avec readr. Les tableaux de classe data.frame, comme ceux de la plus moderne classe tibble, peuvent √™tre manipul√©s √† travers le flux de travail pour l‚Äôanalyse et la mod√©lisation. Comme c‚Äô√©tait le cas pour le chapitre sur la visualisation, ce chapitre est loin de couvrir les nombreuses fonctionnalit√©s qui sont offertes dans le tidyverse. 3.5.1 Importer vos donn√©es dans votre session de travail Supposons que vous avec bien organis√© vos donn√©es en mode tidy. Pour les importer dans votre session et commencer √† les inspecter, vous lancerez une des commandes du module readr, d√©crites dans la documentation d√©di√©e. read_csv() si le s√©parateur de colonne est une virgule read_csv2() si le s√©parateur de colonne est un point-virgule et que le s√©parateur d√©cimal est une virgule read_tsv() si le s√©parateur de colonne est une tabulation read_table() si le s√©parateur de colonne est un espace blanc read_delim() si le s√©parateur de colonne est un autre caract√®re (comme le point-virgule) que vous sp√©cifierez dans l‚Äôargument delim = &quot;;&quot; Les principaux arguments sont les suivants. file: le chemin vers le fichier. Ce chemin peut aussi bien √™tre une adresse locale (data/‚Ä¶) qu‚Äôune adresse internet (https://‚Ä¶). delim: le symbole d√©limitant les colonnes dans le cas de read_delim. col_names: si TRUE, la premi√®re ligne est l‚Äôent√™te du tableau, sinon FALSE. Si vous sp√©cifiez un vecteur num√©rique, ce sont les num√©ros des lignes utilis√©es pour le nom de l‚Äôent√™te. Si vous utilisez un vecteur de caract√®res, ce sont les noms des colonnes que vous d√©sirez donner √† votre tableau. na: le symbole sp√©cifiant une valeur manquante. L‚Äôargument na='' signifie que les cellules vides sont des donn√©es manquantes. Si les valeurs manquantes ne sont pas uniformes, vous pouvez les indiquer dans un vecteur, par exemple na = c(&quot;&quot;, &quot;NA&quot;, &quot;NaN&quot;, &quot;.&quot;, &quot;-&quot;). local: cet argument prend une fonction local() qui peut inclure des arguments de format de temps, mais aussi d‚Äôencodage (voir documentation) D‚Äôautres arguments peuvent √™tre sp√©cifi√©s au besoin, et les r√©p√©ter ici dupliquerait l‚Äôinformation de la documentation de la fonction read_csv de readr. Je d√©conseille d‚Äôimporter des donn√©es en format xls ou xlsx. Si toutefois cela vous convient, je vous r√©f√®re au module readxl. L‚Äôaide-m√©moire de readr (figure 3.1) est √† afficher pr√®s de soi. Figure 3.1: Aide-m√©moire de readr, Source: https://github.com/rstudio/cheatsheets/raw/master/data-import.pdf Nous allons charger des donn√©es de culture de la chicout√© (Rubus chamaemorus), un petit fruit nordique, tir√© de Parent et al. (2013). Ouvrons d‚Äôabord le fichier pour v√©rifier les s√©parateurs de colonne et de d√©cimale (figure 3.2). Figure 3.2: Aper√ßu brut d‚Äôun fichier csv. Le s√©parateur de colonne est un point-virgule et le d√©cimal est une virgule. Avec Atom, mon √©diteur texte pr√©f√©r√© (il y en a d‚Äôautres), je vais dans Edit &gt; Select Encoding et j‚Äôobtiens bien le UTF-8 (figure 3.3). Figure 3.3: Changer l‚Äôencodage d‚Äôun fichier csv. Nous allons donc utiliser read_csv2() avec ses arguments par d√©faut. library(&quot;tidyverse&quot;) chicoute &lt;- read_csv2(&quot;data/chicoute.csv&quot;) ## Using &#39;,&#39; as decimal and &#39;.&#39; as grouping mark. Use read_delim() for more control. ## Parsed with column specification: ## cols( ## .default = col_double(), ## CodeTourbiere = col_character(), ## Ordre = col_character(), ## Traitement = col_character(), ## DemiParcelle = col_character(), ## SousTraitement = col_character() ## ) ## See spec(...) for full column specifications. Quelques commandes utiles inspecter le tableau: head() pr√©sente l‚Äôent√™te du tableau, soit ses 6 premi√®res lignes str() et glimpse() pr√©sentent les variables du tableau et leur type - glimpse()est la fonction tidyverse et str() est la fonction classique (je pr√©f√®re str()) summary() pr√©sente des statistiques de base du tableau names() ou colnames() sort les noms des colonnes sous forme d‚Äôun vecteur dim() donne les dimensions du tableau, ncol() son nombre de colonnes et nrow() son nombre de lignes skim est une fonction du module skimr montrant un portrait graphique et num√©rique du tableau Extra 1. Plusieurs modules ne se trouvent pas dans les d√©p√¥ts CRAN, mais sont disponibles sur GitHub. Pour les installer, installez d‚Äôabord le module devtools disponible sur CRAN. Vous pourrez alors installer les packages de GitHub comme on le fait avec le package skimr. Extra 2. Lorsque je d√©sire utiliser une fonction, mais sans charger le module dans la session, j‚Äôutilise la notation module::fonction. Comme dans ce cas, pour skimr. # devtools::install_github(&quot;ropenscilabs/skimr&quot;) skimr::skim(chicoute) Table 3.5: Data summary Name chicoute Number of rows 90 Number of columns 31 _______________________ Column type frequency: character 5 numeric 26 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace CodeTourbiere 0 1.00 1 4 0 12 0 Ordre 0 1.00 1 2 0 20 0 Traitement 50 0.44 6 11 0 2 0 DemiParcelle 50 0.44 4 5 0 2 0 SousTraitement 50 0.44 1 7 0 3 0 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist ID 0 1.00 45.50 26.12 1.00 23.25 45.50 67.75 90.00 ‚ñá‚ñá‚ñá‚ñá‚ñá Site 0 1.00 6.33 5.49 1.00 2.00 4.00 9.00 20.00 ‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ Latitude_m 0 1.00 5701839.86 1915.50 5695688.00 5701868.50 5702129.00 5702537.00 5706394.00 ‚ñÅ‚ñÇ‚ñÖ‚ñá‚ñÅ Longitude_m 0 1.00 485295.54 6452.33 459873.00 485927.00 486500.00 486544.75 491955.00 ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñá Rendement_g_5m2 50 0.44 13.33 21.56 0.00 0.00 0.94 15.63 72.44 ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ TotalRamet_nombre_m2 0 1.00 251.26 156.06 40.74 122.70 212.92 347.80 651.90 ‚ñá‚ñá‚ñÉ‚ñÇ‚ñÇ TotalVegetatif_nombre_m2 4 0.96 199.02 139.13 22.92 86.26 161.25 263.78 580.60 ‚ñá‚ñá‚ñÇ‚ñÇ‚ñÅ TotalFloral_nombre_m2 4 0.96 52.08 40.41 4.80 22.92 43.00 69.52 198.62 ‚ñá‚ñÖ‚ñÇ‚ñÅ‚ñÅ TotalMale_nombre_m2 4 0.96 24.40 26.87 0.00 3.30 15.28 36.51 104.41 ‚ñá‚ñÇ‚ñÇ‚ñÅ‚ñÅ TotalFemelle_nombre_m2 4 0.96 27.53 29.83 2.55 10.34 17.19 31.96 187.17 ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ FemelleFruit_nombre_m2 18 0.80 19.97 23.79 0.40 7.64 11.46 22.83 157.88 ‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ FemelleAvorte_nombre_m2 4 0.96 8.49 14.52 0.00 1.27 3.07 10.14 76.80 ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ SterileFleur_nombre_m2 4 0.96 0.26 0.71 0.00 0.00 0.00 0.00 3.82 ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ C_pourc 0 1.00 50.28 1.61 46.72 49.14 50.45 51.58 53.83 ‚ñÉ‚ñÜ‚ñÖ‚ñá‚ñÅ N_pourc 0 1.00 2.20 0.40 1.53 1.89 2.12 2.58 3.10 ‚ñÉ‚ñá‚ñÉ‚ñÉ‚ñÇ P_pourc 0 1.00 0.14 0.04 0.07 0.12 0.14 0.16 0.23 ‚ñÉ‚ñÜ‚ñá‚ñÇ‚ñÇ K_pourc 0 1.00 0.89 0.27 0.35 0.69 0.86 1.13 1.54 ‚ñÉ‚ñá‚ñá‚ñá‚ñÅ Ca_pourc 0 1.00 0.39 0.10 0.19 0.32 0.37 0.44 0.88 ‚ñÖ‚ñá‚ñÇ‚ñÅ‚ñÅ Mg_pourc 0 1.00 0.50 0.08 0.36 0.45 0.48 0.52 0.86 ‚ñá‚ñá‚ñÇ‚ñÅ‚ñÅ S_pourc 0 1.00 0.13 0.04 0.07 0.11 0.13 0.14 0.28 ‚ñÖ‚ñá‚ñÇ‚ñÅ‚ñÅ B_pourc 0 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ‚ñÇ‚ñÖ‚ñÉ‚ñá‚ñÉ Cu_pourc 0 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ Zn_pourc 0 1.00 0.01 0.00 0.00 0.01 0.01 0.01 0.02 ‚ñá‚ñá‚ñÇ‚ñÅ‚ñÅ Mn_pourc 0 1.00 0.03 0.03 0.00 0.01 0.03 0.05 0.10 ‚ñá‚ñÖ‚ñÉ‚ñÇ‚ñÅ Fe_pourc 0 1.00 0.02 0.01 0.01 0.01 0.01 0.02 0.05 ‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ Al_pourc 0 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 ‚ñá‚ñÖ‚ñÅ‚ñÅ‚ñÅ Exercice. Inspectez le tableau. 3.5.2 Comment s√©lectionner et filtrer des donn√©es ? On utilise le terme s√©lectionner lorsque l‚Äôon d√©sire choisir une ou plusieurs lignes et colonnes d‚Äôun tableau (la plupart du temps des colonnes). L‚Äôaction de filtrer signifie de s√©lectionner des lignes selon certains crit√®res. 3.5.2.1 S√©lectionner Voici 4 mani√®res de s√©lectionner une colonne en R. Une m√©thode rapide mais peu expressive consiste √† indiquer les valeurs num√©riques de l‚Äôindice de la colonne entre des crochets. Il s‚Äôagit d‚Äôappeler le tableau suivit de crochets. L‚Äôint√©rieur des crochets comprend deux √©l√©ments s√©par√©s par une virgule. Le premier √©l√©ment sert √† filtrer selon l‚Äôindice, le deuxi√®me sert √† s√©lectionner selon l‚Äôindice. Ainsi: chicoute[, 1]: s√©lectionner la premi√®re colonne chicoute[, 1:10]: s√©lectionner les 10 premi√®res colonnes chicoute[, c(2, 4, 5)]: s√©lectionner les colonnes 2, 4 et 5 chicoute[c(10, 13, 20), c(2, 4, 5)]: s√©lectionner les colonnes 2, 4 et 5 et les lignes 10, 13 et 20. Une autre m√©thode rapide, mais plus expressive, consiste √† appeler le tableau, suivi du symbole $, puis le nom de la colonne, e.g. chicoute$Site. Ou bien d‚Äôinscrire le nom de la colonne, ou du vecteur des colonnes, entre des crochets suivant le nom du tableau, c‚Äôest-√†-dire chicoute[c(&quot;Site&quot;, &quot;Latitude_m&quot;, &quot;Longitude_m&quot;)]. Enfin, dans une s√©quence d‚Äôop√©rations en mode pipeline (chaque op√©ration est mise √† la suite de la pr√©c√©dente en pla√ßant le pipe %&gt;% entre chacune), il peut √™tre pr√©f√©rable de s√©lectionner des colonnes avec la fonction select(), i.e. chicoute %&gt;% select(Site, Latitude_m, Longitude_m) Truc. La plupart des IDE, comme RStudio, peuvent vous proposer des colonnes dans une liste. Apr√®s avoir entrer le $, taper sur la touche de tabulation: vous pourrez s√©lectionner la colonne dans une liste d√©filante (figure 3.4). Figure 3.4: Autocompl√©tion dans RStudio. La fonction select() permet aussi de travailler en exclusion. Ainsi pour enlever des colonnes, on placera un - (signe de soustraction) devant le nom de la colonne. ‚ö†Ô∏è Attention. Plusieurs modules utilisent la fonction select (et filter, plus bas). Lorsque vous lancez select et que vous obtenez un message d‚Äôerreur comme Error in select(., ends_with(&quot;pourc&quot;)) : argument inutilis√© (ends_with(&quot;pourc&quot;)) il se pourrait bien que R utilise la fonction select d‚Äôun autre module. Pour sp√©cifier que vous d√©sirez la fonction select du module dplyr, sp√©cifiez dplyr::select. D‚Äôautre arguments de select() permettent une s√©lection rapide. Par exemple, pour obtenir les colonnes contenant des pourcentages: chicoute %&gt;% select(ends_with(&quot;pourc&quot;)) %&gt;% head(3) ## # A tibble: 3 x 13 ## C_pourc N_pourc P_pourc K_pourc Ca_pourc Mg_pourc S_pourc B_pourc Cu_pourc Zn_pourc Mn_pourc Fe_pourc Al_pourc ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 51.5 1.72 0.108 1.21 0.435 0.470 0.0976 0.00258 0.000175 0.00470 0.101 0.0133 0.00156 ## 2 51.3 2.18 0.0985 1.22 0.337 0.439 0.0996 0.00258 0.000407 0.00729 0.0783 0.0148 0.00189 ## 3 50.6 2.12 0.0708 1.05 0.373 0.420 0.104 0.00258 0.000037 0.00713 0.0722 0.0148 0.00160 3.5.2.2 Filtrer Comme c‚Äôest le cas de la s√©lection, on pourra filtrer un tableau de plusieurs mani√®res. J‚Äôai d√©j√† pr√©sent√© comment filtrer selon les indices des lignes. Les autres mani√®res reposent n√©anmoins sur une op√©ration logique ==, &lt;, &gt; ou %in% (le %in% signifie se trouve parmi et peut √™tre suivi d‚Äôun vecteur de valeur que l‚Äôon d√©sire accepter). Les conditions bool√©ennes peuvent √™tre combin√©es avec les op√©rateurs et, &amp;, et ou, |. Pour rappel, Op√©ration R√©sultat Vrai et Vrai Vrai Vrai et Faux Faux Faux et Faux Faux Vrai ou Vrai Vrai Vrai ou Faux Vrai Faux ou Faux Faux La m√©thode classique consiste √† appliquer une op√©ration logique entre les crochets, par exemple chicoute[chicoute$CodeTourbiere == &quot;BEAU&quot;, ] La m√©thode tidyverse, plus pratique en mode pipeline, passe par la fonction filter(), i.e. chicoute %&gt;% filter(CodeTourbiere == &quot;BEAU&quot;) Combiner le tout. chicoute %&gt;% filter(Ca_pourc &lt; 0.4 &amp; CodeTourbiere %in% c(&quot;BEAU&quot;, &quot;MB&quot;, &quot;WTP&quot;)) %&gt;% select(contains(&quot;pourc&quot;)) ## # A tibble: 4 x 13 ## C_pourc N_pourc P_pourc K_pourc Ca_pourc Mg_pourc S_pourc B_pourc Cu_pourc Zn_pourc Mn_pourc Fe_pourc Al_pourc ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 51.3 2.18 0.0985 1.22 0.337 0.439 0.0996 0.00258 0.000407 0.00729 0.0783 0.0148 0.00189 ## 2 50.6 2.12 0.0708 1.05 0.373 0.420 0.104 0.00258 0.000037 0.00713 0.0722 0.0148 0.00160 ## 3 53.8 2.04 0.115 0.947 0.333 0.472 0.106 0.00258 0.000037 0.00510 0.0345 0.0120 0.00102 ## 4 52.6 2.11 0.0847 0.913 0.328 0.376 0.111 0.00296 0.000037 0.00679 0.0491 0.0141 0.00151 3.5.3 Le format long et le format large Dans le tableau chicoute, chaque √©l√©ment poss√®de sa propre colonne. Si l‚Äôon voulait mettre en graphique les boxplot des facettes de concentrations d‚Äôazote, de phosphore et de potassium dans les diff√©rentes tourbi√®res, il faudrait obtenir une seule colonne de concentrations. Pour ce faire, nous utiliserons la fonction pivot_longer(). L‚Äôargument obligatoire (excluant le tableau, qui est implicite dans la cha√Æne d‚Äôop√©rations), est cols, le nom des colonnes √† allonger. Pour obtenir des noms de colonnes allong√©es personnalis√©es, on sp√©cifie le nom des variables consistant aux anciens noms de colonnes avec names_to et celui de la nouvelle colonne contenant les valeurs dans values_to. La suite consiste √† d√©crire les colonnes √† inclure ou √† exclure. Dans le cas qui suit, j‚Äôexclue CodeTourbiere de la refonte j‚Äôutilise sample_n() pour pr√©senter un √©chantillon du r√©sultat. Notez la ligne comprenant la fonction mutate, que l‚Äôon verra plus loin. Cette fonction ajoute une colonne au tableau. Dans ce cas-ci, j‚Äôajoute une colonne constitu√©e d‚Äôune s√©quence de nombres allant de 1 au nombre de lignes du tableau (il y en a 90). Cet identifiant unique pour chaque ligne permettra de reconstituer par la suite le tableau initial. chicoute_long &lt;- chicoute %&gt;% select(CodeTourbiere, N_pourc, P_pourc, K_pourc) %&gt;% mutate(ID = 1:nrow(.)) %&gt;% # mutate ajoute une colonne au tableau pivot_longer(cols = contains(&quot;pourc&quot;), names_to = &quot;nutrient&quot;, values_to = &quot;concentration&quot;) chicoute_long %&gt;% sample_n(10) ## # A tibble: 10 x 4 ## CodeTourbiere ID nutrient concentration ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 BS2 64 N_pourc 1.89 ## 2 BEAU 2 K_pourc 1.22 ## 3 1 84 P_pourc 0.161 ## 4 1 83 P_pourc 0.132 ## 5 2 21 K_pourc 1.17 ## 6 NTP 51 P_pourc 0.104 ## 7 BS2 65 N_pourc 1.68 ## 8 MB 35 N_pourc 2.11 ## 9 2 23 N_pourc 2.86 ## 10 SSP 58 K_pourc 0.686 L‚Äôop√©ration inverse est pivot_wider(), avec laquelle nous s√©lectionnons une colonne sp√©cifiant les nouvelles colonnes √† construire (names_from) ainsi que les valeurs √† placer dans ces colonnes (values_from). chicoute_large &lt;- chicoute_long %&gt;% pivot_wider(names_from = nutrient, values_from = concentration) chicoute_large %&gt;% sample_n(10) ## # A tibble: 10 x 5 ## CodeTourbiere ID N_pourc P_pourc K_pourc ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 66 2.63 0.186 0.988 ## 2 2 22 2.92 0.226 1.22 ## 3 2 20 2.80 0.224 1.20 ## 4 MB 34 2.35 0.115 0.738 ## 5 1 82 1.95 0.141 0.937 ## 6 NTP 54 1.67 0.0869 0.456 ## 7 MB 32 2.09 0.117 0.454 ## 8 NESP 45 1.86 0.142 0.807 ## 9 NTP 51 2.05 0.104 0.398 ## 10 1 70 2.47 0.160 1.01 3.5.4 Combiner des tableaux Nous avons introduit plus haut la notion de base de donn√©es. Nous voudrions peut-√™tre utiliser le code des tourbi√®res pour inclure leur nom, le type d‚Äôessai men√© √† ces tourbi√®res, etc. Importons d‚Äôabord le tableau des noms li√©s aux codes. tourbieres &lt;- read_csv2(&quot;data/chicoute_tourbieres.csv&quot;) ## Using &#39;,&#39; as decimal and &#39;.&#39; as grouping mark. Use read_delim() for more control. ## Parsed with column specification: ## cols( ## Tourbiere = col_character(), ## CodeTourbiere = col_character(), ## Type = col_character(), ## TypeCulture = col_character() ## ) tourbieres ## # A tibble: 11 x 4 ## Tourbiere CodeTourbiere Type TypeCulture ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Beaulieu BEAU calibration naturel ## 2 Brador Path BP calibration naturel ## 3 Lichen (BS2E) 2 validation cultive sec ## 4 Mannys Brook MB calibration naturel ## 5 Middle Bay Road MR calibration naturel ## 6 North Est of Smelt Pond NESP calibration naturel ## 7 North of Blue Moon NBM calibration naturel ## 8 South of Smelt Pond SSP calibration naturel ## 9 Sphaigne (BS2F) BS2 validation cultive sec ## 10 Sphaigne (BS2F) 1 calibration naturel ## 11 West of Trout Pond WTP calibration naturel Notre information est organis√©e en deux tableaux, li√©s par la colonne CodeTourbiere. Comment fusionner l‚Äôinformation pour qu‚Äôelle puisse √™tre utilis√©e dans son ensemble? La fonction left_join effectue cette op√©ration typique avec les bases de donn√©es. chicoute_merge &lt;- left_join(x = chicoute, y = tourbieres, by = &quot;CodeTourbiere&quot;) # ou bien chicoute %&gt;% left_join(y = tourbieres, by = &quot;CodeTourbiere&quot;) chicoute_merge %&gt;% sample_n(4) ## # A tibble: 4 x 34 ## ID CodeTourbiere Ordre Site Traitement DemiParcelle SousTraitement Latitude_m Longitude_m Rendement_g_5m2 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 89 WTP E 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 5700770 487058 NA ## 2 88 WTP E 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 5700775 487060 NA ## 3 86 WTP E 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 5700788 487075 NA ## 4 12 2 6 11 fertilisa‚Ä¶ right Cu 5702582 486522 0 ## # ‚Ä¶ with 24 more variables: TotalRamet_nombre_m2 &lt;dbl&gt;, TotalVegetatif_nombre_m2 &lt;dbl&gt;, TotalFloral_nombre_m2 &lt;dbl&gt;, ## # TotalMale_nombre_m2 &lt;dbl&gt;, TotalFemelle_nombre_m2 &lt;dbl&gt;, FemelleFruit_nombre_m2 &lt;dbl&gt;, FemelleAvorte_nombre_m2 &lt;dbl&gt;, ## # SterileFleur_nombre_m2 &lt;dbl&gt;, C_pourc &lt;dbl&gt;, N_pourc &lt;dbl&gt;, P_pourc &lt;dbl&gt;, K_pourc &lt;dbl&gt;, Ca_pourc &lt;dbl&gt;, Mg_pourc &lt;dbl&gt;, ## # S_pourc &lt;dbl&gt;, B_pourc &lt;dbl&gt;, Cu_pourc &lt;dbl&gt;, Zn_pourc &lt;dbl&gt;, Mn_pourc &lt;dbl&gt;, Fe_pourc &lt;dbl&gt;, Al_pourc &lt;dbl&gt;, ## # Tourbiere &lt;chr&gt;, Type &lt;chr&gt;, TypeCulture &lt;chr&gt; D‚Äôautres types de jointures sont possibles, et d√©crites en d√©tails dans la documentation. Garrick Aden-Buie a pr√©par√© de jolies animations pour d√©crire les diff√©rents types de jointures. left_join(x, y) colle y √† x seulement ce qui dans y correspond √† ce que l‚Äôon trouve dans x. right_join(x, y) colle y √† x seulement ce qui dans x correspond √† ce que l‚Äôon trouve dans y. inner_join(x, y) colle x et y en excluant les lignes o√π au moins une variable de joint est absente dans x et y. full_join(x, y)garde toutes les lignes et les colonnes de x et y. 3.5.5 Op√©rations sur les tableaux Les tableaux peuvent √™tre segment√©s en √©l√©ments sur lesquels on calculera ce qui nous chante. On pourrait vouloir obtenir : la somme avec la function sum() la moyenne avec la function mean() ou la m√©diane avec la fonction median() l‚Äô√©cart-type avec la function sd() les maximum et minimum avec les fonctions min() et max() un d√©compte d‚Äôoccurrence avec la fonction n() ou count() Par exemple, mean(chicoute$Rendement_g_5m2, na.rm = TRUE) ## [1] 13.32851 En mode classique, pour effectuer des op√©rations sur des tableaux, on utilisera la fonction apply(). Cette fonction prend, comme arguments, le tableau, l‚Äôaxe (op√©ration par ligne = 1, op√©ration par colonne = 2), puis la fonction √† appliquer. apply(chicoute %&gt;% select(contains(&quot;pourc&quot;)), 2, mean) ## C_pourc N_pourc P_pourc K_pourc Ca_pourc Mg_pourc S_pourc B_pourc Cu_pourc ## 5.027911e+01 2.199411e+00 1.388959e-01 8.887000e-01 3.884391e-01 4.980142e-01 1.347177e-01 3.090922e-03 4.089891e-04 ## Zn_pourc Mn_pourc Fe_pourc Al_pourc ## 6.662155e-03 3.345239e-02 1.514885e-02 2.694979e-03 Les op√©rations peuvent aussi √™tre effectu√©es par ligne, par exemple une somme (je garde seulement les 10 premiers r√©sultats). apply(chicoute %&gt;% select(contains(&quot;pourc&quot;)), 1, sum)[1:10] ## [1] 55.64299 55.76767 54.78856 55.84453 57.89671 55.53603 55.62526 55.10991 55.06295 55.16774 La fonction √† appliquer peut √™tre personnalis√©e, par exemple: apply( chicoute %&gt;% select(contains(&quot;pourc&quot;)), 2, function(x) (prod(x))^(1 / length(x)) ) ## C_pourc N_pourc P_pourc K_pourc Ca_pourc Mg_pourc S_pourc B_pourc Cu_pourc ## 50.253429104 2.165246915 0.133754530 0.846193827 0.376192724 0.491763884 0.129900753 0.003014675 0.000000000 ## Zn_pourc Mn_pourc Fe_pourc Al_pourc ## 0.006408775 0.024140327 0.014351745 0.002450982 Vous reconnaissez cette fonction? C‚Äô√©tait la moyenne g√©om√©trique (la fonction prod() √©tant le produit d‚Äôun vecteur). En mode tidyverse, on aura besoin principalement des fonction suivantes: group_by() pour effectuer des op√©rations par groupe, l‚Äôop√©ration group_by() s√©pare le tableau en plusieurs petits tableaux, en attendant de les recombiner. C‚Äôest un peu l‚Äô√©quivalent des facettes avec le module de visualisation ggplot2, que nous explorons au chapitre 4. summarise() pour r√©duire plusieurs valeurs en une seule, il applique un calcul sur le tableau ou s‚Äôil y a lieu sur chaque petit tableau segment√©. Il en existe quelques variantes. summarise_all() applique la fonction √† toutes les colonnes summarise_at() applique la fonction aux colonnes sp√©cifi√©es summarise_if() applique la fonction aux colonnes qui ressortent comme TRUE selon une op√©ration bool√©enne mutate() pour ajouter une nouvelle colonne Si l‚Äôon d√©sire ajouter une colonne √† un tableau, par exemple le sommaire calcul√© avec summarise(). √Ä l‚Äôinverse, la fonction transmute() retournera seulement le r√©sultat, sans le tableau √† partir duquel il a √©t√© calcul√©. De m√™me que summarise(), mutate() et transmute() poss√®dent leurs √©quivalents _all(), _at() et _if(). arrange() pour r√©ordonner le tableau On a d√©j√† couvert arrange() dans le chapitre 3. Rappelons que cette fonction n‚Äôest pas une op√©ration sur un tableau, mais plut√¥t un changement d‚Äôaffichage en changeant l‚Äôordre d‚Äôapparition des donn√©es. Ces op√©rations sont d√©crites dans l‚Äôaide-m√©moire Data Transformation Cheat Sheet (figure 3.5)). Figure 3.5: Aide-m√©moire pour la transformation des donn√©es, https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf Pour effectuer des statistiques par colonne, on utilisera summarise pour des statistiques effectu√©es sur une seule colonne. summarise peut prendre le nombre d√©sir√© de statistiques dont la sortie est un scalaire. chicoute %&gt;% summarise( moyenne = mean(TotalFloral_nombre_m2, na.rm = TRUE), ecart_type = sd(TotalFloral_nombre_m2, na.rm = TRUE) ) ## # A tibble: 1 x 2 ## moyenne ecart_type ## &lt;dbl&gt; &lt;dbl&gt; ## 1 52.1 40.4 Si l‚Äôon d√©sire un sommaire sur toutes les variables s√©lectionn√©es, on utilisera summarise_all(). Pour sp√©cifier que l‚Äôon d√©sire la moyenne et l‚Äô√©cart-type on inscrit les noms des fonctions dans list(). chicoute %&gt;% select(contains(&quot;pourc&quot;)) %&gt;% summarise_all(list(mean, sd)) ## # A tibble: 1 x 26 ## C_pourc_fn1 N_pourc_fn1 P_pourc_fn1 K_pourc_fn1 Ca_pourc_fn1 Mg_pourc_fn1 S_pourc_fn1 B_pourc_fn1 Cu_pourc_fn1 Zn_pourc_fn1 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 50.3 2.20 0.139 0.889 0.388 0.498 0.135 0.00309 0.000409 0.00666 ## # ‚Ä¶ with 16 more variables: Mn_pourc_fn1 &lt;dbl&gt;, Fe_pourc_fn1 &lt;dbl&gt;, Al_pourc_fn1 &lt;dbl&gt;, C_pourc_fn2 &lt;dbl&gt;, N_pourc_fn2 &lt;dbl&gt;, ## # P_pourc_fn2 &lt;dbl&gt;, K_pourc_fn2 &lt;dbl&gt;, Ca_pourc_fn2 &lt;dbl&gt;, Mg_pourc_fn2 &lt;dbl&gt;, S_pourc_fn2 &lt;dbl&gt;, B_pourc_fn2 &lt;dbl&gt;, ## # Cu_pourc_fn2 &lt;dbl&gt;, Zn_pourc_fn2 &lt;dbl&gt;, Mn_pourc_fn2 &lt;dbl&gt;, Fe_pourc_fn2 &lt;dbl&gt;, Al_pourc_fn2 &lt;dbl&gt; On utilisera group_by() pour segmenter le tableau, et ainsi obtenir des statistiques pour chaque groupe. chicoute %&gt;% group_by(CodeTourbiere) %&gt;% summarise( moyenne = mean(TotalFloral_nombre_m2, na.rm = TRUE), ecart_type = sd(TotalFloral_nombre_m2, na.rm = TRUE) ) ## # A tibble: 12 x 3 ## CodeTourbiere moyenne ecart_type ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 72.1 32.7 ## 2 2 37.1 32.9 ## 3 BEAU 149. 53.2 ## 4 BP 60.4 30.6 ## 5 BS2 27.2 15.5 ## 6 MB 64.7 40.8 ## 7 MR 35.1 10.5 ## 8 NBM 35.1 16.6 ## 9 NESP 21.4 4.88 ## 10 NTP 47.6 15.9 ## 11 SSP 25.7 11.1 ## 12 WTP 50.2 28.3 Dans le cas de summarise_all, les r√©sultats s‚Äôaffichent de la m√™me mani√®re. chicoute %&gt;% group_by(CodeTourbiere) %&gt;% select(N_pourc, P_pourc, K_pourc) %&gt;% summarise_all(list(mean, sd)) ## Adding missing grouping variables: `CodeTourbiere` ## # A tibble: 12 x 7 ## CodeTourbiere N_pourc_fn1 P_pourc_fn1 K_pourc_fn1 N_pourc_fn2 P_pourc_fn2 K_pourc_fn2 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2.26 0.156 0.880 0.250 0.0193 0.201 ## 2 2 2.76 0.181 1.12 0.178 0.0283 0.179 ## 3 BEAU 2.00 0.0967 1.12 0.179 0.0172 0.120 ## 4 BP 2.05 0.158 0.747 0.161 0.00625 0.0652 ## 5 BS2 2.08 0.103 1.12 0.420 0.0218 0.333 ## 6 MB 2.15 0.109 0.675 0.114 0.0165 0.183 ## 7 MR 1.99 0.127 0.830 0.0802 0.0131 0.142 ## 8 NBM 2.01 0.127 0.854 0.310 0.0202 0.177 ## 9 NESP 1.76 0.135 0.945 0.149 0.0108 0.152 ## 10 NTP 1.83 0.0873 0.402 0.166 0.0103 0.0404 ## 11 SSP 1.83 0.130 0.700 0.160 0.00383 0.0487 ## 12 WTP 1.79 0.0811 0.578 0.132 0.00587 0.102 Pour obtenir des statistiques √† chaque ligne, mieux vaut utiliser apply(), tel que vu pr√©c√©demment. Le point, ., repr√©sente le tableau dans une fonction qui n‚Äôa pas √©t√© con√ßu pour fonctionner de facto avec dplyr. chicoute %&gt;% select(contains(&quot;pourc&quot;)) %&gt;% apply(., 1, sum) ## [1] 55.64299 55.76767 54.78856 55.84453 57.89671 55.53603 55.62526 55.10991 55.06295 55.16774 56.41123 55.47917 55.43537 ## [14] 55.79175 55.44561 54.85448 54.34262 55.03075 54.40533 51.89319 54.70172 54.62176 54.30250 53.86976 53.44731 53.86244 ## [27] 52.43280 54.34978 53.96756 51.46672 55.44267 54.70350 55.30711 56.16200 56.64710 55.95499 54.76370 54.32775 54.95419 ## [40] 53.37094 53.07855 53.04541 52.09520 52.40456 51.92376 53.33248 56.56405 56.35004 56.27185 55.56986 53.81654 55.39638 ## [53] 55.51961 54.88098 54.74774 51.08921 51.31462 53.46819 53.15640 52.82020 57.78038 57.94636 56.65558 56.28845 55.54463 ## [66] 56.51751 55.36497 56.00594 55.64247 56.56967 56.81674 55.87070 55.72308 56.14116 56.42611 55.35650 54.90469 54.03674 ## [79] 53.42991 53.99334 53.09085 53.23222 53.28212 53.63192 53.48102 52.31131 51.72026 51.10534 51.49055 51.59297 Prenons ce tableau des esp√®ces menac√©es issu de l‚ÄôUnion internationale pour la conservation de la nature distribu√©es par l‚ÄôOCDE. library(&quot;tidyverse&quot;) especes_menacees &lt;- read_csv(&quot;data/WILD_LIFE_14012020030114795.csv&quot;) ## Parsed with column specification: ## cols( ## IUCN = col_character(), ## `IUCN Category` = col_character(), ## SPEC = col_character(), ## Species = col_character(), ## COU = col_character(), ## Country = col_character(), ## `Unit Code` = col_character(), ## Unit = col_character(), ## `PowerCode Code` = col_double(), ## PowerCode = col_character(), ## `Reference Period Code` = col_logical(), ## `Reference Period` = col_logical(), ## Value = col_double(), ## `Flag Codes` = col_logical(), ## Flags = col_logical() ## ) Nous ex√©cutons le pipeline suivant. especes_menacees %&gt;% dplyr::filter(IUCN == &quot;CRITICAL&quot;, SPEC == &quot;VASCULAR_PLANT&quot;) %&gt;% dplyr::select(Country, Value) %&gt;% dplyr::group_by(Country) %&gt;% dplyr::summarise(n_critical_plants = sum(Value)) %&gt;% dplyr::arrange(desc(n_critical_plants)) %&gt;% dplyr::top_n(10) ## Selecting by n_critical_plants ## # A tibble: 10 x 2 ## Country n_critical_plants ## &lt;chr&gt; &lt;dbl&gt; ## 1 United States 1222 ## 2 Japan 525 ## 3 Canada 315 ## 4 Czech Republic 284 ## 5 Spain 271 ## 6 Belgium 253 ## 7 Austria 172 ## 8 Slovak Republic 155 ## 9 Australia 148 ## 10 Italy 128 Ce pipeline consiste √†: prendre le tableau especes_menacees, puis filtrer pour n&#39;obtenir que les esp√®ces critiques dans la cat√©gorie des plantes vascularies, puis s√©lectionner les colonnes des pays et des valeurs (nombre d&#39;esp√®ces), puis segmenter le tableaux en plusieurs tableaux selon le pays, puis appliquer la fonction sum pour chacun de ces petits tableaux (puis de recombiner ces sommaires), puis trier les pays en nombre d√©croissant de d√©compte d&#39;esp√®ces, puis afficher le top 10 3.5.6 Exemple (difficile) Pour revenir √† notre tableau chicoute, imaginez que vous aviez une station m√©t√©o (station_A) situ√©e aux coordonn√©es (490640, 5702453) et que vous d√©siriez calculer la distance entre l‚Äôobservation et la station. Prenez du temps pour r√©fl√©chir √† la mani√®re dont vous proc√©derez‚Ä¶ On pourra cr√©er une fonction qui mesure la distance entre un point x, y et les coordonn√©es de la station A‚Ä¶ dist_station_A &lt;- function(x, y) { return(sqrt((x - 490640)^2 + (y - 5702453)^2)) } ‚Ä¶ puis ajouter une colonne avec mutate gr√¢ce √† une fonction prenant les arguments x et y sp√©cifi√©s. chicoute %&gt;% mutate(dist = dist_station_A(x = Longitude_m, y = Latitude_m)) %&gt;% select(ID, CodeTourbiere, Longitude_m, Latitude_m, dist) %&gt;% top_n(10) ## Selecting by dist ## # A tibble: 10 x 5 ## ID CodeTourbiere Longitude_m Latitude_m dist ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7 BP 484054 5706307 7631. ## 2 36 MR 459875 5701988 30769. ## 3 37 MR 459873 5701987 30771. ## 4 38 MR 459880 5701971 30764. ## 5 39 MR 459894 5701966 30750. ## 6 40 MR 459915 5701994 30728. ## 7 46 NBM 485975 5695688 8218. ## 8 48 NBM 485912 5696607 7519. ## 9 49 NBM 485903 5696611 7521. ## 10 50 NBM 485884 5696612 7532. Nous pourrions proc√©der de la m√™me mani√®re pour fusionner des donn√©es climatiques. Le tableau chicoute ne poss√®de pas d‚Äôindicateurs climatiques, mais il est possible de les soutirer de stations m√©t√©o plac√©es pr√®s des sites. Ces donn√©es ne sont pas disponibles pour le tableau de la chicout√©, alors j‚Äôutiliserai des donn√©es fictives pour l‚Äôexemple. Voici ce qui pourrait √™tre fait. Cr√©er un tableau des stations m√©t√©o ainsi que des indices m√©t√©orologiques associ√©s √† ces stations. Lier chaque site √† une station (√† la main o√π selon la plus petite distance entre le site et la station). Fusionner les indices climatiques aux sites, puis les sites aux mesures de rendement. Ces op√©rations demandent habituellement du t√¢tonnement. Il serait surprenant que m√™me une personne exp√©riment√©e soit en mesure de compiler ces op√©rations sans obtenir de message d‚Äôerreur, et retravailler jusqu‚Äô√† obtenir le r√©sultat souhait√©. L‚Äôobjectif de cette section est de vous pr√©sent√© un flux de travail que vous pourriez √™tre amen√©s √† effectuer et de fournir quelques √©l√©ments nouveaux pour mener √† bien une op√©ration. Il peut √™tre frustrant de ne pas saisir toutes les op√©rations: passez √† travers cette section sans jugement. Si vous devez vous frotter √† probl√®me semblable, vous saurez que vous trouverez dans ce manuel une recette int√©ressante. mes_stations &lt;- data.frame( Station = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), Longitude_m = c(490640, 484870, 485929), Latitude_m = c(5702453, 5701870, 5696421), t_moy_C = c(13.8, 18.2, 16.30), prec_tot_mm = c(687, 714, 732) ) mes_stations ## Station Longitude_m Latitude_m t_moy_C prec_tot_mm ## 1 A 490640 5702453 13.8 687 ## 2 B 484870 5701870 18.2 714 ## 3 C 485929 5696421 16.3 732 La fonction suivante calcule la distance entre des coordonn√©es x et y et chaque station d‚Äôun tableau de stations, puis retourne le nom de la station dont la distance est la moindre. dist_station &lt;- function(x, y, stations_df) { # stations est le tableau des stations √† trois colonnes # 1iere: nom de la station # 2ieme: longitude # 3ieme: latitude distance &lt;- c() for (i in 1:nrow(stations_df)) { distance[i] &lt;- sqrt((x - stations_df[i, 2])^2 + (y - stations_df[i, 3])^2) } nom_station &lt;- as.character(stations_df$Station[which.min(distance)]) return(nom_station) } Testons la fonction avec des coordonn√©es. dist_station(x = 459875, y = 5701988, stations_df = mes_stations) ## [1] &quot;B&quot; Nous appliquons cette fonction √† toutes les lignes du tableau, puis en retournons un √©chantillon. chicoute %&gt;% rowwise() %&gt;% mutate(Station = dist_station(x = Longitude_m, y = Latitude_m, stations_df = mes_stations)) %&gt;% select(ID, CodeTourbiere, Longitude_m, Latitude_m, Station) %&gt;% sample_n(10) ## Source: local data frame [10 x 5] ## Groups: &lt;by row&gt; ## ## # A tibble: 10 x 5 ## ID CodeTourbiere Longitude_m Latitude_m Station ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 61 BS2 486575 5702188 B ## 2 57 SSP 484475 5699785 B ## 3 46 NBM 485975 5695688 C ## 4 70 1 486488 5702120 B ## 5 67 1 486544 5702078 B ## 6 59 SSP 484454 5699796 B ## 7 9 BP 484761 5706324 B ## 8 42 NESP 484870 5701876 B ## 9 6 BP 484865 5706394 B ## 10 31 MB 491931 5699342 A Cela semble fonctionner. On peut y ajouter un left_join() pour joindre les donn√©es m√©t√©o au tableau principal. chicoute_weather &lt;- chicoute %&gt;% rowwise() %&gt;% mutate(Station = dist_station(x = Longitude_m, y = Latitude_m, stations_df = mes_stations)) %&gt;% left_join(y = mes_stations, by = &quot;Station&quot;) ## Warning: Column `Station` joining character vector and factor, coercing into character vector chicoute_weather %&gt;% sample_n(10) ## Source: local data frame [10 x 36] ## Groups: &lt;by row&gt; ## ## # A tibble: 10 x 36 ## ID CodeTourbiere Ordre Site Traitement DemiParcelle SousTraitement Latitude_m.x Longitude_m.x Rendement_g_5m2 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 16 2 7 13 fertilisa‚Ä¶ right B 5702643 486498 0 ## 2 84 1 5 10 temoin left B 5702160 486514 5.19 ## 3 10 BP H 5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 5706364 484780 NA ## 4 44 NESP J 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 5701860 484864 NA ## 5 2 BEAU A 2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 5702452 490634 NA ## 6 77 1 3 6 temoin right Control 5702112 486465 2.91 ## 7 46 NBM D 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 5695688 485975 NA ## 8 17 2 7 14 temoin left Cu 5702627 486501 0 ## 9 88 WTP E 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 5700775 487060 NA ## 10 66 1 1 1 fertilisa‚Ä¶ left B 5702078 486544 64.0 ## # ‚Ä¶ with 26 more variables: TotalRamet_nombre_m2 &lt;dbl&gt;, TotalVegetatif_nombre_m2 &lt;dbl&gt;, TotalFloral_nombre_m2 &lt;dbl&gt;, ## # TotalMale_nombre_m2 &lt;dbl&gt;, TotalFemelle_nombre_m2 &lt;dbl&gt;, FemelleFruit_nombre_m2 &lt;dbl&gt;, FemelleAvorte_nombre_m2 &lt;dbl&gt;, ## # SterileFleur_nombre_m2 &lt;dbl&gt;, C_pourc &lt;dbl&gt;, N_pourc &lt;dbl&gt;, P_pourc &lt;dbl&gt;, K_pourc &lt;dbl&gt;, Ca_pourc &lt;dbl&gt;, Mg_pourc &lt;dbl&gt;, ## # S_pourc &lt;dbl&gt;, B_pourc &lt;dbl&gt;, Cu_pourc &lt;dbl&gt;, Zn_pourc &lt;dbl&gt;, Mn_pourc &lt;dbl&gt;, Fe_pourc &lt;dbl&gt;, Al_pourc &lt;dbl&gt;, ## # Station &lt;chr&gt;, Longitude_m.y &lt;dbl&gt;, Latitude_m.y &lt;dbl&gt;, t_moy_C &lt;dbl&gt;, prec_tot_mm &lt;dbl&gt; 3.5.7 Exporter un tableau Simplement avec write_csv(). write_csv(chicoute_weather, &quot;data/chicoute_weather.csv&quot;) 3.5.8 Aller plus loin dans le tidyverse Le livre R for Data Science, de Hadley Wickham et Garrett Grolemund (couverture √† la figure 3.6), est un incontournable. Figure 3.6: Couverture du libre de Hadley Wickham et Garrett Grolemund, Source: https://r4ds.had.co.nz 3.6 R√©f√©rences Parent L.E., Parent, S.√â., Herbert-Gentile, V., Naess, K. et Lapointe, L. 2013. Mineral Balance Plasticity of Cloudberry (Rubus chamaemorus) in Quebec-Labrador Bogs. American Journal of Plant Sciences, 4, 1508-1520. DOI: 10.4236/ajps.2013.47183 "],
["chapitre-visualisation.html", "4 Visualisation 4.1 Pourquoi explorer graphiquement? 4.2 Publier un graphique 4.3 Choisir le type de graphique le plus appropri√© 4.4 Choisir son outil de visualisation 4.5 Visualisation en R 4.6 Module de base pour les graphiques 4.7 La grammaire graphique ggplot2 4.8 Mon premier ggplot 4.9 Les graphiques comme outil d‚Äôexploration des donn√©es 4.10 Extra: R√®gles particuli√®res", " 4 Visualisation Ô∏è¬†Objectifs sp√©cifiques: √Ä la fin de ce chapitre, vous comprendrez l‚Äôimportance de l‚Äôexploration des donn√©es comprendrez les guides g√©n√©raux pour cr√©er un graphique appropri√© comprendrez la diff√©rence entre les modes imp√©ratifs et d√©claratifs pour la cr√©ation de graphique serez en mesure de cr√©er des nuages de points, lignes, histogrammes, diagrammes en barres et boxplots en R saurez exporter un graphique en vue d‚Äôune publication Reconnaissez-vous cette image (figure 4.1)? Figure 4.1: Le b√¢ton de hocket de Mann et al. (2001) Source: GIEC, Bilan 2001 des changements climatiques : Les √©l√©ments scientifiques) Elle a √©t√© con√ßue par Michael E. Mann, Raymond S. Bradley et Malcolm K. Hughes. Le graphique montre l‚Äô√©volution des temp√©ratures en ¬∞C normalis√©es selon la temp√©rature moyenne entre 1961 et 1990 sur l‚Äôaxe des Y en fonction du temps, sur l‚Äôaxe des X. On le connait aujourd‚Äôhui comme le b√¢ton de hockey, et on reconnait son r√¥le cl√© pour sensibiliser la civilisation enti√®re face au r√©chauffement global. Cr√©er des graphiques est une t√¢che courante dans un flux de travail en science. Un graphique bien con√ßu est dense en information. La visualisation des donn√©es permet d‚Äôexplorer des tableaux jusqu‚Äô√† cr√©er des √©l√©ments visuels vou√©s √† la publication, dont l‚Äôinformation serait autrement difficile, voire impossible √† transmettre ad√©quatement. 4.1 Pourquoi explorer graphiquement? La plupart des graphiques que vous g√©n√©rerez ne seront pas destin√©s √† √™tre publi√©s. Ils viseront probablement d‚Äôabord √† explorer des donn√©es. Cela vous permettra de mettre en √©vidence de nouvelles perspectives. Prenons par exemple deux variables, \\(X\\) et \\(Y\\). Vous calculez leur moyenne, √©cart-type et la corr√©lation entre les deux variables (nous verrons les statistiques en plus de d√©tails dans un prochain chapitre). library(&quot;tidyverse&quot;) datasaurus &lt;- read_tsv(&quot;data/DatasaurusDozen.tsv&quot;) cor_datasaurus &lt;- datasaurus %&gt;% group_by(dataset) %&gt;% summarise(cor = cor(x = x, y = y, method = &quot;pearson&quot;)) datasaurus %&gt;% group_by(dataset) %&gt;% summarise_all(list(mean = mean, sd = sd)) %&gt;% left_join(cor_datasaurus, by = &quot;dataset&quot;) ## # A tibble: 13 x 6 ## dataset x_mean y_mean x_sd y_sd cor ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 away 54.3 47.8 16.8 26.9 -0.0641 ## 2 bullseye 54.3 47.8 16.8 26.9 -0.0686 ## 3 circle 54.3 47.8 16.8 26.9 -0.0683 ## 4 dino 54.3 47.8 16.8 26.9 -0.0645 ## 5 dots 54.3 47.8 16.8 26.9 -0.0603 ## 6 h_lines 54.3 47.8 16.8 26.9 -0.0617 ## 7 high_lines 54.3 47.8 16.8 26.9 -0.0685 ## 8 slant_down 54.3 47.8 16.8 26.9 -0.0690 ## 9 slant_up 54.3 47.8 16.8 26.9 -0.0686 ## 10 star 54.3 47.8 16.8 26.9 -0.0630 ## 11 v_lines 54.3 47.8 16.8 26.9 -0.0694 ## 12 wide_lines 54.3 47.8 16.8 26.9 -0.0666 ## 13 x_shape 54.3 47.8 16.8 26.9 -0.0656 Les moyennes, √©carts-types et corr√©lations sont √† peu pr√®s les m√™mes pour tous les groupes. Peut-on conclure que tous les groupes sont semblables? Pas encore. Pour d√©montrer que ces statistiques ne vous apprendront pas grand chose sur la structure des donn√©es, Matejka et Fitzmaurice (2017) ont g√©n√©r√© 12 jeux de donn√©es \\(X\\) et \\(Y\\), ayant chacun pratiquement les m√™mes statistiques. Mais avec des structures bien diff√©rentes (figure 4.2)! Figure 4.2: Animation montrant la progression du jeu de donn√©es Datasaurus pour toutes les formes vis√©es. Source: Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing 4.2 Publier un graphique Vous voil√† sensibilis√© √† l‚Äôimportance d‚Äôexplorer les donn√©es graphiquement. Mais ce qui ultimement √©manera d‚Äôun projet sera le rapport que vous d√©poserez, l‚Äôarticle scientifique que vous ferez publier ou le billet de blogue que vous partagerez su les r√©seaux sociaux. Les graphiques inclus dans vos publications m√©ritent une attention particuli√®re pour que votre audience puisse comprendre les d√©couvertes et perspectives offertes par vos travaux. Pour ce faire, un graphique doit r√©pondre honn√™tement √† la question pos√©e tout en √©tant attrayant. 4.2.1 Cinq qualit√©s d‚Äôun bon graphique Alberto Cairo, chercheur sp√©cialis√© en visualisation de donn√©es, a fait para√Ætre en 2016 le livre The Truthful art, note cinq qualit√©s d‚Äôune visualisation bien con√ßue (les citations de cette section proviennent de ma traduction de Alberto Cairo, The Truthful Art (2016), p.¬†45.). 1- Elle est v√©ritable, puisqu‚Äôelle est bas√©e sur une recherche exhaustive et honn√™te. Cela vaut autant pour les graphiques que pour l‚Äôanalyse de donn√©es. Il s‚Äôagit froidement de pr√©senter les donn√©es selon l‚Äôinterpr√©tation la plus exacte. Les pi√®ges √† √©viter sont le picorage de cerises et la surinterpr√©tation des donn√©es. Le picorage, c‚Äôest lorsqu‚Äôon r√©duit les perspectives afin de soutenir un argumentaire. Par exemple, retirer des donn√©es d‚Äôune r√©gion ou d‚Äôune d√©cennie qui rendraient factice une conclusion fix√©e a priori. Ceci vaut autant pour les graphiques que pour les statistiques (nous parlerons du p-hacking au prochain chapitre). La surinterpr√©tation, c‚Äôest lorsque l‚Äôon saute rapidement aux conclusions: par exemple, que l‚Äôon g√©n√®re des corr√©lations, voire m√™me des relations de causalit√©s √† partir de ce qui n‚Äôest que du bruit de fond. √Ä ce titre, lors d‚Äôune conf√©rence, Heather Krause insiste sur l‚Äôimportance de faire en sorte que les repr√©sentations graphiques r√©pondent correctement aux questions pos√©es dans une √©tude (figure 4.3). Figure 4.3: The F word: Protect your work from four hidden fallacies when working with data, une conf√©rence de Heather Krause, 2018 2- Elle est fonctionnelle, puisqu‚Äôelle constitue une repr√©sentation pr√©cise des donn√©es, et qu‚Äôelle est construite de mani√®re √† laisser les observateurs.trices prendre des initiatives cons√©quentes. ‚ÄúLa seule chose qui est pire qu‚Äôun diagramme en pointe de tarte, c‚Äôest d‚Äôen pr√©senter plusieurs‚Äù (Edward Tufte, designer, cit√© par Alberto Cairo, 2016, p.¬†50). Choisir le bon graphique pour repr√©senter vos donn√©es est beaucoup moins une question de bon go√ªt qu‚Äôune question de d√©marche rationnelle sur l‚Äôobjectif vis√© par la pr√©sentation d‚Äôun graphique. Je pr√©senterai des lignes guides pour s√©lectionner le type de graphique qui pr√©sentera vos donn√©es de mani√®re fonctionnelle en fonction de l‚Äôobjectif d‚Äôun graphique (d‚Äôailleurs, avez-vous vraiment besoin d‚Äôun graphique?). 3- Elle est attrayante et intrigante, et m√™me esth√©tiquement plaisante pour l‚Äôaudience vis√©e - les scientifiques d‚Äôabord, mais aussi le public en g√©n√©ral. En sciences naturelles, la pens√©e rationnelle, la capacit√© √† organiser la connaissance et cr√©er de nouvelles avenues sont des qualit√©s qui sont privil√©gi√©es au talent artistique. Que vous ayez o√π non des aptitudes en art visuel, pr√©sentez de l‚Äôinformation, pas des d√©corations. Excel vous permet d‚Äôajouter une perspective 3D √† un diagramme en barres. La profondeur contient-elle de l‚Äôinformation? Non. Cette d√©coration ne fait qu‚Äôajouter de la confusion. Minimalisez, fournissez le plus d‚Äôinformation possible avec le moins d‚Äô√©l√©ments graphiques possibles. C‚Äôest ce que vous proposent les guides graphiques que j‚Äôintroduirai plus loin. 4- Elle est pertinente, puisqu‚Äôelle r√©v√®le des √©vidences scientifiques autrement difficilement accessibles. Il s‚Äôagit de susciter un eur√™ka, dans le sens qu‚Äôelle g√©n√®re une id√©e, et parfois une initiative, en un coup d‚Äô≈ìil. Le graphique en b√¢ton de hockey est un exemple o√π l‚Äôon a spontan√©ment une id√©e de la situation. Cette situation peut √™tre la pr√©sence d‚Äôun ph√©nom√®ne comme l‚Äôaugmentation de la temp√©rature globale, mais aussi l‚Äôabsence de ph√©nom√®nes pourtant attendus. 5- Elle est instructive, parce que si l‚Äôon saisit et accepte les √©vidences scientifiques qu‚Äôelle d√©crit, cela changera notre perception pour le mieux. En pr√©sentant cette qualit√©, Alberto Cairo voulait insister ses lecteurs.trices √† choisir des sujets de discussion visuelle de mani√®re √† participer √† un monde meilleur. En ce qui nous concerne, il s‚Äôagit de bien s√©lectionner l‚Äôinformation que l‚Äôon d√©sire transmettre. Imaginez que vous avez travaill√© quelques jours pour cr√©er un graphique, sont vous √™tes fier, mais vous (ou un coll√®gue hi√©rarchiquement favoris√©) vous rendez compte que le graphique soutient peu ou pas le propos ou l‚Äôobjectif de votre th√®se/m√©moire/rapport/article. Si c‚Äôest bien le cas, vous feriez mieux de laisser tomber votre oeuvre et consid√©rer votre d√©marche comme une occasion d‚Äôapprentissage. Alberto Cairo r√©sume son livre The Truthful Art dans une entrevue avec le National Geographic. 4.3 Choisir le type de graphique le plus appropri√© De nombreuses mani√®res de pr√©senter les donn√©es sont courrament utilis√©es, comme les nuages de point, les lignes, les histogrammes, les diagrammes en barre et en pointe de tarte. Les principaux types de graphique seront couverts dans ce chapitre. D‚Äôautres types sp√©cialis√©s seront couverts dans les chapitres appropri√©s (graphiques davantage orient√©s vers les statistiques, les biplots, les dendrogrammes, les diagrammes ternaires, les cartes, etc.). La visualisation de donn√©es est aujourd‚Äôhui devenue un m√©tier pour plusieurs personnes ayant des affinit√©s pour la science, les arts et la communication, dont certaines partagent leur expertise sur le web. √Ä ce titre, le site from data to viz est √† conserver dans vos marques-page. Il comprend des arbres d√©cisionnels qui vous guident vers les options appropri√©es pour pr√©senter vos donn√©es, puis fournissent des exemples pour produire ces visualisations en R. √âgalement, je sugg√®re le site internet de Ann K. Emery, qui pr√©sente des lignes guide pour pr√©senter le graphique ad√©quat selon les donn√©es en main. De nombreuses recettes sont √©galement propos√©es sur r-graph-gallery.com. En ce qui a trait aux couleurs, le choix n‚Äôest pas anodin. Si vous avez le souci des d√©tails sur les √©l√©ments esth√©tiques de vos graphiques, je recommande la lecture de ce billet de blog de Lisa Charlotte Rost. Retenez n√©anmois que La couleur est une information. Les couleurs devraient √™tre s√©lectionn√©es d‚Äôabord pour √™tre lisibles par les personnes ne percevant pas les couleurs (figure 4.4), selon le support (apte √† √™tre photocopi√©, lisible √† l‚Äô√©cran, lisible sur des documents imprim√©s en noir et blanc) et selon le type de donn√©es. Donn√©es continues ou cat√©gorielles ordinales: gradient (transition graduelle d‚Äôune couleur √† l‚Äôautre), s√©quence (transition saccad√©e selon des groupes de donn√©es continues) ou divergentes (transition saccad√©e d‚Äôune couleur √† l‚Äôautre vers des couleurs divergentes, par exemple orange vers blanc vers bleu). Donn√©es cat√©gorielles nominales: couleurs √©loign√©es d‚Äôune cat√©gorie √† une autre (plus il y a de cat√©gories, plus les couleurs sont susceptibles de se ressembler). Figure 4.4: Capture d‚Äô√©cran de colorbrewer2.org, qui propose des palettes de couleurs pour cr√©er des cartes, mais l‚Äôinformation est pertinente pour tout type de graphique. Le Financial Times offre √©galement ce guide visuel (figure 4.5). Figure 4.5: Guide de s√©lection de grapjhique du Financial Times Cairo (2016) propose de proc√©der avec ces √©tapes: R√©fl√©chissez au message que vous d√©sirez transmettre: comparer les cat√©gories \\(A\\) et \\(B\\), visualiser une transition ou un changement de \\(A\\) vers \\(B\\), pr√©senter une relation entre \\(A\\) et \\(B\\) ou la distribution de \\(A\\) et \\(B\\) sur une carte. Essayez diff√©rentes repr√©sentations: si le message que vous d√©sirez transmettre a plusieurs volets, il se pourrait que vous ayez besoin de plus d‚Äôun graphique. Mettez de l‚Äôordre dans vos donn√©es. C‚Äô√©tait le sujet du chapitre 3. Testez le r√©sultat. ‚ÄúH√©, qu‚Äôest-ce que tu comprends de cela?‚Äù Si la personne hausse les √©paules, il va falloir r√©√©valuer votre strat√©gie. 4.4 Choisir son outil de visualisation Les modules et logiciels de visualisation sont bas√©s sur des approches que l‚Äôon pourrait placer sur un spectre allant de l‚Äôimp√©ratif au d√©claratif. 4.4.1 Approche imp√©rative Selon cette approche, vous indiquez comment placer l‚Äôinformation dans un espace graphique. Vous indiquer les symboles, les couleurs, les types de ligne, etc. Peu de choses sont automatis√©es, ce qui laisse une grande flexibilit√©, mais demande de vouer beaucoup d‚Äô√©nergie √† la mani√®re de coder pour obtenir le graphique d√©sir√©. Le module graphique de Excel, ainsi que le module graphique de base de R, utilisent des approches imp√©ratives. 4.4.2 Approche d√©clarative Les strat√©gies d‚Äôautomatisation graphique se sont grandement am√©lior√©es au cours des derni√®res ann√©es. Plut√¥t que de vouer vos √©nergies √† cr√©er un graphique, il est maintenant possible de sp√©cifier ce que l‚Äôon veut pr√©senter. La visualisation d√©clarative vous permet de penser aux donn√©es et √† leurs relations, plut√¥t que des d√©tails accessoires. Jake Vanderplas, Declarative Statistical Visualization in Python with Altair (ma traduction) L‚Äôapproche d√©clarative passe souvent par une grammaire graphique, c‚Äôest-√†-dire un langage qui explique ce que l‚Äôon veut pr√©senter - en mode imp√©ratif, on sp√©cifie plut√¥t comment on veut pr√©senter les donn√©es. Le module ggplot2 est le module d√©claratif par excellence en R. 4.5 Visualisation en R En R, votre trousse d‚Äôoutils de visualisation m√©riterait de comprendre les modules suivants. base. Le module de base de R contient des fonctions graphique tr√®s polyvalentes. Les axes sont g√©n√©r√©es automatiquement, on peut y ajouter des titres et des l√©gendes, on peut cr√©er plusieurs graphiques sur une m√™me figure, on peut y ajouter diff√©rentes g√©om√©tries (points, lignes et polygones), avec diff√©rents types de points ou de trait, et diff√©rentes couleurs, etc. Les modules sp√©cialis√©s viennent souvent avec leurs graphiques sp√©cialis√©s, construit √† partir du module de base. En tant que module graphique imp√©ratif, on peut tout faire ou presque (pas d‚Äôinteractivit√©), mais l‚Äô√©criture du code est peut expressive. ggplot2. C‚Äôest le module graphique par excellence en R (et j‚Äôose dire: en calcul scientifique). ggplot2 se base sur une grammaire graphique. √Ä partir d‚Äôun tableau de donn√©es, une colonne peut d√©finir l‚Äôaxe des x, une autre l‚Äôaxe des y, une autre la couleur couleur des points ou leur dimension. Une autre colonne d√©finissant des cat√©gories peut segmenter la visualisation en plusieurs graphiques align√©s horizontalement ou verticalement. Des extensions de ggplot2 permettent de g√©n√©rer des cartes (ggmap), des diagrammes ternaires (ggtern), des animations (gganimate), etc. plotly. plotly est un module graphique particuli√®rement utile pour g√©n√©rer des graphiques interactifs. plotly offre une fonction toute simple pour rendre interactif un graphique ggplot2. Nous survolerons rapidement le module de base, irons plus en profondeur avec ggplot2, puis je pr√©senterai bri√®vement les graphiques interactifs avec plotly. 4.6 Module de base pour les graphiques Nous allons d‚Äôabord survoler le module de base, en mode imp√©ratif. La fonction de base pour les graphiques en R est plot(). Pour nous exercer avec cette fonction, chargeons d‚Äôabord le tableau de donn√©es d‚Äôexercice iris, publi√© en 1936 par le c√©l√®bre biostatisticien Ronald Fisher. data(&quot;iris&quot;) head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa Le tableau iris contient 5 colonnes, les 4 premi√®res d√©crivant les longueurs et largeurs des p√©tales et s√©pales de diff√©rentes esp√®ces d‚Äôiris dont le nom appara√Æt √† la 5i√®me colonne. La mani√®re la plus rapide d‚Äôextraire une colonne d‚Äôun tableau est d‚Äôappeler le tableau, suivit du $, puis du nom de la colonne, par exemple iris$Species. Pour g√©n√©rer un graphique avec la fonction plot(): plot(iris$Sepal.Length, iris$Petal.Length) Par d√©faut, le premier argument est le vecteur d√©finissant l‚Äôaxe des x et le deuxi√®me est celui d√©finissant l‚Äôaxe des y. Vous rencontrerez souvent de telles utilisations d‚Äôarguments implicites, mais je pr√©f√®re √™tre explicite en d√©finissant bien les arguments: plot(x = iris$Sepal.Length, y = iris$Petal.Length). Le graphique pr√©c√©dent peut √™tre amplement personnalis√© en utilisant diff√©rents arguments (figure 4.6). Figure 4.6: √âl√©ments personnalisables d‚Äôun graphique de base, Exercice. Utilisez ces arguments dans la cellule de code de la figure plot(iris$Sepal.Length, iris$Petal.Length). Remarquez que la fonction a d√©cid√© toute seule de cr√©er un nuage de point. La fonction plot() est con√ßue pour cr√©er le graphique appropri√© selon le type des donn√©es sp√©cifi√©es: lignes, boxplot, etc. Si l‚Äôon sp√©cifiait les esp√®ces comme argument x: plot(x = iris$Species, y = iris$Petal.Length) # ou bien # iris %&gt;% # select(Species, Petal.Length) %&gt;% # plot() De m√™me, la fonction plot() appliqu√©e √† un tableau de donn√©es g√©n√©rera une repr√©sentation bivari√©e. plot(iris) Il est possible d‚Äôencoder des attributs gr√¢ce √† des vecteurs de facteurs (cat√©gories). plot(iris, col = iris$Species) L‚Äôargument type = &quot;&quot; permet de personnaliser l‚Äôapparence: type = &quot;p&quot;: points type = &quot;l&quot;: ligne type = &quot;o&quot; et type = &quot;b&quot;: ligne et points type = &quot;n&quot;: ne rien afficher Cr√©ons un jeu de donn√©es. time &lt;- seq(from = 0, to = 100, by = 10) height &lt;- abs(time * 0.1 + rnorm(length(time), 0, 2)) # abs pourvaleur absolue (changement de signe si n√©gatif) plot(x = time, y = height, type = &quot;b&quot;, lty = 2, lwd = 1) Le type de ligne est sp√©cifi√© par l‚Äôargument lty (qui peut prendre un chiffre ou une ch√¢ine de caract√®res, i.e. 1 est √©quivalent de &quot;solid&quot;, 2 de &quot;dashed&quot;, 3 de &quot;dotted&quot;, etc.) et la largeur du trait (valeur num√©rique), par l‚Äôargument lwd. La fonction hist() permet quant √† elle de cr√©er des histogrammes. Parmi ses arguments, breaks est particuli√®rement utile, car il permet d‚Äôajuster la segmentation des incr√©ments. hist(iris$Petal.Length, breaks = 60) Exercice. Ajustez le titre de l‚Äôaxe des x, ainsi que les limites de l‚Äôaxe des x. √ätes-vous en mesure de colorer l‚Äôint√©rieur des barres en bleu? La fonction plot() peut √™tre suivie de plusieurs autres couches comme des lignes (lines() ou abline()), des points (points()), du texte (text()), des polygones (polygon(), des l√©gendes (legend())), etc. On peut aussi personnaliser les couleurs, les types de points, les types de lignes, etc. L‚Äôexemple suivant ajoute une ligne au graphique. Ne pr√™tez pas trop attention aux fonctions predict() et lm() pour l‚Äôinstant: nous les verrons au chapitre 6. plot(x = time, y = height) lines(x = time, y = predict(lm(height ~ time))) Pour exporter un graphique, vous pouvez passer par le menu Export de RStudio. Mais pour des graphiques destin√©s √† √™tre publi√©s, je vous sugg√®re d‚Äôexporter vos graphiques avec une haute r√©solution √† la suite de la commande png() (ou jpg() ou svg()). svg(filename = &quot;images/mon-graphique.svg&quot;, width = 3000, height = 2000) # png(filename = &#39;images/mon-graphique.png&#39;, width = 3000, height=2000, res=300) plot( x = iris$Petal.Length, y = iris$Sepal.Length, col = iris$Species, cex = 3, # dimension des points pch = 16 ) # type de points dev.off() ## png ## 2 Le format svg cr√©e une version vectorielle du graphique, c‚Äôest-√†-dire que l‚Äôimage export√©e est un fichier contenant les formes, non pas les pixels. Cela vous permet d‚Äô√©diter votre graphique dans un logiciel de dessin vectoriel (comme Inkscape). Dans le bloc de code pr√©c√©dent, j‚Äôai mis en commentaire (# ...) le format d‚Äôimage png, utile pour les images de type graphique, avec des changements de couleurs drastiques. J‚Äôy ai sp√©cifi√© une haute r√©solution, √† 300 pixels par pouce. Pour les photos, vous pr√©f√©rerez le format jpg. Des √©diteurs demanderont peut-√™tre des formats vectoriels comme pdf ou eps. Si vous ne trouvez pas de moyen de modifier un aspect du graphique dans le code (bouger des √©tiquettes ou des l√©gendes, ajouter des √©l√©ments graphiques), vous pouvez exporter votre graphique en format svg et √©diter votre graphique dans Inkscape. Le module de base de R comprend une panoplie d‚Äôautres particularit√©s que je ne couvrirai pas ici, en faveur du module ggplot2. 4.7 La grammaire graphique ggplot2 Le module esquisse est une extension de RStudio permettant de g√©n√©rer du code pour le module graphique ggplot2. La vid√©o suivant, o√π j‚Äôutilise esquisse, montre ce en quoi consiste une grammaire graphique. Chaque colonne est un √©l√©ment graphique et peut √™tre encod√© pour former la position en x, en y, la taille des points, leur couleur, ou m√™me le panneau (facet). Mais quelle forme prendra le bidule positionner? Des points, lignes, boxplots, barres? C‚Äôest ce que d√©fini une grammaire graphique. Bri√®vement, une grammaire graphique permet de sch√©matiser des donn√©es avec des marqueurs (points, lignes, etc.) sur des attributs visuels (couleurs, dimension, forme). Cette approche permet de d√©gager 5 composantes. Les donn√©es. Votre tableau est bien s√ªr un argument n√©cessaire pour g√©n√©rer le graphique. Les marqueurs. Un terme abstrait pour d√©signer les points, les lignes, les polygones, les barres, les fl√®ches, etc. En ggplot2, ce sont des g√©om√©tries, par exemple geom_point() pour d√©finir une g√©om√©trie de points. Les attributs encod√©s. La position, la dimension, la couleur ou la forme que prendront les g√©om√©tries. En ggplot2, on les nomme les aesthetics. Les attributs globaux. Les attributs sont globaux lorsqu‚Äôils sont constant (ils ne d√©pendent pas d‚Äôune variable). Les valeurs par d√©faut conviennent g√©n√©ralement, mais certains attributs peuvent √™tre sp√©cifi√©s: par exemple la forme ou la couleur des points, le type de ligne, etc. Les th√®mes. Le th√®me du graphique permet de personnalis√© la mani√®re dont le graphique est rendu. Il existe des th√®mes pr√©d√©finis, que vous pouvez ajuster, mais il est possible de cr√©er vos propres th√®mes (nous ne couvrirons pas cela dans ce cours). Figure 4.7: Cr√©er une oeuvre d‚Äôart avec ggplot2, dessin de [@allison_horst](https://twitter.com/allison_horst). Le flux de travail pour cr√©er un graphique √† partir d‚Äôune grammaire ressemble donc √† ceci: Avec mon tableau, Cr√©er un marqueur ( encoder(position X = colonne A, position Y = colonne B, couleur = colonne C), forme globale = 1) Avec un th√®me noir et blanc Le module tidyverse installera des modules utilis√©s de mani√®re r√©currente dans ce cours, comme ggplot2, dplyr, tidyr et readr. Je recommande de le charger au d√©but de vos sessions de travail. library(&quot;tidyverse&quot;) L‚Äôapproche tidyverse est une grammaire des donn√©es. Le module ggplot2, qui en fait partie, est une grammaire graphique (d‚Äôo√π le gg de ggplot). 4.8 Mon premier ggplot Pour notre premier exercice, je vais charger un tableau depuis le fichier de donn√©es abalone.data. Pour plus de d√©tails sur les tableaux de donn√©es, consultez le chapitre 3. Le fichier de donn√©es porte sur un escargot de mer et comprend le sexe (M: m√¢le, F: femelle et I: enfant), des poids et dimensions des individus observ√©s, et le nombre d‚Äôanneaux compt√©s dans la coquille. abalone &lt;- read_csv(&quot;data/abalone.csv&quot;) ## Parsed with column specification: ## cols( ## Type = col_character(), ## LongestShell = col_double(), ## Diameter = col_double(), ## Height = col_double(), ## WholeWeight = col_double(), ## ShuckedWeight = col_double(), ## VisceraWeight = col_double(), ## ShellWeight = col_double(), ## Rings = col_double() ## ) Inspectons l‚Äôent√™te du tableau avec la fonction head(). head(abalone) ## # A tibble: 6 x 9 ## Type LongestShell Diameter Height WholeWeight ShuckedWeight VisceraWeight ShellWeight Rings ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 M 0.455 0.365 0.095 0.514 0.224 0.101 0.15 15 ## 2 M 0.35 0.265 0.09 0.226 0.0995 0.0485 0.07 7 ## 3 F 0.53 0.42 0.135 0.677 0.256 0.142 0.21 9 ## 4 M 0.44 0.365 0.125 0.516 0.216 0.114 0.155 10 ## 5 I 0.33 0.255 0.08 0.205 0.0895 0.0395 0.055 7 ## 6 I 0.425 0.3 0.095 0.352 0.141 0.0775 0.12 8 Suivant la grammaire graphique ggplot2, on pourra cr√©er ce graphique de points comprenant les attributs suivants suivants. data = abalone, le fichier de donn√©es. mapping = aes(...), sp√©cifi√© comme attribut de la fonction ggplot(), cet encodage (ou aesthetic) reste l‚Äôencodage par d√©faut pour tous les marqueurs du graphique. Toutefois, l‚Äôencodage mapping = aes() peut aussi √™tre sp√©cifi√© dans la fonction du marqueur (par exemple geom_point()). Dans l‚Äôencodage global du graphique, on place en x la longueur de la coquille (x = LongestShell) et on place en y le poids de la coquille (y = ShellWeight). Pour ajouter une fonction √† ggplot, comme une nouvelle couche de marqueur ou des √©l√©ments de th√®me, on utilise le +. G√©n√©ralement, on change aussi de ligne. Le marqueur ajout√© est un point, geom_point(), dans lequel on sp√©cifie un encodage de couleur sur la variable Type (colour = Type) et un encodage de dimension du point sur la variable rings (size = Rings). L‚Äôattribut alpha = 0.5 se situe hors du mapping et de la fonction aes(): c‚Äôest un attribut identique pour tous les points. ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) + geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5) Il existe plusieurs types de marqueurs: geom_point() pour les points geom_line() pour les lignes geom_bar() pour les diagrammes en barre en d√©compte, geom_col en terme de grandeur et geom_histogram pour les histogrammes geom_boxplot() pour les boxplots geom_errorbar(), geom_pointrange() ou geom_crossbar() pour les marges d‚Äôerreur geom_map() pour les cartes etc. Il existe plusieurs attributs d‚Äôencodage: la position x, y et z (z pertinent notamment pour le marqueur geom_tile()) la taille size la forme des points shape la couleur, qui peut √™tre discr√®te ou continue : colour, pour la couleur des contours fill, pour la couleur de remplissage le type de ligne linetype la transparence alpha et d‚Äôautres types sp√©cialis√©s que vous retrouverez dans la documentation des marqueurs Les types de marqueurs et leurs encodages sont d√©crits dans la documentation de ggplot2, qui fournit des feuilles aide-m√©moire qu‚Äôil est commode d‚Äôimprimer et d‚Äôafficher pr√®s de soi (figure 4.8). Figure 4.8: Aide-m√©moire de ggplot2, source: https://www.rstudio.com/resources/cheatsheets/ 4.8.0.1 Les facettes Dans ggplot2, les facetttes sont un type sp√©cial d‚Äôencodage utilis√©s pour d√©finir des grilles de graphique. Elles prennent deux formes: Le collage, facet_wrap(). Une variable cat√©gorielle est utilis√©e pour segmenter les graphiques en plusieurs graphiques, qui sont plac√©s l‚Äôun √† la suite de l‚Äôautre dans un arrangement sp√©cifi√© par un nombre de colonne ou un nombre de ligne. La grille, facet_grid(). Une ou deux variables segmentent les graphiques selon les colonnes et les lignes. Les facettes peuvent √™tre sp√©cifi√©es n‚Äôimporte o√π dans la cha√Æne de commande de ggplot2, mais conventionnellement, on les place tout de suite apr√®s la fonction ggplot(). ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) + facet_wrap(~Type, ncol = 2) + geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5) La fonction cut() permet de discr√©tiser des variables continues en cat√©gories ordonn√©es - les fonctions peuvent √™tre utilis√©es √† l‚Äôint√©rieur de la fonction ggplot. ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) + facet_grid(Type ~ cut(Rings, breaks = seq(0, 30, 5))) + geom_point(mapping = aes(colour = Type), alpha = 0.5) Par d√©faut, les axes des facettes, ainsi que leurs dimensions, sont les m√™mes. Une telle repr√©sentation permet de comparer les facets sur une m√™me √©chelle. Les axes peuvent √™tre d√©finis selon les donn√©es avec l‚Äôargument scales, tandis que l‚Äôespace des facettes peut √™tre conditionn√© selon l‚Äôargument space - pour plus de d√©tails, voir la fiche de documentation. Exercice. Personnalisez le graphique avec les donn√©es abalone en rempla√ßant les variables et en r√©organisant les facettes. 4.8.1 Plusieurs sources de donn√©es Il peut arriver que les donn√©es pour g√©n√©rer un graphique proviennent de plusieurs tableaux. Lorsqu‚Äôon ne sp√©cifie pas la source du tableau dans un marqueur, la valeur par d√©faut est le tableau sp√©cifier dans l‚Äôamorce ggplot(). Il est n√©anmoins possible de d√©finir une source personnalis√©e pour chaque marqueur en sp√©cifiant data = ... comme argument du marqueur. abalone_siteA &lt;- data.frame( LongestShell = c(0.3, 0.8, 0.7), ShellWeight = c(0.05, 0.81, 0.77) ) ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) + geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5) + geom_point(data = abalone_siteA, size = 8, shape = 4) 4.8.2 Exporter avec style Le fond gris est une marque distinctive de ggplot2. Il n‚Äôest toutefois pas appr√©ci√© de tout le monde. D‚Äôautres th√®mes dits complets peuvent √™tre utilis√©s (liste des th√®mes complets). Les th√®mes complets sont appel√©s avant la fonction theme(), qui permet d‚Äôeffectuer des ajustements pr√©cis dont la liste exhaustive se trouve dans la documentation de ggplot2. Vous pouvez aussi personnaliser le titre des axes (xlab() et ylab()) ou du graphique (ggtitle()), ou bien tout sp√©cifier dans une m√™me fonction ou bien tout en m√™me temps dans labs(x = &quot;...&quot;, y = &quot;...&quot;, title = &quot;...&quot;). Il est possible d‚Äôutiliser des exposants dans le titre des axes avec la fonction expression(), par exemple labs(x = expression(&quot;Dose (kg ha&quot;^&quot;-1&quot;~&quot;)&quot;)) pour intituler l‚Äôaxe des x avec \\(Dose~(kg~ha^{-1})\\). Aussi convient parfois de sp√©cifier les limites (xlim() et ylim(), ou expand_limits(x = c(0, 1), y = c(0, 1))). Pour exporter un ggplot, on pourra utiliser les commandes de R png(), svg() ou pdf(), ou les outils de RStudio. Toutefois, ggplot2 offre la fonction ggsave(), que l‚Äôon place en remorque du graphique, en sp√©cifiant les dimensions (width et height) ainsi que la r√©solution (dpi). La r√©solution d‚Äôun graphique destin√© √† la publication est typiquement de plus de 300 dpi. ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) + geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5) + #xlab(&quot;Length (mm)&quot;) + #ylab(&quot;Shell weight (g)&quot;) + #ggtitle(&quot;Abalone&quot;) + # pr√©f√©rablement dans une m√™me ligne labs(x = &quot;Length (mm)&quot;, y = &quot;Shell weight (g)&quot;, title = &quot;Abalone&quot;) + xlim(c(0, 1)) + theme_classic() + theme( axis.title = element_text(size = 20), axis.text = element_text(size = 20), axis.text.y = element_text(size = 20, angle = 90, hjust = 0.5), legend.box = &quot;horizontal&quot; ) ggsave(&quot;images/abalone.png&quot;, width = 8, height = 8, dpi = 300) Nous allons maintenant couvrir diff√©rents types de graphiques, accessibles selon diff√©rents marqueurs: les nuages de points les diagrammes en ligne les boxplots les histogrammes les diagrammes en barres 4.8.3 Nuages de points L‚Äôexemple pr√©c√©dent est un nuage de points, que nous avons g√©n√©r√© avec le marqueur geom_point(), qui a d√©j√† √©t√© passablement introduit. L‚Äôexploration de ces donn√©es a permis de d√©tecter une croissance exponentielle du poids de la coquille en fonction de sa longueur. Il est clair que les abalones juv√©niles (Type I) sont plus petits et moins lourds, mais nous devrons probablement proc√©der √† des tests statistiques pour v√©rifier s‚Äôil y a des diff√©rences entre m√¢les et femelles. Le graphique √©tant tr√®s charg√©, nous avons utilis√© des strat√©gies pour l‚Äôall√©ger en utilisant de la transparence et des facettes. Le marqueur geom_jitter() peut permettre de mieux appr√©cier la dispersion des points en ajoutant une dispersion randomis√©e en x ou en y. ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) + geom_jitter(mapping = aes(colour = Type, size = Rings), alpha = 0.5, width = 0.05, height = 0.1) Dans ce cas-ci, √ßa ne change pas beaucoup, mais retenons-le pour la suite. 4.8.4 Diagrammes en lignes Les lignes sont utilis√©es pour exprimer des liens entre une suite d‚Äôinformation. Dans la plupart des cas, il s‚Äôagit d‚Äôune suite d‚Äôinformation dans le temps que l‚Äôon appelle les s√©ries temporelles. En l‚Äôoccurrence, les lignes devraient √™tre √©vit√©es si la s√©quence entre les variables n‚Äôest pas √©vidente. Nous allons utiliser un tableau de donn√©es de R portant sur la croissance des orangers. data(&quot;Orange&quot;) head(Orange) ## Grouped Data: circumference ~ age | Tree ## Tree age circumference ## 1 1 118 30 ## 2 1 484 58 ## 3 1 664 87 ## 4 1 1004 115 ## 5 1 1231 120 ## 6 1 1372 142 La premi√®re colonne sp√©cifie le num√©ro de l‚Äôarbre mesur√©, la deuxi√®me son √¢ge et la troisi√®me sa circonf√©rence. Le marqueur geom_line() permet de tracer la tendance de la circonf√©rence selon l‚Äô√¢ge. En encodant la couleur de la ligne √† l‚Äôarbre, nous pourrons tracer une ligne pour chacun d‚Äôentre eux. ggplot(data = Orange, mapping = aes(x = age, y = circumference)) + geom_line(aes(colour = Tree)) La l√©gende ne montre pas les num√©ros d‚Äôarbre en ordre croissance. En effet, la l√©gende (tout comme les facettes) classe les cat√©gories prioritairement selon l‚Äôordre des cat√©gories si elles sont ordinales, ou par ordre alphab√©tique si les cat√©gories sont nominales. Inspectons la colonne Tree en inspectant le tableau avec la commande str() - la commande glimpse() du tidyverse donne un sommaire moins complet que str(). str(Orange) ## Classes &#39;nfnGroupedData&#39;, &#39;nfGroupedData&#39;, &#39;groupedData&#39; and &#39;data.frame&#39;: 35 obs. of 3 variables: ## $ Tree : Ord.factor w/ 5 levels &quot;3&quot;&lt;&quot;1&quot;&lt;&quot;5&quot;&lt;&quot;2&quot;&lt;..: 2 2 2 2 2 2 2 4 4 4 ... ## $ age : num 118 484 664 1004 1231 ... ## $ circumference: num 30 58 87 115 120 142 145 33 69 111 ... ## - attr(*, &quot;formula&quot;)=Class &#39;formula&#39; language circumference ~ age | Tree ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; ## - attr(*, &quot;labels&quot;)=List of 2 ## ..$ x: chr &quot;Time since December 31, 1968&quot; ## ..$ y: chr &quot;Trunk circumference&quot; ## - attr(*, &quot;units&quot;)=List of 2 ## ..$ x: chr &quot;(days)&quot; ## ..$ y: chr &quot;(mm)&quot; En effet, la colonne Tree est un facteur ordinal dont les niveaux sont dans le m√™me ordre que celui la l√©gende. 4.8.5 Les histogrammes Nous avons vu les histogrammes dans la br√®ve section sur les fonctions graphiques de base dans R: il s‚Äôagit de segmenter l‚Äôaxe des x en incr√©ments, puis de pr√©senter sur l‚Äôaxe de y le nombre de donn√©es que l‚Äôon retrouve dans cet incr√©ment. Le marqueur √† utiliser est geom_histogram(). Revenons √† nos escargots. Comment pr√©senteriez-vous la longueur de la coquille selon la variable Type? Selon des couleurs ou des facettes? La couleur, dans le cas des histogrammes, est celle du pourtour des barres. Pour colorer l‚Äôint√©rieur des barres, l‚Äôargument √† utiliser est fill. ggplot(data = abalone, mapping = aes(x = LongestShell)) + geom_histogram(mapping = aes(fill = Type), colour = &quot;black&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. On n‚Äôy voit pas grand chose. Essayons plut√¥t les facettes. ggplot(data = abalone, mapping = aes(x = LongestShell)) + facet_grid(Type ~ .) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Les facettes permettent maintenant de bien distinguer la distribution des longueur des juv√©niles. L‚Äôargument bins, tout comme l‚Äôargument breaks du module graphique de base, permet de sp√©cifier le nombre d‚Äôincr√©ments, ce qui peut √™tre tr√®s utile en exploration de donn√©es. ggplot(data = abalone, mapping = aes(x = LongestShell)) + facet_grid(Type ~ .) + geom_histogram(bins = 60, colour = &quot;white&quot;) Le nombre d‚Äôincr√©ments est un param√®tre qu‚Äôil ne faut pas sous-estimer. √Ä preuve, ce tweet de [@NicholasStrayer](https://twitter.com/NicholasStrayer): Histograms are fantastic, but make sure your bin-width/number is chosen well. This is the exact same data, plotted with different bin-widths. Notice that the pattern doesn't necessarily get clearer as bin num increases. #dataviz pic.twitter.com/3MhSFwTVPH ‚Äî Nick Strayer (@NicholasStrayer) 7 ao√ªt 2018 4.8.6 Boxplots Les boxplots sont une autre mani√®re de visualiser des distributions. L‚Äôastuce est de cr√©er une bo√Æte qui s‚Äô√©tant du premier quartile (valeur o√π l‚Äôon retrouve 25% de donn√©es dont la valeur est inf√©rieure) au troisi√®me quartile (valeur o√π l‚Äôon retrouve 75% de donn√©es dont la valeur est inf√©rieure). Une barre √† l‚Äôint√©rieur de cette bo√Æte est plac√©e √† la m√©diane (qui est en fait le second quartile). De part et d‚Äôautre de la bo√Æte, on retrouve des lignes sp√©cifiant l‚Äô√©tendue hors quartile. Cette √©tendue peut √™tre d√©termin√©e de plusieurs mani√®res, mais dans le cas de ggplot2, il s‚Äôagit de 1.5 fois l‚Äô√©tendue de la bo√Æte (l‚Äô√©cart interquartile). Au-del√† de ces lignes, on retrouve les points repr√©sentant les valeurs extr√™mes. Le marqueur √† utiliser est geom_boxplot(). L‚Äôencodage x est la variable cat√©gorielle et l‚Äôencodage y est la variable continue. ggplot(data = abalone, mapping = aes(x = Type, y = LongestShell)) + geom_boxplot() Exercice. On sugg√®re parfois de pr√©senter les mesures sur les boxplots. Utiliser geom_jitter() avec un bruit horizontal. 4.8.7 Les diagrammes en barre Les diagrammes en barre repr√©sente une variable continue associ√©e √† une cat√©gorie. Les barres sont g√©n√©ralement horizontales et ordonn√©es. Nous y reviendrons √† la fin de ce chapitre, mais retenez pour l‚Äôinstant que dans tous les cas, les diagrammes en barre doivent inclure le z√©ro pour √©viter les mauvaises interpr√©tations. Pour les diagrammes en barre, nous allons utiliser les donn√©es de l‚Äôunion internationale pour la conservation de la nature distribu√©es par l‚ÄôOCDE. # Certaines colonnes de caract√®re sont consid√©r√©es comme bool√©ennes # mieux vaut d√©finir leur type pour s&#39;assurer que le bon type # soit attribu√© especes_menacees &lt;- read_csv(&quot;data/WILD_LIFE_14012020030114795.csv&quot;, col_types = list( &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;d&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;d&quot;, &quot;c&quot;, &quot;c&quot; ) ) head(especes_menacees) ## # A tibble: 6 x 15 ## IUCN `IUCN Category` SPEC Species COU Country `Unit Code` Unit `PowerCode Code` PowerCode `Reference Peri‚Ä¶ ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 TOT_‚Ä¶ Total number o‚Ä¶ MAMM‚Ä¶ Mammals AUS Austra‚Ä¶ NBR Numb‚Ä¶ 0 Units &lt;NA&gt; ## 2 ENDA‚Ä¶ Number of enda‚Ä¶ MAMM‚Ä¶ Mammals AUS Austra‚Ä¶ NBR Numb‚Ä¶ 0 Units &lt;NA&gt; ## 3 CRIT‚Ä¶ Number of crit‚Ä¶ MAMM‚Ä¶ Mammals AUS Austra‚Ä¶ NBR Numb‚Ä¶ 0 Units &lt;NA&gt; ## 4 VULN‚Ä¶ Number of vuln‚Ä¶ MAMM‚Ä¶ Mammals AUS Austra‚Ä¶ NBR Numb‚Ä¶ 0 Units &lt;NA&gt; ## 5 THRE‚Ä¶ Total number o‚Ä¶ MAMM‚Ä¶ Mammals AUS Austra‚Ä¶ NBR Numb‚Ä¶ 0 Units &lt;NA&gt; ## 6 TOT_‚Ä¶ Total number o‚Ä¶ MAMM‚Ä¶ Mammals AUT Austria NBR Numb‚Ä¶ 0 Units &lt;NA&gt; ## # ‚Ä¶ with 4 more variables: `Reference Period` &lt;chr&gt;, Value &lt;dbl&gt;, `Flag Codes` &lt;chr&gt;, Flags &lt;chr&gt; L‚Äôexercice consiste √† cr√©er un diagramme en barres horizontales du nombre de plantes vasculaires menac√©es de mani√®re critique pour les 10 pays qui en contiennent le plus. Je vais effectuer quelques op√©rations sur ce tableau afin d‚Äôen arriver avec un tableau que nous pourrons convenablement mettre en graphique: n‚Äôy portez pas trop attention pour l‚Äôinstant: ces op√©rations sont un avant-go√ªt du prochain chapitre. Nous allons filtrer le tableau pour obtenir le nombre de plantes vascularies critiquement menac√©es, s√©lectionner seulement le pays et le nombre d‚Äôesp√®ces, les grouper par pays, additionner toutes les esp√®ces pour chaque pays, les placer en ordre descendant et enfin s√©lectionner les 10 premiers. Comme vous le voyez, la cr√©ation de graphique est li√©e de pr√®s avec la manipulation des tableaux! especes_crit &lt;- especes_menacees %&gt;% filter(IUCN == &quot;CRITICAL&quot;, SPEC == &quot;VASCULAR_PLANT&quot;) %&gt;% dplyr::select(Country, Value) %&gt;% group_by(Country) %&gt;% summarise(n_critical_species = sum(Value)) %&gt;% arrange(desc(n_critical_species)) %&gt;% head(10) especes_crit ## # A tibble: 10 x 2 ## Country n_critical_species ## &lt;chr&gt; &lt;dbl&gt; ## 1 United States 1222 ## 2 Japan 525 ## 3 Canada 315 ## 4 Czech Republic 284 ## 5 Spain 271 ## 6 Belgium 253 ## 7 Austria 172 ## 8 Slovak Republic 155 ## 9 Australia 148 ## 10 Italy 128 Le premier type de diagramme en barre que nous allons couvrir est obtenu par le marqueur geom_col(). ggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) + geom_col() Ce graphique est perfectible. Les barres sont verticales et non ordonn√©es. Souvenons-nous que ggplot2 ordonne par ordre alphab√©tique si aucun autre ordre est sp√©cifi√©. Nous pouvons changer l‚Äôordre en changeant l‚Äôordre des niveaux de la variable Country selon le nombre d‚Äôesp√®ces gr√¢ce √† la fonction fct_reorder. especes_crit &lt;- especes_crit %&gt;% mutate(Country = fct_reorder(Country, n_critical_species)) Pour faire pivoter le graphique, nous ajoutons coord_flip() √† la s√©quence. ggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) + geom_col() + coord_flip() Une autre m√©thode, geom_bar(), est un raccourcis permettant de compter le nombre d‚Äôoccurrence d‚Äôune variable unique. Par exemple, dans le tableau abalone, le nombre de fois que chaque niveau de la variable Type ggplot(data = abalone, mapping = aes(x = Type)) + geom_bar() + coord_flip() Personnellement, j‚Äôaime bien passer par un diagramme en lignes avec le marqueur geom_segment(). Cela me donne la flexibilit√© pour d√©finir un largeur de trait et √©ventuellement d‚Äôajouter un point au bout pour en faire un diagramme en su√ßon. Tenez, j‚Äôen profite aussi pour y ajouter du texte (d√©cal√© horizontalement) et √©tendre les limtes pour m‚Äôassurer que les chiffres apparaissent bien. ggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) + geom_segment(mapping = aes(xend = Country, yend = 0), lwd = 2) + geom_point(size = 5, colour = &quot;black&quot;) + geom_text(aes(label = n_critical_species), hjust = -0.5) + # si ce ne sont pas des valeurs enti√®res, arrondir avec signif() expand_limits(y = c(0, 1300)) + coord_flip() + theme_bw() Les diagrammes en barre peuvent √™tre plac√©s en relation avec d‚Äôautres. Reprenons notre manipulation de donn√©es pr√©c√©dente, mais en incluant tous les pays, pour les trois niveaux d‚Äôalerte, pour les poissons. especes_pays_iucn &lt;- especes_menacees %&gt;% filter(IUCN %in% c(&quot;ENDANGERED&quot;, &quot;VULNERABLE&quot;, &quot;CRITICAL&quot;), SPEC == &quot;FISH_TOT&quot;) %&gt;% dplyr::select(IUCN, Country, Value) %&gt;% group_by(Country, IUCN) %&gt;% summarise(n_species = sum(Value)) %&gt;% group_by(Country) %&gt;% mutate(n_tot = sum(n_species)) %&gt;% ungroup() %&gt;% # pour pouvoir modifier Country, non modifiable tant qu&#39;elle est une variable de regroupement (voir group_by) mutate(Country = fct_reorder(Country, n_tot)) head(especes_pays_iucn) ## # A tibble: 6 x 4 ## Country IUCN n_species n_tot ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Australia CRITICAL 8 48 ## 2 Australia ENDANGERED 16 48 ## 3 Australia VULNERABLE 24 48 ## 4 Austria CRITICAL 6 39 ## 5 Austria ENDANGERED 18 39 ## 6 Austria VULNERABLE 15 39 Pour placer les barres les unes √† c√¥t√© des autres, nous sp√©cifions position = &quot;dodge&quot;. ggplot(data = especes_pays_iucn, mapping = aes(x = Country, y = n_species)) + geom_col(aes(fill = IUCN), position = &quot;dodge&quot;) + coord_flip() Il est parfois plus pratique d‚Äôutiliser les facettes. ggplot(data = especes_pays_iucn, mapping = aes(x = Country, y = n_species)) + facet_grid(IUCN ~ .) + geom_col() + coord_flip() Pour perfectionner encore ce graphique, on pourrait r√©ordonner les facettes individuellement, mais ne nous √©garons par trop. 4.8.8 Exporter un graphique Plus besoin d‚Äôutiliser la fonction png() en mode ggplot2. Utilisons plut√¥t ggsave(). ggplot(data = especes_pays_iucn, mapping = aes(x = Country, y = n_species)) + facet_grid(IUCN ~ .) + geom_col(aes(fill = IUCN)) + coord_flip() ggsave(&quot;images/especes_pays_iucn.png&quot;, width = 6, height = 8, dpi = 300) 4.9 Les graphiques comme outil d‚Äôexploration des donn√©es Figure 4.9: Explorer les donn√©es avec ggplot2, dessin de [@allison_horst](https://twitter.com/allison_horst). La plupart des graphiques que vous cr√©erez ne seront pas destin√©s √† √™tre publi√©s, mais serviront d‚Äôoutil d‚Äôexploration des donn√©es. Le jeu de donn√©es datasaurus, pr√©sent√© en d√©but de chapitre, permet de saisir l‚Äôimportance des outils graphiques pour bien comprendre les donn√©es. datasaurus &lt;- read_tsv(&quot;data/DatasaurusDozen.tsv&quot;) ## Parsed with column specification: ## cols( ## dataset = col_character(), ## x = col_double(), ## y = col_double() ## ) head(datasaurus) ## # A tibble: 6 x 3 ## dataset x y ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 dino 55.4 97.2 ## 2 dino 51.5 96.0 ## 3 dino 46.2 94.5 ## 4 dino 42.8 91.4 ## 5 dino 40.8 88.3 ## 6 dino 38.7 84.9 Projetons d‚Äôabord les coordonn√©es x et y sur un graphique. J‚Äôutilise FacetGrid ici, sachant que ce sera utile pour l‚Äôexploration. ggplot(data = datasaurus, mapping = aes(x = x, y = y)) + geom_point() Ce graphique pourrait ressembler √† une distribution binormale, ou un coup de 12 dans une porte de grange. Mais on aper√ßoit des donn√©es align√©es, parfois de mani√®re rectiligne, parfois en forme d‚Äôellipse. Le tableau datasaurus a une colonne d‚Äôinformation suppl√©mentaire. Utilisons-la comme cat√©gorie pour g√©n√©rer des couleurs diff√©rente. ggplot(data = datasaurus, mapping = aes(x = x, y = y)) + geom_point(mapping = aes(colour = dataset)) Ce n‚Äôest pas vraiment plus clair. Il y a toutefois des formes qui se d√©gage, comme des ellipse et des lignes. Et si je regarde bien, j‚Äôy vois une √©toile. La cat√©gorisation pourrait-elle √™tre mieux utilis√©e si on segmentait par facettes au lieu de des couleurs? ggplot(data = datasaurus, mapping = aes(x = x, y = y)) + facet_wrap(~dataset, nrow = 2) + geom_point(size = 0.5) + coord_equal() Voil√†! Fait int√©ressant, ni les statistiques, ni les algorithmes de regroupement ne nous auraient √©t√© utiles pour diff√©rencier les groupes! 4.9.1 Des graphiques interactifs! Les graphiques sont traditionnellement des images statiques. Toutefois, les graphiques n‚Äô√©tant pas d√©pendants de supports papiers peuvent √™tre utilis√©s de mani√®re diff√©rente, en ajoutant une couche d‚Äôinteraction. Con√ßue √† Montr√©al, plotly est un module graphique interactif en soi. Il peut √™tre utilis√© gr√¢ce √† son outil web, tout comme il peut √™tre interfac√© avec R, Python, javascript, etc. Mais ce qui retient notre attention ici est son interface avec ggplot2. Les graphiques ggplot2 peuvent √™tre enregistr√©s en tant qu‚Äôobjets. Il peuvent cons√©quemment √™tre manipul√©s par des fonctions. La fonction ggplotly permet de rendre votre ggplot interactif. library(&quot;plotly&quot;) ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout especes_crit_bar &lt;- ggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) + geom_segment(mapping = aes(xend = Country, yend = 0), lwd = 2) + geom_point(size = 6) + coord_flip() ggplotly(especes_crit_bar) Vous pouvez publier votre graphique plotly en ligne pour le partager ou l‚Äôinclure dans une publication web. Il vous faudra cr√©er un compte plotly, puis g√©n√©rer une cl√© d‚Äôutilisation dans Settings &gt; API Keys &gt; Generate key. Pour des raisons de s√©curit√©, la cl√© du bloc ci-dessous ne fonctionnera pas. J‚Äôai d√©sactiv√© le bloc de code, mais le r√©sultat se trouve en suivant le lien g√©n√©r√© par plotly: https://plot.ly/~essicolo/152/. Sys.setenv(&quot;plotly_username&quot;=&quot;essicolo&quot;) Sys.setenv(&quot;plotly_api_key&quot;=&quot;iavd1ycE2iiqOp9YD45I&quot;) chart_link &lt;- api_create(x = ggplotly(especes_crit_bar), filename = &quot;public-graph&quot;, sharing = &quot;public&quot;, fileopt = &quot;overwrite&quot;) chart_link 4.9.2 Des extensions de ggplot2 ggplot2 est un module graphique √©l√©gant et polyvalent. Il a pourtant bien des limitations. Justement, le module est con√ßu pour √™tre impl√©ment√© avec des extensions. Vous en trouverez plusieurs sur ggplot2-exts.org, mais en trouverez de nombreuses autres en cherchant avec le terme ggplot2 sur github.com, probablement la plate-forme (voire un r√©seau social) de d√©veloppement de logiciels la plus utilis√©e dans le monde. En voici quelques unes. ggthemr: sp√©cifier un th√®me graphique une seule fois dans votre session, et tout le reste suit. cowplot et patchwork permettent de cr√©er des graphiques pr√™ts pour la publication, par exemple en cr√©ant des grilles de plusieurs ggplots, en les num√©rotant, etc. Si les th√®mes de base ne vous conviennent pas, vous en trouverez d‚Äôautres en installant ggthemes. ggmap et ggspatial sont deux extensions pour cr√©er des cartes. Un chapitre sur les donn√©es spatiales est en d√©veloppement. ggtern permet de cr√©er des diagrammes ternaires, qui sont utiles pour la visualisation de proportions incluant trois composantes. Ce sujet est couvert au chapitre 6, en d√©veloppement. 4.9.3 Aller plus loin avec ggplot2 Claus O. Wilke est professeur en biologie int√©grative √† l‚ÄôUniversit√© du Texas √† Austin. Son livre Fundamentals of Data Visualization est un guide th√©orique et pratique pour la visualisation de donn√©es avec ggplot2. Le site data-to-viz.com vous accompagne dans le choix du graphique √† cr√©er selon vos donn√©es. Le site r-graph-gallery.com offre des recettes pour cr√©er des graphiques avec ggplot2. 4.10 Extra: R√®gles particuli√®res Les mauvais graphiques peuvent survenir √† cause de l‚Äôignorance, bien s√ªr, mais souvent ils existent pour la m√™me raison que la boeuferie [bullhist] verbale ou √©crite. Parfois, les gens ne se soucient pas de la fa√ßon dont ils pr√©sentent les donn√©es aussi longtemps que √ßa appuie leurs arguments et, parfois, ils ne se soucient pas que √ßa porte √† confusion tant qu‚Äôils ont l‚Äôair impressionnant. \\(-\\) Carl Bergstorm et Jevin West, Calling Bullshit Read-Along Week 6: Data Visualization Une repr√©sentation visuelle est un outil tranchant qui peut autant pr√©senter un √©tat v√©ritable des donn√©es qu‚Äôune perspective trompeuse. Bien souvent, une ou plusieurs des 5 qualit√©s ne sont pas respect√©es. Les occasions d‚Äôerreur ne manquent pas - j‚Äôen ferai mention dans la section Choisir le bon type de graphique. Pour l‚Äôinstant, notons quelques r√®gles particuli√®res. 4.10.1 Ne tronquez pas inutilement l‚Äôaxe des \\(y\\) Tronquer l‚Äôaxe vertical peut amener √† porter de fausses conclusions. Effets sur la perception d‚Äôutiliser diff√©rentes r√©f√©rences. Source: Yau (2015), Real Chart Rules to Follow. La r√®gle semble simple: les diagrammes en barre (utilis√©s pour repr√©senter une grandeur) devraient toujours pr√©senter le 0 et les diagrammes en ligne (utilis√©s pour pr√©senter des tendances) ne requiert pas n√©cessairement le z√©ro ((Bergstrom et West, Calling bullshit: Misleading axes on graphs)[http://callingbullshit.org/tools/tools_misleading_axes.html]). Mais le z√©ro n‚Äôest pas toujours li√© √† une quantit√© particuli√®re, par exemple, la temp√©rature ou un log-ratio. De plus, avec un diagramme en ligne on pourra toujours magnifier des tendances en zoomant sur une variation somme toute mineure. On arrive donc moins √† une r√®gle qu‚Äôune qualit√© d‚Äôun bon graphique, en particulier la qualit√© no 1 de Cairo: offrir une repr√©sentation honn√™te des donn√©es. Par exemple, Nathan Yau, auteur du blogue Flowing Data, propose de pr√©senter des r√©sultats de mani√®re relative √† la mesure initiale. C‚Äôest d‚Äôailleurs ce qui a √©t√© fait pour g√©n√©rer le graphique de Michael Mann et al., ci-dessus, o√π le z√©ro correspond √† la moyenne des temp√©ratures enregistr√©es entre 1961 et 1990. Il peut √™tre tentant de tronquer l‚Äôaxe des \\(y\\) lorsque l‚Äôon d√©sire superposer deux axes verticaux. Souvent, l‚Äôutilisation de plusieurs axes verticaux am√®ne une perception de causalit√© dans des situations de fausses corr√©lations. On ne devrait pas utiliser plusieurs axes verticaux. 4.10.2 Utilisez un encrage proportionnel Cette r√®gle a √©t√© propos√©e par Edward Tufte dans Visual Display of Quantitative Information. Une des raisons pour lesquelles on √©vite de tronquer l‚Äôaxe des \\(y\\) en particulier pour les diagrammes en barre est que l‚Äôaire repr√©sentant une mesure (la quantit√© d‚Äô‚Äúencre‚Äù n√©cessaire pour la dessiner) devrait √™tre proportionnelle √† sa magnitude. Les diagrammes en barre sont particuli√®rement sensibles √† cette r√®gle, √©tant donn√©e que la largeur des barres peuvent amplifier l‚Äôaire occup√©e. Deux solutions dans ce cas: (1) utiliser des barres minces ou (2) pr√©f√©rer des ‚Äúdiagrammes de points‚Äù (dot charts, √† ne pas confondre aux nuages de points). L‚Äôencrage a beau √™tre proportionnel, la difficult√© que les humains √©prouvent √† comparer la dimension des cercles, et a fortiori la dimension de parties de cercle, donne peu d‚Äôavantage √† utiliser des diagrammes en pointe de tarte, souvent utilis√©s pour illustrer des proportions. Nathan Yau sugg√®re de les utiliser avec suspicions et d‚Äôexplorer d‚Äôautres options. Pour comparer deux proportions, une avenue int√©ressante est le diagramme en pente, sugg√©r√© notamment par Ann K. Emery. Par extension, le diagramme en pente devient un diagramme en ligne lorsque plusieurs types de proportions sont compar√©es, ou lorsque des proportions √©voluent selon des donn√©es continuent. De la m√™me mani√®re, les diagrammes en bulles ne devraient pas √™tre repr√©sentatifs de la quantit√©, mais plut√¥t de contextualiser des donn√©es. Justement, le graphique tir√© des donn√©es de Gap minder pr√©sent√© plus haut est une contextualisation: l‚Äôaire d‚Äôun cercle ne permet pas de saisir la population d‚Äôun pays, mais de comparer grossi√®rement la population d‚Äôun pays par rapport aux autres. 4.10.3 Publiez vos donn√©es Vous avez peut-√™tre d√©j√† feuillet√© un article et voulu avoir acc√®s aux donn√©es incluses dans un graphique. Il existe des outils pour digitaliser des graphiques pour en extraire les donn√©es. Mais le processus est fastidieux, long, souvent peu pr√©cis. De plus en plus, les chercheurs sont encourag√©s √† publier leurs donn√©es et leurs calculs. Matplotlib et Seaborn sont des outils graphiques classiques qui devraient √™tre accompagn√©s des donn√©es et calculs ayant servi √† les g√©n√©rer. Mais ce n‚Äôest pas id√©al non plus. En revanche, les outils graphiques modernes comme Plotly et Altair peuvent √™tre export√©s en code javascipt, qui contient toutes les informations sur les donn√©es et la mani√®re de les repr√©senter graphiquement. Ce chapitre a pour objectif de vous familiariser avec les outils de base les plus commun√©ment utilis√©s en calcul scientifique avec Python, mais je vous encourage √† explorer la nouvelle g√©n√©ration d‚Äôoutils graphiques. Nous verrons √ßa au chapitre 5. 4.10.4 Visitez www.junkcharts.typepad.com de temps √† autre Le statisticien et blogueur Kaiser Fung s‚Äôaffaire quotidiennement √† proposer des am√©liorations √† de mauvais graphiques sur son blogue Junk Charts. "],
["chapitre-git.html", "5 Science ouverte et reproductibilit√© 5.1 Un code reproductible 5.2 Introduction √† GitHub 5.3 Introduction √† Pakrat üì¶üêÄ 5.4 Pour terminer, le reprex", " 5 Science ouverte et reproductibilit√© Ô∏è¬†Objectifs sp√©cifiques: √Ä la fin de ce chapitre, vous saurez exprimer l‚Äôimportance et les enjeux de la science ouverte saurez arranger vos donn√©es (format csv) et votre code (format notebook) afin de rendre vos recherches reproductibles saurez comment cr√©er un d√©p√¥t sur GitHub, puis administrer son d√©veloppement La science ouverte favorise la diffusion des connaissances √† travers plusieurs aspects. M√©thodologie ouverte. Ce n‚Äôest pas pour rien que les revues scientifiques demandent de la minutie dans la description de la m√©thodologie: c‚Äôest pour s‚Äôassurer de bien comprendre la signification des donn√©es collect√©es et faire en sorte que vos donn√©es puissent √™tre √©chantillonn√©es de la m√™me mani√®re dans une potentielle exp√©rience subs√©quente. √Ä ce titre, la revue Nature a cr√©√© le site de publication de protocoles exp√©rimentaux Protocol exchange, ‚Äúo√π la communaut√© scientifique met en commun son savoir-faire exp√©rimental pour acc√©l√©rer la recherche‚Äù (ma traduction). Donn√©es ouvertes. En rendant nos donn√©es publiques, on permet √† la post√©rit√© de les utiliser pour am√©liorer les connaissances, d√©couvrir des structures qui nous avaient √©chapp√©es, etc. Dans certains cas, l‚Äôouverture des donn√©es peut √™tre contrainte par des enjeux l√©gaux (donn√©es priv√©es) ou √©thiques (donn√©es pouvant √™tre utilis√©es √† mauvais escient). Dans la plupart des cas, les avantages surpassent largement les risques encourus par la publication des donn√©es, et les informations personnelles peuvent √™tre retir√©es. Des journaux comme Plos exigent que les donn√©es minimales √† la reproduction de l‚Äôexp√©rience soient fournies en tant que mat√©riel suppl√©mentaire. Code source ouvert. Les logiciels open source, comme R, sont gratuits pour la plupart. Cela permet √† quiconque de les utiliser, pourvu que l‚Äôon poss√®de le support mat√©riel (un ordinateur) et une connection internet. De la m√™me mani√®re, le code R qui vous a permis de g√©n√©rer des r√©sultats √† partir de vos donn√©es peut √™tre rendu public sous toutes sortes de licenses open source peu restrictive (GPL, BSD, MIT, etc.). Avec les donn√©es et le code, vos travaux pourront √™tre reproduits. R√©vision ouverte. La r√©vision est un travail essentiel en science. Traditionnellement, les publications scientifiques sont r√©vis√©s de mani√®re anonyme, le but √©tant d‚Äô√©viter les conflits. R√©cemment, des revues comme Frontiers ont d√©ploy√© des modes de r√©vision ouverts, permettant (1) des √©changes plus constructifs entre auteurs et r√©viseurs et (2) de remercier ouvertement la contribution des r√©viseurs √† l‚Äôarticle final. Acc√®s ouvert. Les √©diteurs scientifiques sont largement critiqu√©s pour demander des frais usuraires aux biblioth√®ques et pour la consultation √† la pi√®ce, ainsi que des frais de publication d√©mesur√©s. En r√©action √† cela, le site Sci-Hub d√©bloque gratuitement des millions d‚Äôarticles scientifiques. Aussi, des journaux s√©rieux comme Plos et Frontiers publient de facto les articles sur leur site internet, de sorte qu‚Äôils peuvent √™tre librement t√©l√©charg√©s. Le manque d‚Äôouverture dans la science a men√© plusieurs scientifiques √† parler d‚Äôune crise de la reproductibilit√© (Baker, 2016). Dans ce chapitre, nous verrons quelques astuces pour que R devienne un outil favorisant la science ouverte. √Ä la fin de ce chapitre, vous devriez √™tre en mesure de d√©ployer votre code sur une archive en ligne, comme ceci. Figure 5.1: Exemple d‚Äôun dossier de code et de donn√©es ouvertes, (Jeanne et al.¬†2019) 5.1 Un code reproductible Figure 5.2: A Guide to Reproducible Code in Ecology and Evolution, BES 2017 La British ecological society offre des lignes guide pour cr√©er un flux de travail reproductible (BES, 2017). En outre, les principes suivants doivent √™tre respect√©s (ma traduction, avec ajouts). Commencez votre analyse √† partir d‚Äôune copie des donn√©es brutes. Les donn√©es doivent √™tre fournies dans un format ouvert (csv, json, sqlite, etc.). √âvitez de d√©marrer une analyse par un chiffrier √©lectronique ou un logiciel propri√©taire (qui n‚Äôest pas open source). En ce sens, d√©marrer avec Excel (xls ou xlsx) est √† √©viter, tout comme les sont les donn√©es encod√©es pour SPSS ou SAS. Toute op√©ration sur les donn√©es, que ce soit du nettoyage, des fusions, des transformations, etc. devrait √™tre effectu√©e avec du code, non pas manuellement. S‚Äôil s‚Äôagit d‚Äôune erreur de frappe dans un tableau, on peut d√©roger √† la r√®gle. Mais s‚Äôil s‚Äôagit par exemple d‚Äô√©limier des outliers, ne supprimez pas des entr√©es de vos donn√©es brutes. De m√™me, n‚Äôeffectuez pas de transformation de vos donn√©es brutes √† l‚Äôext√©rieur du code. En somme, vos calculs devraient √™tre en mesure d‚Äô√™tre lanc√©s d‚Äôun seul coup, sans op√©rations manuelles interm√©diaires. S√©parez vos op√©rations en unit√©s logiques th√©matiques. Par exemple, vous pourriez s√©parer votre code en parties: (i) charger, fusionner et nettoyer les donn√©es, (ii) analyser les donn√©es, (iii) cr√©er des fichiers comme des tableaux et des figures. √âliminez la duplication du code en cr√©ant des fonctions personnalis√©es. Assurez-vous de commenter vos fonctions en d√©tails, expliquez ce qui est attendu comme entr√©es et comme sorties, ce qu‚Äôelles font et pourquoi. Documentez votre code et vos donn√©es √† m√™me les feuilles de calcul ou dans un fichier de documentation s√©par√©. Tout fichier interm√©diaire devrait √™tre s√©par√© de vos donn√©es brutes. 5.1.1 Structure d‚Äôun projet Un projet de calcul devrait √™tre contenu en un seul dossier. Si vous n‚Äôavez que quelques projets, il est assez facile de garder l‚Äôinfo en m√©moire. Toutefois, en particulier en milieu d‚Äôentreprise, il se pourrait fort bien que vous ayez √† mener plusieurs projets de front. Certaines entreprises cr√©ent des num√©ros de projet: vous aurez avantage √† nommer vos dossiers avec ces num√©ros, incluant une br√®ve description. Pour ma part, j‚Äôordonne mes projets chronologiquement par ann√©e, avec un descriptif. üìÅ 2019_abeille-canneberge Notez que je n‚Äôutilise ni espace, ni caract√®re sp√©cial dans le nom du fichier, pour √©viter les erreurs potentielles avec des logiciels capricieux. √Ä l‚Äôint√©rieur du dossier racine du projet, j‚Äôinclus l‚Äôinformation g√©n√©rale: donn√©es source (souvent des fichiers Excel), manuscrit (m√©moire, th√®se, article, etc.) documentation particuli√®re (pour les articles, j‚Äôutilise Zotero, un gestionnaire de r√©f√©rence), photos et, √©videmment, mon dossier de code (par exemple rstats). üìÅ 2019_abeille-canneberge |-üìÅ documentation |-üìÅ manuscrit |-üìÅ photos |-üìÅ rstats |-üìÅ source Si vous r√©digez votre manuscrit √† m√™me votre code (en Latex, Lyx, markdown ou R markdown que nous verrons cela plus loin), vous pouvez tr√®s bien l‚Äôinclure dans votre fichier de calcul. √Ä l‚Äôint√©rieur du fichier de calcul, vous aurez votre projet RStudio et vos feuilles de calcul s√©quenc√©es. J‚Äôutilise 01-, et non pas 1- pour √©viter que le 10- suive le 1- dans le classement en ordre alpha-num√©rique au cas o√π j‚Äôaurais plus de 10 feuilles de calcul. J‚Äôinclus un fichier README.md (extension md pour markdown), qui contient les informations g√©n√©rales de mes calculs. Les donn√©es brutes (csv) sont plac√©es dans un dossier data, mes graphiques sont export√©s dans un dossier image, mes tableaux sont export√©s dans un dossier tables et mes fonctions externes sont export√©es dans un dossier lib. üìÅ rstats |-üìÅ data |-üìÅ images |-üìÅ lib |-üìÅ tables üìÑ bees.Rproj üìÑ 01_clean-data.R üìÑ 02_data-mining.R üìÑ 03_data-analysis.R üìÑ 04_data-modeling.R üìÑ README.md Je d√©cris les noms de fichiers dans la langue de communication utile pour le rendu final du projet, souvent en anglais lors de publications acad√©miques. J‚Äô√©vite les noms de fichier qui ne sont pas informatifs, par exemple 01.R ou Rplot1.png, ainsi que les majuscules, les caract√®res sp√©ciaux et les espaces comme dans Deuxi√®me essai.R (le README.md est une exception). Pour partager un dossier de projet sur R, on n‚Äôa qu‚Äô√† le compresser (zip), puis l‚Äôenvoyer. Pour que le code fonctionne sur un autre ordinateur, les liens vers les fichiers de donn√©es √† importer ou les graphiques export√©s doivent √™tre relatifs au fichier R ouvert dans votre projet, non pas le chemin complet sur votre ordinateur. Figure 5.3: Retrouvez votre chemin, dessin de Allison Horst Tout comme la BSE, l‚Äôorganisme sans but lucratif rOpenSci offre un guide sur la reproductibilit√©. 5.1.2 Le format R markdown Un code reproductible est un code bien d√©crit. La structure de projet pr√©sent√©e pr√©c√©demment propose de segmenter le code en plusieurs fichiers R. Cette mani√®re de proc√©der est optionnelle. Si le fichier de calcul n‚Äôest pas trop encombrant, on pourra n‚Äôen utiliser qu‚Äôun seul, par exemple stats.R. √Ä l‚Äôint√©rieur m√™me des feuilles de calcul R, vous devrez commenter votre code pour en expliquer les √©tapes, par exemple: ############# ## Titre 1 ## ############# # Titre 2 ## Titre 3 data &lt;- read_csv(&quot;data/abeilles.csv&quot;) # commentaire particulier RStudio a d√©velopp√© une approche plus conviviale avec son format R markdown. Le langage markdown permet de formater un texte avec un minimum de d√©corations, et R markdown permet d‚Äôint√©grer du texte et des codes. Ces notes de cours sont par ailleurs enti√®rement √©crites en R markdown. Figure 5.4: La magie de R markdown, dessin de Allison Horst 5.1.2.1 Le langage markdown Un fichier portant l‚Äôextension .md ou .markdown est un fichier texte clair (que vous pouvez ouvrir et √©diter dans n‚Äôimporte votre √©diteur texte pr√©f√©r√©), tout comme un fichier .R. Il existe n√©anmoins de nombreux √©diteurs de texte sp√©cialis√©s en √©dition markdown - mon pr√©f√©r√© est Typora. Les d√©corations principales en markdown sont les suivantes (les citations utilis√©es ci-apr√®s sont tir√©es du roman Dune, de Frank Herbert). Italique. Pour emphaser en italique, balisez le texte avec des ast√©risques. Par exemple, ‚ÄúPourrais-je porter parmi vous le nom de *Paul-Muad'dib*?‚Äù devient ‚ÄúPourrais-je porter parmi vous le nom de Paul-Muad‚Äôdib?‚Äù Gras. Pour emphaser en gras, balisez le texte avec des doubles ast√©risques. Par exemple, ‚ÄúL'esp√©rance **ternit** l'observation.‚Äù devient ‚ÄúL‚Äôesp√©rance ternit l‚Äôobservation‚Äù. Largeur fixe. Pour un texte √† largeur fixe (signifiant du code), balisez le texte avec des accents graves. Par exemple, ‚ÄúQuel nom donnez-vous √† la petite `souris`, celle qui saute ?‚Äù devient ‚ÄúQuel nom donnez-vous √† la petite souris, celle qui saute?‚Äù Listes. Pour effectuer une liste num√©rot√©e, utilisez le chiffre 1. Par exemple, 1. Paul 1. Leto 1. Alia devient Paul Jessica Alia De m√™me, pour une liste √† puces, changez le 1. par le - ou le *. Ent√™tes. Les titres sont pr√©c√©d√©s par des #. Un # pour un titre 1, deux ## pour un titre 2, etc. Par exemple, # Imperium ## Landsraad ### Maison des Atr√©ides ### Maison des Harkonnen ## CHOAM # Guilde des navigateurs Ins√©rera les titres appropri√©s (que je n‚Äôins√®re pas pour ne pas bousiller la structure de ce texte). Liens. Pour ins√©rer des liens, le texte est entre crochet directement suivi du lien entre parenth√®ses. Par exemple, ‚ÄúLongue vie aux [combattants](https://youtu.be/Cv87NJ2xX0k?t=59)‚Äù devient ‚ÄúLongue vie aux combattants‚Äù. √âquations. Les √©quations suivent la syntaxe Latex entre deux $$ pour les √©quations sur une ligne et entre des doubles $$$$ pour les √©quations sur un paragraphe. Par exemple, $c = \\sqrt{a^2 + b^2}$ devient \\(c = \\sqrt{a^2 + b^2}\\). Images. Pour ins√©rer une image, ![nom de l'image](images/spice-must-flow.png). Une liste exhaustive des balises markdown est disponible sous forme d‚Äôaide-m√©moire. L‚Äôextension de RStudio remedy, installable tout comme un module, fera appara√Ætre une section REMEDY dans le menu Addins, o√π vous trouverez toutes sortes d‚Äôoptions de formatage automatique (figure 5.5). Figure 5.5: Menu des extensions de RStudio, avec l‚Äôextension remedy 5.1.2.2 R markdown Dans RStudio, ouvrez un R markdown par File &gt; New file &gt; R Markdown. Si le module rmarkdown n‚Äôest pas install√©, RStudio vous demandera de l‚Äôinstaller. Une fen√™tre appara√Ætra. Figure 5.6: Nouveau fichier R markdown Les options d‚Äôexportation pourront √™tre modifi√©es par la suite. Un fichier d‚Äôexemple sera cr√©√©, et vous pourrez le modifier. Les parties de texte sont √©crits en markdown, et le code R est ench√¢ss√© entre les balises ```{r} et ```. Je nommerai ces parties de code des cellules de code. Des options de code l‚Äôint√©rieur peuvent √™tre utilis√©es √† l‚Äôint√©rieur des accolades {r}. Par exemple {r, filtre-outliers} donne le nom filtre-outliers au bloc de code, qui permet nomm√©ment de nommer les images cr√©er dans le bloc de code. {r, eval = FALSE} permet d‚Äôactiver (TRUE, valeur par d√©faut) ou de d√©sactiver (FALSE) le calcul de la cellule. {r, echo = FALSE} permet de n‚Äôafficher que la sortie de la cellule de code en n‚Äôaffichant pas le code, par exemple un graphique ou le sommaire d‚Äôune r√©gression. {r, results = FALSE} permet de n‚Äôafficher que le code, mais pas la sortie. {r, warning = FALSE, message = FALSE, error = FALSE} n‚Äôaffichera pas les avertissements, les messages automatiques et les messages d‚Äôerreur. {r, fig.width = 10, fig.height = 5, fig.align = &quot;center&quot;} affichera les graphiques dans les dimensions voulues, align√©e au centre (&quot;center&quot;), √† gauche (&quot;left&quot;) ou √† droite (&quot;right&quot;). Notez que vous pouvez ex√©cuter rapidement du code sur une ligne avec la formulation `r `, par exemple la moyenne des nombres `\\r a&lt;-round(runif(4, 0, 10)); a` est de `\\r mean(a)`, en enlevant les \\ devant les r (ajout√©es artificiellement pour √©viter que le code soit calcul√©) sera la moyenne des nombres 7, 2, 2, 8 est de 4.75 Une fois que vous serez satisfait de votre document, cliquer sur Knit et le fichier de sortie sera g√©n√©r√©. Le guide qui permet de g√©n√©rer le fichier de sortie est tout en haut du fichier. Nous l‚Äôappelons le YAML (acronyme r√©cursif de YAML Ain‚Äôt Markup Language). Prenez le YAML suivant. --- title: &quot;Dune&quot; author: &quot;Frank Herbert&quot; date: &quot;1965-08-01&quot; output: github_document --- Le titre, l‚Äôauteur et la date sont sp√©cifi√©es. Pour indiquer la date courante, on peut simplement la g√©n√©rer avec R en rempla√ßant &quot;1965-08-01&quot; par 2020-02-11. La sp√©cification output indique le type de document √† g√©n√©rer, par exemple html_document pour une page web, pdf_document pour un pdf, ou word_document pour un docx. Dans ce cas-ci, j‚Äôindique github_document pour cr√©er un fichier markdown comprenant nomm√©ment des liens relatifs vers les images des graphiques g√©n√©r√©s. Pourquoi un github_document? C‚Äôest le sujet de la prochaine sous-section. Mais avant cela, je vous r√©f√®re √† un autre aide-m√©moire. Figure 5.7: Aide-m√©moire pour R Markdown, Source: RStudio 5.2 Introduction √† GitHub Le system de suivi de version git (open source) a √©t√© cr√©√© par Linus Torvalds, aussi connu pour avoir cr√©√© Linux. git prend une photo de votre r√©pertoire de projet √† chaque fois que vous commettez un changement. Vous pourrez revenir sans probl√®me sur d‚Äôanciennes versions si quelque chose tourne mal, et vous pourrez publier le r√©sultat final sur un service d‚Äôh√©bergement utilisant git. Il existe plusieurs services pour rendre git utilisable en ligne, mais GitHub est d√©finitivement le plus utilis√© d‚Äôentre tous. La plateforme GitHub est presque devenue un r√©seau social de d√©veloppement. GitHub, maintenant la propri√©t√© de Microsoft, n‚Äôest en soi pas open source. Si comme moi vous avez un penchant pour l‚Äôopen source, je vous redirige vers la plateforme GitLab, qui fonctionne √† peu pr√®s de la m√™me mani√®re que GitHub, mais dans sa version gratuite GitLab vous octroie autant de r√©pertoires priv√©s que vous d√©sirez. Seul hic, alors que la plateforme GitHub sera fort probablement toujours vivante dans plusieurs ann√©es, on en est moins s√ªr pour GitLab. C‚Äôest pourquoi, en r√®gle g√©n√©rale, j‚Äôutilise GitHub √† des fins professionnelles mais GitLab √† des fins personnelles. Pour suivre cette partie du cours, je vous invite √† cr√©er un compte sur GitHub ou GitLab, √† votre choix. Cr√©ez un nouveau d√©p√¥t (New repository). Figure 5.8: Nouveau d√©p√¥t avec GitHub Figure 5.9: Nouveau d√©p√¥t avec GitLab Pour utiliser git, vous pourrez toujours travailler en ligne de commande, mais je vous sugg√®re d‚Äôutiliser GitHub Desktop (qui fonctionne aussi sur GitLab) - √©videmment, d‚Äôautres logiciels similaires existent. Github Desktop vous permettra d‚Äôabord de cloner un r√©pertoire en ligne. Le clonage vous permet de cr√©er une copie locale (sur votre ordinateur) du r√©pertoire. Figure 5.10: Cloner d√©p√¥t avec GitHub Figure 5.11: Cloner d√©p√¥t avec GitLab Une fois que le d√©p√¥t est clon√©, il est sur votre ordinateur. Lorsque vous effectuez un changement, vous devez commettre (commit), puis envoyer (push) vos changements vers le d√©p√¥t en ligne. Pour que votre document markdown soit lisible par GitHub et GitLab, il doit √™tre export√© sous forme de github_document. Un fichier .md sera cr√©√©, et inclura les d√©tails de votre feuille de calculs, images y compris! Figure 5.12: Commettre et d√©ployer un d√©p√¥t avec GitHub L‚Äôinterface de GitHub Desktop vous permet de revenir en arri√®re en √©liminant des commits pr√©c√©dents. Figure 5.13: Revenir en arri√®re avec GitHub desktop Vous pourrez ajouter des collaborateurs √† votre d√©p√¥t, pour que plusieurs personnes travaillent de front sur un m√™me d√©p√¥t. Il est aussi possible de cr√©er une branche d‚Äôun d√©p√¥t, fusionner la branche de d√©veloppement avec la branche principale, commenter les codes, sugg√©rer des changements, etc., mais cela sort du cadre d‚Äôun cours sur la reproductibilit√©. Enfin, pour renvoyer un article vers votre mat√©riel suppl√©mentaire, ins√©rez le lien dans la section m√©thodologie. Il peut s‚Äôagit du lien complet, ou bien d‚Äôun lien raccourci avec git.io. Par exemple, The data and the R code used to compute the results are both available as supplementary material at https://git.io/fhHEj. Notez que RStudio offre une interface pour utiliser git via un onglet afich√© en haut √† droite dans l‚Äôaffichage par d√©faut. Ne l‚Äôayant jamais utilis√©, et je ne me sens pas √† l‚Äôaise d‚Äôen sugg√©rer l‚Äôutilisation, mais libre √† vous d‚Äôexplorer cet outil et de vous l‚Äôapproprier! Figure 5.14: L‚Äôoutil Git de RStudio 5.3 Introduction √† Pakrat üì¶üêÄ Alors que les modules sont continuellement mis √† jour, on doit s‚Äôassurer que l‚Äôon sache exactement quelle version a √©t√© utilis√©e si l‚Äôon d√©sire √™tre stricte sur la reproductibilit√©. Lorsque je r√©vise un article, je demande √† ce que le nom des modules utilis√©s et leur num√©ro de version soient explicitement cit√©s et r√©f√©renc√©s. Par exemple, dans un article sur l‚Äôanalyse de compositions foliaires de laitues inocul√©es par une bact√©rie, j‚Äô√©crivais: Computations were performed in the R statistical language version 3.4.1 (R Development Core Team, 2017). The main packages used in the data analysis workflow were the vegan package version 2.4-3 (Oksanen et al., 2017) for ordination, the compositions package version 1.40-1 (van den Boogaart and Tolosana-Delgado, 2013) for ilr transformations, the nlme version 3.1-131 (Pinheiro et al., 2017) package to compute the random experimental effect, the mvoutlier package version 2.0.8 (Filzmoser and Gschwandtner, 2017) for multivariate outlier detection, and the ggplot2 package version 2.2.1 (Wickham and Chang, 2017) for data visualization. The data and computations are publicly available at https://github.com/essicolo/Nicolas-et-al_Infected-lettuce-ionomics. Nicolas et al., 2019 De cette mani√®re, une personne (que ce soit vos coll√®gues, quiconque voudra auditer ou √©valuer votre code ou vous-m√™me dans le futur) pourra reproduire le code publi√© sur GitHub en installant les versions de R et des modules cit√©s. Mais cela est fastidieux. C‚Äôest pourquoi l‚Äô√©quipe de RStudio (oui, encore ceux-l√†) ont d√©velopp√© le module packrat, qui permet d‚Äôinstaller les modules √† m√™me voter dossier de projet (le dossier contenant le fichier .Rproj). Pour l‚Äôutiliser √† tout moment en cours de projet, Figure 5.15: L‚Äôoutil Packrat de RStudio Le .gitignore contient tous les documents et les types de documents qui sont ignor√©s par git. L‚Äôoption par d√©faut est d‚Äôignorer le dossier lib, qui contient les modules install√©s, mais de garder le dossier src, qui contient la source des modules non install√©s (qui devront √™tre install√©s par les autres personnes utilisant votre projet). Mieux vaut garder les options par d√©faut. Initialiser Packrat revient √† scanner vos documents de projet pour trouver les modules utilis√©s et cr√©er un paquet contenant tout cela √† m√™me votre projet, dans un dossier packrat. üìÅ rstats |-üìÅ data |-üìÅ images |-üìÅ lib |-üìÅ packrat |-üìÅ tables üìÑ sentier-d-or.Rproj üìÑ stats.Rmd üìÑ README.md Ce dossier contiendra tout ce qu‚Äôil faut pour utiliser les modules du projet d‚Äôune personne que l‚Äôon nommera Leto. Lorsqu‚Äôune autre personne, appellons-la Ghanima, utilisera le projet de Leto, RStudio v√©rifiera si le module packrat est bien install√©, et l‚Äôinstallera s‚Äôil ne l‚Äôest pas (Leto et Ghanima sont deux personnage de la s√©rie de science-fiction Dune). Pour utiliser les modules du projet et non pas les modules de son ordinateur, Ghanima lancera la fonction packrat::restore(). Si Leto d√©cide de mettre √† jour ses modules en cours de projet, il lancera la fonction packrat::snapshot() pour que ces nouveaux modules soit int√©gr√©s √† son projet. Lorsque Leto commettra (commit) ses changements dans git et les publiera (push) sur GitHub, puis lorsque Ghanima mettra √† jour (fetch) son d√©p√¥t local git li√© au d√©p√¥t GitHub, elle devra √† nouveau lancer packrat::restore() pour que les modules soient bel et bien ceux utilis√©s par Leto. 5.4 Pour terminer, le reprex Lorsque j‚Äôai d√©couvert un bogue dans le module weathercan, j‚Äôai ouvert une issue sur GitHub en indiquant le message d‚Äôerreur obtenu, en esp√©rant que l‚Äôorigine du bogue puisse √™tre facilement d√©duit. Un d√©veloppeur de weathercan m‚Äôa demand√© un reprex. J‚Äôai √©t√© d√©√ßu lorsque j‚Äôai compris que le reprex n‚Äô√©tait pas une esp√®ce de dinosaure, mais plut√¥t un exemple reproductible (reproducible example). üìó Reprex: Un exemple reproductible. J‚Äôai essay√© d‚Äôisoler le probl√®me pour reproduire l‚Äôerreur avec le minimum de code possible. √Ä partir d‚Äôun code de plus de 7000 lignes (les pr√©sentes notes de cours), j‚Äôen suis arriv√© √† ceci: stations &lt;- data.frame(A = 1) library(&quot;weathercan&quot;) mont_bellevue &lt;- weather_dl( station_ids = c(5397, 48371), start = &quot;2019-02-01&quot;, end = &quot;2019-02-07&quot;, interval = &quot;hour&quot;, verbose = TRUE ) , qui me retournait l‚Äôerreur Getting station: 5397 Formatting station data: 5397 Error in strptime(xx, f, tz = tz) : valeur &#39;tz&#39; incorrecte Le bogue: la fonction weather_dl() utilisait √† l‚Äôinterne un objet nomm√© stations, qui entrait en conflit avec un objet stations s‚Äôil √©tait d√©fini hors de la fonction. Synth√©tiser une question n‚Äôest pas facile (cr√©er cet exemple reprductible m‚Äôa pris pr√®s de 2 heures). Mais r√©pondre √† une question non synth√©tis√©e, c‚Äôest encore plus difficile. C‚Äôest pourquoi on (moi y compris) vous demandera syst√©matiquement un reprex lorsque vous poserez une question li√©e √† une erreur syst√©matique, le plus souvent en programmation. Un exemple reproductible permet √† quelqu‚Äôun de recr√©er l‚Äôerreur que vous avez obtenue simplement en copiant-collant votre code. - Hadley Wickham Selon Hadley Wickham (gourou de R), un reprex devrait comprendre quatre √©l√©ments (je joue √† l‚Äôh√©r√©tique en me permettant d‚Äôadapter le document du gourou): Les modules devraient √™tre charg√©s en d√©but de code. Puis vous chargez des donn√©es, qui peuvent √™tre des donn√©es d‚Äôexemple ou des donn√©es incluses √† m√™me le code R (comme des donn√©es g√©n√©r√©es au hasard). Assurez-vous que voter code est un exemple minimal (retirer le superflu) et qu‚Äôil soit facilement lisible. Incluez la sortie de la fonction sessionInfo(), qui indique la plateforme mat√©rielle et logicielle sur laquelle vous avez g√©n√©r√© l‚Äôerreur. Ceci est important en particulier s‚Äôil s‚Äôagit d‚Äôun bogue. Lorsque vous pensez avoir g√©n√©r√© votre reprex, red√©marrez R (Session &gt; Restart R dans RStudio), puis lancez votre code pour vous assurer que l‚Äôerreur puisse √™tre g√©n√©r√©e dans un nouvel environnement tout propre. "],
["chapitre-biostats.html", "6 Biostatistiques 6.1 Populations et √©chantillons 6.2 Les variables 6.3 Les probabilit√©s 6.4 Les distributions 6.5 Statistiques descriptives 6.6 Tests d‚Äôhypoth√®ses √† un et deux √©chantillons 6.7 L‚Äôanalyse de variance 6.8 Les mod√®les statistiques", " 6 Biostatistiques Ô∏è¬†Objectifs sp√©cifiques: √Ä la fin de ce chapitre, vous serez en mesure de d√©finir les concepts de base en statistique: population, √©chantillon, variable, probabilit√© et distribution serez en mesure de calculer des statistiques descriptives de base: moyenne et √©cart-type, quartiles, maximum et minimum comprendrez les notions de test d‚Äôhypoth√®se, d‚Äôeffet et de p-value, ainsi qu‚Äô√©viter les erreurs communes dans leur interpr√©tation saurez effectuer une mod√©lisation statistique lin√©aire simple, multiple et mixte, entre autre sur des cat√©gories saurez effectuer une mod√©lisation statistique non lin√©aire simple, multiple et mixte Aux chapitres pr√©c√©dents, nous avons vu comment visualiser, organiser et manipuler des tableaux de donn√©es. La statistique est une collection de disciplines li√©es √† la collecte, l‚Äôorganisation, l‚Äôanalyse, l‚Äôinterpr√©tation et la pr√©sentation de donn√©es. Les biostatistiques sont des applications de ces disciplines √† la biosph√®re. Dans Principles and procedures of statistics: A biometrical approach, Steel, Torie et Dickey (1997) d√©finissent les statistiques ainsi: Les statistiques forment la science, pure et appliqu√©e, de la cr√©ation, du d√©veloppement, et de l‚Äôapplication de techniques par lesquelles l‚Äôincertitude de l‚Äôinduction inf√©rentielle peut √™tre √©valu√©e. (ma traduction) Alors que l‚Äôinf√©rence consiste √† g√©n√©raliser des √©chantillons √† l‚Äôensemble d‚Äôune population, l‚Äôinduction est un type de raisonnement qui permet de g√©n√©raliser des observations sous forme de th√©ories. En d‚Äôautres mots, les statistiques permettent d‚Äô√©valuer l‚Äôincertitude sur des processus, de passer par infrence de l‚Äô√©chantillon √† la population, puis par induction de passer de cette repr√©sentation d‚Äôune population en lois g√©n√©rales la concernant. La d√©finition de Whitlock et Schuluter (2015), dans The Analysis of Biological Data, est plus simple et n‚Äôinsiste que sur l‚Äôinf√©rence: La statistique est l‚Äô√©tude des m√©thodes pour mesurer des aspects de populations √† partir d‚Äô√©chantillons et pour quantifier l‚Äôincertitude des mesures. (ma traduction) Les statistiques consistent √† faire du sens (anglicisme assum√©) avec des observations dans l‚Äôobjectif de r√©pondre √† une question que vous aurez formul√©e clairement, pr√©alablement √† votre exp√©rience. The more time I spend as The Statistician in the room, the more I think the best skill you can cultivate is the ability to remain calm and repeatedly ask ‚ÄúWhat question are you trying to answer?‚Äù ‚Äî Bryan Howie (@bryan_howie) 13 d√©cembre 2018 Le flux de travail conventionnel en statistiques consiste √† collecter des √©chantillons, transformer (pr√©traiter) les donn√©es, effectuer des tests, analyser les r√©sultats, les interpr√©ter et les visualiser. \\[Collecte \\rightarrow Pr√©traitement \\rightarrow Tests~statistiques \\rightarrow Analyse \\rightarrow Interpr√©tation \\rightarrow Visualisation \\] Ce chapitre √† lui seul est trop court pour permettre d‚Äôint√©grer toutes les connaissances n√©cessaires √† une utilisation raisonn√©e des statistiques, mais fourni les bases pour aller plus loin. Notez que les erreurs d‚Äôinterpr√©tation statistiques sont courantes et la consultation de sp√©cialistes n‚Äôest souvent pas un luxe. Mais bien que les statistiques soient complexes, la plupart des op√©rations statistiques peuvent √™tre effectu√©es sans l‚Äôassistance de statisticien.ne.s‚Ä¶ √† condition de comprendre suffisamment les concepts utilis√©s. Dans ce chapitre, nous verrons comment r√©pondre correctement √† une question valide et ad√©quate avec l‚Äôaide d‚Äôoutils de calcul scientifique. Nous couvrirons les notions de bases des distributions et des variables al√©atoires qui nous permettront d‚Äôeffectuer des tests statistiques communs avec R. Nous couvrirons aussi les erreurs commun√©ment commises en recherche acad√©mique et les moyens simples de les √©viter. En plus des modules de base de R nous utiliserons les modules de la tidyverse, le module de donn√©es agricoles agridat, ainsi que le module nlme sp√©cialis√© pour la mod√©lisation mixte. Avant de survoler les applications statistiques avec R, je vais rapidement pr√©senter quelques notions importantes en statistiques : populations et √©chantillons, variables, probabilit√©s et distributions. Puis nous allons effectuer des tests d‚Äôhypoth√®se univari√©s (notamment les tests de t et les analyses de variance) et d√©tailler la notion controvers√©e de p-value. Je vais m‚Äôattarder plus longuement aux mod√®les lin√©aires g√©n√©ralis√©s, incluant en particulier des effets fixes et al√©atoires (mod√®les mixtes), qui fournissent une trousse d‚Äôanalyse polyvalente en analyse multivari√©e. Je terminerai avec les perspectives multivari√©es que sont les matrices de covariance et de corr√©lation. 6.1 Populations et √©chantillons Le principe d‚Äôinf√©rence consiste √† g√©n√©raliser des conclusions √† l‚Äô√©chelle d‚Äôune population √† partir d‚Äô√©chantillons issus de cette population. Alors qu‚Äôune population contient tous les √©l√©ments √©tudi√©s, un √©chantillon d‚Äôune population est une observation unique. Une exp√©rience bien con√ßue fera en sorte que les √©chantillons soient repr√©sentatifs de la population qui, la plupart du temps, ne peut √™tre observ√©e enti√®rement pour des raisons pratiques. Les principes d‚Äôexp√©rimentation servant de base √† la conception d‚Äôune bonne m√©thodologie sont pr√©sent√©s dans le cours Dispositifs exp√©rimentaux (BVG-7002). √âgalement, je recommande le livre Principes d‚Äôexp√©rimentation: planification des exp√©riences et analyse de leurs r√©sultats de Pierre Dagnelie (2012), disponible en ligne en format PDF. Un bon aper√ßu des dispositifs exp√©rimentaux est aussi pr√©sent√© dans Introductory Statistics with R, de Peter Dalgaard (2008), que vous pouvez t√©l√©charger du site de la biblioth√®que de l‚ÄôUniversit√© Laval vous avez un identifiant autoris√©. Une population est √©chantillonn√©e pour induire des param√®tres: un rendement typique dans des conditions m√©t√©orologiques, √©daphiques et manag√©riales donn√©es, la masse typique des faucons p√®lerins, m√¢les et femelles, le microbiome typique d‚Äôun sol agricole ou forestier, etc. Une statistique est une estimation d‚Äôun param√®tre calcul√©e √† partir des donn√©es, par exemple une moyenne et un √©cart-type, ou un intercept et une pente. Par exemple, la moyenne (\\(\\mu\\)) et l‚Äô√©cart-type (\\(\\sigma\\)) d‚Äôune population sont estim√©s par les moyennes (\\(\\bar{x}\\)) et √©carts-types (\\(s\\)) calcul√©s sur les donn√©es issues de l‚Äô√©chantillonnage. Chaque param√®tre est li√©e √† une perspective que l‚Äôon d√©sire conna√Ætre chez une population. Ces angles d‚Äôobservations sont les variables. 6.2 Les variables Nous avons abord√© au chapitre 3 la notion de variable par l‚Äôinterm√©diaire d‚Äôune donn√©e. Une variable est l‚Äôobservation d‚Äôune caract√©ristique d√©crivant un √©chantillon. Si la charact√©ristique varie d‚Äôun √©chantillon √† un autre sans que vous en expliquiez la raison (i.e.¬†si identifier la source de la variabilit√© ne fait pas partie de votre exp√©rience), on parlera de variable al√©atoire. M√™me le hasard est r√©gi par certaines lois: ce qui est al√©atoire dans une variable peut √™tre d√©crit par des lois de probabilit√©, que nous verrons plus bas. Mais restons aux variables pour l‚Äôinstant. Par convention, on peut attribuer aux variables un symbole math√©matique. Par exemple, on peut donner √† la masse volumique d‚Äôun sol (qui est le r√©sultat d‚Äôune m√©thodologie pr√©cise) le symbole \\(\\rho\\). Lorsque l‚Äôon attribue une valeur √† \\(\\rho\\), on parle d‚Äôune donn√©e. Chaque donn√©e d‚Äôune observation a un indice qui lui est propre, que l‚Äôon d√©signe souvent par \\(i\\), que l‚Äôon place en indice \\(\\rho_i\\). Pour la premi√®re donn√©e, on a \\(i=1\\), donc \\(\\rho_1\\). Pour un nombre \\(n\\) d‚Äô√©chantillons, on aura \\(\\rho_1\\), \\(\\rho_2\\), \\(\\rho_3\\), ‚Ä¶, \\(\\rho_n\\), formant le vecteur \\(\\rho = \\left[\\rho_1, \\rho_2, \\rho_3, ..., \\rho_n \\right]\\). En R, une variable est associ√©e √† un vecteur ou une colonne d‚Äôun tableau. rho &lt;- c(1.34, 1.52, 1.26, 1.43, 1.39) # matrice 1D data &lt;- data.frame(rho = rho) # tableau data ## rho ## 1 1.34 ## 2 1.52 ## 3 1.26 ## 4 1.43 ## 5 1.39 Il existe plusieurs types de variables, qui se regroupe en deux grandes cat√©gories: les variables quantitatives et les variables qualitatives. 6.2.1 Variables quantitatives Ces variables peuvent √™tre continues dans un espace √©chantillonnal r√©el ou discr√®tes dans un espace √©chantillonnal ne consid√©rant que des valeurs fixes. Notons que la notion de nombre r√©el est toujours une approximation en sciences exp√©rimentales comme en calcul num√©rique, √©tant donn√©e que l‚Äôon est limit√© par la pr√©cision des appareils comme par le nombre d‚Äôoctets √† utiliser. Bien que les valeurs fixes des distributions discr√®tes ne soient pas toujours des valeurs enti√®res, c‚Äôest bien souvent le cas en biostatistiques comme en d√©mographie, o√π les d√©comptes d‚Äôindividus sont souvent pr√©sents (et o√π la notion de fraction d‚Äôindividus n‚Äôest pas accept√©e). 6.2.2 Variables qualitatives On exprime parfois qu‚Äôune variable qualitative est une variable impossible √† mesurer num√©riquement: une couleur, l‚Äôappartenance √† esp√®ce ou √† une s√©rie de sol. Pourtant, dans bien des cas, les variables qualitatives peut √™tre encod√©es en variables quantitatives. Par exemple, on peut accoler des pourcentages de sable, limon et argile √† un loam sableux, qui autrement est d√©crit par la classe texturale d‚Äôun sol. Pour une couleur, on peut lui associer une longueur d‚Äôonde ou des pourcentages de rouge, vert et bleu, ainsi qu‚Äôun ton. En ce qui a trait aux variables ordonn√©es, il est possible de supposer un √©talement. Par exemple, une variable d‚Äôintensit√© faible-moyenne-forte peut √™tre transform√©e lin√©airement en valeurs quantitatives -1, 0 et 1. Attention toutefois, l‚Äô√©talement peut parfois √™tre quadratique ou logarithmique. Les s√©ries de sol peuvent √™tre encod√©es par la proportion de gleyfication (Parent et al., 2017). Quant aux cat√©gories difficilement transformables en quantit√©s, on pourra passer par l‚Äôencodage cat√©goriel, souvent appel√© dummyfication, qui nous verrons plus loin. L‚Äôanalyse qualitative consiste en l‚Äôanalyse de verbatims, essentiellement utile en sciences sociales: nous n‚Äôen n‚Äôaurons pas besoin ici. Nous consid√©rerons les variables qualitatives comme des variables quantitatives qui n‚Äôont pas subi de pr√©traitement. 6.3 Les probabilit√©s ¬´ Nous sommes si √©loign√©s de conna√Ætre tous les agens de la nature, et leurs divers modes d‚Äôaction ; qu‚Äôil ne serait pas philosophique de nier les ph√©nom√®nes, uniquement parce qu‚Äôils sont inexplicables dans l‚Äô√©tat actuel de nos connaissances. Seulement, nous devons les examiner avec une attention d‚Äôautant plus scrupuleuse, qu‚Äôil para√Æt plus difficile de les admettre ; et c‚Äôest ici que le calcul des probabilit√©s devient indispensable, pour d√©terminer jusqu‚Äô√† quel point il faut multiplier les observations ou les exp√©riences, afin d‚Äôobtenir en faveur des agens qu‚Äôelles indiquent, une probabilit√© sup√©rieure aux raisons que l‚Äôon peut avoir d‚Äôailleurs, de ne pas les admettre. ¬ª ‚Äî Pierre-Simon de Laplace Une probabilit√© est la vraisemblance qu‚Äôun √©v√®nement se r√©alise chez un √©chantillon. Les probabilit√©s forment le cadre des syst√®mes stochastiques, c‚Äôest-√†-dire des syst√®mes trop complexes pour en conna√Ætre exactement les aboutissants, auxquels on attribue une part de hasard. Ces syst√®mes sont pr√©dominants dans les processus vivants. On peut d√©gager deux perspectives sur les probabilit√©s: l‚Äôune passe par une interpr√©tation fr√©quentielle, l‚Äôautre bay√©sienne. L‚Äôinterpr√©tation fr√©quentielle repr√©sente la fr√©quence des occurrences apr√®s un nombre infini d‚Äô√©v√®nements. Par exemple, si vous jouez √† pile ou face un grand nombre de fois, le nombre de pile sera √©gal √† la moiti√© du nombre de lanc√©s. L‚Äôapproche fr√©quentielle teste si les donn√©es concordent avec un mod√®le du r√©el. Il s‚Äôagit de l‚Äôinterpr√©tation commun√©ment utilis√©e. L‚Äôinterpr√©tation bay√©sienne vise √† quantifier l‚Äôincertitude des ph√©nom√®nes. Dans cette perspective, plus l‚Äôinformation s‚Äôaccumule, plus l‚Äôincertitude diminue. Cette approche gagne en notori√©t√© notamment parce qu‚Äôelle permet de d√©crire des ph√©nom√®nes qui, intrins√®quement, ne peuvent √™tre r√©p√©t√©s infiniment (absence d‚Äôasymptote), comme celles qui sont bien d√©finis dans le temps ou sur des populations limit√©s. L‚Äôapproche bay√©sienne √©value la probabilit√© que le mod√®le soit r√©el. Une erreur courante consiste √† aborder des statistiques fr√©quentielles comme des statistiques bay√©siennes. Par exemple, si l‚Äôon d√©sire √©valuer la probabilit√© de l‚Äôexistence de vie sur Mars, on devra passer par le bay√©sien, car avec les stats fr√©quentielles, l‚Äôon devra plut√¥t conclure si les donn√©es sont conformes ou non avec l‚Äôhypoth√®se de la vie sur Mars (exemple tir√©e du blogue Dynamic Ecology). Des rivalit√©s factices s‚Äôinstallent enter les tenants des diff√©rentes approches, dont chacune, en r√©alit√©, r√©pond √† des questions diff√©rentes dont il convient r√©fl√©chir sur les limitations. Bien que les statistiques bay√©siennes soient de plus en plus utilis√©es, nous ne couvrirons dans ce chapitre que l‚Äôapproche fr√©quentielle. L‚Äôapproche bay√©sienne est n√©anmoins trait√©e dans le chapitre 7, qui est facultatif au cours. 6.4 Les distributions Une variable al√©atoire peut prendre des valeurs selon des mod√®les de distribution des probabilit√©s. Une distribution est une fonction math√©matique d√©crivant la probabilit√© d‚Äôobserver une s√©rie d‚Äô√©v√®nements. Ces √©v√®nements peuvent √™tre des valeurs continues, des nombres entiers, des cat√©gories, des valeurs bool√©ennes (Vrai/Faux), etc. D√©pendemment du type de valeur et des observations obtenues, on peut associer des variables √† diff√©rentes lois de probabilit√©. Toujours, l‚Äôaire sous la courbe d‚Äôune distribution de probabilit√© est √©gale √† 1. En statistiques inf√©rentielles, les distributions sont les mod√®les, comprenant certains param√®tres comme la moyenne et la variance pour les distributions normales, √† partir desquelles les donn√©es sont g√©n√©r√©es. Il existe deux grandes familles de distribution: discr√®tes et continues. Les distributions discr√®tes sont contraintes √† des valeurs pr√©d√©finies (finies ou infinies), alors que les distributions continues prennent n√©cessairement un nombre infini de valeur, dont la probabilit√© ne peut pas √™tre √©valu√©e ponctuellement, mais sur un intervalle. L‚Äôesp√©rance math√©matique est une fonction de tendance centrale, souvent d√©crite par un param√®tre. Il s‚Äôagit de la moyenne d‚Äôune population pour une distribution normale. La variance, quant √† elle, d√©crit la variabilit√© d‚Äôune population, i.e.¬†son √©talement autour de l‚Äôesp√©rance. Pour une distribution normale, la variance d‚Äôune population est aussi appel√©e variance, souvent pr√©sent√©e par l‚Äô√©cart-type (√©gal √† la racine carr√©e de la variance). 6.4.1 Distribution binomiale En tant que sc√©nario √† deux issues possibles, des tirages √† pile ou face suivent une loi binomiale, comme toute variable bool√©enne prenant une valeur vraie ou fausse. En biostatistiques, les cas communs sont la pr√©sence/absence d‚Äôune esp√®ce, d‚Äôune maladie, d‚Äôun trait phylog√©n√©tique, ainsi que les cat√©gories encod√©es. Lorsque l‚Äôop√©ration ne comprend qu‚Äôun seul √©chantillon (i.e.¬†un seul tirage √† pile ou face), il s‚Äôagit d‚Äôun cas particulier d‚Äôune loi binomiale que l‚Äôon nomme une loi de Bernouilli. Pour 25 tirages √† pile ou face ind√©pendants (i.e.¬†dont l‚Äôordre des tirages ne compte pas), on peut dessiner une courbe de distribution dont la somme des probabilit√©s est de 1. La fonction dbinom est une fonction de distribution de probabilit√©s. Les fonctions de distribution de probabilit√©s discr√®tes sont appel√©es des fonctions de masse. library(&quot;tidyverse&quot;) x &lt;- 0:25 y &lt;- dbinom(x = x, size = 25, prob = 0.5) print(paste(&#39;La somme des probabilit√©s est de&#39;, sum(y))) ## [1] &quot;La somme des probabilit√©s est de 1&quot; ggplot(data = tibble(x, y), mapping = aes(x, y)) + geom_segment(aes(x = x, xend = x, y = 0, yend = y), color = &quot;grey50&quot;) + geom_point() 6.4.2 Distribution de Poisson La loi de Poisson (avec un P majuscule, introduite par le math√©maticien fran√ßais Sim√©on Denis Poisson et non pas l‚Äôanimal) d√©crit des distributions discr√®tes de probabilit√© d‚Äôun nombre d‚Äô√©v√®nements se produisant dans l‚Äôespace ou dans le temps. Les distributions de Poisson d√©crivent ce qui tient du d√©compte. Il peut s‚Äôagir du nombre de grenouilles traversant une rue quotidiennement, du nombre de plants d‚Äôascl√©piades se trouvant sur une terre cultiv√©e, ou du nombre d‚Äô√©v√®nements de pr√©cipitation au mois de juin, etc. La distribution de Poisson n‚Äôa qu‚Äôun seul param√®tre, \\(\\lambda\\), qui d√©crit tant la moyenne des d√©comptes. Par exemple, en un mois de 30 jours, et une moyenne de 8 √©v√®nements de pr√©cipitation pour ce mois, on obtient la distribution suivante. x &lt;- 1:30 y &lt;- dpois(x, lambda = 8) print(paste(&#39;La somme des probabilit√©s est de&#39;, sum(y))) ## [1] &quot;La somme des probabilit√©s est de 0.999664536835124&quot; ggplot(data = data.frame(x, y), mapping = aes(x, y)) + geom_segment(aes(x = x, xend = x, y = 0, yend = y), color = &quot;grey50&quot;) + geom_point() 6.4.3 Distribution uniforme La distribution la plus simple est probablement la distribution uniforme. Si la variable est discr√®te, chaque cat√©gorie est associ√©e √† une probabilit√© √©gale. Si la variable est continue, la probabilit√© est directement proportionnelle √† la largeur de l‚Äôintervalle. On utilise rarement la distribution uniforme en biostatistiques, sinon pour d√©crire des a priori vagues pour l‚Äôanalyse bay√©sienne (ce sujet est trait√© dans le chapitre 7). Nous utilisons la fonction dunif. √Ä la diff√©rence des distributions discr√®tes, les fonctions de distribution de probabilit√©s continues sont appel√©es des fonctions de densit√© d‚Äôune loi de probabilit√© (probability density function). increment &lt;- 0.01 x &lt;- seq(-4, 4, by = increment) y1 &lt;- dunif(x, min = -3, max = 3) y2 &lt;- dunif(x, min = -2, max = 2) y3 &lt;- dunif(x, min = -1, max = 1) print(paste(&#39;La somme des probabilit√©s est de&#39;, sum(y3 * increment))) ## [1] &quot;La somme des probabilit√©s est de 1.005&quot; gg_unif &lt;- data.frame(x, y1, y2, y3) %&gt;% gather(variable, value, -x) ggplot(data = gg_unif, mapping = aes(x = x, y = value)) + geom_line(aes(colour = variable)) 6.4.4 Distribution normale La plus r√©pandue de ces lois est probablement la loi normale, parfois nomm√©e loi gaussienne et plus rarement loi laplacienne. Il s‚Äôagit de la distribution classique en forme de cloche. La loi normale est d√©crite par une moyenne, qui d√©signe la tendance centrale, et une variance, qui d√©signe l‚Äô√©talement des probabilit√©s autour de la moyenne. La racine carr√©e de la variance est l‚Äô√©cart-type. Les distributions de mesures exclusivement positives (comme le poids ou la taille) sont parfois avantageusement approxim√©es par une loi log-normale, qui est une loi normale sur le logarithme des valeurs: la moyenne d‚Äôune loi log-normale est la moyenne g√©om√©trique. increment &lt;- 0.01 x &lt;- seq(-10, 10, by = increment) y1 &lt;- dnorm(x, mean = 0, sd = 1) y2 &lt;- dnorm(x, mean = 0, sd = 2) y3 &lt;- dnorm(x, mean = 0, sd = 3) print(paste(&#39;La somme des probabilit√©s est de&#39;, sum(y3 * increment))) ## [1] &quot;La somme des probabilit√©s est de 0.999147010743368&quot; gg_norm &lt;- data.frame(x, y1, y2, y3) %&gt;% gather(variable, value, -x) ggplot(data = gg_norm, mapping = aes(x = x, y = value)) + geom_line(aes(colour = variable)) Quelle est la probabilit√© d‚Äôobtenir le nombre 0 chez une observation continue distribu√©e normalement dont la moyenne est 0 et l‚Äô√©cart-type est de 1? R√©ponse: 0. La loi normale √©tant une distribution continue, les probabilit√©s non-nulles ne peuvent √™tre calcul√©s que sur des intervalles. Par exemple, la probabilit√© de retrouver une valeur dans l‚Äôintervalle entre -1 et 2 est calcul√©e en soustrayant la probabilit√© cumul√©e √† -1 de la probabilit√© cumul√©e √† 2. increment &lt;- 0.01 x &lt;- seq(-5, 5, by = increment) y &lt;- dnorm(x, mean = 0, sd = 1) prob_between &lt;- c(-1, 2) gg_norm &lt;- data.frame(x, y) gg_auc &lt;- gg_norm %&gt;% filter(x &gt; prob_between[1], x &lt; prob_between[2]) %&gt;% rbind(c(prob_between[2], 0)) %&gt;% rbind(c(prob_between[1], 0)) ggplot(data.frame(x, y), aes(x, y)) + geom_polygon(data = gg_auc, fill = &#39;#71ad50&#39;) + # #71ad50 est un code de couleur format hexad√©cimal geom_line() prob_norm_between &lt;- pnorm(q = prob_between[2], mean = 0, sd = 1) - pnorm(q = prob_between[1], mean = 0, sd = 1) print(paste(&quot;La probabilit√© d&#39;obtenir un nombre entre&quot;, prob_between[1], &quot;et&quot;, prob_between[2], &quot;est d&#39;environ&quot;, round(prob_norm_between, 2) * 100, &quot;%&quot;)) ## [1] &quot;La probabilit√© d&#39;obtenir un nombre entre -1 et 2 est d&#39;environ 82 %&quot; La courbe normale peut √™tre utile pour √©valuer la distribution d‚Äôune population. Par exemple, on peut calculer les limites de r√©gion sur la courbe normale qui contient 95% des valeurs possibles en tranchant 2.5% de part et d‚Äôautre de la moyenne. Il s‚Äôagit ainsi de l‚Äôintervalle de confiance sur la d√©viation de la distribution. increment &lt;- 0.01 x &lt;- seq(-5, 5, by = increment) y &lt;- dnorm(x, mean = 0, sd = 1) alpha &lt;- 0.05 prob_between &lt;- c(qnorm(p = alpha/2, mean = 0, sd = 1), qnorm(p = 1 - alpha/2, mean = 0, sd = 1)) gg_norm &lt;- data.frame(x, y) gg_auc &lt;- gg_norm %&gt;% filter(x &gt; prob_between[1], x &lt; prob_between[2]) %&gt;% rbind(c(prob_between[2], 0)) %&gt;% rbind(c(prob_between[1], 0)) ggplot(data = data.frame(x, y), mapping = aes(x, y)) + geom_polygon(data = gg_auc, fill = &#39;#71ad50&#39;) + # #71ad50 est un code de couleur format hexad√©cimal geom_line() + geom_text(data = data.frame(x = prob_between, y = c(0, 0), labels = round(prob_between, 2)), mapping = aes(label = labels)) On pourrait aussi √™tre int√©ress√© √† l‚Äôintervalle de confiance sur la moyenne. En effet, la moyenne suit aussi une distribution normale, dont la tendance centrale est la moyenne de la distribution, et dont l‚Äô√©cart-type est not√© erreur standard. On calcule cette erreur en divisant la variance par le nombre d‚Äôobservation, ou en divisant l‚Äô√©cart-type par la racine carr√©e du nombre d‚Äôobservations. Ainsi, pour 10 √©chantillons: increment &lt;- 0.01 x &lt;- seq(-5, 5, by = increment) y &lt;- dnorm(x, mean = 0, sd = 1) alpha &lt;- 0.05 prob_between &lt;- c(qnorm(p = alpha/2, mean = 0, sd = 1) / sqrt(10), qnorm(p = 1 - alpha/2, mean = 0, sd = 1) / sqrt(10)) gg_norm &lt;- data.frame(x, y) gg_auc &lt;- gg_norm %&gt;% filter(x &gt; prob_between[1], x &lt; prob_between[2]) %&gt;% rbind(c(prob_between[2], 0)) %&gt;% rbind(c(prob_between[1], 0)) ggplot(data = data.frame(x, y), mapping = aes(x, y)) + geom_polygon(data = gg_auc, fill = &#39;#71ad50&#39;) + # #71ad50 est un code de couleur format hexad√©cimal geom_line() + geom_text(data = data.frame(x = prob_between, y = c(0, 0), labels = round(prob_between, 2)), mapping = aes(label = labels)) 6.5 Statistiques descriptives On a vu comment g√©n√©rer des statistiques sommaires en R avec la fonction summary(). Reprenons les donn√©es d‚Äôiris. data(&quot;iris&quot;) summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 setosa :50 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 versicolor:50 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 virginica :50 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 Pour pr√©cis√©ment effectuer une moyenne et un √©cart-type sur un vecteur, passons par les fonctions mean() et sd(). mean(iris$Sepal.Length) ## [1] 5.843333 sd(iris$Sepal.Length) ## [1] 0.8280661 Pour effectuer un sommaire de tableau pilot√© par une fonction, nous passons par la gamme de fonctions summarise(), de dplyr. Dans ce cas, avec group_by(), nous fragmentons le tableau par esp√®ce pour effectuer un sommaire sur toutes les variables. iris %&gt;% group_by(Species) %&gt;% summarise_all(mean) ## # A tibble: 3 x 5 ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.01 3.43 1.46 0.246 ## 2 versicolor 5.94 2.77 4.26 1.33 ## 3 virginica 6.59 2.97 5.55 2.03 Vous pourriez √™tre int√©ress√© par les quartiles √† 25, 50 et 75%. Mais la fonction summarise() n‚Äôautorise que les fonctions dont la sortie est d‚Äôun seul objet, alors faisons sorte que l‚Äôobjet soit une liste - lorsque l‚Äôon imbrique une fonction funs, le tableau √† ins√©rer dans la fonction est indiqu√© par un .. iris %&gt;% group_by(Species) %&gt;% summarise_all(list(q25 = ~ quantile(., probs = 0.25), q50 = ~ quantile(., probs = 0.50), q75 = ~ quantile(., probs = 0.75))) ## # A tibble: 3 x 13 ## Species Sepal.Length_q25 Sepal.Width_q25 Petal.Length_q25 Petal.Width_q25 Sepal.Length_q50 Sepal.Width_q50 Petal.Length_q50 ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 4.8 3.2 1.4 0.2 5 3.4 1.5 ## 2 versic‚Ä¶ 5.6 2.52 4 1.2 5.9 2.8 4.35 ## 3 virgin‚Ä¶ 6.22 2.8 5.1 1.8 6.5 3 5.55 ## # ‚Ä¶ with 5 more variables: Petal.Width_q50 &lt;dbl&gt;, Sepal.Length_q75 &lt;dbl&gt;, Sepal.Width_q75 &lt;dbl&gt;, Petal.Length_q75 &lt;dbl&gt;, ## # Petal.Width_q75 &lt;dbl&gt; En mode programmation classique de R, on pourra g√©n√©rer les quartiles √† la pi√®ce. quantile(iris$Sepal.Length[iris$Species == &#39;setosa&#39;]) ## 0% 25% 50% 75% 100% ## 4.3 4.8 5.0 5.2 5.8 quantile(iris$Sepal.Length[iris$Species == &#39;versicolor&#39;]) ## 0% 25% 50% 75% 100% ## 4.9 5.6 5.9 6.3 7.0 quantile(iris$Sepal.Length[iris$Species == &#39;virginica&#39;]) ## 0% 25% 50% 75% 100% ## 4.900 6.225 6.500 6.900 7.900 La fonction table() permettra d‚Äôobtenir des d√©comptes par cat√©gorie, ici par plages de longueurs de s√©pales. Pour obtenir les proportions du nombre total, il s‚Äôagit d‚Äôencapsuler le tableau crois√© dans la fonction prop.table(). tableau_croise &lt;- table(iris$Species, cut(iris$Sepal.Length, breaks = quantile(iris$Sepal.Length))) tableau_croise ## ## (4.3,5.1] (5.1,5.8] (5.8,6.4] (6.4,7.9] ## setosa 35 14 0 0 ## versicolor 4 20 17 9 ## virginica 1 5 18 26 prop.table(tableau_croise) ## ## (4.3,5.1] (5.1,5.8] (5.8,6.4] (6.4,7.9] ## setosa 0.234899329 0.093959732 0.000000000 0.000000000 ## versicolor 0.026845638 0.134228188 0.114093960 0.060402685 ## virginica 0.006711409 0.033557047 0.120805369 0.174496644 6.6 Tests d‚Äôhypoth√®ses √† un et deux √©chantillons Un test d‚Äôhypoth√®se permet de d√©cider si une hypoth√®se est confirm√©e ou rejet√©e √† un seuil de probabilit√© pr√©d√©termin√©. Cette section est inspir√©e du chapitre 5 de Dalgaard, 2008. Information: l‚Äôhypoth√®se nulle. Les tests d‚Äôhypoth√®se √©valuent des effets statistiques (qui ne sont pas n√©cessairement des effets de causalit√©). L‚Äôeffet √† √©valuer peut √™tre celui d‚Äôun traitement, d‚Äôindicateurs m√©t√©orologiques (e.g.¬†pr√©cipitations totales, degr√©-jour, etc.), de techniques de gestion des paysages, etc. Une recherche est men√©e pour √©valuer l‚Äôhypoth√®se que l‚Äôon retrouve des diff√©rences entre des unit√©s exp√©rimentales. Par convention, l‚Äôhypoth√®se nulle (√©crite \\(H_0\\)) est l‚Äôhypoth√®se qu‚Äôil n‚Äôy ait pas d‚Äôeffet (c‚Äôest l‚Äôhypoth√®se de l‚Äôavocat du diable üòà) √† l‚Äô√©chelle de la population (et non pas √† l‚Äô√©chelle de l‚Äô√©chantillon). √Ä l‚Äôinverse, l‚Äôhypoth√®se alternative (√©crite \\(H_1\\)) est l‚Äôhypoth√®se qu‚Äôil y ait un effet √† l‚Äô√©chelle de la population. √Ä titre d‚Äôexercice en stats, on d√©bute souvent par en testant si deux vecteurs de valeurs continues proviennent de populations √† moyennes diff√©rentes ou si un vecteur de valeurs a √©t√© g√©n√©r√© √† partir d‚Äôune population ayant une moyenne donner. Dans cette section, nous utiliserons la fonction t.test() pour les tests de t et la fonction wilcox.test() pour les tests de Wilcoxon (aussi appel√© de Mann-Whitney). 6.6.1 Test de t √† un seul √©chantillon Nous devons assumer, pour ce test, que l‚Äô√©chantillon est recueillit d‚Äôune population dont la distribution est normale, \\(\\mathcal{N} \\sim \\left( \\mu, \\sigma^2 \\right)\\), et que chaque √©chantillon est ind√©pendant l‚Äôun de l‚Äôautre. L‚Äôhypoth√®se nulle est souvent celle de l‚Äôavocat du diable, que la moyenne soit √©gale √† une valeur donn√©e (donc la diff√©rence entre la moyenne de la population et une moyenne donn√©e est de z√©ro): ici, que \\(\\mu = \\bar{x}\\). L‚Äôerreur standard sur la moyenne (ESM) de l‚Äô√©chantillon, \\(\\bar{x}\\) est calcul√©e comme suit. \\[ESM = \\frac{s}{\\sqrt{n}}\\] o√π \\(s\\) est l‚Äô√©cart-type de l‚Äô√©chantillon et \\(n\\) est le nombre d‚Äô√©chantillons. Pour tester l‚Äôintervalle de confiance de l‚Äô√©chantillon, on multiplie l‚ÄôESM par l‚Äôaire sous la courbe de densit√© couvrant une certaine proportion de part et d‚Äôautre de l‚Äô√©chantillon. Pour un niveau de confiance de 95%, on retranche 2.5% de part et d‚Äôautre. set.seed(33746) x &lt;- rnorm(20, 16, 4) level &lt;- 0.95 alpha &lt;- 1-level x_bar &lt;- mean(x) s &lt;- sd(x) n &lt;- length(x) error &lt;- qnorm(1 - alpha/2) * s / sqrt(n) error ## [1] 1.483253 intervalle de confiance est l‚Äôerreur de par et d‚Äôautre de la moyenne. c(x_bar - error, x_bar + error) ## [1] 14.35630 17.32281 Si la moyenne de la population est de 16, un nombre qui se situe dans l‚Äôintervalle de confiance on accepte l‚Äôhypoth√®se nulle au seuil 0.05. Si le nombre d‚Äô√©chantillon est r√©duit (g√©n√©ralement &lt; 30), on passera plut√¥t par une distribution de t, avec \\(n-1\\) degr√©s de libert√©. error &lt;- qt(1 - alpha/2, n-1) * s / sqrt(n) c(x_bar - error, x_bar + error) ## [1] 14.25561 17.42351 Plus simplement, on pourra utiliser la fonction t.test() en sp√©cifiant la moyenne de la population. Nous avons g√©n√©r√© 20 donn√©es avec une moyenne de 16 et un √©cart-type de 4. Nous savons donc que la vraie moyenne de l‚Äô√©chantillon est de 16. Mais disons que nous testons l‚Äôhypoth√®se que ces donn√©es sont tir√©es d‚Äôune population dont la moyenne est 18 (et implicitement que sont √©cart-type est de 4). t.test(x, mu = 18) ## ## One Sample t-test ## ## data: x ## t = -2.8548, df = 19, p-value = 0.01014 ## alternative hypothesis: true mean is not equal to 18 ## 95 percent confidence interval: ## 14.25561 17.42351 ## sample estimates: ## mean of x ## 15.83956 La fonction retourne la valeur de t (t-value), le nombre de degr√©s de libert√© (\\(n-1 = 19\\)), une description de l‚Äôhypoth√®se alternative (alternative hypothesis: true mean is not equal to 18), ainsi que l‚Äôintervalle de confiance au niveau de 95%. Le test contient aussi la p-value. Bien que la p-value soit largement utilis√©e en science 6.6.1.1 Information: la p-value La p-value, ou valeur-p ou p-valeur, est utilis√©e pour trancher si, oui ou non, un r√©sultat est significatif. En langage scientifique, le mot significatif ne devrait √™tre utilis√© que lorsque l‚Äôon r√©f√®re √† un test d‚Äôhypoth√®se statistique. Vous retrouverez des p-value partout en stats. Les p-values indiquent la confiance que les donn√©es ait √©t√© √©chantillonn√©es d‚Äôune population o√π un effet est observable selon le mod√®le statistique utilis√©s. La p-value est la probabilit√© que les donn√©es aient √©t√© g√©n√©r√©es pour obtenir un effet √©quivalent ou plus prononc√© si l‚Äôhypoth√®se nulle est vraie. Une p-value √©lev√©e indique que le mod√®le appliqu√© √† vos donn√©es concordent avec la conclusion que l‚Äôhypoth√®se nulle est vraie, et inversement si la p-value est faible. Le seuil arbitraire utilis√©e en √©cologie et en agriculture, comme dans plusieurs domaines, est 0.05. L‚Äôutilisation d‚Äôun seuil est toutefois contest√©e, car une cat√©gorisation de la p-value avec un seuil de significativit√© brouille le jugement sur l‚Äôimportance des effets et de leur incertitude (Amrhein et al., 2019). Les six principes de l‚ÄôAmerican Statistical Association guident l‚Äôinterpr√©tation des p-values. [ma traduction] Les p-values indique l‚Äôampleur de l‚Äôincompatibilit√© des donn√©es avec le mod√®le statistique Les p-values ne mesurent pas la probabilit√© que l‚Äôhypoth√®se √©tudi√©e soit vraie, ni la probabilit√© que les donn√©es ont √©t√© g√©n√©r√©es uniquement par la chance. Les conclusions scientifiques et d√©cisions d‚Äôaffaire ou politiques ne devraient pas √™tre bas√©es sur si une p-value atteint un seuil sp√©cifique. Une inf√©rence appropri√©e demande un rapport complet et transparent. Une p-value, ou une signification statistique, ne mesure pas l‚Äôampleur d‚Äôun effet ou l‚Äôimportance d‚Äôun r√©sultat. En tant que tel, une p-value n‚Äôoffre pas une bonne mesure des √©vidences d‚Äôun mod√®le ou d‚Äôune hypoth√®se. Dans le cas pr√©c√©dent, la p-value √©tait de 0.01014. Pour aider notre interpr√©tation, prenons l‚Äôhypoth√®se alternative: true mean is not equal to 18. L‚Äôhypoth√®se nulle √©tait bien que la vraie moyenne est √©gale √† 18. Ins√©rons la p-value dans la d√©finition: la probabilit√© que les donn√©es aient √©t√© g√©n√©r√©es pour obtenir un effet √©quivalent ou plus prononc√© si l‚Äôhypoth√®se nulle est vraie est de 0.01014. Il est donc tr√®s peu probable que les donn√©es soient tir√©es d‚Äôun √©chantillon dont la moyenne est de 18. Au seuil de signification de 0.05, on rejette l‚Äôhypoth√®se nulle et l‚Äôon conclut qu‚Äô√† ce seuil de confiance, l‚Äô√©chantillon ne provient pas d‚Äôune population ayant une moyenne de 18. 6.6.2 Attention: mauvaises interpr√©tations des p-values ‚ÄúLa p-value n‚Äôa jamais √©t√© con√ßue comme substitut au raisonnement scientifique‚Äù Ron Wasserstein, directeur de l‚ÄôAmerican Statistical Association [ma traduction]. Un r√©sultat montrant une p-value plus √©lev√©e que 0.05 est-il pertinent? Lors d‚Äôune conf√©rence, Dr Evil ne pr√©sentent que les r√©sultats significatifs de ses essais au seuil de 0.05. Certains essais ne sont pas significatifs, mais bon, ceux-ci ne sont pas importants‚Ä¶ En √©cartant ces r√©sultats, Dr Evil commet 3 erreurs: La p-value n‚Äôest pas un bon indicateur de l‚Äôimportance d‚Äôun test statistique. L‚Äôimportance d‚Äôune variable dans un mod√®le devrait √™tre √©valu√©e par la valeur de son coefficient. Son incertitude devrait √™tre √©valu√©e par sa variance. Une mani√®re d‚Äô√©valuer plus intuitive la variance est l‚Äô√©cart-type ou l‚Äôintervalle de confiance. √Ä un certain seuil d‚Äôintervalle de confiance, la p-value traduira la probabilit√© qu‚Äôun coefficient soit r√©ellement nul ait pu g√©n√©rer des donn√©es d√©montrant un coefficient √©gal ou sup√©rieur. Il est tout aussi important de savoir que le traitement fonctionne que de savoir qu‚Äôil ne fonctionne pas. Les r√©sultats d√©montrant des effets sont malheureusement davantage soumis aux journaux et davantage publi√©s que ceux ne d√©montrant pas d‚Äôeffets (Decullier et al., 2005). Le seuil de 0.05 est arbitraire. 6.6.2.1 Attention au p-hacking Le p-hacking (ou data dredging) consiste √† manipuler les donn√©es et les mod√®les pour faire en sorte d‚Äôobtenir des p-values favorables √† l‚Äôhypoth√®se test√©e et, √©ventuellement, aux conclusions recherch√©es. √Ä √©viter dans tous les cas. Toujours. Toujours. Toujours. Figure 6.1: Un sketch humoristique de John Oliver sur le p-hacking, Last week tonight, 2016 (en anglais) 6.6.3 Test de Wilcoxon √† un seul √©chantillon Le test de t suppose que la distribution des donn√©es est normale‚Ä¶ ce qui est rarement le cas, surtout lorsque les √©chantillons sont peu nombreux. Le test de Wilcoxon ne demande aucune supposition sur la distribution: c‚Äôest un test non-param√©trique bas√© sur le tri des valeurs. wilcox.test(x, mu = 18) ## ## Wilcoxon signed rank test ## ## data: x ## V = 39, p-value = 0.01208 ## alternative hypothesis: true location is not equal to 18 Le V est la somme des rangs positifs. Dans ce cas, la p-value est semblable √† celle du test de t, et les m√™mes conclusions s‚Äôappliquent. 6.6.4 Tests de t √† deux √©chantillons Les tests √† un √©chantillon servent plut√¥t √† s‚Äôexercer: rarement en aura-t-on besoin en recherche, o√π plus souvent, on voudra comparer les moyennes de deux unit√©s exp√©rimentales. L‚Äôexp√©rience comprend donc deux s√©ries de donn√©es continues, \\(x_1\\) et \\(x_2\\), issus de lois de distribution normale \\(\\mathcal{N} \\left( \\mu_1, \\sigma_1^2 \\right)\\) et \\(\\mathcal{N} \\left( \\mu_2, \\sigma_2^2 \\right)\\), et nous testons l‚Äôhypoth√®se nulle que \\(\\mu_1 = \\mu_2\\). La statistique t est calcul√©e comme suit. \\[t = \\frac{\\bar{x_1} - \\bar{x_2}}{ESDM}\\] L‚ÄôESDM est l‚Äôerreur standard de la diff√©rence des moyennes: \\[ESDM = \\sqrt{ESM_1^2 + ESM_2^2}\\] Si vous supposez que les variances sont identiques, l‚Äôerreur standard (s) est calcul√©e pour les √©chantillons des deux groupes, puis ins√©r√©e dans le calcul des ESM. La statistique t sera alors √©valu√©e √† \\(n_1 + n_2 - 2\\) degr√©s de libert√©. Si vous supposez que la variance est diff√©rente (proc√©dure de Welch), vous calculez les ESM avec les erreurs standards respectives, et la statistique t devient une approximation de la distribution de t avec un nombre de degr√©s de libert√© calcul√© √† partir des erreurs standards et du nombre d‚Äô√©chantillon dans les groupes: cette proc√©dure est consid√©r√©e comme plus prudente (Dalgaard, 2008, page 101). Prenons les donn√©es d‚Äôiris pour l‚Äôexemple en excluant l‚Äôiris setosa √©tant donn√©e que les tests de t se restreignent √† deux groupes. Nous allons tester la longueur des p√©tales. iris_pl &lt;- iris %&gt;% filter(Species != &quot;setosa&quot;) %&gt;% select(Species, Petal.Length) sample_n(iris_pl, 5) ## Species Petal.Length ## 1 virginica 5.1 ## 2 versicolor 4.0 ## 3 virginica 5.0 ## 4 versicolor 4.6 ## 5 versicolor 4.1 Dans la prochaine cellule, nous introduisons l‚Äôinterface-formule de R, o√π l‚Äôon retrouve typiquement le ~, entre les variables de sortie √† gauche et les variables d‚Äôentr√©e √† droite. Dans notre cas, la variable de sortie est la variable test√©e, Petal.Length, qui varie en fonction du groupe Species, qui est la variable d‚Äôentr√©e (variable explicative) - nous verrons les types de variables plus en d√©tails dans la section Les mod√®les statistiques, plus bas. t.test(formula = Petal.Length ~ Species, data = iris_pl, var.equal = FALSE) ## ## Welch Two Sample t-test ## ## data: Petal.Length by Species ## t = -12.604, df = 95.57, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.49549 -1.08851 ## sample estimates: ## mean in group versicolor mean in group virginica ## 4.260 5.552 Nous obtenons une sortie similaire aux pr√©c√©dentes. L‚Äôintervalle de confiance √† 95% exclu le z√©ro, ce qui est coh√©rent avec la p-value tr√®s faible, qui nous indique le rejet de l‚Äôhypoth√®se nulle au seuil 0.05. Les groupes ont donc des moyennes de longueurs de p√©tale significativement diff√©rentes. 6.6.4.1 Enregistrer les r√©sultats d‚Äôun test Il est possible d‚Äôenregistrer un test dans un objet. tt_pl &lt;- t.test(formula = Petal.Length ~ Species, data = iris_pl, var.equal = FALSE) summary(tt_pl) ## Length Class Mode ## statistic 1 -none- numeric ## parameter 1 -none- numeric ## p.value 1 -none- numeric ## conf.int 2 -none- numeric ## estimate 2 -none- numeric ## null.value 1 -none- numeric ## stderr 1 -none- numeric ## alternative 1 -none- character ## method 1 -none- character ## data.name 1 -none- character str(tt_pl) ## List of 10 ## $ statistic : Named num -12.6 ## ..- attr(*, &quot;names&quot;)= chr &quot;t&quot; ## $ parameter : Named num 95.6 ## ..- attr(*, &quot;names&quot;)= chr &quot;df&quot; ## $ p.value : num 4.9e-22 ## $ conf.int : num [1:2] -1.5 -1.09 ## ..- attr(*, &quot;conf.level&quot;)= num 0.95 ## $ estimate : Named num [1:2] 4.26 5.55 ## ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;mean in group versicolor&quot; &quot;mean in group virginica&quot; ## $ null.value : Named num 0 ## ..- attr(*, &quot;names&quot;)= chr &quot;difference in means&quot; ## $ stderr : num 0.103 ## $ alternative: chr &quot;two.sided&quot; ## $ method : chr &quot;Welch Two Sample t-test&quot; ## $ data.name : chr &quot;Petal.Length by Species&quot; ## - attr(*, &quot;class&quot;)= chr &quot;htest&quot; 6.6.5 Comparaison des variances Pour comparer les variances, on a recours au test de F (F pour Fisher). var.test(formula = Petal.Length ~ Species, data = iris_pl) ## ## F test to compare two variances ## ## data: Petal.Length by Species ## F = 0.72497, num df = 49, denom df = 49, p-value = 0.2637 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.411402 1.277530 ## sample estimates: ## ratio of variances ## 0.7249678 Il semble que l‚Äôon pourrait relancer le test de t sans la proc√©dure Welch, avec var.equal = TRUE. 6.6.6 Tests de Wilcoxon √† deux √©chantillons Cela ressemble au test de t! wilcox.test(formula = Petal.Length ~ Species, data = iris_pl, var.equal = TRUE) ## ## Wilcoxon rank sum test with continuity correction ## ## data: Petal.Length by Species ## W = 44.5, p-value &lt; 2.2e-16 ## alternative hypothesis: true location shift is not equal to 0 6.6.7 Les tests pair√©s Les tests pair√©s sont utilis√©s lorsque deux √©chantillons proviennent d‚Äôune m√™me unit√© exp√©rimentale: il s‚Äôagit en fait de tests sur la diff√©rence entre deux observations. set.seed(2555) n &lt;- 20 avant &lt;- rnorm(n, 16, 4) apres &lt;- rnorm(n, 18, 3) Il est important de sp√©cifier que le test est pair√©, la valeur par d√©faut de paired √©tant FALSE. t.test(avant, apres, paired = TRUE) ## ## Paired t-test ## ## data: avant and apres ## t = -1.5168, df = 19, p-value = 0.1458 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -4.5804586 0.7311427 ## sample estimates: ## mean of the differences ## -1.924658 L‚Äôhypoth√®se nulle qu‚Äôil n‚Äôy ait pas de diff√©rence entre l‚Äôavant et l‚Äôapr√®s traitement est accept√©e au seuil 0.05. Exercice. Effectuer un test de Wilcoxon pair√©. 6.7 L‚Äôanalyse de variance L‚Äôanalyse de variance consiste √† comparer des moyennes de plusieurs groupe distribu√©s normalement et de m√™me variance. Cette section sera √©labor√©e prochainement plus en profondeur. Consid√©rons-la pour le moment comme une r√©gression sur une variable cat√©gorielle. pl_aov &lt;- aov(Petal.Length ~ Species, iris) summary(pl_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 2 437.1 218.55 1180 &lt;2e-16 *** ## Residuals 147 27.2 0.19 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 La prochaine section, justement, est vou√©e aux mod√®les statistiques explicatifs, qui incluent la r√©gression. 6.8 Les mod√®les statistiques La mod√©lisation statistique consiste √† lier de mani√®re explicite des variables de sortie \\(y\\) (ou variables-r√©ponse ou variables d√©pendantes) √† des variables explicatives \\(x\\) (ou variables pr√©dictives / ind√©pendantes / covariables). Les variables-r√©ponse sont mod√©lis√©es par une fonction des variables explicatives ou pr√©dictives. Pourquoi garder les termes explicatives et pr√©dictives? Parce que les mod√®les statistiques (bas√©s sur des donn√©es et non pas sur des m√©canismes) sont de deux ordres. D‚Äôabord, les mod√®les pr√©dictifs sont con√ßus pour pr√©dire de mani√®re fiable une ou plusieurs variables-r√©ponse √† partir des informations contenues dans les variables qui sont, dans ce cas, pr√©dictives. Ces mod√®les sont couverts dans le chapitre 11 de ce manuel (en d√©veloppement). Lorsque l‚Äôon d√©sire tester des hypoth√®ses pour √©valuer quelles variables expliquent la r√©ponse, on parlera de mod√©lisation (et de variables) explicatives. En inf√©rence statistique, on √©valuera les corr√©lations entre les variables explicatives et les variables-r√©ponse. Un lien de corr√©lation n‚Äôest pas un lien de causalit√©. L‚Äôinf√©rence causale peut en revanche √™tre √©valu√©e par des mod√®les d‚Äô√©quations structurelles, sujet qui fera √©ventuellement partie de ce cours. Cette section couvre la mod√©lisation explicative. Les variables qui contribuent √† cr√©er les mod√®les peuvent √™tre de diff√©rentes natures et distribu√©es selon diff√©rentes lois de probabilit√©. Alors que les mod√®les lin√©aires simples (lm) impliquent une variable-r√©ponse distribu√©e de mani√®re continue, les mod√®les lin√©aires g√©n√©ralis√©s peuvent aussi expliquer des variables de sorties discr√®tes. Dans les deux cas, on distinguera les variables fixes et les variables al√©atoires. Les variables fixes sont les variables test√©es lors de l‚Äôexp√©rience: dose du traitement, esp√®ce/cultivar, m√©t√©o, etc. Les variables al√©atoires sont les sources de variation qui g√©n√®rent du bruit dans le mod√®le: les unit√©s exp√©rimentales ou le temps lors de mesures r√©p√©t√©es. Les mod√®les incluant des effets fixes seulement sont des mod√®les √† effets fixes. G√©n√©ralement, les mod√®les incluant des variables al√©atoires incluent aussi des variables fixes: on parlera alors de mod√®les mixtes. Nous couvrirons ces deux types de mod√®le. 6.8.1 Mod√®les √† effets fixes Les tests de t et de Wilcoxon, explor√©s pr√©c√©demment, sont des mod√®les statistiques √† une seule variable. Nous avons vu dans l‚Äôinterface-formule qu‚Äôune variable-r√©ponse peut √™tre li√©e √† une variable explicative avec le tilde ~. En particulier, le test de t est r√©gression lin√©aire univari√©e (√† une seule variable explicative) dont la variable explicative comprend deux cat√©gories. De m√™me, l‚Äôanova est une r√©gression lin√©aire univari√©e dont la variable explicative comprend plusieurs cat√©gories. Or l‚Äôinterface-formule peut √™tre utilis√© dans plusieurs circonstances, notamment pour ajouter plusieurs variables de diff√©rents types: on parlera de r√©gression multivari√©e. La plupart des mod√®les statistiques peuvent √™tre approxim√©s comme une combinaison lin√©aire de variables: ce sont des mod√®les lin√©aires. Les mod√®les non-lin√©aires impliquent des strat√©gies computationnelles complexes qui rendent leur utilisation plus difficile √† man≈ìuvrer. Un mod√®le lin√©aire univari√© prendra la forme \\(y = \\beta_0 + \\beta_1 x + \\epsilon\\), o√π \\(\\beta_0\\) est l‚Äôintercept et \\(\\beta_1\\) est la pente et \\(\\epsilon\\) est l‚Äôerreur. Vous verrez parfois la notation \\(\\hat{y} = \\beta_0 + \\beta_1 x\\). La notation avec le chapeau \\(\\hat{y}\\) exprime qu‚Äôil s‚Äôagit des valeurs g√©n√©r√©es par le mod√®le. En fait, \\(y = \\hat{y} - \\epsilon\\). 6.8.1.1 Mod√®le lin√©aire univari√© avec variable continue Prenons les donn√©es lasrosas.corn incluses dans le module agridat, o√π l‚Äôon retrouve le rendement d‚Äôune production de ma√Øs √† dose d‚Äôazote variable, en Argentine. library(&quot;agridat&quot;) data(&quot;lasrosas.corn&quot;) sample_n(lasrosas.corn, 10) ## year lat long yield nitro topo bv rep nf ## 1 1999 -33.05207 -63.84230 69.57 0.0 LO 185.67 R1 N0 ## 2 1999 -33.05137 -63.84383 67.41 53.0 E 175.12 R2 N2 ## 3 1999 -33.05104 -63.84323 68.33 29.0 LO 168.70 R3 N1 ## 4 1999 -33.05162 -63.84456 68.06 53.0 E 171.71 R1 N2 ## 5 1999 -33.05180 -63.84386 63.99 0.0 LO 172.46 R1 N0 ## 6 2001 -33.05065 -63.84578 35.85 50.6 HT 194.85 R1 N2 ## 7 1999 -33.05170 -63.84553 58.89 131.5 HT 187.98 R1 N5 ## 8 2001 -33.05077 -63.84502 50.95 124.6 HT 184.66 R2 N5 ## 9 1999 -33.05181 -63.84202 78.75 106.0 LO 169.25 R2 N4 ## 10 1999 -33.05154 -63.84468 68.58 29.0 E 169.35 R1 N1 Ces donn√©es comprennent plusieurs variables. Prenons le rendement (yield) comme variable de sortie et, pour le moment, ne retenons que la dose d‚Äôazote (nitro) comme variable explicative: il s‚Äôagit d‚Äôune r√©gression univari√©e. Les deux variables sont continues. Explorons d‚Äôabord le nuage de points de l‚Äôune et l‚Äôautre. ggplot(data = lasrosas.corn, mapping = aes(x = nitro, y = yield)) + geom_point() L‚Äôhypoth√®se nulle est que la dose d‚Äôazote n‚Äôaffecte pas le rendement, c‚Äôest √† dire que le coefficient de pente et nul. Une autre hypoth√®se est que l‚Äôintercept est nul: donc qu‚Äô√† dose de 0, rendement de 0. Un mod√®le lin√©aire √† variable de sortie continue est cr√©√© avec la fonction lm(), pour linear model. modlin_1 &lt;- lm(yield ~ nitro, data = lasrosas.corn) summary(modlin_1) ## ## Call: ## lm(formula = yield ~ nitro, data = lasrosas.corn) ## ## Residuals: ## Min 1Q Median 3Q Max ## -53.183 -15.341 -3.079 13.725 45.897 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 65.843213 0.608573 108.193 &lt; 2e-16 *** ## nitro 0.061717 0.007868 7.845 5.75e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 19.66 on 3441 degrees of freedom ## Multiple R-squared: 0.01757, Adjusted R-squared: 0.01728 ## F-statistic: 61.54 on 1 and 3441 DF, p-value: 5.754e-15 Le diagnostic du mod√®le comprend plusieurs informations. D‚Äôabord la formule utilis√©e, affich√©e pour la tra√ßabilit√©. Viens ensuite un aper√ßu de la distribution des r√©sidus. La m√©diane devrait s‚Äôapprocher de la moyenne des r√©sidus (qui est toujours de 0). Bien que le -3.079 peut sembler important, il faut prendre en consid√©ration de l‚Äô√©chelle de y, et ce -3.079 est exprim√© en terme de rendement, ici en quintaux (i.e.¬†100 kg) par hectare. La distribution des r√©sidus m√©rite d‚Äô√™tre davantage investigu√©e. Nous verrons cela un peu plus tard. Les coefficients apparaissent ensuite. Les estim√©s sont les valeurs des effets. R fournit aussi l‚Äôerreur standard associ√©e, la valeur de t ainsi que la p-value (la probabilit√© d‚Äôobtenir cet effet ou un effet plus extr√™me si en r√©alit√© il y avait absence d‚Äôeffet). L‚Äôintercept est bien s√ªr plus √©lev√© que 0 (√† dose nulle, on obtient 65.8 quintaux par hectare en moyenne). La pente de la variable nitro est de ~0.06: pour chaque augmentation d‚Äôun kg/ha de dose, on a obtenu ~0.06 quintaux/ha de plus de ma√Øs. Donc pour 100 kg/ha de N, on a obtenu un rendement moyen de 6 quintaux de plus que l‚Äôintercept. Soulignons que l‚Äôampleur du coefficient est tr√®s important pour guider la fertilisation: ne rapporter que la p-value, ou ne rapporter que le fait qu‚Äôelle est inf√©rieure √† 0.05 (ce qui arrive souvent dans la litt√©rature), serait tr√®s insuffisant pour l‚Äôinterpr√©tation des statistiques. La p-value nous indique n√©anmoins qu‚Äôil serait tr√®s improbable qu‚Äôune telle pente ait √©t√© g√©n√©r√©e alors que celle-ci est nulle en r√©alit√©. Les √©toiles √† c√¥t√© des p-values indiquent l‚Äôampleur selon l‚Äô√©chelle Signif. codes indiqu√©e en-dessous du tableau des coefficients. Sous ce tableau, R offre d‚Äôautres statistiques. En outre, les R¬≤ et R¬≤ ajust√©s indiquent si la r√©gression passe effectivement par les points. Le R¬≤ prend un maximum de 1 lorsque la droite passe exactement sur les points. Enfin, le test de F g√©n√®re une p-value indiquant la probabilit√© que les coefficients de pente ait √©t√© g√©n√©r√©s si les vrais coefficients √©taient nuls. Dans le cas d‚Äôune r√©gression univari√©e, cela r√©p√®te l‚Äôinformation sur l‚Äôunique coefficient. On pourra √©galement obtenir les intervalles de confiance avec la fonction confint(). confint(modlin_1, level = 0.95) ## 2.5 % 97.5 % ## (Intercept) 64.65001137 67.03641474 ## nitro 0.04629164 0.07714271 Ou soutirer l‚Äôinformation de diff√©rentes mani√®res, comme avec la fonction coefficients(). coefficients(modlin_1) ## (Intercept) nitro ## 65.84321305 0.06171718 √âgalement, on pourra ex√©cuter le mod√®le sur les donn√©es qui ont servi √† le g√©n√©rer: predict(modlin_1)[1:5] ## 1 2 3 4 5 ## 73.95902 73.95902 73.95902 73.95902 73.95902 Ou sur des donn√©es externes. nouvelles_donnees &lt;- data.frame(nitro = seq(from = 0, to = 100, by = 5)) predict(modlin_1, newdata = nouvelles_donnees)[1:5] ## 1 2 3 4 5 ## 65.84321 66.15180 66.46038 66.76897 67.07756 6.8.1.2 Analyse des r√©sidus Les r√©sidus sont les erreurs du mod√®le. C‚Äôest le vecteur \\(\\epsilon\\), qui est un d√©calage entre les donn√©es et le mod√®le. Le R¬≤ est un indicateur de l‚Äôampleur du d√©calage, mais une r√©gression lin√©aire explicative en bonne et due forme devrait √™tre accompagn√©e d‚Äôune analyse des r√©sidus. On peut les calculer par \\(\\epsilon = y - \\hat{y}\\), mais aussi bien utiliser la fonction residuals(). res_df &lt;- data.frame(nitro = lasrosas.corn$nitro, residus_lm = residuals(modlin_1), residus_calcul = lasrosas.corn$yield - predict(modlin_1)) sample_n(res_df, 10) ## nitro residus_lm residus_calcul ## 1 124.6 24.666827 24.666827 ## 2 124.6 11.126827 11.126827 ## 3 99.8 25.417413 25.417413 ## 4 66.0 -11.636547 -11.636547 ## 5 131.5 11.460978 11.460978 ## 6 75.4 -18.686688 -18.686688 ## 7 29.0 -1.763011 -1.763011 ## 8 131.5 -11.289022 -11.289022 ## 9 131.5 -5.639022 -5.639022 ## 10 131.5 -13.129022 -13.129022 Dans une bonne r√©gression lin√©aire, on ne retrouvera pas de structure identifiable dans les r√©sidus, c‚Äôest-√†-dire que les r√©sidus sont bien distribu√©s de part et d‚Äôautre du mod√®le de r√©gression. ggplot(res_df, aes(x = nitro, y = residus_lm)) + geom_point() + labs(x = &quot;Dose N&quot;, y = &quot;R√©sidus&quot;) + geom_hline(yintercept = 0, col = &quot;red&quot;, size = 1) Bien que le jugement soit subjectif, on peut dire avec confiance qu‚Äôil n‚Äôy a pas structure particuli√®re. En revanche, on pourrait g√©n√©rer un \\(y\\) qui varie de mani√®re quadratique avec \\(x\\), un mod√®le lin√©aire montrera une structure √©vidente. set.seed(36164) x &lt;- 0:100 y &lt;- 10 + x*1 + x^2 * 0.05 + rnorm(length(x), 0, 50) modlin_2 &lt;- lm(y ~ x) ggplot(data.frame(x, residus = residuals(modlin_2)), aes(x = x, y = residus)) + geom_point() + labs(x = &quot;x&quot;, y = &quot;R√©sidus&quot;) + geom_hline(yintercept = 0, col = &quot;red&quot;, size = 1) De m√™me, les r√©sidus ne devraient pas cro√Ætre avec \\(x\\). set.seed(3984) x &lt;- 0:100 y &lt;- 10 + x + x * rnorm(length(x), 0, 2) modlin_3 &lt;- lm(y ~ x) ggplot(data.frame(x, residus = residuals(modlin_3)), aes(x = x, y = residus)) + geom_point() + labs(x = &quot;x&quot;, y = &quot;R√©sidus&quot;) + geom_hline(yintercept = 0, col = &quot;red&quot;, size = 1) On pourra aussi inspecter les r√©sidus avec un graphique de leur distribution. Reprenons notre mod√®le de rendement du ma√Øs. ggplot(res_df, aes(x = residus_lm)) + geom_histogram(binwidth = 2, color = &quot;white&quot;) + labs(x = &quot;Residual&quot;) L‚Äôhistogramme devrait pr√©senter une distribution normale. Les tests de normalit√© comme le test de Shapiro-Wilk peuvent aider, mais ils sont g√©n√©ralement tr√®s s√©v√®res. shapiro.test(res_df$residus_lm) ## ## Shapiro-Wilk normality test ## ## data: res_df$residus_lm ## W = 0.94868, p-value &lt; 2.2e-16 L‚Äôhypoth√®se nulle que la distribution est normale est rejet√©e au seuil 0.05. Dans notre cas, il est √©vident que la s√©v√©rit√© du test n‚Äôest pas en cause, car les r√©sidus semble g√©n√©rer trois ensembles. Ceci indique que les variables explicatives sont insuffisantes pour expliquer la variabilit√© de la variable-r√©ponse. 6.8.1.3 R√©gression multiple Comme c‚Äôest le cas pour bien des ph√©nom√®nes en √©cologie, le rendement d‚Äôune culture n‚Äôest certainement pas expliqu√© seulement par la dose d‚Äôazote. Lorsque l‚Äôon combine plusieurs variables explicatives, on cr√©e un mod√®le de r√©gression multivari√©e, ou une r√©gression multiple. Bien que les tendances puissent sembler non-lin√©aires, l‚Äôajout de variables et le calcul des coefficients associ√©s reste un probl√®me d‚Äôalg√®bre lin√©aire. On pourra en effet g√©n√©raliser les mod√®les lin√©aires, univari√©s et multivari√©s, de la mani√®re suivante. \\[ y = X \\beta + \\epsilon \\] o√π: \\(X\\) est la matrice du mod√®le √† \\(n\\) observations et \\(p\\) variables. \\[ X = \\left( \\begin{matrix} 1 &amp; x_{11} &amp; \\cdots &amp; x_{1p} \\\\ 1 &amp; x_{21} &amp; \\cdots &amp; x_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; \\cdots &amp; x_{np} \\end{matrix} \\right) \\] \\(\\beta\\) est la matrice des \\(p\\) coefficients, \\(\\beta_0\\) √©tant l‚Äôintercept qui multiplie la premi√®re colonne de la matrice \\(X\\). \\[ \\beta = \\left( \\begin{matrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{matrix} \\right) \\] \\(\\epsilon\\) est l‚Äôerreur de chaque observation. \\[ \\epsilon = \\left( \\begin{matrix} \\epsilon_0 \\\\ \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n \\end{matrix} \\right) \\] 6.8.1.4 Mod√®les lin√©aires univari√©s avec variable cat√©gorielle nominale Une variable cat√©gorielle nominale (non ordonn√©e) utilis√©e √† elle seule dans un mod√®le comme variable explicative, est un cas particulier de r√©gression multiple. En effet, l‚Äôencodage cat√©goriel (ou dummyfication) transforme une variable cat√©gorielle nominale en une matrice de mod√®le comprenant une colonne d√©signant l‚Äôintercept (une s√©rie de 1) d√©signant la cat√©gorie de r√©f√©rence, ainsi que des colonnes pour chacune des autres cat√©gories d√©signant l‚Äôappartenance (1) ou la non appartenance (0) de la cat√©gorie d√©sign√©e par la colonne. 6.8.1.4.1 L‚Äôencodage cat√©goriel Une variable √† \\(C\\) cat√©gories pourra √™tre d√©clin√©e en \\(C\\) variables dont chaque colonne d√©signe par un 1 l‚Äôappartenance au groupe de la colonne et par un 0 la non-appartenance. Pour l‚Äôexemple, cr√©ons un vecteur d√©signant le cultivar de pomme de terre. data &lt;- data.frame(cultivar = c(&#39;Superior&#39;, &#39;Superior&#39;, &#39;Superior&#39;, &#39;Russet&#39;, &#39;Kenebec&#39;, &#39;Russet&#39;)) model.matrix(~cultivar, data) ## (Intercept) cultivarRusset cultivarSuperior ## 1 1 0 1 ## 2 1 0 1 ## 3 1 0 1 ## 4 1 1 0 ## 5 1 0 0 ## 6 1 1 0 ## attr(,&quot;assign&quot;) ## [1] 0 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$cultivar ## [1] &quot;contr.treatment&quot; Nous avons trois cat√©gories, encod√©es en trois colonnes. La premi√®re colonne est un intercept et les deux autres d√©crivent l‚Äôabsence (0) ou la pr√©sence (1) des cultivars Russet et Superior. Le cultivar Kenebec est absent du tableau. En effet, en partant du principe que l‚Äôappartenance √† une cat√©gorie est mutuellement exclusive, c‚Äôest-√†-dire qu‚Äôun √©chantillon ne peut √™tre assign√© qu‚Äô√† une seule cat√©gorie, on peut d√©duire une cat√©gorie √† partir de l‚Äôinformation sur toutes les autres. Par exemple, si cultivar_Russet et cultivar_Superior sont toutes deux √©gales √† \\(0\\), on conclura que cultivar_Kenebec est n√©cessairement √©gal √† \\(1\\). Et si l‚Äôun d‚Äôentre cultivar_Russet et cultivar_Superior est √©gal √† \\(1\\), cultivar_Kenebec est n√©cessairement √©gal √† \\(0\\). L‚Äôinformation contenue dans un nombre \\(C\\) de cat√©gorie peut √™tre encod√©e dans un nombre \\(C-1\\) de colonnes. C‚Äôest pourquoi, dans une analyse statistique, on d√©signera une cat√©gorie comme une r√©f√©rence, que l‚Äôon d√©tecte lorsque toutes les autres cat√©gories sont encod√©es avec des \\(0\\): cette r√©f√©rence sera incluse dans l‚Äôintercept. La cat√©gorie de r√©f√©rence par d√©faut en R est celle la premi√®re cat√©gorie dans l‚Äôordre alphab√©tique. On pourra modifier cette r√©f√©rence avec la fonction relevel(). data$cultivar &lt;- relevel(data$cultivar, ref = &quot;Superior&quot;) model.matrix(~cultivar, data) ## (Intercept) cultivarKenebec cultivarRusset ## 1 1 0 0 ## 2 1 0 0 ## 3 1 0 0 ## 4 1 0 1 ## 5 1 1 0 ## 6 1 0 1 ## attr(,&quot;assign&quot;) ## [1] 0 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$cultivar ## [1] &quot;contr.treatment&quot; Pour certains mod√®les, vous devrez vous assurer vous-m√™me de l‚Äôencodage cat√©goriel. Pour d‚Äôautre, en particulier avec l‚Äôinterface par formule de R, ce sera fait automatiquement. 6.8.1.4.2 Exemple d‚Äôapplication Prenons la topographie du terrain, qui peut prendre plusieurs niveaux. levels(lasrosas.corn$topo) ## [1] &quot;E&quot; &quot;HT&quot; &quot;LO&quot; &quot;W&quot; Explorons le rendement selon la topographie. ggplot(lasrosas.corn, aes(x = topo, y = yield)) + geom_boxplot() Les diff√©rences sont √©videntes, et la mod√©lisation devrait montrer des effets significatifs. L‚Äôencodage cat√©goriel peut √™tre visualis√© en g√©n√©rant la matrice de mod√®le avec la fonction model.matrix() et l‚Äôinterface-formule - sans la variable-r√©ponse. model.matrix(~ topo, data = lasrosas.corn) %&gt;% tbl_df() %&gt;% # tbl_df pour transformer la matrice en tableau sample_n(10) ## # A tibble: 10 x 4 ## `(Intercept)` topoHT topoLO topoW ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 0 0 ## 2 1 0 1 0 ## 3 1 0 0 1 ## 4 1 0 0 1 ## 5 1 0 1 0 ## 6 1 0 0 1 ## 7 1 1 0 0 ## 8 1 0 1 0 ## 9 1 0 0 1 ## 10 1 0 0 0 Dans le cas d‚Äôun mod√®le avec une variable cat√©gorielle nominale seule, l‚Äôintercept repr√©sente la cat√©gorie de r√©f√©rence, ici E. Les autres colonnes sp√©cifient l‚Äôappartenance (1) ou la non-appartenance (0) de la cat√©gorie pour chaque observation. Cette matrice de mod√®le utilis√©e pour la r√©gression donnera un intercept, qui indiquera l‚Äôeffet de la cat√©gorie de r√©f√©rence, puis les diff√©rences entre les cat√©gories subs√©quentes et la cat√©gorie de r√©f√©rence. modlin_4 &lt;- lm(yield ~ topo, data = lasrosas.corn) summary(modlin_4) ## ## Call: ## lm(formula = yield ~ topo, data = lasrosas.corn) ## ## Residuals: ## Min 1Q Median 3Q Max ## -47.371 -11.933 -1.593 11.080 44.119 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 78.6653 0.5399 145.707 &lt;2e-16 *** ## topoHT -30.0526 0.7500 -40.069 &lt;2e-16 *** ## topoLO 6.2832 0.7293 8.615 &lt;2e-16 *** ## topoW -11.8841 0.7039 -16.883 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.59 on 3439 degrees of freedom ## Multiple R-squared: 0.4596, Adjusted R-squared: 0.4591 ## F-statistic: 975 on 3 and 3439 DF, p-value: &lt; 2.2e-16 Le mod√®le lin√©aire est √©quivalent √† l‚Äôanova, mais les r√©sultats de lm sont plus √©labor√©s. summary(aov(yield ~ topo, data = lasrosas.corn)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## topo 3 622351 207450 975 &lt;2e-16 *** ## Residuals 3439 731746 213 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 L‚Äôanalyse de r√©sidus peut √™tre effectu√©e de la m√™me mani√®re. 6.8.1.5 Mod√®les lin√©aires univari√©s avec variable cat√©gorielle ordinale Bien que j‚Äôintroduise la r√©gression sur variable cat√©gorielle ordinale √† la suite de la section sur les variables nominales, nous revenons dans ce cas √† une r√©gression simple, univari√©e. Voyons un cas √† 5 niveaux. statut &lt;- c(&quot;Totalement en d√©saccord&quot;, &quot;En d√©saccord&quot;, &quot;Ni en accord, ni en d√©saccord&quot;, &quot;En accord&quot;, &quot;Totalement en accord&quot;) statut_o &lt;- factor(statut, levels = statut, ordered=TRUE) model.matrix(~statut_o) # ou bien, sans passer par model.matrix, contr.poly(5) o√π 5 est le nombre de niveaux ## (Intercept) statut_o.L statut_o.Q statut_o.C statut_o^4 ## 1 1 -0.6324555 0.5345225 -3.162278e-01 0.1195229 ## 2 1 -0.3162278 -0.2672612 6.324555e-01 -0.4780914 ## 3 1 0.0000000 -0.5345225 -4.095972e-16 0.7171372 ## 4 1 0.3162278 -0.2672612 -6.324555e-01 -0.4780914 ## 5 1 0.6324555 0.5345225 3.162278e-01 0.1195229 ## attr(,&quot;assign&quot;) ## [1] 0 1 1 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$statut_o ## [1] &quot;contr.poly&quot; La matrice de mod√®le a 5 colonnes, soit le nombre de niveaux: un intercept, puis 4 autres d√©signant diff√©rentes valeurs que peuvent prendre les niveaux. Ces niveaux croient-ils lin√©airement? De mani√®re quadratique, cubique ou plus loin dans des distributions polynomiales? modmat_tidy &lt;- data.frame(statut, model.matrix(~statut_o)[, -1]) %&gt;% gather(variable, valeur, -statut) modmat_tidy$statut &lt;- factor(modmat_tidy$statut, levels = statut, ordered=TRUE) ggplot(data = modmat_tidy, mapping = aes(x = statut, y = valeur)) + facet_wrap(. ~ variable) + geom_point() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) R√®gle g√©n√©rale, pour les variables ordinales, on pr√©f√©rera une distribution lin√©aire, et c‚Äôest l‚Äôoption par d√©faut de la fonction lm(). L‚Äôutilisation d‚Äôune autre distribution peut √™tre effectu√©e √† la mitaine en utilisant dans le mod√®le la colonne d√©sir√©e de la sortie de la fonction model.matrix(). 6.8.1.6 R√©gression multiple √† plusieurs variables Reprenons le tableau de donn√©es du rendement de ma√Øs. head(lasrosas.corn) ## year lat long yield nitro topo bv rep nf ## 1 1999 -33.05113 -63.84886 72.14 131.5 W 162.60 R1 N5 ## 2 1999 -33.05115 -63.84879 73.79 131.5 W 170.49 R1 N5 ## 3 1999 -33.05116 -63.84872 77.25 131.5 W 168.39 R1 N5 ## 4 1999 -33.05117 -63.84865 76.35 131.5 W 176.68 R1 N5 ## 5 1999 -33.05118 -63.84858 75.55 131.5 W 171.46 R1 N5 ## 6 1999 -33.05120 -63.84851 70.24 131.5 W 170.56 R1 N5 Pour ajouter des variables au mod√®le dans l‚Äôinterface-formule, on additionne les noms de colonne. La variable lat d√©signe la latitude, la variable long d√©signe la latitude et la variable bv (brightness value) d√©signe la teneur en mati√®re organique du sol (plus bv est √©lev√©e, plus faible est la teneur en mati√®re organique). modlin_5 &lt;- lm(yield ~ lat + long + nitro + topo + bv, data = lasrosas.corn) summary(modlin_5) ## ## Call: ## lm(formula = yield ~ lat + long + nitro + topo + bv, data = lasrosas.corn) ## ## Residuals: ## Min 1Q Median 3Q Max ## -48.405 -11.071 -1.251 10.592 40.078 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.946e+05 3.309e+04 5.882 4.45e-09 *** ## lat 5.541e+03 4.555e+02 12.163 &lt; 2e-16 *** ## long 1.776e+02 4.491e+02 0.395 0.693 ## nitro 6.867e-02 5.451e-03 12.597 &lt; 2e-16 *** ## topoHT -2.665e+01 1.087e+00 -24.520 &lt; 2e-16 *** ## topoLO 5.565e+00 1.035e+00 5.378 8.03e-08 *** ## topoW -1.465e+01 1.655e+00 -8.849 &lt; 2e-16 *** ## bv -5.089e-01 3.069e-02 -16.578 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.47 on 3435 degrees of freedom ## Multiple R-squared: 0.5397, Adjusted R-squared: 0.5387 ## F-statistic: 575.3 on 7 and 3435 DF, p-value: &lt; 2.2e-16 L‚Äôampleur des coefficients est relatif √† l‚Äô√©chelle de la variable. En effet, un coefficient de 5541 sur la variable lat n‚Äôest pas comparable au coefficient de la variable bv, de -0.5089, √©tant donn√© que les variables ne sont pas exprim√©es avec la m√™me √©chelle. Pour les comparer sur une m√™me base, on peut centrer (soustraire la moyenne) et r√©duire (diviser par l‚Äô√©cart-type). lasrosas.corn_sc &lt;- lasrosas.corn %&gt;% mutate_at(c(&quot;lat&quot;, &quot;long&quot;, &quot;nitro&quot;, &quot;bv&quot;), scale) modlin_5_sc &lt;- lm(yield ~ lat + long + nitro + topo + bv, data = lasrosas.corn_sc) summary(modlin_5_sc) ## ## Call: ## lm(formula = yield ~ lat + long + nitro + topo + bv, data = lasrosas.corn_sc) ## ## Residuals: ## Min 1Q Median 3Q Max ## -48.405 -11.071 -1.251 10.592 40.078 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 78.9114 0.6666 118.376 &lt; 2e-16 *** ## lat 3.9201 0.3223 12.163 &lt; 2e-16 *** ## long 0.3479 0.8796 0.395 0.693 ## nitro 2.9252 0.2322 12.597 &lt; 2e-16 *** ## topoHT -26.6487 1.0868 -24.520 &lt; 2e-16 *** ## topoLO 5.5647 1.0347 5.378 8.03e-08 *** ## topoW -14.6487 1.6555 -8.849 &lt; 2e-16 *** ## bv -4.9253 0.2971 -16.578 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.47 on 3435 degrees of freedom ## Multiple R-squared: 0.5397, Adjusted R-squared: 0.5387 ## F-statistic: 575.3 on 7 and 3435 DF, p-value: &lt; 2.2e-16 Typiquement, les variables cat√©gorielles, qui ne sont pas mises √† l‚Äô√©chelle, donneront des coefficients plus √©lev√©es, et devrons √™tre √©valu√©es entre elles et non comparativement aux variables mises √† l‚Äô√©chelle. Une mani√®re conviviale de repr√©senter des coefficients consiste √† cr√©er un tableau (fonction tibble()) incluant les coefficients ainsi que leurs intervalles de confiance, puis √† les porter graphiquement. intervals &lt;- tibble(Estimate = coefficients(modlin_5_sc)[-1], # [-1] enlever l&#39;intercept LL = confint(modlin_5_sc)[-1, 1], # [-1, ] enlever la premi√®re ligne, celle de l&#39;intercept UL = confint(modlin_5_sc)[-1, 2], variable = names(coefficients(modlin_5_sc)[-1])) intervals ## # A tibble: 7 x 4 ## Estimate LL UL variable ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 3.92 3.29 4.55 lat ## 2 0.348 -1.38 2.07 long ## 3 2.93 2.47 3.38 nitro ## 4 -26.6 -28.8 -24.5 topoHT ## 5 5.56 3.54 7.59 topoLO ## 6 -14.6 -17.9 -11.4 topoW ## 7 -4.93 -5.51 -4.34 bv ggplot(data = intervals, mapping = aes(x = Estimate, y = variable)) + geom_vline(xintercept = 0, lty = 2) + geom_segment(mapping = aes(x = LL, xend = UL, y = variable, yend = variable)) + geom_point() + labs(x = &quot;Coefficient standardis√©&quot;, y = &quot;&quot;) On y voit qu‚Äô√† l‚Äôexception de la variable long, tous les coefficients sont diff√©rents de 0. Le coefficient bv est n√©gatif, indiquant que plus la valeur de bv est √©lev√© (donc plus le sol est pauvre en mati√®re organique), plus le rendement est faible. Plus la latitude est √©lev√©e (plus on se dirige vers le Nord de l‚ÄôArgentine), plus le rendement est √©lev√©. La dose d‚Äôazote a aussi un effet statistique positif sur le rendement. Quant aux cat√©gories topographiques, elles sont toutes diff√©rentes de la cat√©gorie E, ne croisant pas le z√©ro. De plus, les intervalles de confiance ne se chevauchant pas, on peut conclure en une diff√©rence significative d‚Äôune √† l‚Äôautre. Bien s√ªr, tout cela au seuil de confiance de 0.05. On pourra retrouver des cas o√π l‚Äôeffet combin√© de plusieurs variables diff√®re de l‚Äôeffet des deux variables prises s√©par√©ment. Par exemple, on pourrait √©valuer l‚Äôeffet de l‚Äôazote et celui de la topographie dans un m√™me mod√®le, puis y ajouter une interaction entre l‚Äôazote et la topographie, qui d√©finira des effets suppl√©mentaires de l‚Äôazote selon chaque cat√©gorie topographique. C‚Äôest ce que l‚Äôon appelle une interaction. Dans l‚Äôinterface-formule, l‚Äôinteraction entre l‚Äôazote et la topographie est not√©e nitro:topo. Pour ajouter cette interaction, la formule deviendra yield ~ nitro + topo + nitro:topo. Une approche √©quivalente est d‚Äôutiliser le raccourci yield ~ nitro*topo. modlin_5_sc &lt;- lm(yield ~ nitro*topo, data = lasrosas.corn_sc) summary(modlin_5_sc) ## ## Call: ## lm(formula = yield ~ nitro * topo, data = lasrosas.corn_sc) ## ## Residuals: ## Min 1Q Median 3Q Max ## -47.984 -11.985 -1.388 10.339 40.636 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 78.6999 0.5322 147.870 &lt; 2e-16 *** ## nitro 1.8131 0.5351 3.388 0.000711 *** ## topoHT -30.0052 0.7394 -40.578 &lt; 2e-16 *** ## topoLO 6.2026 0.7190 8.627 &lt; 2e-16 *** ## topoW -11.9628 0.6939 -17.240 &lt; 2e-16 *** ## nitro:topoHT 1.2553 0.7461 1.682 0.092565 . ## nitro:topoLO 0.5695 0.7186 0.792 0.428141 ## nitro:topoW 0.7702 0.6944 1.109 0.267460 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.38 on 3435 degrees of freedom ## Multiple R-squared: 0.4756, Adjusted R-squared: 0.4746 ## F-statistic: 445.1 on 7 and 3435 DF, p-value: &lt; 2.2e-16 Les r√©sultats montre des effets de l‚Äôazote et des cat√©gories topographiques, mais il y a davantage d‚Äôincertitude sur les interactions, indiquant que l‚Äôeffet statistique de l‚Äôazote est sensiblement le m√™me ind√©pendamment des niveaux topographiques. 6.8.1.7 Attention √† ne pas surcharger le mod√®le Il est possible d‚Äôajouter des interactions doubles, triples, quadruples, etc. Mais plus il y a d‚Äôinteractions, plus votre mod√®le comprendra de variables et vos tests d‚Äôhypoth√®se perdront en puissance statistique. 6.8.1.8 Les mod√®les lin√©aires g√©n√©ralis√©s Dans un mod√®le lin√©aire ordinaire, un changement constant dans les variables explicatives r√©sulte en un changement constant de la variable-r√©ponse. Cette supposition ne serait pas ad√©quate si la variable-r√©ponse √©tait un d√©compte, si elle est bool√©enne ou si, de mani√®re g√©n√©rale, la variable-r√©ponse ne suivait pas une distribution continue. Ou, de mani√®re plus sp√©cifique, il n‚Äôy a pas moyen de retrouver une distribution normale des r√©sidus? On pourra bien s√ªr transformer les variables (sujet du chapitre 6, en d√©veloppement). Mais il pourrait s‚Äôav√©rer impossible, ou tout simplement non souhaitable de transformer les variables. Le mod√®le lin√©aire g√©n√©ralis√© (MLG, ou generalized linear model - GLM) est une g√©n√©ralisation du mod√®le lin√©aire ordinaire chez qui la variable-r√©ponse peut √™tre caract√©ris√© par une distribution de Poisson, de Bernouilli, etc. Prenons d‚Äôabord cas d‚Äôun d√©compte de vers fil-de-fer (worms) retrouv√©s dans des parcelles sous diff√©rents traitements (trt). Les d√©comptes sont typiquement distribu√© selon une loi de Poisson. cochran.wireworms %&gt;% ggplot(aes(x = worms)) + geom_histogram(bins = 10) Explorons les d√©comptes selon les traitements. cochran.wireworms %&gt;% ggplot(aes(x = trt, y = worms)) + geom_boxplot() Les traitements semble √† premi√®re vue avoir un effet comparativement au contr√¥le. Lan√ßons un MLG avec la fonction glm(), et sp√©cifions que la sortie est une distribution de Poisson. Bien que la fonction de lien (link = &quot;log&quot;) soit explictement impos√©e, le log est la valeur par d√©faut pour les distributions de Poisson. Ainsi, les coefficients du mod√®les devront √™tre interpr√©t√©s selon un mod√®le \\(log \\left(worms \\right) = intercept + pente \\times coefficient\\). modglm_1 &lt;- glm(worms ~ trt, cochran.wireworms, family = stats::poisson(link=&quot;log&quot;)) summary(modglm_1) ## ## Call: ## glm(formula = worms ~ trt, family = stats::poisson(link = &quot;log&quot;), ## data = cochran.wireworms) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.8279 -0.9455 -0.2862 0.6916 3.1888 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.1823 0.4082 0.447 0.655160 ## trtM 1.6422 0.4460 3.682 0.000231 *** ## trtN 1.7636 0.4418 3.991 6.57e-05 *** ## trtO 1.5755 0.4485 3.513 0.000443 *** ## trtP 1.3437 0.4584 2.931 0.003375 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 64.555 on 24 degrees of freedom ## Residual deviance: 38.026 on 20 degrees of freedom ## AIC: 125.64 ## ## Number of Fisher Scoring iterations: 5 L‚Äôinterpr√©tation sp√©cifique des coefficients d‚Äôune r√©gression de Poisson doit passer par la fonction de lien \\(log \\left(worms \\right) = intercept + pente \\times coefficient\\). Le traitement de r√©f√©rence (K), qui correspond √† l‚Äôintercept, sera accompagn√© d‚Äôun nombre de vers de \\(exp \\left(0.1823\\right) = 1.20\\) vers, et le traitement M, √† \\(exp \\left(1.6422\\right) = 5.17\\) vers. Cela correspond √† ce que l‚Äôon observe sur les boxplots plus haut. Il est tr√®s probable (p-value de ~0.66) qu‚Äôun intercept (traitement K) de 0.18 ayant une erreur standard de 0.4082 ait √©t√© g√©n√©r√© depuis une population dont l‚Äôintercept est nul. Quant aux autres traitements, leurs effets sont tous significatifs au seuil 0.05, mais peuvent-ils √™tre consid√©r√©s comme √©quivalents? intervals &lt;- tibble(Estimate = coefficients(modglm_1), # [-1] enlever l&#39;intercept LL = confint(modglm_1)[, 1], # [-1, ] enlever la premi√®re ligne, celle de l&#39;intercept UL = confint(modglm_1)[, 2], variable = names(coefficients(modglm_1))) ## Waiting for profiling to be done... ## Waiting for profiling to be done... intervals ## # A tibble: 5 x 4 ## Estimate LL UL variable ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.182 -0.740 0.888 (Intercept) ## 2 1.64 0.840 2.62 trtM ## 3 1.76 0.972 2.74 trtN ## 4 1.58 0.766 2.56 trtO ## 5 1.34 0.509 2.34 trtP ggplot(data = intervals, mapping = aes(x = Estimate, y = variable)) + geom_vline(xintercept = 0, lty = 2) + geom_segment(mapping = aes(x = LL, xend = UL, y = variable, yend = variable)) + geom_point() + labs(x = &quot;Coefficient&quot;, y = &quot;&quot;) Les intervalles de confiance se superposant, on ne peut pas conclure qu‚Äôun traitement est li√© √† une r√©duction plus importante de vers qu‚Äôun autre, au seuil 0.05. Maintenant, √† d√©faut de trouver un tableau de donn√©es plus appropri√©, prenons le tableau mtcars, qui rassemble des donn√©es sur des mod√®les de voitures. La colonne vs, pour v-shaped, inscrit 0 si les pistons sont droit et 1 s‚Äôils sont plac√©s en V dans le moteur. Peut-on expliquer la forme des pistons selon le poids du v√©hicule (wt)? mtcars %&gt;% sample_n(6) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## 2 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 ## 3 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## 4 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## 5 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## 6 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 mtcars %&gt;% ggplot(aes(x = wt, y = vs)) + geom_point() Il semble y avoir une tendance: les v√©hicules plus lourds ont plut√¥t des pistons droits (vs = 0). V√©rifions cela. modglm_2 &lt;- glm(vs ~ wt, data = mtcars, family = stats::binomial()) summary(modglm_2) ## ## Call: ## glm(formula = vs ~ wt, family = stats::binomial(), data = mtcars) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.9003 -0.7641 -0.1559 0.7223 1.5736 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 5.7147 2.3014 2.483 0.01302 * ## wt -1.9105 0.7279 -2.625 0.00867 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.860 on 31 degrees of freedom ## Residual deviance: 31.367 on 30 degrees of freedom ## AIC: 35.367 ## ## Number of Fisher Scoring iterations: 5 Exercice. Analyser les r√©sultats. 6.8.1.9 Les mod√®les non-lin√©aires La hauteur d‚Äôun arbre en fonction du temps n‚Äôest typiquement pas lin√©aire. Elle tend √† cro√Ætre de plus en plus lentement jusqu‚Äô√† un plateau. De m√™me, le rendement d‚Äôune culture trait√© avec des doses croissantes de fertilisants tend √† atteindre un maximum, puis √† se stabiliser. Ces ph√©nom√®nes ne peuvent pas √™tre approxim√©s par des mod√®les lin√©aires. Examinons les donn√©es du tableau engelstad.nitro. engelstad.nitro %&gt;% sample_n(10) ## loc year nitro yield ## 1 Knoxville 1966 0 63.0 ## 2 Knoxville 1965 335 61.2 ## 3 Jackson 1965 335 73.0 ## 4 Jackson 1966 201 61.3 ## 5 Jackson 1966 335 59.8 ## 6 Knoxville 1964 0 60.9 ## 7 Knoxville 1964 67 75.9 ## 8 Jackson 1966 67 45.2 ## 9 Jackson 1962 201 73.1 ## 10 Jackson 1964 335 67.8 engelstad.nitro %&gt;% ggplot(aes(x = nitro, y = yield)) + facet_grid(year ~ loc) + geom_line() + geom_point() Le mod√®le de Mitscherlich pourrait √™tre utilis√©. \\[ y = A \\left( 1 - e^{-R \\left( E + x \\right)} \\right) \\] o√π \\(y\\) est le rendement, \\(x\\) est la dose, \\(A\\) est l‚Äôasymptote vers laquelle la courbe converge √† dose croissante, \\(E\\) est l‚Äô√©quivalent de dose fourni par l‚Äôenvironnement et \\(R\\) est le taux de r√©ponse. Explorons la fonction. mitscherlich_f &lt;- function(x, A, E, R) { A * (1 - exp(-R*(E + x))) } x &lt;- seq(0, 350, by = 5) y &lt;- mitscherlich_f(x, A = 75, E = 30, R = 0.02) ggplot(tibble(x, y), aes(x, y)) + geom_point(data = engelstad.nitro, aes(x = nitro, y = yield)) + geom_line() + ylim(c(0, 100)) Exercice. Changez les param√®tres pour visualiser comment la courbe r√©agit. Nous pouvons d√©crire le mod√®le gr√¢ce √† l‚Äôinterface formule dans la fonction nls(). Notez que les mod√®les non-lin√©aires demandent des strat√©gies de calcul diff√©rentes de celles des mod√®les lin√©aires. En tout temps, nous devons identifier des valeurs de d√©part raisonnables pour les param√®tres dans l‚Äôargument start. Vous r√©ussirez rarement √† obtenir une convergence du premier coup avec vos param√®tres de d√©part. Le d√©fi est d‚Äôen trouver qui permettront au mod√®le de converger. Parfois, le mod√®le ne convergera jamais. D‚Äôautres fois, il convergera vers des solutions diff√©rentes selon les variables de d√©part choisies. modnl_1 &lt;- nls(yield ~ A * (1 - exp(-R*(E + nitro))), data = engelstad.nitro, start = list(A = 50, E = 10, R = 0.2)) Le mod√®le ne converge pas (le bloc de calcul est d√©sactiv√©). Essayons les valeurs prises plus haut, lors de la cr√©ation du graphique, qui semblent bien s‚Äôajuster. modnl_1 &lt;- nls(yield ~ A * (1 - exp(-R*(E + nitro))), data = engelstad.nitro, start = list(A = 75, E = 30, R = 0.02)) Bingo! Voyons maintenant le sommaire. summary(modnl_1) ## ## Formula: yield ~ A * (1 - exp(-R * (E + nitro))) ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## A 75.023427 3.331860 22.517 &lt;2e-16 *** ## E 66.164110 27.251591 2.428 0.0184 * ## R 0.012565 0.004881 2.574 0.0127 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12.34 on 57 degrees of freedom ## ## Number of iterations to convergence: 5 ## Achieved convergence tolerance: 8.067e-06 Les param√®tres sont significativement diff√©rents de z√©ro au seuil 0.05, et donnent la courbe suivante. x &lt;- seq(0, 350, by = 5) y &lt;- mitscherlich_f(x, A = coefficients(modnl_1)[1], E = coefficients(modnl_1)[2], R = coefficients(modnl_1)[3]) ggplot(tibble(x, y), aes(x, y)) + geom_point(data = engelstad.nitro, aes(x = nitro, y = yield)) + geom_line() + ylim(c(0, 100)) Et les r√©sidus‚Ä¶ tibble(res = residuals(modnl_1)) %&gt;% ggplot(aes(x = res)) + geom_histogram(bins = 20) tibble(nitro = engelstad.nitro$nitro, res = residuals(modnl_1)) %&gt;% ggplot(aes(x = nitro, y = res)) + geom_point() + geom_hline(yintercept = 0, colour = &quot;red&quot;) Les r√©sidus ne sont pas distribu√©s normalement, mais semble bien partag√©s de part et d‚Äôautre de la courbe. 6.8.2 Mod√®les √† effets mixtes Lorsque l‚Äôon combine des variables fixes (test√©es lors de l‚Äôexp√©rience) et des variables al√©atoire (variation des unit√©s exp√©rimentales), on obtient un mod√®le mixte. Les mod√®les mixtes peuvent √™tre univari√©s, multivari√©s, lin√©aires ordinaires ou g√©n√©ralis√©s ou non lin√©aires. √Ä la diff√©rence d‚Äôun effet fixe, un effet al√©atoire sera toujours distribu√© normalement avec une moyenne de 0 et une certaine variance. Dans un mod√®le lin√©aire o√π l‚Äôeffet al√©atoire est un d√©calage d‚Äôintercept, cet effet s‚Äôadditionne aux effets fixes: \\[ y = X \\beta + Z b + \\epsilon \\] o√π: \\(Z\\) est la matrice du mod√®le √† \\(n\\) observations et \\(p\\) variables al√©atoires. Les variables al√©atoires sont souvent des variables nominales qui subissent un encodage cat√©goriel. \\[ Z = \\left( \\begin{matrix} z_{11} &amp; \\cdots &amp; z_{1p} \\\\ z_{21} &amp; \\cdots &amp; z_{2p} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ z_{n1} &amp; \\cdots &amp; z_{np} \\end{matrix} \\right) \\] \\(b\\) est la matrice des \\(p\\) coefficients al√©atoires. \\[ b = \\left( \\begin{matrix} b_0 \\\\ b_1 \\\\ \\vdots \\\\ b_p \\end{matrix} \\right) \\] Le tableau lasrosas.corn, utilis√© pr√©c√©demment, contenait trois r√©p√©titions effectu√©s au cours de deux ann√©es, 1999 et 2001. √âtant donn√© que la r√©p√©tition R1 de 1999 n‚Äôa rien √† voir avec la r√©p√©tition R1 de 2001, on dit qu‚Äôelle est embo√Æt√©e dans l‚Äôann√©e. Le module nlme nous aidera √† monter notre mod√®le mixte. library(&quot;nlme&quot;) mmodlin_1 &lt;- lme(fixed = yield ~ lat + long + nitro + topo + bv, random = ~ 1|year/rep, data = lasrosas.corn) √Ä ce stade vous devriez commencer √† √™tre familier avec l‚Äôinterface formule et vous deviez saisir l‚Äôargument fixed, qui d√©signe l‚Äôeffet fixe. L‚Äôeffet al√©atoire, random, suit un tilde ~. √Ä gauche de la barre verticale |, on place les variables d√©signant les effets al√©atoire sur la pente. Nous n‚Äôavons pas couvert cet aspect, alors nous le laissons √† 1. √Ä droite, on retrouve un structure d‚Äôembo√Ætement d√©signant l‚Äôeffet al√©atoire: le premier niveau est l‚Äôann√©e, dans laquelle est embo√Æt√©e la r√©p√©tition. {r biostats-multivariate-nlme-model}-summ summary(mmodlin_1) La sortie est semblable √† celle de la fonction lm(). 6.8.2.1 Mod√®les mixtes non-lin√©aires Le mod√®le non lin√©aire cr√©√© plus haut liait le rendement √† la dose d‚Äôazote. Toutefois, les unit√©s exp√©rimentales (le site loc et l‚Äôann√©e year) n‚Äô√©taient pas pris en consid√©ration. Nous allons maintenant les consid√©rer. Nous devons d√©cider la structure de l‚Äôeffet al√©atoire, et sur quelles variables il doit √™tre appliqu√© - la d√©cision appartient √† l‚Äôanalyste. Il me semble plus convenable de supposer que le site et l‚Äôann√©e affectera le rendement maximum plut√¥t que l‚Äôenvironnement et le taux: les effets al√©atoires seront donc affect√©s √† la variable A. Les effets al√©atoires n‚Äôont pas de structure d‚Äôembo√Ætement. L‚Äôeffet de l‚Äôann√©e sur A sera celui d‚Äôune pente et l‚Äôeffet de site sera celui de l‚Äôintercept. La fonction que nous utiliserons est nlme(). mm &lt;- nlme(yield ~ A * (1 - exp(-R*(E + nitro))), data = engelstad.nitro, start = c(A = 75, E = 30, R = 0.02), fixed = list(A ~ 1, E ~ 1, R ~ 1), random = A ~ year | loc) summary(mm) ## Nonlinear mixed-effects model fit by maximum likelihood ## Model: yield ~ A * (1 - exp(-R * (E + nitro))) ## Data: engelstad.nitro ## AIC BIC logLik ## 477.2286 491.889 -231.6143 ## ## Random effects: ## Formula: A ~ year | loc ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## A.(Intercept) 2.611534836 A.(In) ## A.year 0.003066832 -0.556 ## Residual 11.152757999 ## ## Fixed effects: list(A ~ 1, E ~ 1, R ~ 1) ## Value Std.Error DF t-value p-value ## A.(Intercept) 74.58222 4.722715 56 15.792234 0.0000 ## E 65.56721 25.533993 56 2.567840 0.0129 ## R 0.01308 0.004808 56 2.720215 0.0087 ## Correlation: ## A.(In) E ## E 0.379 ## R -0.483 -0.934 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -1.83373132 -0.89293033 0.07418165 0.68353577 1.82434347 ## ## Number of Observations: 60 ## Number of Groups: 2 Et sur graphique: engelstad.nitro %&gt;% ggplot(aes(x = nitro, y = yield)) + facet_grid(year ~ loc) + geom_line(data = tibble(nitro = engelstad.nitro$nitro, yield = predict(mm, level = 0)), colour = &quot;grey35&quot;) + geom_point() + ylim(c(0, 95)) Les mod√®les mixtes non lin√©aires peuvent devenir tr√®s complexes lorsque les param√®tres, par exemple A, E et R, sont eux-m√™me affect√©s lin√©airement par des variables (par exemple A ~ topo). Pour aller plus loin, consultez Parent et al. (2017) ainsi que les calculs associ√©s √† l‚Äôarticle. Ou √©crivez-moi un courriel pour en discuter! Note. L‚Äôinterpr√©tation de p-values sur les mod√®les mixtes est controvers√©e. √Ä ce sujet, Douglas Bates a √©crit une longue lettre √† la communaut√© de d√©veloppement du module lme4, une alternative √† nlme, qui remet en cause l‚Äôutilisation des p-values, ici. De plus en plus, pour les mod√®les mixtes, on se tourne vers les statistiques bay√©siennes, couvertes dans le chapitre 7 avec le module greta. Mais en ce qui a trait aux mod√®les mixtes, le module brms automatise bien des aspects de l‚Äôapproche bay√©sienne. 6.8.3 Aller plus loin 6.8.3.1 Statistiques g√©n√©rales: The analysis of biological data 6.8.3.2 Statistiques avec R Disponibles en version √©lectronique √† la biblioth√®que de l‚ÄôUniversit√© Laval: Introduction aux statistiques avec R: Introductory statistics with R Approfondir les statistiques avec R: The R Book, Second edition Approfondir les mod√®les √† effets mixtes avec R: Mixed Effects Models and Extensions in Ecology with R ModernDive, un livre en ligne offrant une approche moderne avec le package moderndive. "],
["chapitre-biostats-bayes.html", "7 Introduction √† l‚Äôanalyse bay√©sienne en √©cologie 7.1 Qu‚Äôest-ce que c‚Äôest? 7.2 Pourquoi l‚Äôutiliser? 7.3 Comment l‚Äôutiliser? 7.4 Faucons p√©lerins 7.5 Statistiques d‚Äôune population 7.6 Test de t: Diff√©rence entre des groupes 7.7 Mod√©lisation multiniveau 7.8 Pour aller plus loin", " 7 Introduction √† l‚Äôanalyse bay√©sienne en √©cologie Ô∏è¬†Objectifs sp√©cifiques: Ce chapitre est un extra. Il ne fait pas partie des objectifs du cours. Il ne sera pas √©valu√©. √Ä la fin de ce chapitre, vous serez en mesure de d√©finir ce que sont les statistiques bay√©siennes serez en mesure de calculer des statistiques descriptives de base en mode bay√©sien avec le module greta. Les statistiques bay√©siennes forment une trousse d‚Äôoutils √† garder dans votre pack sack. 7.1 Qu‚Äôest-ce que c‚Äôest? En deux mots: mod√©lisation probabiliste. Un approche de mod√©lisation probabiliste se servant au mieux de l‚Äôinformation disponible. Pour calculer les probabilit√©s d‚Äôune variable inconnu en mode bay√©sien, nous avons besoin: De donn√©es D‚Äôun mod√®le D‚Äôune id√©e plus ou moins pr√©cise du r√©sultat avant d‚Äôavoir analys√© les donn√©es De mani√®re plus formelle, le th√©or√®me de Bayes (qui forme la base de l‚Äôanalyse bay√©seienne), dit que la distribution de probabilit√© des param√®tres d‚Äôun mod√®le (par exemple, la moyenne ou une pente) est proportionnelle √† la mutliplication de la distribution de probabilit√© estim√©e des param√®tres et la distribution de probabilit√© √©mergeant des donn√©es. Plus formellement, \\[P\\left(\\theta | y \\right) = \\frac{P\\left(y | \\theta \\right) \\times P\\left(\\theta\\right)}{P\\left(y \\right)}\\], o√π \\(P\\left(\\theta | y \\right)\\) \\(-\\) la probabilit√© d‚Äôobtenir des param√®tres \\(\\theta\\) √† partir des donn√©es \\(y\\) \\(-\\) est la distribution de probabilit√© a posteriori, calcul√©e √† partir de votre a prioti \\(P\\left(\\theta\\right)\\) \\(-\\) la probabilit√© d‚Äôobtenir des param√®tres \\(\\theta\\) sans √©gard aux donn√©es, selon votre connaissance du ph√©nom√®ne \\(-\\) et vos donn√©es observ√©es \\(P\\left(y | \\theta \\right)\\) \\(-\\) la probabilit√© d‚Äôobtenir les donn√©es \\(y\\) √©tant donn√©s les param√®tres \\(\\theta\\) qui r√©gissent le ph√©nom√®ne. \\(P\\left(y\\right)\\), la probabilit√© d‚Äôobserver les donn√©es, est appell√©e la vraissemblance marginale, et assure que la somme des probabilit√©s est nulle. 7.2 Pourquoi l‚Äôutiliser? Avec la notion fr√©quentielle de probabilit√©, on teste la probabilit√© d‚Äôobserver les donn√©es recueillies √©tant donn√©e l‚Äôabsence d‚Äôeffet r√©el (qui est l‚Äôhypoth√®se nulle g√©n√©ralement adopt√©e). La notion bay√©sienne de probabilit√© combine la connaissance que l‚Äôon a d‚Äôun ph√©nom√®ne et les donn√©es observ√©es pour estimer la probabilit√© qu‚Äôil existe un effet r√©el. En d‚Äôautre mots, les stats fr√©quentielles testent si les donn√©es concordent avec un mod√®le du r√©el, tandis que les stats bay√©siennes √©valuent, selon les donn√©es, la probabilit√© que le mod√®le soit r√©el. Le hic, c‚Äôest que lorsqu‚Äôon utilise les statistiques fr√©quentielles pour r√©pondre √† une question bay√©sienne, on s‚Äôexpose √† de mauvaises interpr√©tations. Par exemple, lors d‚Äôun projet consid√©rant la vie sur Mars, les stats fr√©quentielles √©valueront si les donn√©es recueillies sont conformes ou non avec l‚Äôhypoth√®se de la vie sur Mars. Par contre, pour √©valuer la probabilit√© de l‚Äôexistance de vie sur Mars, on devra passer par les stats bay√©siennes (exemple tir√©e du billet Dynamic Ecology ‚Äì Frequentist vs.¬†Bayesian statistics: resources to help you choose). 7.3 Comment l‚Äôutiliser? Bien que la formule du th√©or√®me de Bayes soit plut√¥t simple, calculer une fonction a posteriori demandera de passer par des algorithmes de simulation, ce qui pourrait demander une bonne puissance de calcul, et des outils appropri√©s. R comporte une panoplie d‚Äôoutils pour le calcul bay√©sien g√©n√©rique (rstan, rjags, MCMCpack, etc.), et d‚Äôautres outils pour des besoins particuliers (brms: R package for Bayesian generalized multivariate non-linear multilevel models using Stan). Nous utiliserons ici le module g√©n√©rique greta, qui permet de g√©n√©rer de mani√®re conviviale plusieurs types de mod√®les bay√©siens. Pour installer greta, vous devez pr√©alablement installer Python, gr√©√© des modules tensorflow et tensorflow-probability en suivant le guide. En somme, vous devez d‚Äôabord installer greta (install.packages(&quot;greta&quot;)). Puis vous devez installer une distribution de Python ‚Äì je vous sugg√®re Anaconda (~500 Mo) ou Miniconda pour une installation minimale (~60 Mo). Enfin, lancez les commandes suivantes (une connection internet est n√©cessaire pour t√©l√©charger les modules). Si vous avez install√© la version compl√®te d‚ÄôAnaconda, vous avez acc√®s √† Anaconda-navigator, une interface pour la gestion de vos environnements de calcul: assurez-vous qu‚Äôil soit fermer pour √©viter que la commande se butte √† des fichiers verouill√©s. greta::install_tensorflow( method = &quot;conda&quot;, envname = &quot;r-greta&quot;, version = &quot;1.14.0&quot;, extra_packages = &quot;tensorflow-probability==0.7.0&quot; ) Puis, vous devez installer une distribution de Python ‚Äì je vous sugg√®re Anaconda (~500 Mo) ou Miniconda pour une installation minimale (~60 Mo). Enfin, lancez les commandes suivantes pour installer Python, tensorflow et tensorflow-probability dans un nouvel environnement de calcul (nomm√© r-greta). reticulate::conda_create(envname = &quot;r-greta&quot;, packages = c(&quot;python&quot;, &quot;tensorflow=1.14&quot;, &quot;tensorflow-probability=0.7&quot;)) 7.4 Faucons p√©lerins Empruntons un exemple du livre Introduction to WinBUGS for Ecologists: A Bayesian Approach to Regression, ANOVA and Related Analyses, de Marc K√©ry et examinons la masse de faucons p√©lerins. Mais alors que Marc K√©ry utilise WinBUGS, un logiciel de r√©solution de probl√®me en mode bay√©sien, nous utiliserons greta. Source: Wikimedia Commons Pour une premi√®re approche, nous allons estimer la masse moyenne d‚Äôune population de faucons p√©lerins. √Ä titre de donn√©es, g√©n√©rons des nombres al√©atoires. Cette strat√©gie permet de valider les statistiques en les comparant aux param√®tre que l‚Äôon impose. Ici, nous imposons une moyenne de 600 grammes et un √©cart-type de 30 grammes. G√©n√©rons une s√©ries de donn√©es avec 20 √©chantillons. library(&quot;tidyverse&quot;) set.seed(5682) y20 &lt;- rnorm(n = 20, mean=600, sd = 30) y200 &lt;- rnorm(n = 200, mean=600, sd = 30) par(mfrow = c(1, 2)) hist(y20, breaks=5) hist(y200, breaks=20) Je cr√©e une fonction qui retourne la moyenne et l‚Äôerreur sur la moyenne ou sur la distribution. Calculons les statistiques classiques. confidence_interval &lt;- function(x, on=&quot;deviation&quot;, distribution=&quot;t&quot;, level=0.95) { m &lt;- mean(x) se &lt;- sd(x) n &lt;- length(x) if (distribution == &quot;t&quot;) { error &lt;- se * qt((1+level)/2, n-1) } else if (distribution == &quot;normal&quot;) { error &lt;- se * qnorm((1+level)/2) } if (on == &quot;error&quot;) { error &lt;- error/sqrt(n) } return(c(ll = m-error, mean = m, ul = m+error)) } print(&quot;D√©viation, 95%&quot;) ## [1] &quot;D√©viation, 95%&quot; print(round(confidence_interval(y20, on=&#39;deviation&#39;, level=0.95), 2)) ## ll mean ul ## 532.23 598.85 665.47 print(&quot;Erreur, 95%&quot;) ## [1] &quot;Erreur, 95%&quot; print(round(confidence_interval(y20, on=&#39;error&#39;, level=0.95), 2)) ## ll mean ul ## 583.96 598.85 613.75 print(&quot;√âcart-type&quot;) ## [1] &quot;√âcart-type&quot; print(round(sd(y20), 2)) ## [1] 31.83 En faisant cela, nous prenons pour acquis que les donn√©es sont distribu√©es normalement. En fait, nous savons qu‚Äôelles devraient l‚Äô√™tre pour de grands √©chantillons, puisque nous avons nous-m√™me g√©n√©r√© les donn√©es. Par contre, comme observateur par exemple de la s√©rie de 20 donn√©es g√©n√©r√©es, la distribution est d√©finitivement asym√©trique. Sous cet angle, la moyenne, ainsi que l‚Äô√©cart-type, pourraient √™tre des param√®tres biais√©s. Nous pouvons justifier le choix d‚Äôune loi normale par des connaissances a priori des distributions de masse parmi des esp√®ces d‚Äôoiseau. Ou bien transformer les donn√©es pour rendre leur distribution normale (chapitre 8). 7.5 Statistiques d‚Äôune population 7.5.1 Calcul analytique Supposer une distribution normale d‚Äôune population implique d‚Äôestimer deux param√®tres: sa moyenne et son √©cart-type. Toutefois, pour cet exemple, nous supposons que l‚Äô√©cart-type est connu, ce qui n‚Äôest √† toute fin pratique jamais le cas, mais vous d√©couvrirez bient√¥t pourquoi nous laisse tomber l‚Äô√©cart-type √† cette √©tape. Nous allons donc estimer la moyenne d‚Äôune population de faucons dont l‚Äô√©cart-type est de 30: \\(X \\sim \\mathcal{N}(\\mu, 30)\\). On sait qu‚Äôune distribution normale est d√©finir par la fonction suivante. \\[f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\] Ou, en R: normdist &lt;- function(x, mean, sd) { f &lt;- 1 / (sd * sqrt(2*pi)) * exp(-0.5 * ((x-mean)/sd)^2) return(f) } Ce qui n‚Äôest utile que pour une petite d√©monstration, √©tant donn√©e cette op√©ration peut √™tre effectu√©e avec la fonction dnorm(), qui vient avec le module stats charg√© en R par d√©faut. x_ &lt;- seq(0, 2000, 100) plot(x_, dnorm(x = x_, mean = 750, sd = 300), &quot;l&quot;, lwd = 4, col = &quot;pink&quot;) points(x_, normdist(x = x_, mean = 750, sd = 300)) Reprenons notre √©quation de Bayes. \\[P\\left(\\theta | y \\right) = \\frac{P\\left(y | \\theta \\right) \\times P\\left(\\theta\\right)}{P\\left(y \\right)}\\], En mode bay√©sien, nous devons d√©finir la connaissance a priori, P(), sous forme de variables al√©atoires non-observ√©es selon une distribution. Prenons l‚Äôexemple des faucons p√©lerins. Disons que nous ne savons pas √† quoi ressemble la moyenne du groupe a priori. Nous pouvons utiliser un a priori peu informatif, o√π la masse moyenne peut prendre n‚Äôimporte quelle valeur entre 0 et 2000 grammes, sans pr√©f√©rence: nous lui imposons donc un a priori selon une distribution uniforme. Idem pour l‚Äô√©cart-type. C‚Äôest ce qu‚Äôon appelle des a priori plats. Mais il est plut√¥t conseiller (Gelman et al., 2013) d‚Äôutiliser des a priori vagues plut√¥t que plats ou non-informatifs. En effet, des masses de 0 g ou 2000 g ne sont pas aussi probables qu‚Äôune masse de 750 g. Si vous √©tudiez les faucons p√©lerins (ce qui n‚Äôest pas mon cas), vous aurez une id√©e de sa masse, ne serait-ce qu‚Äôen arcourrant la litt√©rature √† son sujet. Mais disons que j‚Äôestime tr√®s vaguement qu‚Äôune masse moyenne √©chantillonale devrait √™tre autour de 750 g, avec un large √©cart-type de 200 g sur la moyenne. Il s‚Äôagit de l‚Äô√©cart-type de la moyenne, pas de l‚Äô√©cart-type de l‚Äô√©chantillon que nous supposons √™tre connu. Notez que l‚Äôa priori peu avoir la forme que l‚Äôon d√©sire: il s‚Äôagit seulement de cr√©er un vecteur. Toutefois, g√©n√©rer ce vecteur avec des distributions connues est aussi pratique qu‚Äô√©l√©gant. x_mean &lt;- seq(400, 1200, 5) prob &lt;- tibble(mass = x_mean, Prior = dnorm(x = x_mean, mean = 750, sd = 200)) prob$Prior &lt;- prob$Prior / sum(prob$Prior) prob %&gt;% ggplot(aes(mass, Prior))+ geom_line() + expand_limits(y=0) Note sur le jargon: √©tant donn√©e que cet a priori aura la m√™me distribution que l‚Äôa posteriori, on dit que cet a priori est conjugu√©. Nous allons √©galement utiliser nos donn√©es pour cr√©er une fonction de vraissemblance (likelihood), \\(P\\left(y | \\theta \\right)\\), qui est la distribution de probabilit√© issue des donn√©es: une distribution normale avec une moyenne calcul√©e et une variance connue. prob$Likelihood &lt;- dnorm(x = x_mean, mean = mean(y20), sd = 30) prob$Likelihood &lt;- prob$Likelihood / sum(prob$Likelihood) prob %&gt;% pivot_longer(-mass, names_to = &quot;type&quot;, values_to = &quot;probability&quot;) %&gt;% ggplot(aes(mass, probability, colour = type)) + geom_line() + expand_limits(y=0) Noter distribution a posteriori est proportionnelle √† la multiplication de l‚Äôa priori et de la vraissemblance. Puis nous allons normaliser l‚Äôa posteriori pour faire en sorte que la somme des probabilit√©s soit de 1. prob$Posterior &lt;- prob$Likelihood * prob$Prior prob$Posterior &lt;- prob$Posterior / sum(prob$Posterior) prob %&gt;% pivot_longer(-mass, names_to = &quot;type&quot;, values_to = &quot;probability&quot;) %&gt;% ggplot(aes(mass, probability, colour = type)) + geom_line() + expand_limits(y=0) La distribution a posteriori est presque call√©e sur les donn√©es. Pas √©tonnant, √©tant donn√©e que l‚Äôa priori est tr√®s vague. En revanche, un a priori plus affirm√©, avec un √©cart-type plus faible, aurait davantage de poids sur l‚Äôa posteriori. Exercice. Changez l‚Äôa priori et visualisez l‚Äôeffet sur l‚Äôa posteriori. Maintenant, imaginez ajouter l‚Äô√©cart-type. Cela reste faisable en calcul analytique, mais √ßa complique le calcul pour normaliser les proabilit√©. Ajoutez encore une variable et le calcul bay√©sien devient un v√©ritable casse-t√™te. En fait, en bay√©sien, la difficult√© de mettre √† l‚Äô√©chelle de plus d‚Äôun param√®tre rend rare la multiplication distributions de probabilit√©. C‚Äôest pourquoi l‚Äôon pr√©f√®re les simuler et √©chantillonnant, √† l‚Äôaide de diff√©rents algorithmes, la distribution a posteriori. En R, le module greta est con√ßu pour cela. 7.5.2 greta Chargeons d‚Äôabord les modules n√©cessaires. Avant de charger greta, il faut s√©lectionner l‚Äôenvironnement coda (Python) auquel se connecter. Lors de l‚Äôinstallation, nous avions sp√©cifi√© que l‚Äôinstallation se fasse dans l‚Äôenvironnement nomm√© r-greta. reticulate::use_condaenv(&quot;r-greta&quot;, required = TRUE) library(&quot;greta&quot;) ## ## Attaching package: &#39;greta&#39; ## The following object is masked from &#39;package:plotly&#39;: ## ## slice ## The following object is masked from &#39;package:dplyr&#39;: ## ## slice ## The following objects are masked from &#39;package:stats&#39;: ## ## binomial, cov2cor, poisson ## The following objects are masked from &#39;package:base&#39;: ## ## %*%, apply, backsolve, beta, chol2inv, colMeans, colSums, diag, eigen, forwardsolve, gamma, identity, ## rowMeans, rowSums, sweep, tapply library(&quot;DiagrammeR&quot;) library (&quot;bayesplot&quot;) ## This is bayesplot version 1.7.1 ## - Online documentation and vignettes at mc-stan.org/bayesplot ## - bayesplot theme set to bayesplot::theme_default() ## * Does _not_ affect other ggplot2 plots ## * See ?bayesplot_theme_set for details on theme setting library(&quot;tidybayes&quot;) Reprenons l‚Äôa piori utilis√© pr√©c√©demment. Dans greta, nous d√©finissons notre a priori ainsi. param_mean &lt;- normal(mean = 750, 200) L‚Äô√©cart-type d‚Äôun √©chantillon ne peut pas √™tre n√©gatif. Il est commun pour les √©carts-types d‚Äôutiliser une distribution en tronqu√©e √† 0. On pourrait utiliser une normale tronqu√©e, mais la cauchy tronqu√©e est souvent recommand√©e (e.g. Gelman, 2006) puisque la queue, plus √©paisse que la distribution normale, permet davantage de flexibilit√©. Disons que nous supposons un √©cart-type d‚Äôune moyenne de 50, et d‚Äôun √©cart-type de 100, tronqu√© √† 0. x_ &lt;- seq(0, 500, 10) plot(x_, dcauchy(x = x_, location = 50, scale = 100), &quot;l&quot;) param_sd &lt;- cauchy(location = 50, scale = 100, dim = NULL, truncation = c(0, Inf)) La fonction a porteriori inclue la fonction de vraissemblance ainsi que la connaissancew a priori. distribution(y20) &lt;- normal(param_mean, param_sd) Le tout forme un mod√®le pour appr√©cier y, la masse des faucons p√©lerins. m &lt;- model(param_mean, param_sd) plot(m) L√©gende: Le graphique du mod√®le montre que deux param√®tres sont attach√©s √† des distributions pour g√©n√©rer une distribution de sortie. Nous pouvons enfin lancer le mod√®le. draws &lt;- mcmc(m, n_samples = 1000) ## ## running 4 chains simultaneously on up to 4 cores ## warmup 0/1000 | eta: ?s warmup == 50/1000 | eta: 33s | 24% bad warmup ==== 100/1000 | eta: 22s | 12% bad warmup ====== 150/1000 | eta: 17s | 8% bad warmup ======== 200/1000 | eta: 15s | 6% bad warmup ========== 250/1000 | eta: 13s | 5% bad warmup =========== 300/1000 | eta: 12s | 4% bad warmup ============= 350/1000 | eta: 10s | 4% bad warmup =============== 400/1000 | eta: 9s | 3% bad warmup ================= 450/1000 | eta: 8s | 3% bad warmup =================== 500/1000 | eta: 7s | 2% bad warmup ===================== 550/1000 | eta: 7s | 2% bad warmup ======================= 600/1000 | eta: 6s | 2% bad warmup ========================= 650/1000 | eta: 5s | 2% bad warmup =========================== 700/1000 | eta: 4s | 2% bad warmup ============================ 750/1000 | eta: 3s | 2% bad warmup ============================== 800/1000 | eta: 3s | 2% bad warmup ================================ 850/1000 | eta: 2s | 1% bad warmup ================================== 900/1000 | eta: 1s | 1% bad warmup ==================================== 950/1000 | eta: 1s | 1% bad warmup ====================================== 1000/1000 | eta: 0s | 1% bad ## sampling 0/1000 | eta: ?s sampling == 50/1000 | eta: 6s sampling ==== 100/1000 | eta: 6s sampling ====== 150/1000 | eta: 5s sampling ======== 200/1000 | eta: 5s sampling ========== 250/1000 | eta: 5s sampling =========== 300/1000 | eta: 5s sampling ============= 350/1000 | eta: 4s sampling =============== 400/1000 | eta: 4s sampling ================= 450/1000 | eta: 4s sampling =================== 500/1000 | eta: 3s sampling ===================== 550/1000 | eta: 3s sampling ======================= 600/1000 | eta: 3s sampling ========================= 650/1000 | eta: 2s sampling =========================== 700/1000 | eta: 2s sampling ============================ 750/1000 | eta: 2s sampling ============================== 800/1000 | eta: 1s sampling ================================ 850/1000 | eta: 1s sampling ================================== 900/1000 | eta: 1s sampling ==================================== 950/1000 | eta: 0s sampling ====================================== 1000/1000 | eta: 0s L‚Äôinspection de l‚Äô√©chantillonnage peut √™tre effectu√©e gr√¢ce au module bayesplot. mcmc_combo(draws, combo = c(&quot;hist&quot;, &quot;trace&quot;)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. √Ä gauche, nous obtenons la distribution des param√®tres. √Ä droite, nous pouvons observer que l‚Äô√©chantillonnage semble stable. Dans le cas o√π il ne le serait pas, il faudrait revoir le mod√®le. Nous pouvons calculer des intervales de cr√©dibilt√©. draws_tidy &lt;- draws %&gt;% spread_draws(param_mean, param_sd) draws_mean &lt;- confidence_interval(x = draws_tidy$param_mean, on = &quot;deviation&quot;, distribution = &quot;normal&quot;, level = 0.95) print(&quot;Moyenne:&quot;) ## [1] &quot;Moyenne:&quot; draws_mean ## ll mean ul ## 583.5554 598.7328 613.9103 draws_sd &lt;- confidence_interval(x = draws_tidy$param_sd, on = &quot;deviation&quot;, distribution = &quot;normal&quot;, level = 0.95) print(&quot;√âcart-type:&quot;) ## [1] &quot;√âcart-type:&quot; draws_sd ## ll mean ul ## 22.18403 34.16479 46.14555 L‚Äôa priori √©tant vague, les r√©sultats de l‚Äôanalyse bay√©sienne sont comparables aux statistiques fr√©quentielles. print(&quot;Erreur, 95%&quot;) ## [1] &quot;Erreur, 95%&quot; print(round(confidence_interval(y20, on=&#39;error&#39;, level=0.95), 2)) ## ll mean ul ## 583.96 598.85 613.75 Les r√©sultats des deux approches doivent n√©anmoins √™tre interpr√©t√©s de mani√®re diff√©rente. En ce qui a trait √† la moyenne: Fr√©quentiel. Il y a une probabilit√© de 95% que mes donn√©es aient √©t√© g√©n√©r√©es √† partir d‚Äôune moyenne se situant entre 584 et 614 grammes. Bay√©sien. √âtant donn√©e mes connaissances (vagues) de la moyenne et de l‚Äô√©cart-type avant de proc√©der √† l‚Äôanalyse (a priori), il y a une probabilit√© de 95% que la moyenne de la masse de la population se situe entre 583.6 et 613.9 grammes. Nous avons une id√©e de la distribution des param√®tres‚Ä¶ mais pas de la masse dans la population. Pas de probl√®me: nous avons des √©chantillons de moyennes et d‚Äô√©cart-type. Nous pouvons les √©chantilonn√©s avec remplacement pour g√©n√©rer des possibilit√©s de distrbution, puis √©chantillonn√© une masse selon ces distributions √©chantillonn√©es. Disons‚Ä¶ 10000? Figure 7.1: Source: Star Wars, a new hope Yep, 10 000. n_mass &lt;- 10000 sim_mass &lt;- rep(0, n_mass) for (i in 1:n_mass) { sim_mean &lt;- sample(draws_tidy$param_mean) sim_sd &lt;- sample(draws_tidy$param_sd) sim_mass[i] &lt;- rnorm(1, sim_mean, sim_sd) } La distribution avec laquelle j‚Äôai cr√©√© les donn√©es y20 plus haut avait une moyenne de 600 et un √©cart-type de 30. Je la superpose ici avec La distribution mod√©lis√©e avec notre petit mod√®le bay√©sien. x_ &lt;- seq(450, 750, 5) plot(x_, dnorm(x_, 600, 30), lty = 3, col = &quot;red&quot;, type = &quot;l&quot;, xlab = &quot;Mass (g)&quot;, ylab = &quot;Density&quot;) lines(density(sim_mass), col = &quot;blue&quot;) sim_mass_limits &lt;- confidence_interval(x = sim_mass, on = &quot;deviation&quot;, distribution = &quot;normal&quot;, level = 0.95) abline(v = sim_mass_limits[1], lty = 2, col = &quot;blue&quot;) abline(v = sim_mass_limits[3], lty = 2, col = &quot;blue&quot;) text(x = sim_mass_limits[1], y = 0.01, labels = round(sim_mass_limits[1]), pos = 2, col = &quot;blue&quot;) text(x = sim_mass_limits[3], y = 0.01, labels = round(sim_mass_limits[3]), pos = 4, col = &quot;blue&quot;) Raisonnement bay√©sien: √âtant donn√©e mes connaissances vagues de la moyenne et de l‚Äô√©cart-type avant de proc√©der √† l‚Äôanalyse, il y a une probabilit√© de 95% que la masse de la population se situe entre 529.1 et 668.9 grammes. Nous avons maintenant une id√©e de la distribution de moyenne de la population. Mais, rarement, une analyse s‚Äôarr√™tera √† ce stade. Il arrive souvent que l‚Äôon doive comparer les param√®tres de deux, voire plusieurs groupes. Par exemple, comparer des populations vivants dans des √©cosyst√®mes diff√©rents, ou comparer un traitement √† un plac√©bo. Ou bien, comparer, dans une m√™me population de faucons p√©lerins, l‚Äôenvergure des ailes des m√¢les et celle des femelles. 7.6 Test de t: Diff√©rence entre des groupes Pour comparer des groupes, on exprime g√©n√©ralement une hypoth√®se nulle, qui typiquement pose qu‚Äôil n‚Äôy a pas de diff√©rence entre les groupes. Puis, on choisit un test statistique pour d√©terminer si les distributions des donn√©es observ√©es sont plausibles dans si l‚Äôhypoth√®se nulle est vraie. En d‚Äôautres mots, le test statistique exprime la probabilit√© que l‚Äôon obtienne les donn√©es obtenues s‚Äôil n‚Äôy avait pas de diff√©rence entre les groupes. Par exemple, si vous obtenez une p-value de moins de 0.05 apr√®s un test de comparaison et l‚Äôhypoth√®se nulle pose qu‚Äôil n‚Äôy a pas de diff√©rence entre les groupes, cela signifie qu‚Äôil y a une probabilit√© de 5% que vous ayiez obtenu ces donn√©es s‚Äôil n‚Äôy avait en fait pas de diff√©rence entre les groupe. Il serait donc peu probable que vos donn√©es euent √©t√© g√©n√©r√©es comme telles s‚Äôil n‚Äôy avait en fait pas de diff√©rence. n_f &lt;- 30 moy_f &lt;- 105 n_m &lt;- 20 moy_m &lt;- 77.5 sd_fm &lt;- 2.75 set.seed(21526) envergure_f &lt;- rnorm(mean=moy_f, sd=sd_fm, n=n_f) envergure_m &lt;- rnorm(mean=moy_m, sd=sd_fm, n=n_m) envergure_f_df &lt;- data.frame(Sex = &quot;Female&quot;, Wingspan = envergure_f) envergure_m_df &lt;- data.frame(Sex = &quot;Male&quot;, Wingspan = envergure_m) envergure_df &lt;- rbind(envergure_f_df, envergure_m_df) envergure_df %&gt;% ggplot(aes(x=Wingspan)) + geom_histogram(aes(y=..density.., fill=Sex)) + geom_density(aes(value=Sex, y=..density..)) ## Warning: Ignoring unknown aesthetics: value ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Et les statistiques des deux groupesL envergure_df %&gt;% group_by(Sex) %&gt;% summarise(mean = mean(Wingspan), sd = sd(Wingspan), n = n()) ## # A tibble: 2 x 4 ## Sex mean sd n ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Female 105. 2.46 30 ## 2 Male 77.0 3.19 20 √âvaluer s‚Äôil y a une diff√©rence significative peut se faire avec un test de t (ou de Student). t.test(envergure_f, envergure_m) ## ## Welch Two Sample t-test ## ## data: envergure_f and envergure_m ## t = 33.235, df = 33.665, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 26.29232 29.71848 ## sample estimates: ## mean of x mean of y ## 105.04267 77.03727 La probabilit√© que les donn√©es ait √©t√© g√©n√©r√©es de la sorte si les deux groupes n‚Äô√©tait semblables est tr√®s faible (p-value &lt; 2.2e-16). On obtiendrait sensiblement les m√™mes r√©sultats avec une r√©gression lin√©aire. linmod &lt;- lm(Wingspan ~ Sex, envergure_df) summary(linmod) ## ## Call: ## lm(formula = Wingspan ~ Sex, data = envergure_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.221 -1.938 0.219 2.046 4.686 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 105.0427 0.5062 207.51 &lt;2e-16 *** ## SexMale -28.0054 0.8004 -34.99 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.773 on 48 degrees of freedom ## Multiple R-squared: 0.9623, Adjusted R-squared: 0.9615 ## F-statistic: 1224 on 1 and 48 DF, p-value: &lt; 2.2e-16 Le mod√®le lin√©aire est plus informatif. Il nous apprend que l‚Äôenvergure des ailes des m√¢les est en moyenne plus faible de 28.0 cm que celle des femelles‚Ä¶ confint(linmod, level = 0.95) ## 2.5 % 97.5 % ## (Intercept) 104.02487 106.06047 ## SexMale -29.61468 -26.39612 ‚Ä¶ avec un intervalle de confiance entre -29.6 cm √† -26.4 cm. Utilisons l‚Äôinformation d√©riv√©e de statistiques classiques dans nos a priori. Oui-oui, on peut faire √ßa. Mais attention, un a priori trop pr√©cis ou trop coll√© sur nos donn√©es orientera le mod√®le vers une solution pr√©alablement √©tablie: ce qui constituerait aucune avanc√©e par rapport √† l‚Äôa priori. Nous allons utiliser a priori pour les deux groupes la moyenne des deux groupes, et comme dispersion la moyenne le double de l‚Äô√©cart-type. Rappelons que cet √©cart-type est l‚Äôa priori de √©cart-type sur la moyenne, non pas de la population. Proc√©dons √† la cr√©ation d‚Äôun mod√®le greta. Nous utiliserons la r√©gression lin√©aire pr√©f√©rablement au test de t. is_female &lt;- model.matrix(~envergure_df$Sex)[, 2] int &lt;- normal(600, 30) coef &lt;- normal(30, 10) sd &lt;- cauchy(0, 10, truncation = c(0, Inf)) mu &lt;- int + coef * is_female distribution(envergure_df$Wingspan) &lt;- normal(mu, sd) m &lt;- model(int, coef, sd, mu) plot(m) Go! draws &lt;- mcmc(m, n_samples = 1000) Et les r√©sultats. mcmc_combo(draws, combo = c(&quot;dens&quot;, &quot;trace&quot;), pars = c(&quot;int&quot;, &quot;coef&quot;, &quot;sd&quot;)) draws_tidy &lt;- draws %&gt;% spread_draws(int, coef, sd) draws_tidy print(&quot;Intercept:&quot;) confidence_interval(x = draws_tidy$int, on = &quot;deviation&quot;, distribution = &quot;normal&quot;, level = 0.95) print(&quot;Pente:&quot;) confidence_interval(x = draws_tidy$coef, on = &quot;deviation&quot;, distribution = &quot;normal&quot;, level = 0.95) 7.7 Mod√©lisation multiniveau Vous souvenez-vous en quoi consiste un effet al√©atoire? Pour rappel, il s‚Äôagit d‚Äôun effet global nul mais variable d‚Äôun groupe √† l‚Äôautre, alors qu‚Äôun effet fixe ne subit pas la contrainte d‚Äôeffet nul. En mod√©lisation lin√©aire, l‚Äôeffet al√©atoire peut se trouver sur l‚Äôintercept ou sur une pente (ou plusieurs pentes). Ce concept peut √™tre port√© naturellement en mod√©lisation bay√©sienne en ajoutant √† l‚Äôintercept ou √† une pente un effet dont l‚Äôa priori est une distribution √©tal√©e autour de z√©ro (effet global nul, mais variable). Reprenons le mod√®le consid√©r√© √† la section 6. mmodlin_1 &lt;- lme(fixed = yield ~ lat + long + nitro + topo + bv, random = ~ 1|year/rep, data = lasrosas.corn) data(lasrosas.corn, package = &quot;agridat&quot;) lasrosas.corn$year_rep &lt;- paste0(lasrosas.corn$year, &quot;_&quot;, lasrosas.corn$rep) lasrosas.corn_sc &lt;- lasrosas.corn %&gt;% select(lat, long, nitro, bv) %&gt;% mutate_all(scale) %&gt;% bind_cols(lasrosas.corn %&gt;% select(-lat, -long, -nitro, -bv)) %&gt;% mutate(year = as.factor(year)) corn_modmat &lt;- model.matrix(~lat + long + nitro + topo + bv + year + year_rep, data = lasrosas.corn_sc) head(corn_modmat) Nous devons d√©finir nos a priori sur les param√®tres du mod√®le. Nous avons l‚Äôintercept, ainsi que les pentes des effets fixes et des effets al√©atoires. Nous pourrions cr√©er un a priori par param√®tre, ou bien s√©parer l‚Äôintercept des autres pentes, mais nous allons plut√¥t cr√©er un jeu d‚Äôa priori pour les effets fixes et un autre pour les effets al√©atoires. coef_fixed &lt;- normal(0, 10, dim = 8) # intercept et pentes fixes mu &lt;- corn_modmat[, 1:8] %*% coef_fixed sd &lt;- cauchy(0, 3, truncation = c(0, Inf)) distribution(lasrosas.corn_sc$yield) &lt;- normal(mu, sd) m &lt;- model(coef_fixed, sd, mu) plot(m) #draws &lt;- greta::mcmc(m, n_samples = 100, warmup = 10) #mcmc_combo(draws, combo = c(&quot;dens&quot;, &quot;trace&quot;), pars = c(&quot;mu&quot;)) Chapitre en cours de r√©daction‚Ä¶ 7.8 Pour aller plus loin Le module greta est con√ßu et maintenu par Nick Golding, du Quantitative &amp; Applied Ecology Group de l‚ÄôUniversity of Melbourne, Australie. La documentation de greta offre des recettes pour toutes sortes d‚Äôanalyses en √©cologie. Les livres de Mark K√©ry, bien que r√©dig√©s pour les calculs en langage R et WinBUGS, offre une approche bien structur√©e et traduisible en greta, qui est plus moderne que WinBUGS. Introduction to WinBUGS for Ecologists (2010) Bayesian Population Analysis using WinBUGS: A Hierarchical Perspective (2011) Applied Hierarchical Modeling in Ecology: Analysis of distribution, abundance and species richness in R and BUGS (2015) "],
["chapitre-explorer.html", "8 Explorer R 8.1 R sur le web 8.2 R en chaire et en os 8.3 Quelques outils en √©cologie math√©matique avec R", " 8 Explorer R L‚Äôapprentissage de R peut √™tre √©tourdissant. Cette section est une petite pause fourre-tout qui vous introduira aux nombreuses possibilit√©s de R. Ô∏è¬†Objectifs sp√©cifiques: √Ä la fin de ce chapitre, vous serez en mesure d‚Äôidentifier les sources d‚Äôinformation principales sur le d√©veloppement de R et de ses modules comprendrez l‚Äôimportance du pr√©traitement des donn√©es, en particulier dans le cadre de l‚Äôanalyse de donn√©es compositionnelles, et saurez effectuer un pr√©traitement ad√©quat saurez comment acqu√©rir des donn√©es m√©t√©o d‚ÄôEnvironnement Canada avec le module weathercan saurez identifier les modules d‚Äôanalyse de sols (soiltexture et aqp) saurez comment d√©buter un projet de m√©ta-analyse et de d√©ploiement d‚Äôun logiciel sur R Pour certains, le langage R est un labyrinthe. Pour d‚Äôautres, c‚Äôest une myriade de portes ouvertes. Si vous lisez ce manuel, vous vous √™tes peut-√™tre engag√© dans un labyrinthe dans l‚Äôobjectif d‚Äôy trouver la cl√© qui d√©v√©rouillera une porte bien pr√©cise qui m√®ne √† un tr√©sor, un objet magique‚Ä¶ ou un dipl√¥me. Peut-√™tre aussi prendrez-vous le go√ªt d‚Äôerrer dans ce labyrinthe, explorant ses d√©bouch√©s, pour y d√©nicher au hasard des petits outils et des d√©bouch√©s. S√©quence du jeu vid√©o The legend of Zelda. Cette section est un amalgame de plusieurs outils de R pertinents en analyse √©cologique. 8.1 R sur le web Dans un environnement de travail en √©volution rapide et constante, il est difficile de consid√©rer que ses comp√©tences sont abouties. Rester inform√© sur le d√©veloppement de R vous permettra de d√©nicher de r√©soudre des probl√®mes persistants de mani√®re plus efficace ou par de nouvelles avenues, et vous offrira m√™me l‚Äôoccasion de d√©nicher des probl√®mes dont vous ne soup√ßonniez pas l‚Äôexistance. Plusieurs sources d‚Äôinformation vous permettront de vous tenir √† jour sur le d√©veloppement de R, de ses environnement de travail (RStudio, Jupyter, Atom, etc.) et des nouveaux modules qui s‚Äôy greffent. Plus largement, vous gagnerez √† vous informer sur les derni√®res tendances en calcul scientifique sur d‚Äôautres plate-forme que R (Python, Javascript, Julia, etc.). √âvidemment, nos t√¢ches quotidiennes ne nous permettent pas de tout suivre. M√™me si vous pouviez n‚Äôattrapper qu‚Äô1% du d√©filement, ce sera d√©j√† 1% de plus que rien du tout. √âvidemment, rester au courant aide parce que vous en apprenez davantage sur les outils et leurs applications. Mais √ßa aide aussi parce que √ßa vous permet de conna√Ætre des gens et des organisations! Il est tr√®s utile de savoir qui travaille sur quoi et o√π se d√©roulent les d√©veloppements sur un sujet donn√©, car si vous cherchez consciemment quelque chose plus tard, √ßa vous aidera √† trouver votre chemin plus facilement. - Ma√´lle Salmon, Keeping up to date with R news (ma traduction) Je vous propose une liste de ressources. Ne vous y tenez surtout pas: discartez ce qui ne vous convient pas, et partez √† l‚Äôaventure! The Hobbit: An Unexpected Journey, Peter Jackson (2012) 8.1.1 GitHub Nous verrons au chapitre 5 l‚Äôimportance d‚Äôutilser des outils d‚Äôarchivage et de suivi de version, comme git, dans le d√©ploiement de la science ouverte. Pour l‚Äôinstant, retenons que GitHub est une plate-forme git en ligne acquise par Microsoft qui est devenue un r√©seau social de d√©veloppement informatique. De nombreux modules de R y sont d√©velopp√©s. Au chapitre 5, vous serez invit√©s √† y ouvrir un compte et √† y archiver du contenu. Vous pourrez alors suivre (dans le m√™me sens que sur Facebook ou Twitter) le d√©veloppement de projets et suivre les travaux des personnes qui vous semblent d‚Äôint√©r√™t. 8.1.2 Twitter Le hashtag #rstats rassemble sur Twitter ce qui se tweete sur le sujet. On y retrouve les comptes de R-bloggers, RStudio et rOpenSci. Certaines communaut√© y sont aussi actives, comme R4DS online learning community, qui partage des nouvelles sur R, et R-Ladies Global, qui vise √† amener davantage de diversit√© √† la communaut√© de R. Des comptes th√©matiques comme Daily R Cheatsheets et One R Package a Day permettent de d√©couvrir quotidiennement de nouvelles possibilit√©s. Enfin, plusieurs personnes contribuent positivement √† la communaut√© R. Hadley Wickham brille parmi les √©toiles de R. Les comptes de Mara Averick, Claus Wilke et David Robinson sont aussi int√©ressants. 8.1.3 Nouvelles Le site d‚Äôaggr√©gation R-bloggers, mis √† jour quotidiennement, republie des articles en anglais tir√©s d‚Äôun peu partout sur la toile. On y trouve principalement des tutoriels et des annonces de nouveaux d√©veloppement. Deux fois par mois, l‚Äôorganisation rOpenSci offre un portrait de l‚Äôuniv-R (üí©), ce que R Weekely offre de mani√®re hebdomadaire (l‚Äôinformation sera probablement redondante). Le tidyverse a quant √† lui son propre blogue. 8.1.4 Des questions? Bien que davantage vou√©s √† la r√©solution de probl√®me qu‚Äô √† l‚Äôexploration de nouvelles opportunit√©s, Stackoverflow et Cross Validated sont des plate-forme pris√©es. De plus, la liste de courriels r-sig-ecology permet des √©changes entre professionnels et novices en analyse de donn√©es √©cologiques avec R. 8.1.5 Participer R est un logiciel bas√© sur une communaut√© de d√©veloppement, d‚Äôutilisation et de vulgarisation. Des personnes offrent g√©n√©reusement du temps de support. Si vous vous sentez √† l‚Äôaise, offrez aussi le v√¥tre! 8.1.6 Mise en garde Les modules de R sont d√©velopp√©s par quiconque le veut bien: leur qualit√© n‚Äôest pas n√©cessairement audit√©e. Souvent, ils ne sont v√©rifi√©s que par une vigilance communautaire: dans ce cas, vous √™tes les cobailles. Ce qui n‚Äôest pas n√©cessairement une mauvaise chose, mais cela n√©cessite de prendre ses pr√©cautions. Dans sa conf√©rence How to be a resilient R user, Ma√´lle Salmon propose quelques guides pour juger de la qualit√© d‚Äôun module. 1. Le module est-il activement d√©velopp√©? Bien! Attention! 2. Le module est-il bien test√©? V√©rifiez si le module a fait l‚Äôobjet d‚Äôune publication scientifique, s‚Äôil a √©t√© utilis√© avec succ√®s dans la lit√©rature ou dans des documents cr√©dibles. 3. Le module est-il bien document√©? Un site internet d√©di√© est-il utilis√© pour documenter l‚Äôutilisation du module? Les fichiers d‚Äôaide sont-ils complets, et sont-ils de bonne qualit√©? 4. Le module est-il largement utilis√©? Un module peu populaire n‚Äôest pas n√©cessessairement de mauvaise qualit√©: peut-√™tre est-il seulement destin√© √† des applications de niche. S‚Äôil n‚Äôest pas un indicateur √† lui seul de la solidit√© ou la validit√© d‚Äôun module, une masse critique indique que le module a pass√© sous la surveillance de plusieurs utilisateurs. Dans GitHub, ceci peut √™tre √©valu√© par le nombre d‚Äô√©toiles attribu√© au module (√©quivalent √† un J‚Äôaime). 5. Le module est-il d√©velopp√© par une personne ou une organisation cr√©dible? On peut affirmer sans trop se compromettre que l‚Äô√©quipe de RStudio d√©veloppe des modules de confiance. Tout comme il faudrait se m√©fier d‚Äôun module d√©velopp√© par une personne anonyme. Le module packagemetrics permet d‚Äô√©valuer ces crit√®res. # devtools::install_github(&quot;ropenscilabs/packagemetrics&quot;) library(&quot;packagemetrics&quot;) pm &lt;- package_list_metrics(c(&quot;dplyr&quot;, &quot;ggplot2&quot;, &quot;vegan&quot;, &quot;greta&quot;)) metrics_table(pm) 8.1.7 Prendre tout √ßa en note Un logiciel de prise de notes (comme Evernote, OneNote, Notion, Simplenote, Turtl, etc.) pourrait vous √™tre utile pour retrouver l‚Äôinformation soutir√©e de vos flux d‚Äôinformation. Mais certaines personnes consignent simplement leurs informations dans un carnet ou un document de traitement de texte. 8.2 R en chaire et en os L‚ÄôUniversit√© Laval (institution aupr√®s de laquelle ce manuel est d√©velopp√©) sera haute en mai 2019 de la conf√©rence R √† Qu√©bec 2019. Des ateliers seront offerts pour les utilisateurs novices et avanc√©s. 8.3 Quelques outils en √©cologie math√©matique avec R 8.3.1 Pr√©traitement des donn√©es Il arrive souvent ques les donn√©es brutes ne soient pas exprim√©es de mani√®re appropri√©e ou optimale pour l‚Äôanalyse statistique ou la mod√©lisation. Vous devrez alors effectuer un pr√©traitement sur ces donn√©es. Lors du chapitre 6, nous avons abord√© la mise √† l‚Äô√©chelle, o√π des variables num√©riques √©taient transform√©es pour avoir une moyenne de z√©ro et un √©cart-type de 1. Cette op√©ration permettait d‚Äôappr√©cier les coefficients et leur incertitude sur une m√™me √©chelle. L‚Äôencodage cat√©gorielle a quant √† lui permi d‚Äôutiliser des m√©thodes quantitatives sur des donn√©es qualitatives. Dans les deux cas, nous n‚Äôavons pas utilis√© le terme, mais il s‚Äôagissait d‚Äôun pr√©traitement, c‚Äôest-√†-dire une transformation des donn√©es pr√©alable √† l‚Äôanalyse ou la mod√©lisation. Un pr√©traitement peut consister simplement en une transformation logarithmique ou exponentielle. En particulier, si vos donn√©es forment une partie d‚Äôun tout (exprim√©es en pourcentages ou fractions), vous devriez probablement utiliser un pr√©traitement gr√¢ce aux outils de l‚Äôanalyse compositionnelle. Avant de les aborder, nous allons traiter des transformations de base. 8.3.1.1 Standardisation La standardisation consiste √† centrer vos donn√©es √† une moyenne de 0 et √† les √©chelonner √† une variance de 1, c‚Äôest-√†-dire \\[x_{standard} = \\frac{x - \\bar{x}}{\\sigma}\\] o√π \\(\\bar{x}\\) est la moyenne du vecteur \\(x\\) et o√π \\(\\sigma\\) est son √©cart-type. Ce pr√©traitement des donn√©es peut s‚Äôav√©r√©r utile lorsque la mod√©lisation tient compte de l‚Äô√©chelle de vos mesures (par exemple, les param√®tres de r√©gression vus au chapitre 6 ou les distances que nous verrons au chapitre 9). En effet, les pentes d‚Äôune r√©gression lin√©aire multiple ne pourront √™tre compar√©es entre elles que si elles sont une m√™me √©chelle. Par exemple, on veut mod√©liser la consommation en miles au gallon (mpg) de voitures en fonction de leur puissance (hp), le temps en secondes pour parcourir un quart de mile (qsec) et le nombre de cylindre. data(&quot;mtcars&quot;) modl &lt;- lm(mpg ~ hp + qsec + cyl, mtcars) summary(modl) ## ## Call: ## lm(formula = mpg ~ hp + qsec + cyl, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.3223 -1.9483 -0.5656 1.5452 7.7773 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 55.30540 9.03697 6.120 1.33e-06 *** ## hp -0.03552 0.01622 -2.190 0.03700 * ## qsec -0.89424 0.42755 -2.092 0.04567 * ## cyl -2.26960 0.54505 -4.164 0.00027 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.003 on 28 degrees of freedom ## Multiple R-squared: 0.7757, Adjusted R-squared: 0.7517 ## F-statistic: 32.29 on 3 and 28 DF, p-value: 3.135e-09 Les pentes signifient que la distance parcourue par gallon d‚Äôessence diminue de 0.03552 miles au gallon pour chaque HP, de 0.89242 par seconde au quart de mile et de 2.2696 par cyclindre additionnel. L‚Äôinterpr√©tation est conviviale √† cette √©chelle. Mais lequel de ces effets est le plus important? L t value indique que ce seraient les cylindres. Mais pour juger l‚Äôimportance en terme de pente, il vaudrait mieux standardiser. library(&quot;tidyverse&quot;) standardise &lt;- function(x) (x-mean(x))/sd(x) mtcars_sc &lt;- mtcars %&gt;% mutate_if(is.numeric, standardise) # ou bien scale(mtcars, center = TRUE, scale = TRUE) modl_sc &lt;- lm(mpg ~ hp + qsec + cyl, mtcars_sc) summary(modl_sc) ## ## Call: ## lm(formula = mpg ~ hp + qsec + cyl, data = mtcars_sc) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.71716 -0.32326 -0.09384 0.25639 1.29042 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.061e-16 8.808e-02 0.000 1.00000 ## hp -4.041e-01 1.845e-01 -2.190 0.03700 * ## qsec -2.651e-01 1.268e-01 -2.092 0.04567 * ## cyl -6.725e-01 1.615e-01 -4.164 0.00027 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4983 on 28 degrees of freedom ## Multiple R-squared: 0.7757, Adjusted R-squared: 0.7517 ## F-statistic: 32.29 on 3 and 28 DF, p-value: 3.135e-09 Les valeurs des pentes ne peuvent plus √™tre interpr√©t√©es directement, mais peuvent maintenant √™tre compar√©es entre elles. Dans ce cas, le nombre de cilyndres a en effet une importance plus grande que la puissance et le temps pour parcourir un 1/4 de mile. Les algorithmes bas√©s sur des distances auront, de m√™me, avantage √† √™tre standardis√©s. 8.3.1.2 √Ä l‚Äô√©chelle de la plage Si vous d√©sirez pr√©server le z√©ro dans le cas de donn√©es positives ou plus g√©n√©ralement vous voulez que vos donn√©es pr√©trait√©es soient positives, vous pouvez les transformer √† l‚Äô√©chelle de la plage, c‚Äôest-√†-dire les forcer √† s‚Äô√©taler de 0 √† 1: \\[ x_{range01} = \\frac{x - x_{min}}{x_{max} - x_{min}} \\] Cette transformation est sensible aux valeurs aberrantes, et une fois le vecteur transform√© les valeurs aberrantes seront toutefois plus difficiles √† d√©tecter. range_01 &lt;- function(x) (x-min(x))/(max(x) - min(x)) mtcars %&gt;% mutate_if(is.numeric, range_01) %&gt;% # en fait, toutes les colonnes sont num√©riques, alors mutate_all aurait pu √™tre utilis√© au lieu de mutate_if sample_n(4) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 0.3531915 1 0.7206286 0.4346290 0.17972350 0.49271286 0.3000000 0 0 0.0 0.1428571 ## 2 1.0000000 0 0.0000000 0.0459364 0.67281106 0.08233188 0.6428571 1 1 0.5 0.0000000 ## 3 0.5276596 0 0.1738588 0.1519435 0.53456221 0.41856303 1.0000000 1 0 0.5 0.1428571 ## 4 0.0000000 1 1.0000000 0.5406360 0.07834101 0.95551010 0.4142857 0 0 0.0 0.4285714 8.3.1.3 Normaliser Le terme normaliser est associer √† des op√©rations diff√©rentes dans la litt√©rature. Nous prendrons la nomenclature de scikit-learn, pour qui la normalisation consiste √† faire en sorte que la longueur du vecteur (sa norme, d‚Äôo√π normaliser) soit unitaire. Cette op√©ration est le plus souvent utilis√©e par observation (ligne), non pas par variable (colonne). Il existe plusieurs mani√®res de mesures la distance d‚Äôun vecteur, mais la plus commune est la distance euclidienne. La seule fois que j‚Äôai eu √† utiliser ce pr√©traitement √©tait en analyse spectrale (Chemometrics with R, Ron Wehrens, 2011, chapitre 3.5). En R, library(&quot;pls&quot;) ## ## Attaching package: &#39;pls&#39; ## The following object is masked from &#39;package:vegan&#39;: ## ## scores ## The following object is masked from &#39;package:stats&#39;: ## ## loadings data(&quot;gasoline&quot;) spectro &lt;- gasoline$NIR %&gt;% unclass() %&gt;% as_tibble() normalise &lt;- function(x) x/sqrt(sum(x^2)) spectro_norm &lt;- spectro %&gt;% rowwise() %&gt;% # diff√©rentes approches possibles pour les op√©rations sur les lignes normalise() spectro_norm[1:4, 1:4] ## 900 nm 902 nm 904 nm 906 nm ## 1 -0.0011224834 -0.0010265446 -0.0009434425 -0.0008314021 ## 2 -0.0009890637 -0.0008856332 -0.0007977676 -0.0006912734 ## 3 -0.0010481029 -0.0009227116 -0.0008269742 -0.0007035061 ## 4 -0.0010444801 -0.0009446277 -0.0008623530 -0.0007718261 8.3.1.4 Analyse compositionnelle en R En 1898, le statisticien Karl Pearson nota que des corr√©lations √©taient induites lorsque l‚Äôon effectuait des ratios par rapport √† une variable commune. Source Karl Pearson, 1897. Mathematical contributions to the theory of evolution.‚Äîon a form of spurious correlation which may arise when indices are used in the measurement of organs. Proceedings of the royal society of London Faisons l‚Äôexercice! Nous g√©n√©rons au hasard 1000 donn√©es (comme le proposait Pearson) pour trois dimensions: le f√©mur, le tibia et l‚Äôhum√©rus. Ces dimensions ne sont pas g√©n√©r√©es par des distributions corr√©l√©es. set.seed(3570536) n &lt;- 1000 bones &lt;- tibble(femur = rnorm(n, 10, 3), tibia = rnorm(n, 8, 2), humerus = rnorm(n, 6, 2)) plot(bones) cor(bones) ## femur tibia humerus ## femur 1.000000000 -0.069006171 0.002652292 ## tibia -0.069006171 1.000000000 -0.008994704 ## humerus 0.002652292 -0.008994704 1.000000000 Pourtant, si j‚Äôutilise des ratios allom√©triques avec l‚Äôhum√©rus comme base, bones_r &lt;- bones %&gt;% transmute(fh = femur/humerus, th = tibia/humerus) plot(bones_r) text(30, 20, paste(&quot;corr√©lation =&quot;, round(cor(bones_r$fh, bones_r$th), 2)), col = &quot;blue&quot;) Nous avons induit ce que Pearson appelait une fausse corr√©lation (spurious correlation). En 1960, Chayes proposa que de telles fausses corr√©lations sont induites non seulement sur des ratios de valeurs absolues, mais aussi sur des ratios d‚Äôune somme totale. Par exemple, dans une composition simple de deux types d‚Äôutilisation du territoire, si une proportion augmente, l‚Äôautre doit n√©cessairement diminuer. n &lt;- 100 tibble(A = runif(n, 0, 1)) %&gt;% mutate(B = 1 - A) %&gt;% ggplot(aes(x=A, y=B)) + geom_point() Les variables exprim√©es relativement √† une somme totale sont dites compositionnelles. Elles poss√®dent les caract√©ristiques suivantes. Redondance d‚Äôinformation. Un syst√®me de deux proportions ne contient qu‚Äôune seule variable du fait que l‚Äôon puisse d√©duire l‚Äôune en soutrayant l‚Äôautre de la somme totale. Un vecteur compositionnel contient de l‚Äôinformation redondante. Pourtant, effectuer des statistiques sur l‚Äôune plut√¥t que sur l‚Äôautre donnera des r√©sultats diff√©rents. D√©pendance d‚Äô√©chelle. Les statistiques devraient √™tre ind√©pendantes de la somme totale utilis√©e. Pourtant, elles diff√©reront sur l‚Äôon utilise par exemple, une proportion des m√¢les d‚Äôune part et des femelles d‚Äôautre part, ou la proportion de la somme des deux, de m√™me que les r√©sultats d‚Äôun test sanguin diff√©rera si l‚Äôon utilise une base s√®che ou une base humide. Distribution th√©orique des donn√©es. √âtant donn√©e que les proportions sont confin√©es entre 0 et 1 (ou 100%, ou une somme totale quelconque), la distribution normale (qui s‚Äô√©tend de -‚àû √† +‚àû) n‚Äôest souvent pas appropri√©e. On pourra utiliser la distribution de Dirichlet ou la distribution logitique-normale, mais d‚Äôautres approches sont souvent plus pratiques. Pour illustrer l‚Äôeffet de la distribution, voyons un diagramme ternaire incluant le sable, le limon et l‚Äôargile. En utilisant des √©cart-types univari√©s, nous obtenons l‚Äôellipse en rouge, qui non seulement repr√©sente peu l‚Äô√©talement des donn√©es, mais elle d√©passe les bornes du triangle, admettant ainsi des proportions n√©gatives. En bleu, la distribution logistique normale (issue des m√©thodes pr√©sent√©es plus loin dans cette section) convient davantage. Les cons√©quences d‚Äôeffectuer des statistiques lin√©aires sur des donn√©es compositionnelles brutes peuvent √™tre majeures. En outre, Pawlowksy-Glahn et Egozcue (2006), s‚Äôappuyant en outre sur Rock (1988), note les probl√®mes suivants (exprim√©s en mes mots). les r√©gressions, les regroupements et les analyses en composantes principales peuvent avoir peu ou pas de signification les propri√©t√©s des distributions peuvent √™tre g√©n√©r√©es par l‚Äôop√©ration de fermeture de la composition (s‚Äôassurer que le total des proportions donne 100%) les r√©sultats d‚Äôanalyses discriminantes lin√©aires sont propices √† √™tre illusoires tous les coefficients de corr√©lation seront affect√©s √† des degr√©s inconnus les r√©sultats des tests d‚Äôhypoth√®ses seront intrins√®quement fauss√©s Pour contourner ces probl√®mes, il faut d‚Äôabord aborder les donn√©es compositionnelles pour ce qu‚Äôelles sont: des donn√©es intrins√®quement multivari√©es. Elles sont un nuage de point, et non pas une collection de variables individuelles. Ceci qui n‚Äôemp√™che pas d‚Äôeffectuer des analyses consciencieusement sous des angles particuliers. En R, on pourra ais√©ment rapporter une composition en somme unitaire gr√¢ce √† la fonction apply. Mais auparavant, chargeons le module compositions (n‚Äôoubliez pas de l‚Äôinstaller au pr√©alable) pour acc√©der √† des donn√©es fictives de proportions de sable, limon et argile dans des s√©diments. library(&quot;compositions&quot;) data(&quot;ArcticLake&quot;) ArcticLake &lt;- ArcticLake %&gt;% as_tibble() head(ArcticLake) ## # A tibble: 6 x 4 ## sand silt clay depth ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 77.5 19.5 3 10.4 ## 2 71.9 24.9 3.2 11.7 ## 3 50.7 36.1 13.2 12.8 ## 4 52.2 40.9 6.6 13 ## 5 70 26.5 3.5 15.7 ## 6 66.5 32.2 1.3 16.3 comp &lt;- ArcticLake %&gt;% select(-depth) %&gt;% apply(., 1, function(x) x/sum(x)) %&gt;% t() comp[1:5, ] ## sand silt clay ## [1,] 0.7750000 0.1950000 0.0300000 ## [2,] 0.7190000 0.2490000 0.0320000 ## [3,] 0.5070000 0.3610000 0.1320000 ## [4,] 0.5235707 0.4102307 0.0661986 ## [5,] 0.7000000 0.2650000 0.0350000 On pourra aussi utiliser la fonction acomp (pour Aitchison-composition) pour fermer la composition √† une somme de 1. comp &lt;- ArcticLake %&gt;% select(-depth) %&gt;% acomp(.) comp[1:5, ] ## sand silt clay ## [1,] 0.7750000 0.1950000 0.0300000 ## [2,] 0.7190000 0.2490000 0.0320000 ## [3,] 0.5070000 0.3610000 0.1320000 ## [4,] 0.5235707 0.4102307 0.0661986 ## [5,] 0.7000000 0.2650000 0.0350000 Cette strat√©gie a pour avantage d‚Äôattribuer √† la variable comp la classe acomp, qui automatise les op√©rations dans l‚Äôespace compositionnel (que l‚Äôon nomme aussi le simplex). La repr√©sentation ternaire est souvent utilis√©e pour pr√©senter des compositions. Toutefois, il est difficile d‚Äôinterpr√©ter les compositions de plus de trois parties. La classe acomp automatise aussi la repr√©sentation teranaire. plot(comp) Afin de transposer cet espace cl√¥t en un espace ouvert, on pourra diviser chaque proportion par une proportion de r√©f√©rence choisie parmi n‚Äôimporte quelle proportion. Du coup, on retire une dimension redondante! Dans ce ratio, on choisit d‚Äôutiliser la proportion de r√©f√©rence au d√©nominateur, ce qui est arbitraire. En utilisant le log du ratio, l‚Äôinverse du ratio ne sera qu‚Äôun changement de signe, ce qui est pratique en statistiques lin√©aries. Cette solution, propos√©e par Aitchison (1986), s‚Äôapplique non seulement sur les compositions √† deux composantes, mais sur toute composition. Il s‚Äôagit alors d‚Äôutiliser une composition de r√©f√©rence pour effecteur les ratios. Pour une composition de \\(A\\), \\(B\\), \\(C\\), \\(D\\) et \\(E\\): \\[alr_A = log \\left( \\frac{A}{E} \\right), alr_B = log \\left( \\frac{B}{E} \\right), alr_C = log \\left( \\frac{C}{E} \\right), alr_D = log \\left( \\frac{D}{E} \\right)\\] Dans R, la colonne de r√©f√©rence est par d√©faut la derni√®re colonne de la matrice des compositions. add_lr &lt;- alr(comp) Cette derni√®re strat√©gie se nomme les log-ratios aditifs (\\(alr\\) pour additive log-ratio). Bien que valide pour effectuer des tests statistiques, cette strat√©gie a le d√©savantage de d√©pendre de la d√©cision arbitraire de la composante √† utiliser au num√©rateur. Deuxi√®me restriction des alr: les axes de l‚Äôespace des alr n‚Äô√©tant pas orthogonaux, ils ne peuvent pas √™tre utilis√©s pour effectuer des statistiques bas√©es sur les distances (que nous couvrirons au chapitre 9). L‚Äôautre strat√©gie propos√©e par Aitchison √©tait d‚Äôeffectuer un log-ratio entre chaque composante et la moyenne g√©om√©trique de toutes les composantes. Cette transformation se nomme le log-ratio centr√© (\\(clr\\), pour centered log-ratio) \\[clr_i = log \\left( \\frac{x_i}{g \\left( x \\right)} \\right)\\] En R, cen_lr &lt;- clr(comp) Avec des CLRs, les distances sont valides. Mais‚Ä¶ nous restons avec le probl√®me de la redondance d‚Äôinformation. En fait, la somme de chacunes des lignes d‚Äôune matrice de clr est de 0. Pas tr√®s pratique lorsque l‚Äôon effectue des statistiques incluant une inversion de la matrice de covariance (distance de Mahalanobis, g√©ostatistiques, etc.) cen_lr %&gt;% cov() %&gt;% solve() Error in solve.default(.) : le syst√®me est num√©riquement singulier : conditionnement de la r√©ciproque = 4.44407e-17 Enfin, une autre m√©thode de transformation d√©velopp√©e par Egoscue et al. (2003), les log-ratios isom√©triques (ou isometric log-ratios, ilr) projette les compositions comprenant D composantes dans un espace restreint de D-1 dimensions orthonorm√©es. Ces dimensions doivent doivent √™tre pr√©alablement √©tablie dans un dendrogramme de bifurcation, o√π chaque composante ou groupe de composante est successivement divis√© en deux embranchement. La mani√®re d‚Äôarranger ces balances importe peu, mais on aura avantage √† cr√©er des balances interpr√©tables. Le diagramme de balances peut √™tre encod√© dans une partition binaire s√©quentielle (ou sequential bianry partition, sbp). Une sbp est une matrice de contraste ou chaque ligne repr√©sente une partition entre deux variables ou groupes de variables. Une composante √©tiquett√©e +1 correspondra au groupe du num√©rateur, une composante √©tiquett√©e -1 au d√©nominateur et une composante √©tiquett√©e 0 sera exclue de la partition (Parent et al., 2013). J‚Äôai reformul√© la fonction CoDaDendrogram pour que l‚Äôon puisse ajouter des informations int√©ressantes sur les balants horizontaux. Cette fonction est disponible sur github. source(&quot;https://raw.githubusercontent.com/essicolo/AgFun/master/codadend2.R&quot;) sbp &lt;- matrix(c(1, 1,-1, 1,-1, 0), byrow = TRUE, ncol = 3) CoDaDendrogram2(comp, V = gsi.buildilrBase(t(sbp)), ylim = c(0, 1), equal.height = TRUE) Si la SBP est plus imposante, il pourrait √™tre plus ais√© de monter dans un chiffrier, puis de l‚Äôimporter dans R via un fichier csv. Le calcul des ILRs est effectu√© comme suit. \\[ilr_j = \\sqrt{\\frac{n_j^+ n_j^-}{n_j^+ + n_j^-}} log \\left( \\frac{g \\left( c_j^+ \\right)}{g \\left( c_j^+ \\right)} \\right)\\] ou, √† la ligne \\(j\\) de la SBP, \\(n_j^+\\) et \\(n_j^-\\) sont respectivement le nombre de composantes au num√©rateur et au d√©nominateur, \\(g \\left( c_j^+ \\right)\\) est la moyenne g√©om√©trique des composantes au num√©rateur et \\(g \\left( c_j^- \\right)\\) est la moyenne g√©om√©trique des composantes au d√©nominateur. Les balances sont conventionnellement not√©es [A,B | C,D], ou les composantes A et B au d√©nominateur sont balanc√©es avec les composantes C and D au num√©rateur. Une balance positive signifie que la moyenne g√©om√©trique des concentrations au num√©rateur est sup√©rieur √† celle au d√©nominateur, et inversement, alors qu‚Äôune balance nulle signifie que les moyennes g√©om√©triques sont √©gales (√©quilibre). Ainsi, en mod√©lisation lin√©aire, un coefficient positif sur [A,B | C,D] signifie que l‚Äôaugmentation de l‚Äôimportance de C et D comparativement √† A et B est associ√© √† une augmentation de la variable r√©ponse du mod√®le. En R, iso_lr &lt;- ilr(comp, V = gsi.buildilrBase(t(sbp))) Notez la forme gsi.buildilrBase(t(sbp)) est une op√©ration pour obtenir la matrice d‚Äôorthonormalit√© √† partir de la SBP. Les ILRs sont des balances multivari√©es sur lesquelles on pourra effectuer des statistiques lin√©aries. Bien que l‚Äôinterpr√©tation des r√©sultats comme collection d‚Äôinterpr√©tations sur des balances univari√©es pourra √™tre affect√©e par la structure de la SBP, ni les statistiques lin√©aires multivari√©es, ni la distance entre les points ne seront affect√©s. En effet, chaque variante de la SBP est une rotation (d‚Äôun facteur de 60¬∞) par rapport √† l‚Äôorigine: source(&quot;lib/ilr-rotation-sbp.R&quot;) Pour les transformations inverses, vous pourrez utiliser les fonctions alrInv, clrInv et ilrInv. Dans tous les cas, si vous tenez √† garder la trace de vos donn√©es dans leur format original, vous aurez avantage √† ajouter √† votre vecteur compositionnel la valeur de remplissage, constitu√© d‚Äôun amalgame des composantes non mesur√©es. Par exemple, pourc &lt;- c(N = 0.03, P = 0.001, K = 0.01) acomp(pourc) # vous perdez la trace des proportions originales ## N P K ## 0.73170732 0.02439024 0.24390244 ## attr(,&quot;class&quot;) ## [1] acomp pourc &lt;- c(N = 0.03, P = 0.001, K = 0.01) Fv &lt;- 1 - sum(pourc) comp &lt;- acomp(c(pourc, Fv = Fv)) comp ## N P K Fv ## 0.030 0.001 0.010 0.959 ## attr(,&quot;class&quot;) ## [1] acomp iso_lr &lt;- ilr(comp) # avec une sbp par d√©faut ilrInv(iso_lr) ## 1 2 3 4 ## [1,] 0.03 0.001 0.01 0.959 ## attr(,&quot;class&quot;) ## [1] acomp Si vos donn√©es font partie d‚Äôun tout, je vous recommande chaudement d‚Äôutiliser des m√©thodes compositionnelles autant pour l‚Äôanalyse que la mod√©lisation. Pour en savoir davantage, le livre Compositional data analysis with R, de van den Boogart et Tolosana-Delgado, est disponible en format √©lectronique √† la biblioth√®que de l‚ÄôUniversit√© Laval. Pour aller plus loin, j‚Äôai √©cri un billet √† ce sujet (auquel √† ce jour il manque toujours un cas d‚Äô√©tude): We should use balances and machine learning to diagnose ionomes. 8.3.2 Acqu√©rir des donn√©es m√©t√©o Une t√¢che commune en √©cologie est de lier des observations √† la m√©t√©o‚Ä¶ qui sont rarement collect√©s lors d‚Äôexp√©riences. Environnement Canada poss√®de sont r√©seau de stations. Les donn√©es sont disponibles sur internet en libre acc√®s. Vous pouvez chercher des stations, effectuer des requ√™tes et t√©l√©charger des fichiers csv. Pour un petit tableau, la t√¢che est plut√¥t triviale. Mais √ßa devient rapidement laborieux √† mesure que l‚Äôon doit rechercher de nombreuses donn√©es. Le module weathercan, d√©velopp√© par Steffi LaZerte, permet d‚Äôeffectuer des requ√™tes rapidement √† partir des coordonn√©es de votre site exp√©rimental. Par exemple, si je cherche une station m√©t√©o sfournissant des donn√©es horaires situ√© √† moins de 20 km du sommet du Mont-Bellevue, √† Sherbrooke, aux coordonn√©es [latitude 45.35, longitude -71.90], library(&quot;weathercan&quot;) station_site &lt;- stations_search(coords = c(45.35, -71.90), dist = 20, interval = &quot;hour&quot;) station_site ## # A tibble: 4 x 15 ## prov station_name station_id climate_id WMO_id TC_id lat lon elev tz interval start end normals distance ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 QC LENNOXVILLE 5397 7024280 71611 WQH 45.4 -71.8 181 Etc/GMT+5 hour 1994 2020 TRUE 6.65 ## 2 QC SHERBROOKE 48371 7028123 71610 YSC 45.4 -71.7 241. Etc/GMT+5 hour 2009 2020 FALSE 19.2 ## 3 QC SHERBROOKE A 5530 7028124 71610 YSC 45.4 -71.7 241. Etc/GMT+5 hour 1962 2005 FALSE 19.4 ## 4 QC SHERBROOKE A 30171 7028126 NA GSC 45.4 -71.7 241. Etc/GMT+5 hour 2004 2009 FALSE 19.4 Je prends en note l‚Äôidentifiant de la station d√©sir√©e (ou des stations, disons 5397 et 48371), puis je lance une requ√™te pour obtenir la m√©t√©o horaire entre les dates d√©sir√©es. mont_bellevue &lt;- weather_dl(station_ids = c(5397, 48371), start = &quot;2019-02-01&quot;, end = &quot;2019-02-07&quot;, interval = &quot;hour&quot;, verbose = TRUE, tz_disp = &quot;Etc/GMT+5&quot;) ## Warning in weather_dl(station_ids = c(5397, 48371), start = &quot;2019-02-01&quot;, : &#39;tz_disp&#39; is deprecated, see Details under ? ## weather_dlFALSE ## Getting station: 5397 ## Formatting station data: 5397 ## Adding header data: 5397 ## Getting station: 48371 ## Formatting station data: 48371 ## Adding header data: 48371 ## Trimming missing values before and after ## As of weathercan v0.3.0 time display is either local time or UTC ## See Details under ?weather_dl for more information. ## This message is shown once per session mont_bellevue %&gt;% head(5) ## # A tibble: 5 x 35 ## station_name station_id station_operator prov lat lon elev climate_id WMO_id TC_id date time ## &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dttm&gt; ## 1 LENNOXVILLE 5397 NA QC 45.4 -71.8 181 7024280 71611 WQH 2019-02-01 2019-02-01 00:00:00 ## 2 LENNOXVILLE 5397 NA QC 45.4 -71.8 181 7024280 71611 WQH 2019-02-01 2019-02-01 01:00:00 ## 3 LENNOXVILLE 5397 NA QC 45.4 -71.8 181 7024280 71611 WQH 2019-02-01 2019-02-01 02:00:00 ## 4 LENNOXVILLE 5397 NA QC 45.4 -71.8 181 7024280 71611 WQH 2019-02-01 2019-02-01 03:00:00 ## 5 LENNOXVILLE 5397 NA QC 45.4 -71.8 181 7024280 71611 WQH 2019-02-01 2019-02-01 04:00:00 ## # ‚Ä¶ with 23 more variables: year &lt;chr&gt;, month &lt;chr&gt;, day &lt;chr&gt;, hour &lt;chr&gt;, weather &lt;chr&gt;, hmdx &lt;dbl&gt;, hmdx_flag &lt;chr&gt;, ## # pressure &lt;dbl&gt;, pressure_flag &lt;chr&gt;, rel_hum &lt;dbl&gt;, rel_hum_flag &lt;chr&gt;, temp &lt;dbl&gt;, temp_dew &lt;dbl&gt;, temp_dew_flag &lt;chr&gt;, ## # temp_flag &lt;chr&gt;, visib &lt;dbl&gt;, visib_flag &lt;chr&gt;, wind_chill &lt;dbl&gt;, wind_chill_flag &lt;chr&gt;, wind_dir &lt;dbl&gt;, ## # wind_dir_flag &lt;chr&gt;, wind_spd &lt;dbl&gt;, wind_spd_flag &lt;chr&gt; Et voil√†. mont_bellevue %&gt;% ggplot(aes(x = time, y = temp)) + geom_line(aes(colour = station_name)) 8.3.3 P√©dom√©trie avec R Cette section a √©t√© √©crite par Michael Leblanc. Plusieurs fonctionnalit√©s ont √©t√© d√©velopp√©es sur R afin d‚Äôaider les p√©dom√©triciens √† visualiser, explorer et traiter les donn√©es num√©riques en science des sols. Voici quelques exemples. 8.3.3.1 Texture du sol La texture du sol est d√©finie par sa composition granulom√©trique, habituellement repr√©sent√©e par trois fractions (sable, limon, argile), laquelle peut √™tre g√©n√©ralis√©e en classe texturale. La d√©finition des classes texturales diff√®re d‚Äôun syst√®me ou d‚Äôun pays √† l‚Äôautre comme en t√©moigne l‚Äôarticle Perdus dans le triangle des textures (Richer de Forges et al.¬†2008). La d√©finition des fractions granulom√©triques peut √©galement diff√©rer selon le domaine d‚Äô√©tude (ing√©nierie, p√©dologie) ou le pays. Par exemple, le diam√®tre du limon est de 0,002 mm √† 0,05 mm dans le syst√®me canadien, am√©ricain et fran√ßais alors qu‚Äôil est de 0,002 mm √† 0,02 mm dans le syst√®me australien et de 0,002 mm √† 0,063 mm dans le syst√®me allemand. Il est donc important de v√©rifier la m√©thodologie et le syst√®me de classification utilis√©s pour interpr√©ter les donn√©es de texture du sol. Le module soilTexture propose des fonctions permettant d‚Äôaborder ces multiples d√©finitions. library(&quot;soiltexture&quot;) ## soiltexture 1.5.1 (git revision: 4b25ba2). For help type: help(pack=&#39;soiltexture&#39;) 8.3.3.1.1 Les triangles texturaux Avec la fonction TT.plot, vous pouvez pr√©senter vos donn√©es granulom√©triques dans un triangle textural tel que d√©fini par les diff√©rents syst√®mes nationaux. Auparavant, cr√©ons un objet comprenant des textures al√©atoires. set.seed(848341) # random.org rand_text &lt;- TT.dataset(n=100, seed.val=29) head(rand_text) ## CLAY SILT SAND Z ## 1 54.650857 40.37101 4.978129 13.2477582 ## 2 44.745954 40.81782 14.436221 20.8433109 ## 3 18.192509 48.26752 33.539970 7.1814626 ## 4 17.750492 40.14405 42.105458 -0.2077358 ## 5 65.518360 23.36110 11.120538 10.8656027 ## 6 6.610293 22.45353 70.936173 3.7108567 Avec le module soiltexture, les tableaux de texture doivent inclure les intitull√©s exactes CLAY, SILT et SAND (notez les majuscules). Les points des textures g√©n√©r√©es peuvent √™tre port√©s dans des diagrammes ternaires texturaux de diff√©rents syst√®mes de classification, par exemple le syst√®me canadioen et le syst√®me USDA. par(mfrow=c(1, 2)) TT.plot(class.sys = &quot;CA.FR.TT&quot;, tri.data = rand_text, col = &quot;blue&quot;) TT.plot(class.sys = &quot;USDA.TT&quot;, tri.data = rand_text, col = &quot;blue&quot;) Les param√®tres de la figure (titres, polices, style de la grille, etc.) peuvent √™tre personnalis√©s avec les arguments TT.plot. 8.3.3.1.2 Les classes texturales La fonction TT.points.in.classes est utile pour d√©signer la classe texturale √† partir des donn√©es granulom√©triques, en sp√©cifiant bien le syst√®me de classification d√©sir√©. TT.points.in.classes( tri.data = rand_text[1:10, ], # class.sys = &quot;CA.FR.TT&quot;, PiC.type = &quot;t&quot; ) ## [1] &quot;ALi&quot; &quot;ALi&quot; &quot;L&quot; &quot;L&quot; &quot;ALo&quot; &quot;LS&quot; &quot;ALo&quot; &quot;A&quot; &quot;LLi&quot; &quot;LSA&quot; Plusieurs autres fonctions sont propos√©es par soiltexture afin de visualiser, classifier et transformer les donn√©es de texture du sol : Functions in soiltexture. Julien Moeys (2018) propose √©galement le tutoriel The soil texture wizard: a tutorial. 8.3.3.2 Profils de sols Le profil de sols est une entit√© d√©crite par une s√©quence de couches ou d‚Äôhorizons avec diff√©rentes caract√©ristiques morphologiques. Le module AQP, pour Algorithms for Quantitative Pedology, propose des fonctions de visualisation, d‚Äôagr√©gation et de classification permettant d‚Äôaborder la complexit√© inh√©rente aux informations p√©dologiques. 8.3.3.2.1 La visualisation de profils Vous devez d‚Äôabord structurer vos donn√©es dans un tableau (data.frame) incluant minimalement ces trois colonnes : Identifiant unique du profil (groupes d‚Äôhorizons) (id) Limites sup√©rieures de l‚Äôhorizon (top) Limites inf√©rieures de l‚Äôhorizon (down) Vos donn√©es morphologiques, physico-chimiques, etc., sont incluses dans les autres colonnes. Chargeons un fichier p√©dologique √† titre d‚Äôexemple. profils &lt;- read_csv(&quot;data/06_pedometric-profile.csv&quot;) ## Parsed with column specification: ## cols( ## id = col_double(), ## horizon = col_character(), ## top = col_double(), ## bottom = col_double(), ## hue = col_character(), ## value = col_double(), ## chroma = col_double(), ## pH.CaCl2 = col_double(), ## C.CNS.pc = col_double() ## ) head(profils) ## # A tibble: 6 x 9 ## id horizon top bottom hue value chroma pH.CaCl2 C.CNS.pc ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Ap1 0 23 10YR 2 3 4.78 2.71 ## 2 1 Ap2 23 34 10YR 2 2 4.74 2.2 ## 3 1 Bfcj 34 46 7.5YR 4 5 4.79 2.4 ## 4 1 BC 46 83 2.5Y 4 5 4.93 0.22 ## 5 1 C 83 100 2.5Y 5 4 4.82 0.18 ## 6 2 Ap 0 29 10YR 2 2 4.6 4.22 La fonction munsell2rgb permet de convertir le code de couleur Munsell en format RGB. library(&quot;aqp&quot;) ## This is aqp 1.19 ## ## Attaching package: &#39;aqp&#39; ## The following object is masked from &#39;package:greta&#39;: ## ## slice ## The following object is masked from &#39;package:plotly&#39;: ## ## slice ## The following objects are masked from &#39;package:dplyr&#39;: ## ## slice, union ## The following object is masked from &#39;package:base&#39;: ## ## union profils$soil_color &lt;- with(profils, munsell2rgb(hue, value, chroma)) Pr√©alablement √† la visualisation, le tableau est transform√© en objet SoilProfileCollection par la fonction depths. Pour ce faire, le tableau doit √™tre un pur data.frame, non pas un tibble. profils &lt;- profils %&gt;% as.data.frame() depths(profils) &lt;- id ~ top + bottom La fonction plot d√©tectera le type d‚Äôobjet et appellera la fonction de visualisation en cons√©quence. par(mfrow = c(1, 3)) plot(profils, name=&quot;horizon&quot;) title(&#39;Couleur des horizons&#39;, cex.main=1) plot(profils, name=&quot;horizon&quot;, color=&#39;C.CNS.pc&#39;, col.label=&#39;C total (%)&#39;) plot(profils, name=&quot;horizon&quot;, color=&#39;pH.CaCl2&#39;, col.label=&#39;pH CaCl2&#39;) De multiples figures th√©matiques peuvent √™tre g√©n√©r√©es afin de repr√©senter les particuliarit√©s des profils. Pour aller plus loin, consultez les guides Introduction to SoilProfileCollection Objects et Generating Sketches from SPC Objects. 8.3.3.2.2 Les plans verticaux (depth functions) Les plans verticaux sont des diagrammes qui permettent d‚Äôinterpr√©ter les donn√©es en fonction de la profondeur. La fonction slab permet le calcul de statistiques descriptives par intervalles de profondeur r√©guliers, lesquelles permettent de visualiser la variabilit√© verticale des propri√©t√©s des sols. agg &lt;- slab(profils, fm = ~ C.CNS.pc + pH.CaCl2) La visualisation est g√©n√©r√©e par le module graphique ggplot2 agg %&gt;% ggplot(mapping = aes(x = -top, y = p.q50)) + facet_grid(. ~ variable, scale = &quot;free&quot;) + geom_ribbon(aes(ymin = p.q25, ymax = p.q75), fill = &quot;grey75&quot;, alpha = 0.5) + geom_path() + labs(x = &quot;Profondeur (cm)&quot;, y = &quot;M√©diane bord√©e des 25e and 75e percentiles&quot;) + coord_flip() 8.3.3.2.3 Le regroupement de profils Le calcul des distances de dissimilarit√© entre les profils avec profile_compare permet la construction de dendrogramme et le regroupement des profils. Notez que nous survolerons au chapitre 9 les concepts de dissimilarit√© et de partitionnement. library(&quot;cluster&quot;) library(&quot;mvtnorm&quot;) library(&quot;sharpshootR&quot;) # remotes::install_github(&quot;ncss-tech/sharpshootR&quot;) d &lt;- profile_compare(profils, vars=c(&#39;C.CNS.pc&#39;, &#39;pH.CaCl2&#39;), k=0, max_d=40) ## Computing dissimilarity matrices from 10 profiles [0.08 Mb] d_diana &lt;- diana(d) plotProfileDendrogram(profils, name=&quot;horizon&quot;, d_diana, scaling.factor = 0.3, y.offset = 5, color=&#39;pH.CaCl2&#39;, col.label=&#39;pH CaCl2&#39;) 8.3.3.2.4 Diagramme de relations entre les horizons Il est possible de visualiser les transitions d‚Äôhorizon les plus probables dans un groupe de profils de sols. tp &lt;- hzTransitionProbabilities(profils, name=&quot;horizon&quot;) ## Warning: ties in transition probability matrix par(mar = c(0, 0, 0, 0), mfcol = c(1, 2)) plot(profils, name=&quot;horizon&quot;) plotSoilRelationGraph(tp, graph.mode = &quot;directed&quot;, edge.arrow.size = 0.5, edge.scaling.factor = 2, vertex.label.cex = 0.75, vertex.label.family = &quot;sans&quot;) Consultez AQP project pour des pr√©sentations, des tutoriels et des exemples de figures qui montrent les nombreuses possibilit√©s du package AQP. 8.3.4 M√©ta-analyses en R Je conseille les livres Introduction to Meta-Analysis, Meta-analysis with R et Handbook of Meta-analysis in Ecology and Evolution pour les m√©ta-analyses sur des √©cosyst√®mes. Le module metafor est un ioncournable pour effectuer des m√©taanalyses en R. On ne passe pas tout √† fait √† c√¥t√© si l‚Äôon utilise le module meta, lui-m√™me bas√© en partie sur metafor. Le module meta a touttefois l‚Äôavantage d‚Äô√™tre simple d‚Äôutilisation. Par exemple, pour une m√©ta-analyse d‚Äôune r√©ponse continue, library(&quot;meta&quot;) ## Loading &#39;meta&#39; package (version 4.10-0). ## Type &#39;help(meta)&#39; for a brief overview. ## ## Attaching package: &#39;meta&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## ci meta_data &lt;- read_csv(&quot;https://portal.uni-freiburg.de/imbi/_SUPPRESS_ACCESSRULE/lehre/lehrbuecher/meta-analysis-with-r/dataset02.csv&quot;) ## Parsed with column specification: ## cols( ## author = col_character(), ## Ne = col_double(), ## Me = col_double(), ## Se = col_double(), ## Nc = col_double(), ## Mc = col_double(), ## Sc = col_double() ## ) meta_analyse &lt;- metacont(n.e = Ne, mean.e = Me, sd.e = Se, n.c = Nc, mean.c = Mc, sd.c = Sc, data = meta_data, sm = &quot;SMD&quot;) meta_analyse ## SMD 95%-CI %W(fixed) %W(random) ## 1 -0.5990 [-1.3300; 0.1320] 3.5 5.7 ## 2 -0.9518 [-1.6770; -0.2266] 3.6 5.7 ## 3 -0.5909 [-1.6301; 0.4483] 1.7 4.1 ## 4 -0.7064 [-1.7986; 0.3858] 1.6 3.9 ## 5 -0.2815 [-0.6076; 0.0445] 17.6 8.1 ## 6 -0.5375 [-1.0816; 0.0065] 6.3 6.8 ## 7 -1.3204 [-2.1896; -0.4513] 2.5 4.9 ## 8 -0.4800 [-1.3514; 0.3914] 2.5 4.9 ## 9 0.0918 [-0.2549; 0.4385] 15.6 8.0 ## 10 -3.2433 [-4.2035; -2.2831] 2.0 4.5 ## 11 0.0000 [-0.7427; 0.7427] 3.4 5.6 ## 12 -0.7061 [-1.2020; -0.2102] 7.6 7.1 ## 13 -0.4724 [-1.2537; 0.3089] 3.1 5.4 ## 14 -0.1849 [-0.5071; 0.1373] 18.0 8.2 ## 15 -0.0265 [-0.6045; 0.5515] 5.6 6.6 ## 16 -1.1648 [-2.0828; -0.2468] 2.2 4.7 ## 17 -0.2127 [-0.9651; 0.5397] 3.3 5.6 ## ## Number of studies combined: k = 17 ## ## SMD 95%-CI z p-value ## Fixed effect model -0.3915 [-0.5283; -0.2548] -5.61 &lt; 0.0001 ## Random effects model -0.5858 [-0.8703; -0.3013] -4.04 &lt; 0.0001 ## ## Quantifying heterogeneity: ## tau^2 = 0.2309 [0.1376; 0.9813]; tau = 0.4806 [0.3710; 0.9906]; ## I^2 = 72.5% [55.4%; 83.1%]; H = 1.91 [1.50; 2.43] ## ## Test of heterogeneity: ## Q d.f. p-value ## 58.27 16 &lt; 0.0001 ## ## Details on meta-analytical method: ## - Inverse variance method ## - DerSimonian-Laird estimator for tau^2 ## - Jackson method for confidence interval of tau^2 and tau ## - Hedges&#39; g (bias corrected standardised mean difference) Et pour effectuer un forest plot, forest(meta_analyse) 8.3.5 Cr√©er des applications avec R RStudio vous permet de d√©ployer vos r√©sultats sous forme d‚Äôapplications web gr√¢ce √† son module shiny. Pour ce faire, le seul pr√©alable est de savoir programmer en R. En agen√ßant une interface avec des inputs (listes de s√©lection, des bo√Ætes de dialogue, des s√©lecteurs, des boutons, etc.) avec des mod√®les que vous d√©veloppez, vous pourrez cr√©er des interfaces int√©ractives. Pour cr√©er une application shiny, vous devez cr√©er une partie pour l‚Äôinterface (ui) et une autre pour le calcul (server). Je n‚Äôirai pas dans les d√©tails, √©tant donn√©e qu‚Äôil s‚Äôagit d‚Äôun sujet √† part enti√®re. Pour aller plus loin, visitez le site du projet shiny. library(&quot;shiny&quot;) ui &lt;- basicPage( sliderInput(&quot;A&quot;, &quot;Asymptote:&quot;, min = 0, max = 100, value = 50), sliderInput(&quot;E&quot;, &quot;Environnement:&quot;, min = -10, max = 100, value = 20), sliderInput(&quot;R&quot;, &quot;Taux:&quot;, min = 0, max = 0.1, value = 0.035), sliderInput(&quot;prix_dose&quot;, &quot;Prix dose:&quot;, min = 0, max = 5, value = 1), sliderInput(&quot;prix_vente&quot;, &quot;Prix vente:&quot;, min = 0, max = 200, value = 100), sliderInput(&quot;dose&quot;, &quot;Dose:&quot;, min = 0, max = 300, value = c(0, 200)), plotOutput(&quot;distPlot&quot;) ) server &lt;- function(input, output) { mitsch_f &lt;- reactive({ input$A * (1 - exp(-input$R * (seq(input$dose[1], input$dose[2], length = 100) + input$E))) }) mitsch_opt &lt;- reactive({ (log((input$A * input$R * input$prix_vente) / input$prix_dose - input$E * input$R) / input$R ) }) output$distPlot &lt;- renderPlot({ plot(seq(input$dose[1], input$dose[2], length = 100), mitsch_f(), type = &quot;l&quot;, ylim = c(0, 100)) abline(v = mitsch_opt() ) text(mitsch_opt(), 2, paste(&quot;Dose optimale:&quot;, round(mitsch_opt(), 0))) }) } shinyApp(ui, server) Une fois l‚Äôapplication cr√©√©e, il est possible de la d√©ployer sur le site shninyapps.io. D‚Äôabord cr√©er une application shiny dans RStudio: File &gt; New File &gt; Shiny Web App. √âcrivez votre code dans le fichier app.R (dans ce cas, ce peut √™tre un copier-coller), puis cliquez sur Run App en haut √† droite de la fen√™tre d‚Äô√©dition du code. Lorsque l‚Äôapplication fonctionne, vous pourrez la publier via RStudio en cliquant sur le bouton Publish dans la fen√™tre Viewer (vous devez au pr√©alable avoir un comte sur shinyapp.io). Une application sera publique et sera ouverte. https://essicolo.shinyapps.io/Mitscherlich/ Pour d√©ployer en mode priv√©, vous devrez d√©bourser pour un forfait ou installer votre propre serveur. 8.3.6 Travailler en Python Le chapitre 7 a pr√©sent√© un module pour les statistiques bay√©siennes n√©cessitant un environnement Python. Il s‚Äôagissait de faire fonctionner un module en R qui, √† l‚Äôinterne, effectue ses calculs en Python. Rien ne vous emp√™che d‚Äôeffectuer des calculs directement en Python √† m√™me l‚Äôinterface de RStudio. Il vous faudra d‚Äôabord installer Python et les modules de calcul que vous d√©sirez. Il existe plusieurs distributions de Python. Parmi elles, Anaconda est probablement la plus intuitive √† installer. Choisissez d‚Äôabord Anaconda (~500 Mo) ou Miniconda pour une installation minimale (~60 Mo) - si vous installez Miniconda, vous devrez aussi installer les modules n√©cessaires pour le calcul. Installez aussi le module reticulate de R, de sortte que vous puissiez communiquer avec Python. Anaconda fonctionne avec des environnements de calcul. Chaque environnement poss√®de sa propre version de Python et ses propres modules: cela vous permet d‚Äôisoler vos environnements et de contr√¥ler la version des modules. Vous pouvez connecter R √† l‚Äôenvironnement de base cr√©√© lors de l‚Äôinstallation d‚ÄôAnaconda, ou bien en cr√©er un autre. Pour en cr√©er un nouveau, incluant une liste de modules de calcul, library(&quot;reticulate&quot;) conda_create(envname = &quot;monprojet&quot;, packages = c(&quot;python&quot;, &quot;numpy&quot;, &quot;scipy&quot;, &quot;matplotlib&quot;, &quot;pandas&quot;, &quot;scikit-learn&quot;)) Connectez-vous √† votre environnement Python, par exemple j‚Äôai cr√©√© un environnement r-greta, et je m‚Äôy suis d√©j√† connect√© au chapitre 7. library(&quot;reticulate&quot;) conda_list() ## name python ## 1 anaconda3 /home/essi/anaconda3/bin/python ## 2 npk-model /home/essi/anaconda3/envs/npk-model/bin/python ## 3 pytorch /home/essi/anaconda3/envs/pytorch/bin/python ## 4 r-env /home/essi/anaconda3/envs/r-env/bin/python ## 5 r-greta /home/essi/anaconda3/envs/r-greta/bin/python ## 6 tf2 /home/essi/anaconda3/envs/tf2/bin/python #use_condaenv(&quot;r-greta&quot;, required = TRUE) # use_python(&quot;home/essi/anaconda3/bin/&quot;) # ou le chemin vers l&#39;ex√©cutable pyhton, attention, non reproductible! Supposons que vous travailliez en R markdown. Pour lancer un bloc de code en Python, indiquez {python} au lieu de {r} dans l‚Äôent√™te. # ```{python} import numpy as np import pandas as pd import matplotlib.pyplot as plt a = np.linspace(0, 30, 101) b = np.sin(a) plt.plot(a, b) plt.title(&quot;Un graphique en Matplotlib dans RStudio&quot;) # ``` Pour r√©cup√©rer une variable Python en R, pr√©c√©dez la variable de py$. plot(py$a, py$b, type = &quot;l&quot;, main = &quot;Un graphique en R avec \\n des variables d√©finies en Python&quot;) Idem, pour r√©cup√©rer un objet R en Python, . r.iris.head(6) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 0 5.1 3.5 1.4 0.2 setosa ## 1 4.9 3.0 1.4 0.2 setosa ## 2 4.7 3.2 1.3 0.2 setosa ## 3 4.6 3.1 1.5 0.2 setosa ## 4 5.0 3.6 1.4 0.2 setosa ## 5 5.4 3.9 1.7 0.4 setosa Vous aurez ainsi acc√®s aux fonctionnalit√©s de Python et R dans un m√™me flux de travail. "],
["chapitre-ordination.html", "9 Association, partitionnement et ordination 9.1 Espaces d‚Äôanalyse 9.2 Analyse d‚Äôassociation 9.3 Partitionnement 9.4 Ordination", " 9 Association, partitionnement et ordination Ô∏è¬†Objectifs sp√©cifiques: √Ä la fin de ce chapitre, vous serez en mesure d‚Äôeffectuer des calculs permettant de mesurer des diff√©rence entre des observations, des groupes d‚Äôobservation ou des variables observ√©es serez en mesure d‚Äôeffection des analyses de partitionnement hi√©rarchiques et non-hi√©rarchiques serez en mesure d‚Äôeffectuer des calculs d‚Äôordination √† l‚Äôaide des techniques de r√©duction d‚Äôaxe communes: analyse en composante principale, l‚Äôanalyse de correspondance, l‚Äôanalyse en coordonn√©es principales, analyse discriminante lin√©aire, l‚Äôanalyse de redondance et l‚Äôanalyse canonique des correspondances. Les donn√©es √©cologiques incluent g√©n√©ralement plusieurs variables qui doivent √™tre analys√©es conjointement. Les techniques pour l‚Äôanalyse multivari√©e de donn√©es √©cologiques ont grandi en nombre et en complexit√©, laissant √©merger l‚Äô√©cologie num√©rique, un nouveau domaine d‚Äô√©tude scientifique initi√© par Pierre Legendre et Louis Legendre dont l‚Äôouvrage Numerical Ecology, aujourd‚Äôhui √† sa troisi√®me √©dition, reste un incontournable pour qui s‚Äôint√©resse aux math√©matiques sous-jacentes au domaine. Pour la r√©daction de ces notes, c‚Äôest toutefois le livre Numerical ecology with R, √©crit par Borcard et al. (2011) pour offrir un guide √† qui voudrait une approche plus appliqu√©e. L‚Äô√©cologie num√©rique sera effleur√©e dans ce chapitre, qui introduit √† trois concepts. Les associations permettent de quantifier la ressemblance ou la diff√©rence entre deux observation (√©chantillons) ou variables (descripteurs). Lorsque l‚Äôon a plus de deux variables ou plus de deux site, nous obtenons des matrices d‚Äôassociation. Le partitionnement permet de regrouper des observations ou des variables selon des m√©triques d‚Äôassociation. L‚Äôordination vise par l‚Äôinterm√©diaire de techniques de r√©duction d‚Äôaxe √† mettre de l‚Äôordre dans des donn√©es dont le nombre √©lev√© de variables peut amener √† des difficult√©s d‚Äôappr√©ciation et d‚Äôinterpr√©taion. library(&quot;tidyverse&quot;) 9.1 Espaces d‚Äôanalyse 9.1.1 Abondance et occurence L‚Äôabondance est le d√©compte d‚Äôesp√®ces observ√©es, tandis que l‚Äôoccurence est la pr√©sence ou l‚Äôabsence d‚Äôune esp√®ce. Le tableau suivant contient des donn√©es d‚Äôabondance. abundance &lt;- tibble(&#39;Bruant familier&#39; = c(1, 0, 0, 3), &#39;Citelle √† poitrine rousse&#39; = c(1, 0, 0, 0), &#39;Colibri √† gorge rubis&#39; = c(0, 1, 0, 0), &#39;Geai bleu&#39; = c(3, 2, 0, 0), &#39;Bruant chanteur&#39; = c(1, 0, 5, 2), &#39;Chardonneret&#39; = c(0, 9, 6, 0), &#39;Bruant √† gorge blanche&#39; = c(1, 0, 0, 0), &#39;M√©sange √† t√™te noire&#39; = c(20, 1, 1, 0), &#39;Jaseur bor√©al&#39; = c(66, 0, 0, 0)) Ce tableau peut √™tre rapidement transform√© en donn√©es d‚Äôoccurence, qui ne comprennent que l‚Äôinformation bool√©enne de pr√©sence (not√© 1) et d‚Äôabsence (not√© 0). occurence &lt;- abundance %&gt;% transmute_all(~if_else(. &gt; 0, 1, 0)) L‚Äôespace des esp√®ces (ou des variables ou descripteurs) est celui o√π les esp√®ces forment les axes et o√π les sites sont positionn√©s dans cet espace. Il s‚Äôagit d‚Äôune perspective en mode R, qui permet principalement d‚Äôidentifier quels esp√®ces se retrouvent plus courrament ensemble. abundance %&gt;% select(&quot;Bruant chanteur&quot;, &quot;Chardonneret&quot;, &quot;M√©sange √† t√™te noire&quot;) ## # A tibble: 4 x 3 ## `Bruant chanteur` Chardonneret `M√©sange √† t√™te noire` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 20 ## 2 0 9 1 ## 3 5 6 1 ## 4 2 0 0 Dans l‚Äôespace des sites (ou les √©chantillons ou objets), on transpose la matrice d‚Äôabondance. On passe ici en mode Q, o√π chaque point est une esp√®ce, et o√π l‚Äôon peut observer quels √©chantillons sont similaires. abundance %&gt;% t() ## [,1] [,2] [,3] [,4] ## Bruant familier 1 0 0 3 ## Citelle √† poitrine rousse 1 0 0 0 ## Colibri √† gorge rubis 0 1 0 0 ## Geai bleu 3 2 0 0 ## Bruant chanteur 1 0 5 2 ## Chardonneret 0 9 6 0 ## Bruant √† gorge blanche 1 0 0 0 ## M√©sange √† t√™te noire 20 1 1 0 ## Jaseur bor√©al 66 0 0 0 9.1.2 Environnement L‚Äôespace de l‚Äôenvironnement comprend souvent un autre tableau contenant l‚Äôinformation sur l‚Äôenvironnement o√π se trouve les esp√®ces: les coordonn√©es et l‚Äô√©l√©vation, la pente, le pH du sol, la pluviom√©trie, etc. 9.2 Analyse d‚Äôassociation Nous utiliserons le terme association comme une mesure pour quantifier la ressemblance ou la diff√©rence entre deux objets (√©chantillons) ou variables (descripteurs). Alors que la corr√©lation et la covariance sont des mesures d‚Äôassociation entre des variables (analyse en mode R), la similarit√© et la distance sont deux types de une mesure d‚Äôassociation entre des objets (analyse en mode Q). Une distance de 0 est mesur√©e chez deux objets identiques. La distance augmente au fur et √† mesure que les objets sont dissoci√©s. Une similarit√© ayant une valeur de 0 indique aucune association, tandis qu‚Äôune valeur de 1 indique une association parfaite. √Ä l‚Äôoppos√©, la dissimilarit√© est √©gale √† 1-similarit√©. La distance peut √™tre li√©e √† la similarit√© par la relation: \\[distance=\\sqrt{1-similarit√©}\\] ou \\[distance=\\sqrt{dissimilarit√©}\\] La racine carr√©e permet, pour certains indices de similarit√©, d‚Äôobtenir des propri√©t√©s eucl√©diennes. Pour plus de d√©tails, voyez le tableau 7.2 de Legendre et Legendre (2012). Les matrices d‚Äôassociation sont g√©n√©ralement pr√©sent√©es comme des matrices carr√©es, dont les dimensions sont √©gales au nombre d‚Äôobjets (mode Q) ou de vrariables (mode R) dans le tableau. Chaque √©l√©ment (‚Äúcellule‚Äù) de la matrice est un indice d‚Äôassociation entre un objet (ou une variable) et un autre. Ainsi, la diagonale de la matrice est un vecteur nul (distance ou dissimilarit√©) ou unitaire (similarit√©), car elle correspond √† l‚Äôassociation entre un objet et lui-m√™me. Puisque l‚Äôassociation entre A et B est la m√™me qu‚Äôentre B et A, et puisque la diagonale retourne une valeur convenue, il est possible d‚Äôexprimer une matrice d‚Äôassociation en mode ‚Äúcompact‚Äù, sous forme de vecteur. Le vecteur d‚Äôassociation entre des objets A, B et C contiendra toute l‚Äôinformation n√©cessaire en un vecteur de trois chiffres, [AB, AC, BC], plut√¥t qu‚Äôune matrice de dimension \\(3 \\times 3\\). L‚Äôimpact sur la m√©moire vive peut √™tre consid√©rable pour les calculs comprenant de nombreuses dimensions. En R, les calculs de similarit√© et de distances peuvent √™tre effectu√©s avec le module vegan. La fonction vegdist permet de calculer les indices d‚Äôassociation en forme carr√©e. Nous verons plus tard les m√©thodes de mesure de similarit√© et de distance plus loin. Pour l‚Äôinstant, utilisons la m√©thode de Jaccard pour une d√©monstration sur des donn√©es d‚Äôoccurence. library(&quot;vegan&quot;) vegdist(occurence, method = &quot;jaccard&quot;, diag = TRUE, upper = TRUE) ## 1 2 3 4 ## 1 0.0000000 0.7777778 0.7500000 0.7142857 ## 2 0.7777778 0.0000000 0.6000000 1.0000000 ## 3 0.7500000 0.6000000 0.0000000 0.7500000 ## 4 0.7142857 1.0000000 0.7500000 0.0000000 Remarquez que vegdist retourne une matrice dont la diagonale est de 0 (on l‚Äôaffiche en sp√©cifiant diag = TRUE). La diagonale est l‚Äôassociation d‚Äôun objet avec lui-m√™me. Or la similarit√© d‚Äôun objet avec lui-m√™me devrait √™tre de 1! En fait, par convention vegdist retourne des dissimilarit√©s, non pas des similarit√©s. La matrice de distance serait donc calcul√©e en extrayant la racine carr√©e des √©l√©ments de la matrice de dissimilarit√©: dissimilarity &lt;- vegdist(occurence, method = &quot;jaccard&quot;, diag = TRUE, upper = TRUE) distance &lt;- sqrt(dissimilarity) distance ## 1 2 3 4 ## 1 0.0000000 0.8819171 0.8660254 0.8451543 ## 2 0.8819171 0.0000000 0.7745967 1.0000000 ## 3 0.8660254 0.7745967 0.0000000 0.8660254 ## 4 0.8451543 1.0000000 0.8660254 0.0000000 Dans le chapitre sur l‚Äôanalyse compositionnelle, nous avons abord√© les significations diff√©rentes que peuvent prendre le z√©ro. L‚Äôinformation fournie par un z√©ro peut √™tre diff√©rente selon les circonstances. Dans le cas d‚Äôune variable continue, un z√©ro signifie g√©n√©ralement une mesure sous le seuil de d√©tection. Deux tissus dont la concentration en cuivre est nulle ont une afinit√© sous la perspective de la concentration en cuivre. Dans le cas de mesures d‚Äôabondance (d√©compte) ou d‚Äôoccurence (pr√©sence-absence), on pourra d√©crire comme similaires deux niches √©cologiques o√π l‚Äôon retrouve une esp√®ce en particulier. Mais deux sites o√π l‚Äôon de retouve pas d‚Äôours polaires ne correspondent pas n√©cessairement √† des niches similaires! En effet, il peut exister de nombreuses raisons √©cologiques et m√©thodologiques pour lesquelles l‚Äôesp√®ces ou les esp√®ces n‚Äôont pas √©t√© observ√©es. C‚Äôest le probl√®me des double-z√©ros (esp√®ces non observ√©es √† deux sites), probl√®me qui est amplifi√© avec les grilles comprenant des esp√®ces rares. La ressemblance entre des objets comprenant des donn√©es continues devrait √™tre calcul√©e gr√¢ce √† des indicateurs sym√©triques. Inversement, les affinit√©s entre les objets d√©crits par des donn√©es d‚Äôabondance ou d‚Äôoccurence susceptibles de g√©n√©rer des probl√®mes de double-z√©ros devraient √™tre √©valu√©es gr√¢ce √† des indicateurs asym√©triques. Un d√©fi suppl√©mentaire arrive lorsque les donn√©es sont de type mixte. Nous utiliserons la convention de vegan et nous calculerons la dissimilarit√©, non pas la similarit√©. Les mesures de dissimilarit√© sont calcul√©es sur des donn√©es d‚Äôabondance ou des donn√©es d‚Äôoccurence. Notons qu‚Äôil existe beaucoup de confusion dans la litt√©rature sur la mani√®re de nommer les dissimilarit√©s (ce qui n‚Äôest pas le cas des distances, dont les noms sont reconnus). Dans les sections suivantes, nous noterons la dissimilarit√© avec un \\(d\\) minuscule et la distance avec un \\(D\\) majuscule. 9.2.1 Association entre objets (mode Q) 9.2.1.1 Objets: Abondance La dissimilarit√© de Bray-Curtis est asym√©trique. Elle est aussi appel√©e l‚Äôindice de Steinhaus, de Czekanowski ou de S√∏rensen. Il est important de s‚Äôassurer de bien s‚Äôentendre la m√©thode √† laquelle on fait r√©f√©rence. L‚Äô√©quation enl√®ve toute ambiguit√©. La dissimilarit√© de Bray-Curtis entre les points A et B est calcul√©e comme suit. \\[d_{AB} = \\frac {\\sum \\left| A_{i} - B_{i} \\right| }{\\sum \\left(A_{i}+B_{i}\\right)}\\] Utilisons vegdist pour g√©n√©rer les matrices d‚Äôassociation. Le format ‚Äúliste‚Äù de R est pratique pour enregistrer la collection d‚Äôobjets, dont les matrice d‚Äôassociation que nous allons cr√©er dans cette section. associations_abund &lt;- list() associations_abund[[&#39;BrayCurtis&#39;]] &lt;- vegdist(abundance, method = &quot;bray&quot;) associations_abund[[&#39;BrayCurtis&#39;]] ## 1 2 3 ## 2 0.9433962 ## 3 0.9619048 0.4400000 ## 4 0.9591837 1.0000000 0.7647059 La dissimilarit√© de Bray-Curtis est souvent utilis√©e dans la litt√©rature. Toutefois, la version originale de Bray-Curtis n‚Äôest pas tout √† fait m√©trique (semim√©trique). Cons√©quemment, la dissimilarit√© de Ruzicka (une variante de la dissimilarit√© de Jaccard pour les donn√©es d‚Äôabondance) est m√©trique, et devrait probablement √™tre pr√©f√©r√© √† Bary-Curtis (Oksanen, 2006). \\[d_{AB, Ruzicka} = \\frac { 2 \\times d_{AB, Bray-Curtis} }{1 + d_{AB, Bray-Curtis}}\\] associations_abund[[&#39;Ruzicka&#39;]] &lt;- associations_abund[[&#39;BrayCurtis&#39;]] * 2 / (1 + associations_abund[[&#39;BrayCurtis&#39;]]) La dissimilarit√© de Kulczynski (aussi √©crit Kulsinski) est asym√©trique et semim√©trique, tout comme celle de Bray-Curtis. Elle est calcul√©e comme suit. \\[d_{AB} = 1-\\frac{1}{2} \\times \\left[ \\frac{\\sum min(A_i, B_i)}{\\sum A_i} + \\frac{\\sum min(A_i, B_i)}{\\sum B_i} \\right]\\] associations_abund[[&#39;Kulczynski&#39;]] &lt;- vegdist(abundance, method = &quot;kulczynski&quot;) Une approche commune pour mesurer l‚Äôassociation entre sites d√©crits par des donn√©es d‚Äôabondance est la distance de Hellinger. Notez qu‚Äôil s‚Äôagit ici d‚Äôune distance, non pas d‚Äôune dissimilarit√©. Pour l‚Äôobtenir, on doit d‚Äôabord diviser chaque donn√©e d‚Äôabondance par l‚Äôabondance totale pour chaque site pour obtenir les esp√®ces en tant que proportions, puis on extrait la racine carr√©e de chaque √©l√©ment. Enfin, on calcule la distance euclidienne entre les proportions de chaque site. Pour rappel, une distance euclidienne est la g√©n√©ralisation en plusieurs dimensions du th√©or√®me de Pythagore, \\(c = \\sqrt{a^2 + b^2}\\). \\[D_{AB} = \\sqrt {\\sum \\left( \\frac{A_i}{\\sum A_i} - \\frac{B_i}{\\sum B_i} \\right)^2}\\] üò±¬†Attention La distance d‚ÄôHellinger h√©rite des biais li√©es aux donn√©es compositionnelles. Elle peut √™tre substiti√©e par une matrice de distances d‚ÄôAitchison. associations_abund[[&#39;Hellinger&#39;]] &lt;- dist(decostand(abundance, method=&quot;hellinger&quot;)) Toute comme la distance d‚ÄôHellinger, la distance de chord est calcul√©e par une distance euclidienne sur des donn√©es d‚Äôabondance transform√©es de sorte que chaque ligne ait une longueur (norme) de 1. associations_abund[[&#39;Chord&#39;]] &lt;- dist(decostand(abundance, method=&quot;normalize&quot;)) La m√©trique du chi-carr√©, ou \\(\\chi\\)-carr√©, ou chi-square, donne davantage de poids aux esp√®ces rares qu‚Äôaux esp√®ces communes. Son utilisation est recommand√©e lorsque les esp√®ces rares sont de bons indicateurs de conditions √©cologiques particuli√®res (Legendre et Legendre, 2012, p.¬†308). \\[ d_{AB} = \\sqrt{\\sum _j \\frac{1}{\\sum y_j} \\left( \\frac{A_j}{\\sum A} - \\frac{B_j}{\\sum B} \\right)^2 } \\] La m√©trique peut √™tre transform√©e en distance en la multipliant par la racine carr√©e de la somme totale des esp√®ces dans la matric d‚Äôabondance (\\(X\\)). \\[ D_{AB} = \\sqrt{\\sum X} \\times d_{AB} \\] associations_abund[[&#39;ChiSquare&#39;]] &lt;- dist(decostand(abundance, method=&quot;chi.square&quot;)) Une manni√®re visuellement plus int√©ressante de pr√©senter une matrice d‚Äôassociation est un graphique de type heatmap. associations_abund_df &lt;- list() for (i in 1:length(associations_abund)) { associations_abund_df[[i]] &lt;- data.frame(as.matrix(associations_abund[[i]])) colnames(associations_abund_df[[i]]) &lt;- rownames(associations_abund_df[[i]]) associations_abund_df[[i]]$row &lt;- rownames(associations_abund_df[[i]]) associations_abund_df[[i]] &lt;- associations_abund_df[[i]] %&gt;% gather(key=row) associations_abund_df[[i]]$column = rep(1:4, 4) associations_abund_df[[i]]$dist &lt;- names(associations_abund)[i] } associations_abund_df &lt;- do.call(rbind, associations_abund_df) ggplot(associations_abund_df, aes(x=row, y=column)) + facet_wrap(. ~ dist, nrow = 2) + geom_tile(aes(fill = value)) + geom_text(aes(label = round(value, 2))) + scale_fill_gradient2(low = &quot;#00ccff&quot;, mid = &quot;#aad400&quot;, high = &quot;#ff0066&quot;, midpoint = 2) + labs(x=&quot;Site&quot;, y=&quot;Site&quot;) Peu importe le type d‚Äôassociation utilis√©e, les heatmaps montrent les m√™mes tendances. Les assocaitions de dissimilarit√© (Bray-Curtis, Kulczynski et Ruzicka) s‚Äô√©talent de 0 √† 1, tandis que les distances (Chi-Square, Chord et Hellinger) partent de z√©ro, mais n‚Äôont pas de limite sup√©rieure. On note les plus grandes diff√©rences entre les sites 2 et 4, tandis que les sites 2 et 3 sont les plus semblables pour toutes les mesures d‚Äôassociation √† l‚Äôexception de la dissimilarit√© de Kulczynski. 9.2.1.2 Objets: Occurence (pr√©sence-absence) Des indices d‚Äôassociation diff√©rents devraient √™tre utilis√©s lorsque des donn√©es sont compil√©es sous forme bool√©enne. En g√©n√©ral, les tableaux de donn√©es d‚Äôoccurence seront compil√©s avec des 1 (pr√©sence) et des 0 (absence). La similarit√© de Jaccard entre le site A et le site B est la proportion de double 1 (pr√©sences de 1 dans A et B) parmi les esp√®ces. La dissimilari√© est la proportion compl√©mentaire (comprenant [1, 0], [0, 1] et [0, 0]). La distance de Jaccard est la racine carr√©e de la dissimilarit√©. associations_occ &lt;- list() associations_occ[[&#39;Jaccard&#39;]] &lt;- vegdist(occurence, method = &quot;jaccard&quot;) Les distances d‚ÄôHellinger, de chord et de chi-carr√© sont aussi appropri√©es pour les calculs de distances sur des tableaux d‚Äôoccurence. associations_occ[[&#39;Hellinger&#39;]] &lt;- dist(decostand(occurence, method=&quot;hellinger&quot;)) associations_occ[[&#39;Chord&#39;]] &lt;- dist(decostand(occurence, method=&quot;normalize&quot;)) associations_occ[[&#39;ChiSquare&#39;]] &lt;- dist(decostand(occurence, method=&quot;chi.square&quot;)) Graphiquement, associations_occ_df &lt;- list() for (i in 1:length(associations_occ)) { associations_occ_df[[i]] &lt;- data.frame(as.matrix(associations_occ[[i]])) colnames(associations_occ_df[[i]]) &lt;- rownames(associations_occ_df[[i]]) associations_occ_df[[i]]$row &lt;- rownames(associations_occ_df[[i]]) associations_occ_df[[i]] &lt;- associations_occ_df[[i]] %&gt;% gather(key=row) associations_occ_df[[i]]$column = rep(1:4, 4) associations_occ_df[[i]]$dist &lt;- names(associations_occ)[i] } associations_occ_df &lt;- do.call(rbind, associations_occ_df) ggplot(associations_occ_df, aes(x=row, y=column)) + facet_wrap(. ~ dist) + geom_tile(aes(fill = value)) + geom_text(aes(label = round(value, 2))) + scale_fill_gradient2(low = &quot;#00ccff&quot;, mid = &quot;#aad400&quot;, high = &quot;#ff0066&quot;, midpoint = 1) + labs(x=&quot;Site&quot;, y=&quot;Site&quot;) Il est attendu que les matrices d‚Äôassociation sur l‚Äôoccurence sont semblables √† celles sur l‚Äôabondance. Dans ce cas-ci, la distance d‚ÄôHellinger donne des r√©sultats semblables √† la dissimilarit√© de Jaccard. 9.2.1.3 Objets: Donn√©es quantitatives Les donn√©es quantitative en √©cologie peuvent d√©crire l‚Äô√©tat de l‚Äôenvironnement: le climat, l‚Äôhydrologie, l‚Äôhydrog√©ochimie, la p√©dologie, etc. En r√®gle g√©n√©rale, les coordonn√©es des sites ne sot pas des variables environnementales, √† que l‚Äôon soup√ßonne la coordonn√©e elle-m√™me d‚Äô√™tre responsable d‚Äôeffets sur notre syst√®me: mais il s‚Äôagira la plupart du temps d‚Äôeffets confondants (par exemple, on peut mesurer un effet de lattitude sur le rendement des agrumes, mais il s‚Äôagira probablement avant tout d‚Äôeffets dus aux conditions climatiques, qui elles changent en fonction de la lattitude). D‚Äôautre types de donn√©es quantitative pouvant √™tre appr√©hend√©es par des distances sont les traits ph√©nologiques, les ionomes, les g√©nomes, etc. La distance euclidienne est la racine carr√©e de la somme des carr√©s des distances sur tous les axes. Il s‚Äôagit d‚Äôune application multidimensionnelle du th√©or√®me de Pythagore. La distance d‚ÄôAitchison, couverte dans le chapitre 8, est une distance euclidienne calcul√©e sur des donn√©es compositionnelles pr√©alablement transform√©es. La distance euclidienne est sensible aux unit√©s utilis√©s: utiliser des milim√®tres plut√¥t que des m√®tres enflera la distance euclidienne. Il est recommand√© de porter une attention particuli√®re aux unit√©s, et de standardiser les donn√©es au besoin (par exemple, en centrant la moyenne √† z√©ro et en fixant l‚Äô√©cart-type √† 1). On pourrait, par exemple, mesurer la distance entre des observations des dimensions de diff√©rentes esp√®ces d‚Äôiris. Ce tableau est inclu dans R par d√©faut. data(iris) iris %&gt;% sample_n(5) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 6.6 3.0 4.4 1.4 versicolor ## 2 7.7 3.8 6.7 2.2 virginica ## 3 4.4 3.2 1.3 0.2 setosa ## 4 5.6 3.0 4.1 1.3 versicolor ## 5 6.9 3.1 5.1 2.3 virginica Les mesures du tableau sont en centim√®tres. Pour √©viter de donner davantage de poids aux longueur des s√©pales et en m√™me temps de n√©gliger la largeur des p√©tales, nous allons standardiser le tableau. iris_sc &lt;- iris %&gt;% select(-Species) %&gt;% scale(.)%&gt;% as_tibble(.) %&gt;% mutate(Species = iris$Species) iris_sc ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 -0.898 1.02 -1.34 -1.31 setosa ## 2 -1.14 -0.132 -1.34 -1.31 setosa ## 3 -1.38 0.327 -1.39 -1.31 setosa ## 4 -1.50 0.0979 -1.28 -1.31 setosa ## 5 -1.02 1.25 -1.34 -1.31 setosa ## 6 -0.535 1.93 -1.17 -1.05 setosa ## 7 -1.50 0.786 -1.34 -1.18 setosa ## 8 -1.02 0.786 -1.28 -1.31 setosa ## 9 -1.74 -0.361 -1.34 -1.31 setosa ## 10 -1.14 0.0979 -1.28 -1.44 setosa ## # ‚Ä¶ with 140 more rows Pour les comparaisons des dimensions, prenons la moyenne des dimensions (mises √† l‚Äô√©chelle) par esp√®ce. iris_means &lt;- iris_sc %&gt;% group_by(Species) %&gt;% summarise_all(mean) %&gt;% select(-Species) iris_means ## # A tibble: 3 x 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -1.01 0.850 -1.30 -1.25 ## 2 0.112 -0.659 0.284 0.166 ## 3 0.899 -0.191 1.02 1.08 Nous pouvons utiliser la distance euclidienne, commune en g√©om√©trie, pour comparer les esp√®ces. La distance euclidienne est calcul√©e comme suit. \\[ \\mathcal{E} = \\sqrt{\\Sigma_i \\left( A_i - B_i \\right) ^2 } \\] associations_cont = list() associations_cont[[&#39;Euclidean&#39;]] &lt;- dist(iris_sc %&gt;% select(-Species), method=&quot;euclidean&quot;) La distance de Mahalanobis est semblable √† la distance euclidienne, mais qui tient compte de la covariance de la matrice des objets. Cette covariance peut √™tre utilis√©e pour d√©crire la structure d‚Äôun nuage de points. La diastance de Mahalanobis se calcule comme suit. \\[\\mathcal{M} = \\sqrt{(A - B)^T S^{-1} (A-B)}\\] Notez qu‚Äôil s‚Äôagit d‚Äôune g√©n√©ralisation de la distance euclidienne, qui √©quivaut √† une distance de Mahalanobis dont la matrice de covariance est une matrice identit√©. La distance de Mahalanobis permet de repr√©senter des distances dans un espace fortement corr√©l√©. Elle est courramment utilis√©e pour d√©tecter les valeurs aberrantes selon des crit√®res de distance √† partir du centre d‚Äôun jeu de donn√©es multivari√©es. associations_cont[[&#39;Mahalanobis&#39;]] &lt;- vegdist(iris_sc %&gt;% select(-Species), &#39;mahalanobis&#39;) La distance de Manhattan porte aussi le nom de distance de cityblock ou de taxi. C‚Äôest la distance que vous devrez parcourir pour vous rendre du point A au point B √† Manhattan, c‚Äôest-√†-dire selon une s√©quence de tron√ßons perpendiculaires. \\[ D_{AB} = \\sum _i \\left| A_i - B_i \\right| \\] La distance de Manhattan est appropri√©e lorsque les gradients (changements d‚Äôun √©tat √† l‚Äôautre ou d‚Äôune r√©gion √† l‚Äôautre) ne permettent pas des changements simultan√©s. Mieux vaut standardiser les variables pour √©viter qu‚Äôune dimension soit pr√©pond√©rante. associations_cont[[&#39;Manhattan&#39;]] &lt;- vegdist(iris_sc %&gt;% select(-Species), &#39;manhattan&#39;) Avant de pr√©senter les r√©sultats des esp√®ces d‚Äôiris, voici une repr√©sentation des distances euclidiennes (rouge), de Mahalanobis (bleu) et de Manhattan (vert), chacune de 1 et 2 unit√©s √† partir du centre et, pour ce qui est de la distance de Mahalanobis, selon la covariance. library(&quot;car&quot;) ## Loading required package: carData ## Registered S3 methods overwritten by &#39;car&#39;: ## method from ## influence.merMod lme4 ## cooks.distance.influence.merMod lme4 ## dfbeta.influence.merMod lme4 ## dfbetas.influence.merMod lme4 ## ## Attaching package: &#39;car&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## recode ## The following object is masked from &#39;package:purrr&#39;: ## ## some library(&quot;MASS&quot;) ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:plotly&#39;: ## ## select ## The following object is masked from &#39;package:dplyr&#39;: ## ## select select &lt;- dplyr::select # √©viter les conflits de fonctions entre MASS et dplyr filter &lt;- dplyr::filter sigma &lt;- matrix(c(1, 0.6, 0.6, 1), ncol = 2) # matrice de covariance mu &lt;- c(0, 0) # centre data &lt;- mvrnorm(n = 100, mu, sigma) # g√©n√©rer des donn√©es plot(data, ylim = c(-2, 2), xlim = c(-2, 2), asp = 1) ## cercles t &lt;- seq(0,2*pi,length=100) c1 &lt;- t(rbind(mu[2] + sin(t)*1, mu[1] + cos(t)*1)) c2 &lt;- t(rbind(mu[2] + sin(t)*2, mu[1] + cos(t)*2)) lines(c1, lwd = 2, col = &quot;red&quot;) lines(c2, lwd = 2, col = &quot;red&quot;) ## ellipses e1 &lt;- ellipse(mu, sigma, radius=1, add=TRUE) e2 &lt;- ellipse(mu, sigma, radius=2, add=TRUE) ## carr√©s lines(c(1, 0, -1, 0, 1), c(0, 1, 0, -1, 0), lwd = 2, col = &quot;green&quot;) lines(c(2, 0, -2, 0, 2), c(0, 2, 0, -2, 0), lwd = 2, col = &quot;green&quot;) Et, graphiquement, les r√©sultats des distances des iris. associations_cont_df &lt;- list() for (i in 1:length(associations_cont)) { associations_cont_df[[i]] &lt;- data.frame(as.matrix(associations_cont[[i]])) colnames(associations_cont_df[[i]]) &lt;- rownames(associations_cont_df[[i]]) associations_cont_df[[i]]$row &lt;- rownames(associations_cont_df[[i]]) associations_cont_df[[i]] &lt;- associations_cont_df[[i]] %&gt;% gather(key=row) associations_cont_df[[i]]$column = rep(1:nrow(iris), nrow(iris)) associations_cont_df[[i]]$dist &lt;- names(associations_cont)[i] } associations_cont_df &lt;- do.call(rbind, associations_cont_df) ggplot(associations_cont_df, aes(x=row, y=column)) + facet_wrap(. ~ dist) + geom_tile(aes(fill = value), colour = NA) + #geom_text(aes(label = round(value, 2))) + scale_fill_gradient2(low = &quot;#00ccff&quot;, mid = &quot;#aad400&quot;, high = &quot;#ff0066&quot;, midpoint = 5) + labs(x=&quot;Site&quot;, y=&quot;Site&quot;) Le tableau iris est ordonn√© par esp√®ce. Les distances euclidienne et de Manhattan permettent ais√©ment de distinguer les esp√®ces selon les dimensions des p√©tales et des s√©pales. Toutefois, l‚Äôutilsation de la covariance avec la distance de Mahalanobis cr√©e des distinction moins tranch√©es. 9.2.1.4 Objets: Donn√©es mixtes Les donn√©es cat√©gorielles ordinales peuvent √™tre transform√©es en donn√©es continues par gradations lin√©aires ou quadratiques. Les donn√©es cat√©gorielles nominales, quant √† elles, peuvent √™tre encod√©es (encodage cat√©goriel) en donn√©es similaires √† des occurences. Attention toutefois: contrairement √† la r√©gression lin√©aire qui demande d‚Äôexclure une cat√©gorie, l‚Äôencodage cat√©goriel doit inclure toutes les cat√©gories. Le comportement par d√©faut de la fonction model.matrix est d‚Äôexclure la cat√©gorie de r√©f√©rence: on doit sp√©cifier que l‚Äôintercept est de z√©ro, c‚Äôest-√†-dire model.matrix(~ + categorie). La similarit√© de Gower a √©t√© d√©velopp√©e pour mesurer des associations entre des objets dont les donn√©es sont mixtes: bool√©ennes, cat√©gorielles et continues. La similarit√© de Gower est calcul√©e en additionnant les distances calcul√©es par colonne, individuellement. Si la colonne est bool√©enne, on utilise les distances de Jaccard (qui exclue les double-z√©ro) de mani√®re univari√©e: une variable √† la fois. Pour les variables continues, on utilise la distance de Manhattan divis√©e par la plage de valeurs de la variable (pour fin de standardisation). Puisqu‚Äôelle h√©rite de la particularit√© de la distance de Manhattan et de la similarit√© de Jaccard univari√©e, la similarit√© de Gower reste une combinaison lin√©aire de distances univari√©es. X &lt;- tibble(ID = 1:8, age = c(21, 21, 19, 30, 21, 21, 19, 30), gender = c(&#39;M&#39;,&#39;M&#39;,&#39;N&#39;,&#39;M&#39;,&#39;F&#39;,&#39;F&#39;,&#39;F&#39;,&#39;F&#39;), civil_status = c(&#39;MARRIED&#39;,&#39;SINGLE&#39;,&#39;SINGLE&#39;,&#39;SINGLE&#39;,&#39;MARRIED&#39;,&#39;SINGLE&#39;,&#39;WIDOW&#39;,&#39;DIVORCED&#39;), salary = c(3000.0,1200.0 ,32000.0,1800.0 ,2900.0 ,1100.0 ,10000.0,1500.0), children = c(TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE), available_credit = c(2200,100,22000,1100,2000,100,6000,2200)) X ## # A tibble: 8 x 7 ## ID age gender civil_status salary children available_credit ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 1 21 M MARRIED 3000 TRUE 2200 ## 2 2 21 M SINGLE 1200 FALSE 100 ## 3 3 19 N SINGLE 32000 TRUE 22000 ## 4 4 30 M SINGLE 1800 TRUE 1100 ## 5 5 21 F MARRIED 2900 TRUE 2000 ## 6 6 21 F SINGLE 1100 TRUE 100 ## 7 7 19 F WIDOW 10000 FALSE 6000 ## 8 8 30 F DIVORCED 1500 TRUE 2200 Il faut pr√©alablement proc√©der √† l‚Äôencodage cat√©goriel pour les variables cat√©gorielles nominales. X_dum &lt;- model.matrix(~ 0 + ., X[, -1]) X_dum ## age genderF genderM genderN civil_statusMARRIED civil_statusSINGLE civil_statusWIDOW salary childrenTRUE available_credit ## 1 21 0 1 0 1 0 0 3000 1 2200 ## 2 21 0 1 0 0 1 0 1200 0 100 ## 3 19 0 0 1 0 1 0 32000 1 22000 ## 4 30 0 1 0 0 1 0 1800 1 1100 ## 5 21 1 0 0 1 0 0 2900 1 2000 ## 6 21 1 0 0 0 1 0 1100 1 100 ## 7 19 1 0 0 0 0 1 10000 0 6000 ## 8 30 1 0 0 0 0 0 1500 1 2200 ## attr(,&quot;assign&quot;) ## [1] 1 2 2 2 3 3 3 4 5 6 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$gender ## [1] &quot;contr.treatment&quot; ## ## attr(,&quot;contrasts&quot;)$civil_status ## [1] &quot;contr.treatment&quot; ## ## attr(,&quot;contrasts&quot;)$children ## [1] &quot;contr.treatment&quot; Calculons la dissimilarit√© de Gower (cette fois le graphique est fait avec pheatmap). library(&quot;pheatmap&quot;) d_gow &lt;- as.matrix(vegdist(X_dum, &#39;gower&#39;)) colnames(d_gow) &lt;- rownames(d_gow) &lt;- X$ID pheatmap(d_gow) Les dendrogrammes apparaissants sur les axes du graphique sont issus d‚Äôun processus de partitionnement bas√© sur la distance, que nous verrons plus loin dans ce chapiter. Les profils des clients 4 et 7, ainsi que ceux des clients 3 et 7 diff√®rent le plus. Les profils 3 et 4 sont n√©anmoins plut√¥t diff√©rents. 9.2.2 Associations entre variables (mode R) Il existe de nombreuses approches pour mesurer les associations entre variables. La plus connue est la corr√©lation. Mais les donn√©es d‚Äôabondance et d‚Äôoccurence demandent des approches diff√©rentes. 9.2.2.1 Variables: Abondance La distance du chi-carr√© est sugg√©r√©e par Borcard et al. (2011). abundance_r &lt;- t(abundance) D_chisq_R &lt;- as.matrix(dist(decostand(abundance_r, method=&quot;chi.square&quot;))) pheatmap(D_chisq_R, display_numbers = round(D_chisq_R, 2)) Des coabondances sont notables pour la m√©sange √† t√™te noire, le jaseur bor√©al, la citelle √† poitrine rousse et le bruant √† gorge blanche (tache bleu au centre). 9.2.2.2 Variables: Occurence La dissimilarit√© de Jaccard peut √™tre utilis√©e. occurence_r &lt;- t(occurence) D_jacc_R &lt;- as.matrix(vegdist(occurence_r, method = &quot;jaccard&quot;)) pheatmap(D_jacc_R, display_numbers = round(D_jacc_R, 2)) Des cooccurences sont notables pour le jaseur bor√©al, la citelle √† poitrine rousse et le bruant √† gorge blanche (tache bleu au centre). 9.2.2.3 Variables: Quantit√©s La matrice des corr√©lations de Pearson peut √™tre utilis√©e pour les donn√©es continues. Quant aux variables ordinales, elles devraient id√©alement √™tre li√©es lin√©airement ou quadratiquement. Si ce n‚Äôest pas le cas, c‚Äôest-√†-dire que les cat√©gories sont ordonn√©es par rang seulement, vous pourrez avoir recours aux coefficients de corr√©lation de Spearman ou de Kendall. iris_cor &lt;- iris %&gt;% select(-Species) %&gt;% cor(.) pheatmap(cor(iris[, -5]), cluster_rows = FALSE, cluster_cols = FALSE, display_numbers = round(iris_cor, 2)) 9.2.3 Conclusion sur les associations Il n‚Äôexiste pas de r√®gle claire pour d√©terminer quelle technique d‚Äôassociation utiliser. Cela d√©pend en premier lieu de vos donn√©es. Vous s√©lectionnerez votre m√©thode d‚Äôassociation selon le type de donn√©es que vous abordez, la question √† laquelle vous d√©sirez r√©pondre ainsi l‚Äôexp√©rience dans la litt√©rature comme celle de vos coll√®gues scientifiques. S‚Äôil n‚Äôexiste pas de r√®gle clair, c‚Äôest qu‚Äôil existe des dizaines de m√©thodes diff√©rentes, et la plupart d‚Äôentre elles vous donneront une perspective juste et valide. Il faut n√©anmoins faire attention pour √©viter de s√©lectionner les m√©thodes qui ne sont pas appropri√©es. 9.3 Partitionnement Les donn√©es suivantes ont √©t√© g√©n√©r√©es par Leland McInnes (Tutte institute of mathematics, Ottawa). √ätes-vous en mesure d‚Äôidentifier des groupes? Combien en trouvez-vous? df_mcinnes &lt;- read_csv(&quot;data/clusterable_data.csv&quot;, col_names = c(&quot;x&quot;, &quot;y&quot;), skip = 1) ## Parsed with column specification: ## cols( ## x = col_double(), ## y = col_double() ## ) ggplot(df_mcinnes, aes(x=x, y=y)) + geom_point() + coord_fixed() En 2D, l‚Äôoeil humain peut facilement d√©tecter les groupes. En 3D, c‚Äôest toujours possible, mais au-del√† de 3D, le partitionnement cognitive devient rapidement maladroite. Les algorithmes sont alors d‚Äôune aide pr√©cieuse. Mais ils transportent en pratique tout un baggage de limitations. Quel est le crit√®re d‚Äôassociation entre les groupes? Combien de groupe devrions-nous cr√©er? Comment distinguer une donn√©e trop bruit√©e pour √™tre classifi√©e? Le partitionnement de donn√©es (clustering en anglais), et inversement leur regroupement, permet de cr√©er des ensembles selon des crit√®res d‚Äôassociation. On suppose donc que Le partitionnement permet de cr√©er des groupes selon l‚Äôinformation que l‚Äôon fait √©merger des donn√©es. Il est cons√©quemment entendu que les donn√©es ne sont pas cat√©goris√©es √† priori: il ne s‚Äôagit pas de pr√©dire la cat√©gorie d‚Äôun objet, mais bien de cr√©er des cat√©gories √† partir des objets par exemple selon leurs dimensions, leurs couleurs, leurs signature chimique, leurs comportements, leurs g√®nes, etc. Plusieurs m√©thodes sont aujourd‚Äôhui offertes aux analystes pour partitionner leurs donn√©es. Dans le cadre de ce manuel, nous couvrirons ici deux grandes tendances dans les algorithmes. M√©thodes hi√©rarchique et non hi√©rarchiques. Dans un partitionnement hi√©rarchique, l‚Äôensemble des objets forme un groupe, comprenant des sous-regroupements, des sous-sous-regroupements, etc., dont les objets forment l‚Äôultime partitionnement. On pourra alors identifier comment se d√©cline un partitionnement. √Ä l‚Äôinverse, un partitionnement non-hi√©rarchique des algorhitmes permettent de cr√©er les groupes non hi√©rarchis√©s les plus diff√©rents que possible. Membership exclusif ou flou. Certaines techniques attribuent √† chaque objet une classe unique: l‚Äôappartenance sera indiqu√©e par un 1 et la non appartenance par un 0. D‚Äôautres techniques vont attribuer un membership flou o√π le degr√© d‚Äôappartenance est une variable continue de 0 √† 1. Parmi les m√©thodes floues, on retrouve les m√©thodes probabilistes. 9.3.1 √âvaluation d‚Äôun partitionnement Le choix d‚Äôune technique de partitionnement parmi de nombreuses disponibles, ainsi que le choix des param√®tres gouvernant chacune d‚Äôentre elles, est avant tout bas√© sur ce que l‚Äôon d√©sire d√©finir comme √©tant un groupe, ainsi que la mani√®re d‚Äôinterpr√©ter les groupes. En outre, le nombre de groupe √† d√©partager est toujours une d√©cision de l‚Äôanalyste. N√©anmoins, on peut se fier des indicateurs de performance de partitionnement. Parmis ceux-ci, retenons le score silouhette ainsi que l‚Äôindice de Calinski-Harabaz. 9.3.1.1 Score silouhette En anglais, le h dans silouhette se trouve apr√®s le l: on parle donc de silhouette coefficient pour d√©signer le score de chacun des objets dans le partitionnement. Pour chaque objet, on calcule la distance moyenne qui le s√©pare des autres points de son groupe (\\(a\\)) ainsi que la distance moyenne qui le s√©pare des points du groupe le plus rapproch√©. \\[s = \\frac{b-a}{max \\left(a, b \\right)}\\] Un coefficient de -1 indique le pire classement, tandis qu‚Äôun coefficient de 1 indique le meilleur classement. La moyenne des coefficients silouhette est le score silouhette. 9.3.1.2 Indice de Calinski-Harabaz L‚Äôindice de Calinski-Harabaz est proportionnel au ratio des dispersions intra-groupe et la moyenne des dispersions inter-groupes. Plus l‚Äôindice est √©lev√©, mieux les groupes sont d√©finis. La math√©matique est d√©crite dans la documentation de scikit-learn, un module d‚Äôanalyse et autoapprentissage sur Python. Note. Les coefficients silouhette et l‚Äôindice de Calinski-Harabaz sont plus appropri√©s pour les formes de groupes convexes (cercles, sph√®res, hypersph√®res) que pour les formes irr√©guli√®res (notamment celles obtenues par la DBSCAN, discut√©e ci-desssous). 9.3.2 Partitionnement non hi√©rarchique Il peut arriver que vous n‚Äôayez pas besoin de comprendre la structure d‚Äôagglom√©ration des objets (ou variables). Plusieurs techniques de partitionnement non hi√©rarchique sont disponibles sur R. On s‚Äôint√©ressera en particulier aux k-means et au dbscan. 9.3.2.1 Kmeans L‚Äôobjectif des kmeans est de minimiser la distance eucl√©dienne entre un nombre pr√©d√©fini de k groupes exclusifs. L‚Äôalgorhitme commence par placer une nombre k de centroides au hasard dans l‚Äôespace d‚Äôun nombre p de variables (vous devez fixer k, et p est le nombre de colonnes de vos donn√©es). Ensuite, chaque objet est √©tiquett√© comme appartenant au groupe du centroid le plus pr√®s. La position du centroide est d√©plac√©e √† la moyenne de chaque groupe. Recommencer √† partir de l‚Äô√©tape 2 jusqu‚Äô√† ce que l‚Äôassignation des objets aux groupes ne change plus. Source: David Sheehan La technique des kmeans suppose que les groupes ont des distributions multinormales - repr√©sent√©es par des cercles en 2D, des sph√®res en 3D, des hypersph√®res en plus de 3D. Cette limitation est probl√©matique lorsque les groupes se pr√©sentent sous des formes irr√©guli√®res, comme celles du nuage de points de Leland McInnes, pr√©sent√© plus haut. De plus, la technique classique des kmeans est bas√©e sur des distances euclidiennes: l‚Äôutilisation des kmeans n‚Äôest appropri√©e pour les donn√©es comprenant beaucoup de z√©ros, comme les donn√©es d‚Äôabondance, qui devraient pr√©alablement √™tre transform√©es en variables centr√©es et r√©duites (Legendre et Legendre, 2012). La technique des mixtures gaussiennes (gaussian mixtures) est une g√©n√©ralisation des kmeans permettant d‚Äôint√©grer la covariance des groupes. Les groupes ne sont plus des hyper-sph√®res, mais des hyper-ellipso√Ødes. 9.3.2.1.1 Application Nous pouvons utilis√© la fonction kmeans de R. Toutefois, puisque l‚Äôon d√©sire ici effectuer des tests de partitionnement pour plusieurs nombres de groupes, nous utiliserons cascadeKM, du module vegan. Notez que de nombreux param√®tres par d√©faut sont utilis√©s dans les ex√©cutions ci-dessous. Ces notes de cours ne forment pas un travail de recherche scientifique. Lors de travaux de recherche, l‚Äôutilsation d‚Äôun argument ou d‚Äôun autre dans une fonction doit √™tre justifi√©: qu‚Äôun param√®tre soit utilis√© par d√©faut dans une fonction n‚Äôest a priori pas une justification convainquante. Pour les kmeans, on doit fixer le nombre de groupes. Le graphique des donn√©es de Leland McInnes montrent 6 groupes. Toutefois, il est rare que l‚Äôon puisse visualiser des d√©marquations aussi tranch√©es que celles de l‚Äôexemple, qui plus est dans des cas o√π l‚Äôon doit traiter de plus de deux dimensions. Je vais donc lancer le partitionnement en boucle pour plusieurs nombres de groupes, de 3 √† 10 et pour chaque groupe, √©valuer le score silouhette et de Calinski-Habaraz. J‚Äôutilise un argument random_state pour m‚Äôassurer que les groupes seront les m√™mes √† chaque fois que la cellule sera lanc√©e. library(&quot;vegan&quot;) mcinnes_kmeans &lt;- cascadeKM(df_mcinnes, inf.gr = 3, sup.gr = 10, criterion = &quot;calinski&quot;) str(mcinnes_kmeans) ## List of 4 ## $ partition: int [1:2309, 1:8] 1 1 1 1 1 1 1 1 1 1 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:2309] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. ..$ : chr [1:8] &quot;3 groups&quot; &quot;4 groups&quot; &quot;5 groups&quot; &quot;6 groups&quot; ... ## $ results : num [1:2, 1:8] 85.1 2164.5 61.4 2294.6 51.4 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:2] &quot;SSE&quot; &quot;calinski&quot; ## .. ..$ : chr [1:8] &quot;3 groups&quot; &quot;4 groups&quot; &quot;5 groups&quot; &quot;6 groups&quot; ... ## $ criterion: chr &quot;calinski&quot; ## $ size : int [1:10, 1:8] 1243 505 561 NA NA NA NA NA NA NA ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:10] &quot;Group 1&quot; &quot;Group 2&quot; &quot;Group 3&quot; &quot;Group 4&quot; ... ## .. ..$ : chr [1:8] &quot;3 groups&quot; &quot;4 groups&quot; &quot;5 groups&quot; &quot;6 groups&quot; ... ## - attr(*, &quot;class&quot;)= chr &quot;cascadeKM&quot; L‚Äôobjet mcinnes_kmeans, de type cascadeKM, peut √™tre visualis√© directement avec la fonction plot. plot(mcinnes_kmeans) On obtient un maximum de Calinski √† 4 groupes, qui correspons √† la deuxi√®me simulation effectu√©e de 3 √† 10. Examinons les scores silouhette (module: cluster). library(&quot;cluster&quot;) asw &lt;- c() for (i in 1:ncol(mcinnes_kmeans$partition)) { mcinnes_kmeans_silhouette &lt;- silhouette(mcinnes_kmeans$partition[, i], dist = vegdist(df_mcinnes, method = &quot;euclidean&quot;)) asw[i] &lt;- summary(mcinnes_kmeans_silhouette)$avg.width } plot(3:10, asw, type = &#39;b&#39;) Le score silouhette maximum est √† 3 groupes. La forme des groupes n‚Äô√©tant pas convexe, il fallait s‚Äôattendre √† ce que indicateurs maximaux pour les deux indicateurs soient diff√©rents. C‚Äôest d‚Äôailleurs souvent le cas. Cet exemple supporte que le choix du nombre de groupe √† d√©partager repose sur l‚Äôanalyste, non pas uniquement sur les indicateurs de performance. Choisissons 6 groupes, puisque que c‚Äôest visuellement ce que l‚Äôon devrait chercher pour ce cas d‚Äô√©tude. kmeans_group &lt;- mcinnes_kmeans$partition[, 4] mcinnes_kmeans$partition %&gt;% head(3) ## 3 groups 4 groups 5 groups 6 groups 7 groups 8 groups 9 groups 10 groups ## 1 1 4 5 3 1 6 7 10 ## 2 1 4 4 1 1 3 7 10 ## 3 1 1 5 3 6 6 3 3 df_mcinnes %&gt;% mutate(kmeans_group = kmeans_group) %&gt;% # ajouter une colonne de regoupement ggplot(aes(x=x, y=y)) + geom_point(aes(colour = factor(kmeans_group))) + coord_fixed() L‚Äôalgorithme kmeans est loin d‚Äô√™tre statisfaisant. Cela est attendu, puisque les kmeans recherchent des distribution gaussiennes sur des groupes vraisemblablement non-gaussiens. Nous pouvons cr√©er un graphique silouhette pour nos 6 groupes. Notez qu‚Äô√† cause d‚Äôun bogue, il n‚Äôest pas possible de pr√©senter les donn√©es clairement lorsqu‚Äôelles sont nombreuses. sil &lt;- silhouette(mcinnes_kmeans$partition[, 6], dist = vegdist(df_mcinnes[, ], method = &quot;euclidean&quot;)) sil &lt;- sortSilhouette(sil) plot(sil, col = &#39;black&#39;) 9.3.2.2 DBSCAN La technique DBSCAN (* Density-Based Spatial Clustering of Applications with Noise) sousentend que les groupes sont compos√©s de zones o√π l‚Äôon retrouve plus de points (zones denses) s√©par√©es par des zones de faible densit√©. Pour lancer l‚Äôalgorithme, nous devons sp√©cifier une mesure d‚Äôassociation critique (distance ou dissimilarit√©) d* ainsi qu‚Äôun nombre de point critique k dans le voisinage de cette distance. L‚Äôalgorithme commence par √©tiqueter chaque point selon l‚Äôune de ces cat√©gories: Noyau: le point a au moins k points dans son voisinage, c‚Äôest-√†-dire √† une distance inf√©rieure ou √©gale √† d. Bordure: le point a moins de k points dans son voisinage, mais l‚Äôun de des points voisins est un noyau. Bruit: le cas √©ch√©ant. Ces points sont consid√©r√©s comme des outliers. Les noyaux distanc√©s de d ou moins sont connect√©s entre eux en englobant les bordures. Le nombre de groupes est prescrit par l‚Äôalgorithme DBSCAN, qui permet du coup de d√©tecter des donn√©es trop bruit√©es pour √™tre class√©es. Damiani et al. (2014) a d√©velopp√© une approche utilisant la technique DBSCAN pour partitionner des zones d‚Äôescale pour les flux de populations migratoires. 9.3.2.2.1 Application La technique DBSCAN n‚Äôest pas bas√©e sur le nombre de groupe, mais sur la densit√© des points. L‚Äôargument x ne constitue pas les donn√©es, mais une matrice d‚Äôassociation. L‚Äôargument minPts sp√©cifie le nombre minimal de points qui l‚Äôon doit retrouver √† une distance critique d* pour la formation des *noyaux et la propagation des groupes, sp√©cifi√©e dans l‚Äôargument eps. La distance d peut √™tre estim√©e en prenant une fraction de la moyenne, mais on aura volontiers recours √† sont bon jugement. library(&quot;dbscan&quot;) mcinnes_dbscan &lt;- dbscan(x = vegdist(df_mcinnes[, ], method = &quot;euclidean&quot;), eps = 0.03, minPts = 10) dbscan_group &lt;- mcinnes_dbscan$cluster unique(dbscan_group) ## [1] 1 0 2 6 3 4 5 Les param√®tres sp√©cifi√©s donnent 5 groupes (1, 2, ..., 5) et des points trop bruit√©s pour √™tre classifi√©s (√©tiquet√©s 0). Voyons comment les groupes ont √©t√© form√©s. df_mcinnes %&gt;% mutate(dbscan_group = dbscan_group) %&gt;% # ajouter une colonne de regoupement ggplot(aes(x=x, y=y)) + geom_point(aes(colour = factor(dbscan_group))) + coord_fixed() Le partitionnement semble plus conforme √† ce que l‚Äôon recherche. N√©anmoins, DBSCAN cr√© quelques petits groupes ind√©sirables (groupe 6, en rose) ainsi qu‚Äôun grand groupe (violet) qui auraient lieu d‚Äô√™tre partitionn√©. Ces d√©faut pourraient √™tre r√©gl√©s en jouant sur les param√®tres eps et minPts. 9.3.3 Partitionnement hi√©rarchique Les techniques de partitionnement hi√©rarchique sont bas√©es sur les matrices d‚Äôassociation. La technique pour mesurer l‚Äôassociation (entre objets ou variables) d√©terminera en grande partie le paritionnement des donn√©es. Les partitionnements hi√©rarchiques ont l‚Äôavantage de pouvoir √™tre repr√©sent√©s sous forme de dendrogramme (ou arbre) de partition. Un tel dendrogramme pr√©sente des sous-groupes qui se joignent en groupes jusqu‚Äô√† former un seul ensemble. Le partitionnement hi√©rarchique est abondamment utilis√© en phylog√©nie, pour √©tudier les relations de parent√© entre organismes vivants, populations d‚Äôorganismes et esp√®ces. La ph√©n√©tique, branche empirique de la phylog√©n√®se intersp√©cifique, fait usage du partitionnement hi√©rarchique √† partir d‚Äôassociations g√©n√©tiques entre unit√©s taxonomiques. On retrouve de nombreuses ressources acad√©miques en phylog√©n√©tique ainsi que des outils pour R et Python. Toutefois, la phylog√©n√©tique en particulier ne fait pas partie de la pr√©sente itt√©ration de ce manuel. 9.3.3.1 Techniques de partitionnement hi√©rarchique Le partitionnement hi√©rarchique est typiquement effectu√© avec une des quatres m√©thodes suivantes, dont chacune poss√®de ses particularit√©s, mais sont toutes agglom√©ratives: √† chaque √©tape d‚Äôagglom√©ration, on fusionne les deux groupes ayant le plus d‚Äôaffinit√© sur la base des deux sous-groupes les plus rapproch√©s. Single link (single). Les groupes sont agglom√©r√©s sur la base des deux points parmi les groupes, qui sont les plus proches. Complete link (complete). √Ä la diff√©rence de la m√©thode single, on consid√®re comme crit√®re d‚Äôagglom√©ration les √©l√©ments les plus √©loign√©s de chaque groupe. Agglom√©ration centrale. Il s‚Äôagit d‚Äôune fammlle de m√©thode bas√©es sur les diff√©rences entre les tendances centrales des objets ou des groupes. Average (average). Appel√©e UPGMA (Unweighted Pair-Group Method unsing Average), les groupes sont agglom√©r√©s selon un centre calcul√©s par la moyenne et le nombre d‚Äôobjet pond√®re l‚Äôagglom√©ration (le poids des groupes est retir√©). Cette technique est historiquement utilis√©e en bioinformatique pour partitionner des groupes phylog√©n√©tiques (Sneath et Sokal, 1973). Weighted (weighted). La version de average, mais non pond√©r√©e (WPGMA). Centroid (centroid). Tout comme average, mais le centro√Øde (centre g√©om√©trique) est utilis√© au lieu de la moyenne. Accronyme: UPGMC. Median (median). Appel√©e WPGMC. Devinez! ;) Ward (ward). L‚Äôoptimisation vise √† minimiser les sommes des carr√©s par regroupement. 9.3.3.2 Quel outil de partitionnement hi√©rarchique utiliser? Alors que le choix de la matrice d‚Äôassociation d√©pend des donn√©es et de leur contexte, la technique de partitionnement hi√©rarchique peut, quant √† elle, √™tre bas√©e sur un crit√®re num√©rique. Il en existe plusieurs, mais le crit√®re recommand√© pour le choix d‚Äôune technique de partitionnement hi√©rarchique est la corr√©lation coph√©n√©tique. La distance coph√©n√©tique est la distance √† laquelle deux objets ou deux sous-groupes deviennent membres d‚Äôun m√™me groupe. La corr√©lation coph√©n√©tique est la corr√©lation de Pearson entre le vecteur d‚Äôassociation des objets et le vecteur de distances coph√©n√©tiques. 9.3.3.3 Application Les techniques de partitionnement hi√©rarchique pr√©sent√©es ci-dessus sont disponibles dans le module stats de R, qui est charg√© automatiquement lors de l‚Äôouversture de R. Nous allons classifier les dimensions des iris gr√¢ce √† la distance de Manhattan. mcinnes_hclust_distmat &lt;- vegdist(df_mcinnes, method = &quot;manhattan&quot;) clustering_methods &lt;- c(&#39;single&#39;, &#39;complete&#39;, &#39;average&#39;, &#39;centroid&#39;, &#39;ward&#39;) clust_l &lt;- list() coph_corr_l &lt;- c() for (i in seq_along(clustering_methods)) { clust_l[[i]] &lt;- hclust(mcinnes_hclust_distmat, method = clustering_methods[i]) coph_corr_l[i] &lt;- cor(mcinnes_hclust_distmat, cophenetic(clust_l[[i]])) } ## The &quot;ward&quot; method has been renamed to &quot;ward.D&quot;; note new &quot;ward.D2&quot; tibble(clustering_methods, coph_corr = coph_corr_l) %&gt;% ggplot(aes(x = fct_reorder(clustering_methods, -coph_corr), y = coph_corr)) + geom_col() + labs(x = &quot;M√©thode de partitionnement&quot;, y = &quot;Corr√©lation coph√©n√©tique&quot;) La m√©thode average retourne la corr√©lation la plus √©lev√©e. Pour plus de flexibilit√©, ench√¢ssons le nom de la m√©thode dans une variable. Ainsi, en chageant le nom de cette variable, le reste du code sera cons√©quent. names(clust_l) &lt;- clustering_methods best_method &lt;- &quot;average&quot; Le partitionnement hi√©rarchique peut √™tre visualis√© par un dendrogramme. plot(clust_l[[best_method]]) 9.3.3.4 Combien de groupes utiliser? La longueur des lignes verticales est la distance s√©parant les groupes enfants. Bien que la s√©lection du nombre de groupe soit avant tout bas√©e sur les besoins du probl√®me, nous pouvons nous appuyer sur certains outils. La hauteur totale peut servir de crit√®re pour d√©finir un nombre de groupes ad√©quat. On pourra s√©lectionner le nombre de groupe o√π la hauteur se stabilise en fonction du nombre de groupe. On pourra aussi utiliser le graphique silhouette, comprenant une collection de largeurs de silouhette, repr√©sentant le degr√© d‚Äôappartenance √† son groupe. La fonction sklearn.metrics.silhouette_score, du module scikit-learn, s‚Äôen occupe. asw &lt;- c() num_groups &lt;- 3:10 for(i in seq_along(num_groups)) { sil &lt;- silhouette(cutree(clust_l[[best_method]], k = num_groups[i]), mcinnes_hclust_distmat) asw[i] &lt;- summary(sil)$avg.width } plot(num_groups, asw, type = &quot;b&quot;) Le nombre optimal de groupes serait de 5. Coupons le dendrorgamme √† la hauteur correspondant √† 5 groupes avec la fonction cutree. k_opt &lt;- num_groups[which.max(asw)] hclust_group &lt;- cutree(clust_l[[best_method]], k = k_opt) plot(clust_l[[best_method]]) rect.hclust(clust_l[[best_method]], k = k_opt) La classification hi√©rarchique, uniquement bas√©e sur la distance, peut √™tre inappropri√©e pour d√©finir des formes complexes. df_mcinnes %&gt;% mutate(hclust_group = hclust_group) %&gt;% # ajouter une colonne de regoupement ggplot(aes(x=x, y=y)) + geom_point(aes(colour = factor(hclust_group))) + coord_fixed() 9.3.4 Partitionnement hi√©rarchique bas√©e sur la densit√© des points La tecchinque HDBSCAN, dont l‚Äôalgorithme est relativement r√©cent (Campello et al., 2013), permet une partitionnement hi√©rarchique sur le m√™me principe des zones de densit√© de la technique DBSCAN. Le HDBSCAN a √©t√© utilis√©e pour partitionner les lieux d‚Äôescale d‚Äôoiseaux migrateurs en Chine (Xu et al., 2013). Avec DBSCAN, un rayon est fix√© dans une m√©trique appropri√©e. Pour chaque point, on compte le nombre de point voisins, c‚Äôest √† dire le nombre de point se situant √† une distance (ou une dissimilarit√©) √©gale ou inf√©rieure au rayon fix√©. Avec HDBSCAN, on sp√©cifie le nombre de points devant √™tre recouverts et on calcule le rayon n√©cessaire pour les recouvrir. Ainsi, chaque point est associ√© √† un rayon critique que l‚Äôon nommera \\(d_{noyau}\\). La m√©trique initiale est ensuite alt√©r√©e: on remplace les associations entre deux objets A et B par la valeur maximale entre cette association, le rayon critique de A et le rayon critique de B. Cette nouvelle distance est appel√©e la distance d‚Äôatteinte mutuelle: elle accentue les distances pour les points se trouvant dans des zones peu denses. On applique par la suite un algorithme semblable √† la partition hi√©rarchique single link: En s‚Äô√©largissant, les rayons se superposent, chaque superposition de rayon forment graduellement des groupes qui s‚Äôagglom√®rent ainsi de mani√®re hi√©rarchique. Au lieu d‚Äôeffectuer une tranche √† une hauteur donn√©e dans un dendrogramme de partitionnement, la technique HDBSCAN se base sur un dendrogramme condens√© qui discarte les sous-groupes comprenant moins de n objets (\\(n_{gr min}\\)). Dans nouveau dendrogramme, on recherche des groupes qui occupent bien l‚Äôespace d‚Äôanalyse. Pour ce faitre, on utilise l‚Äôinverse de la distance pour cr√©er un indicateur de persistance (semblable √† la similarit√©), \\(\\lambda\\). Pour chaque groupe hi√©rarchique dans le dendrogramme condens√©, on peut calculer la persistance o√π le groupe prend naissance. De plus, pour chaque objet d‚Äôun groupe, on peut aussi calculer une distance √† laquelle il quitte le groupe. La stabilit√© d‚Äôun groupe est la domme des diff√©rences de persistance entre la persistance √† la naissance et les persistances des objets. On descend dans le dendrogramme. Si la somme des stabilit√© des groupes enfants est plus grande que la stabilit√© du groupe parent, on accepte la division. Sinon, le parent forme le groupe. La documentation du module hdbscan pour Python offre une description intuitive et plus exhaustive des principes et algorithme de HDBSCAN. 9.3.4.1 Param√®tres Outre la m√©trique d‚Äôassociation dont nous avons discut√©, HDBSCAN demande d‚Äô√™tre nourri avec quelques param√®tres importants. En particulier, le nombre minimum d‚Äôobjets par groupe, \\(n_{gr min}\\) d√©pend de la quantit√© de donn√©es que vous avez √† votre disposition, ainsi que de la quantit√© d‚Äôobjets que vous jugez suffisante pour cr√©er des groupes. Nous utiliserons l‚Äôimpl√©mentation de HDBSCAN du module dbscan. Si vous d√©sirez davantage d‚Äôoptions, vous pr√©f√©rerez probablement l‚Äôimpl√©mentation du module largeVis. mcinnes_hdbscan &lt;- hdbscan(x = vegdist(df_mcinnes, method = &quot;euclidean&quot;), minPts = 20, gen_hdbscan_tree = TRUE, gen_simplified_tree = FALSE) hdbscan_group &lt;- mcinnes_hdbscan$cluster unique(hdbscan_group) ## [1] 6 0 4 3 5 1 2 Nous avons 6 groupes, num√©rot√©s de 1 √† 6, ainsi que des √©tiquettes identifiant des objets d√©sign√©s comme √©tant du bruit de fond, num√©rot√© 0. Le dendrogramme non condens√© peu √™tre produit. plot(mcinnes_hdbscan$hdbscan_tree) Difficile d‚Äôy voir clair avec autant d‚Äôobjets. L‚Äôobjet mcinnes_hdbscan a un nombre minimum d‚Äôobjets par groupe de 20. Ce qui permet de pr√©senter le dendrogramme de mani√®re condens√©e. plot(mcinnes_hdbscan) Enfin, un aper√ßu des strat√©gies de partitionnement utilis√©s jusqu‚Äôici. clustering_group &lt;- df_mcinnes %&gt;% mutate(kmeans_group, hclust_group, dbscan_group, hdbscan_group) %&gt;% gather(-x, -y, key = &quot;method&quot;, value = &quot;cluster&quot;) ## Warning: attributes are not identical across measure variables; ## they will be dropped clustering_group$cluster &lt;- factor(clustering_group$cluster) clustering_group %&gt;% ggplot(aes(x = x, y = y)) + geom_point(aes(colour = cluster)) + facet_wrap(~method, ncol = 2) + coord_equal() + theme_bw() Clairement, le partitionnement avec HDBSCAN donne les meilleurs r√©sultats. 9.3.5 Conclusion sur le partitionnement Au chapitre 4, nous avons vu avec le jeu de donn√©es ‚Äúdatasaurus‚Äù que la visualisation peut permettre de d√©tecter des structures en segmentant les donn√©es selon des groupes. Or, si les donn√©es n‚Äô√©taient pas √©tiquet√©es, leur structure serait ind√©tectable avec les algorithmes disponibles actuellement. Le partitionnement permet d‚Äôexplorer des donn√©es, de d√©tecter des tendances et de d√©gager des groupes permettant la prise de d√©cision. Plusieurs techniques de partitionnement ont √©t√© pr√©sent√©es. Le choix de la technique sera d√©terminante sur la mani√®re dont les groupes seront partitionn√©s. La d√©finition d‚Äôun groupe variant d‚Äôun cas √† l‚Äôautre, il n‚Äôexiste pas de r√®gle pour prescrire une m√©thode ou une autre. La partitionnement hi√©rarchique a l‚Äôavantage de permetre de visualiser comment les groupes s‚Äôagglom√®rent. Parmi les m√©thodes de partitionnement hi√©rarchique disponibles, les m√©thodes bas√©es sur la densit√© permettent une grande flexibilit√©, ainsi qu‚Äôune d√©tection d‚Äôobservations ne faisant partie d‚Äôaucun goupe. 9.4 Ordination En √©cologie, biologie, agronommie comme en foresterie, la plupart des tableaux de donn√©es comprennent de nombreuses variables: pH, nutriments, climat, esp√®ces ou cultivars, etc. L‚Äôordination vise √† mettre de l‚Äôordre dans des donn√©es dont le nombre √©lev√© de variables peut amener √† des difficult√©s d‚Äôappr√©ciation et d‚Äôinterpr√©taion (Legendre et Legendre, 2012). Plus pr√©cis√©ment, le terme ordination est utilis√© en √©cologie pour d√©signer les techniques de r√©duction d‚Äôaxe. L‚Äôanalyse en composante principale est probablement la plus connue de ces techniques. Mais de nombreuses techniques d‚Äôordination ont √©t√© d√©velopp√©es au cours des derni√®res ann√©es, chacune ayant ses domaines d‚Äôapplication. Les techniques de r√©duction d‚Äôaxe permettent de d√©gager l‚Äôinformation la plus importante en projetant une synth√®se des relations entre les observations et entre les variables. Les techniques ne supposant aucune structure a priori sont dites non-contraignantes: elles ne comprennent pas de tests statistiques. √Ä l‚Äôinverse, les ordinations contraignantes lient des variables descriptives avec une ou plusieurs variables pr√©dictives. La r√©f√©rence en la mati√®re est indiscutablement (Legendre et Legendre, 2012). Cette section en couvrira quelques unes et vous guidera vers la technique la plus appropri√©e pour vos donn√©es. 9.4.1 Ordination non contraignante Cette section couvrira l‚Äôanalyse en composantes principales (ACP), l‚Äôanalyse de correspondance (AC), l‚Äôanalyse factorielle (AF) ainsi que l‚Äôanalyse en coordonn√©es principales (ACoP). M√©thode Distance pr√©serv√©e Variables Analyse en composantes principales (ACP) Distance euclidienne Donn√©es quantitatives, relations lin√©aires (attention aux double-z√©ros) Analyse de correspondance (AC) Distance de \\(\\chi^2\\) Donn√©es non-n√©gatives, dimentionnellement homog√®nes ou binaires, abondance ou occurence Positionnement multidimensionnel (PoMd) Toute mesure de dissimilarit√© Donn√©es quantitatives, qualitatives nominales/ordinales ou mixtes Source: Adapt√© de (Legendre et Legendre, 2012, chapitre 9) 9.4.1.1 Analyse en composantes principales L‚Äôobjectif d‚Äôune ACP est de repr√©senter les donn√©es dans un nombre r√©duit de dimensions repr√©sentant le plus possible la variation d‚Äôun tableau de donn√©es: elle permet de projetter les donn√©es dans un espace o√π les variables sont combin√©es en axes orthogonaux dont le premier axe capte le maximum de variance. L‚ÄôACP peut par exemple √™tre utilis√©e pour analyser des corr√©lations entre variables ou d√©gager l‚Äôinformation la plus pertinente d‚Äôun tableau de donn√©es m√©t√©o ou de signal en un nombre plus retreint de variables. L‚ÄôACP effectue une rotation des axes √† partir du centre (moyenne) du nuage de points effectu√©e de mani√®re √† ce que le premier axe d√©finisse la direction o√π l‚Äôon retrouve la variance maximale. Ce premier axe est une combinaison lin√©aire des variables et forme la premi√®re composante principale. Une fois cet axe d√©finit, on trouve de deuxi√®me axe, orthogonal au premier, o√π l‚Äôon retouve la variance maximale - cet axe forme la deuxi√®me composante principale, et ainsi de suite jusqu‚Äô√† ce que le nombre d‚Äôaxe corresponde au nombre de variables. Les projections des observations sur ces axes principaux sont appel√©s les scores. Les projections des variables sur les axes principaux sont les vecteurs propres (eigenvectors, ou loadings). La variance des composantes principales diminue de la premi√®re √† la derni√®re, et peut √™tre calcul√©e comme une proportion de la variance totale: c‚Äôest le pourcentage d‚Äôinertie. Par convention, on utilise les valeurs propres (eigenvalues) pour mesurer l‚Äôimportance des axes. Si la premi√®re composante principale a une inertie de 50% et la deuxi√®me a une intertie de 30%, la repr√©sentation en 2D des projection repr√©sentera 80% de la variance du nuage de points. L‚Äôh√©t√©rog√©n√©it√© des √©chelles de mesure peut avoir une grande importance sur les r√©sultats d‚Äôune ACP (les donn√©es doivent √™tre dimensionnellement homog√®nes). En effet, la hauteur d‚Äôun ceriser aura une variance plus grande que le diam√®tre d‚Äôune cerise exprim√© dans les m√™mes unit√©s, et cette derni√®re aura plus de variance que la teneur en cuivre d‚Äôune feuille. Il est cons√©quemment avis√© de mettre les donn√©es √† l‚Äô√©chelle en centrant la moyenne √† z√©ro et l‚Äô√©cart-type √† 1 avant de proc√©der √† une ACP. L‚ÄôACP a √©t√© con√ßue pour projetter en un nombre moindre de dimensions des observations dont les distributions sont multinormales. Bien que l‚ÄôACP soit une technique robuste, il est pr√©f√©rable de transformer pr√©alablement les variables dont la distribution est particuli√®rement asym√©triques (Legendre et Legendre, 2012, p.¬†450). Le cas √©ch√©ant, les valeurs extr√™mes pourraient faire d√©vier les vecteurs propres et biaiser l‚Äôanalyse. En particulier, les donn√©es ACP men√©es sur des donn√©es compositionnelles sont r√©put√©es pour g√©n√©rer des analyses biais√©es (Pawlowsky-Glahn and Egozcue, 2006). Le test de Mardia (Korkmaz, 2014) peut √™tre utilis√© pour tester la multinormalit√©. Une distribution multinormale devrait g√©n√©rer des scores en forme d‚Äôhypersph√®re (en forme de cercle sur un biplot: voir plus loin). 9.4.1.1.1 Vecteurs propres et valeurs propres Une matrice carr√©e (comme une matrice de covariance \\(\\Sigma\\)) multipli√©e par un vecteur propre \\(e\\) est √©gale aux valeurs propres \\(\\lambda\\) multipli√©es par les vecteurs propres \\(e\\). \\[ \\Sigma e = \\lambda e \\] De mani√®re intuitive, les vecteurs propres indiquent l‚Äôorientation de la covariance, et les valeurs propres indique la longueur associ√©e √† cette direction. L‚ÄôACP est bas√©e sur le calcul des vecteurs propres et des valeurs propres de la matrice de covariance des variables. Pour d‚Äôabord obtenir les valeurs propres \\(\\lambda\\), il faut r√©soudre l‚Äô√©quation \\[ det(cov(X) - \\lambda I) = 0 \\], o√π \\(det\\) est l‚Äôop√©ration permettant de calculer le d√©terminant, \\(cov\\) est l‚Äôop√©ration pour calculer la covariance, \\(X\\) est la matrice de donn√©es, \\(\\lambda\\) sont les valeurs propres et \\(I\\) est une matrice d‚Äôidentit√©. Pour \\(p\\) variables dans votre tableau \\(X\\), vous obtiendrex \\(p\\) valeurs propres. Ensuite, on trouve les vecteurs propres en r√©solvant l‚Äô√©quation $ e = e $. Bien qu‚Äôil soit possible d‚Äôeffectuer cette op√©ration √† la main pour des cas tr√®s simples, vous aurez avantage √† utiliser un langage de programmation. Chargeons les donn√©es d‚Äôiris, puis isolons seulement les deux dimensions des s√©pales l‚Äôesp√®ce setosa. data(&quot;iris&quot;) setosa_sepal &lt;- iris %&gt;% filter(Species == &quot;setosa&quot;) %&gt;% select(starts_with(&quot;Sepal&quot;)) setosa_sepal ## Sepal.Length Sepal.Width ## 1 5.1 3.5 ## 2 4.9 3.0 ## 3 4.7 3.2 ## 4 4.6 3.1 ## 5 5.0 3.6 ## 6 5.4 3.9 ## 7 4.6 3.4 ## 8 5.0 3.4 ## 9 4.4 2.9 ## 10 4.9 3.1 ## 11 5.4 3.7 ## 12 4.8 3.4 ## 13 4.8 3.0 ## 14 4.3 3.0 ## 15 5.8 4.0 ## 16 5.7 4.4 ## 17 5.4 3.9 ## 18 5.1 3.5 ## 19 5.7 3.8 ## 20 5.1 3.8 ## 21 5.4 3.4 ## 22 5.1 3.7 ## 23 4.6 3.6 ## 24 5.1 3.3 ## 25 4.8 3.4 ## 26 5.0 3.0 ## 27 5.0 3.4 ## 28 5.2 3.5 ## 29 5.2 3.4 ## 30 4.7 3.2 ## 31 4.8 3.1 ## 32 5.4 3.4 ## 33 5.2 4.1 ## 34 5.5 4.2 ## 35 4.9 3.1 ## 36 5.0 3.2 ## 37 5.5 3.5 ## 38 4.9 3.6 ## 39 4.4 3.0 ## 40 5.1 3.4 ## 41 5.0 3.5 ## 42 4.5 2.3 ## 43 4.4 3.2 ## 44 5.0 3.5 ## 45 5.1 3.8 ## 46 4.8 3.0 ## 47 5.1 3.8 ## 48 4.6 3.2 ## 49 5.3 3.7 ## 50 5.0 3.3 library(&quot;MVN&quot;) ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 ## sROC 0.1-2 loaded setosa_sepal_mvn &lt;- mvn(setosa_sepal, mvnTest = &quot;mardia&quot;) setosa_sepal_mvn$multivariateNormality ## Test Statistic p value Result ## 1 Mardia Skewness 0.759503524380438 0.943793240544741 YES ## 2 Mardia Kurtosis 0.0934600553610254 0.925538081956867 YES ## 3 MVN &lt;NA&gt; &lt;NA&gt; YES Pour consid√©rer la distribution comme multinormale, la p-value de la distortion (Mardia Skewness) et la statistique de Kurtosis (Mardia Kurtosis) doit √™tre √©gale ou plus √©lev√©e que 0.05 (Kormaz, 2019, fiche d‚Äôaide de la fonction mvn de R). C‚Äôest bien le cas pour les donn√©es du tableau setosa_sepal. Retirons de la matrice de covariance les valeurs et vecteurs propres avec la fonction eigen. setosa_eigen &lt;- eigen(cov(setosa_sepal)) setosa_eigenval &lt;- setosa_eigen$values setosa_eigenvec &lt;- setosa_eigen$vectors Le premier vecteur propre correspond √† la premi√®re colonne, et le second √† la deuxi√®me. Les coordonn√©es x et y sont les premi√®res et deuxi√®mes lignes. Les vecteurs propres ont une longueur unitaire (norme de 1). Ils peuvent √™tre mis √† l‚Äô√©chelles √† la racine carr√©e des valeurs propres. setosa_eigenvec_sc &lt;- setosa_eigenvec %*% diag(sqrt(setosa_eigen$values)) Pour effectuer une translation des vecteurs propres au centre du nuage de point, nous avons besoin du centro√Øde. centroid &lt;- setosa_sepal %&gt;% apply(., 2, mean) plot(setosa_sepal, asp = 1) # vecteurs propres brutes lines(x=c(centroid[1], centroid[1] + setosa_eigenvec[1, 1]), y=c(centroid[2], centroid[2] + setosa_eigenvec[2, 1]), col = &quot;green&quot;, lwd = 3) # vecteur propre 1 lines(x=c(centroid[1], centroid[1] + setosa_eigenvec[1, 2]), y=c(centroid[2], centroid[2] + setosa_eigenvec[2, 2]), col = &quot;green&quot;, lwd = 3) # vecteur propre 1 # vecteurs propres √† l&#39;√©chelle lines(x=c(centroid[1], centroid[1] + setosa_eigenvec_sc[1, 1]), y=c(centroid[2], centroid[2] + setosa_eigenvec_sc[2, 1]), col = &quot;red&quot;, lwd = 4) # vecteur propre 1 lines(x=c(centroid[1], centroid[1] + setosa_eigenvec_sc[1, 2]), y=c(centroid[2], centroid[2] + setosa_eigenvec_sc[2, 2]), col = &quot;red&quot;, lwd = 4) # vecteur propre 1 points(x=centroid[1], y=centroid[2], pch = 16, cex = 2, col =&quot;blue&quot;) # centroid On peut observer que, comme je l‚Äôai mentionn√© plus haut, les vecteurs propres indiquent l‚Äôorientation de la covariance, et les valeurs propres indique la longueur associ√©e √† cette direction. 9.4.1.1.2 Biplot Imaginez un nuage de points en 3D, axes y compris. Vous tournez votre nuage de points pour trouver la perspective en 2D qui fera en sorte que vos donn√©es soient les plus dispers√©es possibles. Avec une lampe de poche, vous illuminez votre nuage de points dans l‚Äôaxe de cette perspective: vous venez d‚Äôeffectuer une analyse en composantes principales, et l‚Äôombre des points et des axes sur le mur formera votre biplot. Pour cr√©er un biplot, on juxtapose les descripteurs (variables) en tant que vecteurs propres, repr√©sent√©s par des fl√®ches, et les objets (observations) en tant que scores, repr√©sent√©s par des points. Les r√©sultats d‚Äôune ordination peuvent √™tre pr√©sent√©s selon deux types de biplots (Legendre et Legendre, 2012). Biplot de corr√©lation permettant de visualiser les corr√©lations entre des variables m√©t√©orologiques. Source: Parent, 2017 Deux types de projection sont courramment utilis√©s. Biplot de distance. Ce type de projection permet de visualiser la position des objets entre eux et par rapport aux descripteurs et d‚Äôappr√©cier la contribution des descripteurs pour cr√©er les composantes principales. Pour cr√©er un biplot de distance, on projette directement les vecteurs propres (\\(U\\)) en guise de descripteurs. Pour ce qui est des objets, on utilise les scores de l‚ÄôACP (\\(F\\)). De cette mani√®re, les distances euclidiennes entre les scores sont des approximations des distances euclidiennes dans l‚Äôespace multidimentionnel, la projection d‚Äôun objet sur un descripteur perpendiculairement √† ce dernier est une approximation de la position de l‚Äôobjet sur le descripteur et la projection d‚Äôun descripteur sur un axe principal est proportionnelle √† sa contribution pour g√©n√©rer l‚Äôaxe. Biplot de corr√©lation. Cette projection permet d‚Äôappr√©cier les corr√©lations entre les descripteurs. Pour ce faire, les objets et les valeurs propres doivent √™tre transform√©s. Pour g√©n√©rer les descripteurs, les vecteurs propres (\\(U\\)) doivent √™tre multipli√©s par la matrice diagonalis√©e de la racine carr√©e des valeurs propres (\\(\\Lambda\\)), c‚Äôest-√†-dire \\(U \\Lambda ^{\\frac{1}{2}}\\). En ce qui a trait aux objets, on multiplie les scores par (\\(F\\)) par la racine carr√©e n√©gative des valeurs propres diagonalis√©es, c‚Äôest-√†-dire \\(F \\Lambda ^{- \\frac{1}{2}}\\). De cette mani√®re, tout comme c‚Äôest le cas pour le biplot de distance, la projection d‚Äôun objet sur un descripteur perpendiculairement √† ce dernier est une approximation de la position de l‚Äôobjet sur le descripteur, la projection d‚Äôun descripteur sur un axe principal est proportionnelle √† son √©cart-type et les angles entre les descripteurs sont proportionnelles √† leur corr√©lation (et non pas leur proximit√©). En d‚Äôautres mots, le bilot de distances devrait √™tre utilis√© pour appr√©cier la distance entre les objets et le biplot de corr√©lation devrait √™tre utilis√© pour appr√©cier les corr√©lations entre les descripteurs. Mais dans tous les cas, le type de biplot utilis√© doit √™tre indiqu√©. Le triplot est une forme apparent√©e au biplot, auquel on ajoute des variables pr√©dictives. Le triplot est utile pour repr√©senter les r√©sultats des ordinations contraignantes comme les analyses de redondance et les analyse de correspondance canoniques. 9.4.1.1.3 Application Bien que l‚ÄôACP puisse √™tre effectu√©e gr√¢ce √† des modules de base de R, nous utiliserons le module vegan. Le tableau varechem comprend des donn√©es issues d‚Äôanalyse de sols identifi√©s par leur composition chimique, leur pH, leur profondeur totale et la profondeur de l‚Äôhumus publi√©es dans V√§re et al. (1995) et export√©es du module vegan. library(&quot;vegan&quot;) data(&quot;varechem&quot;) varechem %&gt;% sample_n(5) ## N P K Ca Mg S Al Fe Mn Zn Mo Baresoil Humdepth pH ## 1 26.6 36.7 171.4 738.6 94.9 33.8 20.7 2.5 77.6 7.4 0.3 23.0 2.8 2.8 ## 2 26.2 61.9 202.2 741.2 86.3 48.6 124.3 23.6 94.5 10.2 0.6 56.9 2.5 2.9 ## 3 22.8 50.6 151.7 648.0 64.8 30.2 12.1 2.3 122.9 8.1 0.2 23.7 2.6 2.9 ## 4 18.0 64.9 224.5 517.6 59.7 52.9 435.1 101.2 38.0 9.5 1.1 21.3 1.8 2.9 ## 5 19.1 26.4 61.1 259.1 37.0 21.4 155.1 81.4 20.6 4.0 0.6 5.8 1.9 3.0 Comme nous l‚Äôavons vu pr√©cdemment, les donn√©es de concentration sont de type compositionnelles. Les donn√©es compositionnelles du tableau varechem m√©riteraient d‚Äô√™tre transform√©es (Aitchison et Greenacre, 2002). Utilisons les log-ratios centr√©s (clr). library(&quot;compositions&quot;) varecomp &lt;- varechem %&gt;% select(-Baresoil, -Humdepth, -pH) %&gt;% mutate(Fv = apply(., 1, function(x) 1e6 - sum(x))) vareclr &lt;- varecomp %&gt;% acomp(.) %&gt;% clr(.) %&gt;% as_tibble() %&gt;% bind_cols(varechem %&gt;% select(Baresoil, Humdepth, pH)) ## Warning: Calling `as_tibble()` on a vector is discouraged, because the behavior is likely to change in the future. Use `tibble::enframe(name = NULL)` instead. ## This warning is displayed once per session. vareclr %&gt;% sample_n(5) ## # A tibble: 5 x 15 ## N P K Ca Mg S Al Fe Mn Zn Mo Fv Baresoil Humdepth pH ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -1.34 -0.760 0.673 2.07 0.549 -0.762 0.181 -1.60 -0.696 -2.26 -5.39 9.34 40.5 3.8 2.7 ## 2 -1.37 -0.655 0.697 2.01 -0.0269 -0.705 0.547 -1.89 -0.869 -2.35 -5.06 9.67 11.2 2.2 2.9 ## 3 -0.773 -0.988 0.175 1.05 -0.174 -0.960 1.50 0.628 -1.88 -3.09 -5.11 9.62 18.6 1.7 3.1 ## 4 -0.871 -0.623 0.872 1.92 -0.244 -0.757 0.250 -1.77 -0.862 -2.41 -5.26 9.76 29.8 2 2.8 ## 5 -1.68 -0.469 0.833 2.57 0.347 -0.910 1.04 0.0773 -1.27 -2.39 -7.49 9.32 7.6 1.1 3.6 Effectuons l‚ÄôACP. Pour cet exemple, nous standardiserons les donn√©es √©tant donn√©es que les colonnes Baresoil, Humedepth et pH ne sont pas √† la m√™me √©chelle que les colonnes des clr. vareclr_sc &lt;- scale(vareclr) vare_pca &lt;- rda(vareclr_sc) # ou bien rda(vareclr, scale = TRUE, mais la mise √† l&#39;√©chelle pr√©alable est plus explicite) L‚Äôobjet vareclr_pca contient l‚Äôinformation n√©cessaire pour mener notre ACP. summary(vare_pca, scaling = 2) # scaling = 2 pour obtenir les infos pour les biplots de corr√©lation ## ## Call: ## rda(X = vareclr_sc) ## ## Partitioning of variance: ## Inertia Proportion ## Total 15 1 ## Unconstrained 15 1 ## ## Eigenvalues, and their contribution to the variance ## ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 PC10 PC11 PC12 PC13 ## Eigenvalue 7.1523 2.4763 2.1122 0.93015 0.57977 0.48786 0.36646 0.29432 0.19686 0.15434 0.107357 0.095635 0.042245 ## Proportion Explained 0.4768 0.1651 0.1408 0.06201 0.03865 0.03252 0.02443 0.01962 0.01312 0.01029 0.007157 0.006376 0.002816 ## Cumulative Proportion 0.4768 0.6419 0.7827 0.84473 0.88338 0.91590 0.94034 0.95996 0.97308 0.98337 0.990527 0.996902 0.999719 ## PC14 ## Eigenvalue 0.0042200 ## Proportion Explained 0.0002813 ## Cumulative Proportion 1.0000000 ## ## Scaling 2 for species and site scores ## * Species are scaled proportional to eigenvalues ## * Sites are unscaled: weighted dispersion equal on all dimensions ## * General scaling constant of scores: 4.309777 ## ## ## Species scores ## ## PC1 PC2 PC3 PC4 PC5 PC6 ## N 0.1437 0.7606 -0.6792 0.19837 0.1128526 -0.050149 ## P 0.8670 -0.3214 -0.2950 -0.22940 0.1437960 -0.042884 ## K 0.9122 -0.3857 0.2357 0.03469 0.2737020 0.075717 ## Ca 0.9649 -0.3362 -0.2147 0.17757 -0.2188717 0.008051 ## Mg 0.8263 -0.2723 0.1035 0.52135 -0.1495399 -0.342214 ## S 0.8825 -0.3169 0.3539 -0.21216 0.1176279 -0.191386 ## Al -1.0105 -0.2442 0.2146 0.02674 -0.1005560 -0.043569 ## Fe -1.0338 -0.2464 0.1492 0.13162 0.1512218 0.081571 ## Mn 0.9556 0.1041 -0.1256 -0.21300 0.2565831 0.275275 ## Zn 0.7763 -0.1031 -0.3123 -0.36493 -0.5665691 0.153089 ## Mo -0.2152 0.8717 0.4065 -0.33643 -0.2134335 -0.167725 ## Fv 0.2360 0.5776 -0.8112 0.12736 0.1280097 -0.109737 ## Baresoil 0.5147 0.4210 0.4472 0.54980 -0.1438570 0.463148 ## Humdepth 0.7455 0.4379 0.5194 0.16493 0.0004757 -0.273056 ## pH -0.5754 -0.5864 -0.5957 0.23408 -0.1517661 -0.056641 ## ## ## Site scores (weighted sums of species scores) ## ## PC1 PC2 PC3 PC4 PC5 PC6 ## sit1 0.16862 0.423777 0.46731 0.91175 1.10380 1.06421 ## sit2 -0.09705 -0.097482 0.61143 -0.29049 1.14916 0.40622 ## sit3 0.02831 -0.795737 0.74176 -0.19097 -2.43337 -0.81762 ## sit4 1.39081 -0.354376 -0.19377 -0.45160 0.46020 -0.31446 ## sit5 1.30346 0.357866 0.29887 0.76856 0.20913 -0.64145 ## sit6 0.43636 0.495037 1.21722 1.18128 -0.98242 -0.74474 ## sit7 1.07306 0.467575 -0.32245 0.03717 0.13956 -0.64972 ## sit8 0.02545 0.659714 -0.28861 -0.01424 0.47105 0.45173 ## sit9 1.42005 0.007356 -0.29000 -0.78474 0.97592 -0.80263 ## sit10 -0.50638 -0.220909 1.52981 0.26289 0.42135 0.94054 ## sit11 0.45392 0.649297 0.44573 -0.26620 -0.74522 -0.53228 ## sit12 0.18623 0.259640 0.89112 0.21096 -0.51393 2.24361 ## sit13 1.26264 0.225744 -0.96668 -0.69334 0.61990 0.43312 ## sit14 -1.48685 0.739545 -0.20926 1.09256 0.61856 -0.87999 ## sit15 -0.50622 1.108685 -2.61287 -1.00433 -1.35383 1.21964 ## sit16 -1.28653 0.898663 -0.38778 -0.47556 -0.02449 -0.29419 ## sit17 -1.72773 0.476962 -0.48878 0.71156 1.06398 -1.33473 ## sit18 -0.82844 -0.296515 1.20315 -1.49821 -0.18330 1.05231 ## sit19 -1.00247 -0.609253 0.25185 -0.85420 0.71031 0.14854 ## sit20 -0.43405 -0.338912 0.55348 -1.35776 -0.81986 -1.02468 ## sit21 -0.05083 0.122645 -0.04611 -0.56047 -0.26151 -0.98053 ## sit22 0.17891 -2.315489 -0.69084 -0.19547 0.80628 0.04291 ## sit23 -0.46443 -2.592018 -1.21615 1.56359 -0.62334 0.28748 ## sit24 0.46316 0.728185 -0.49843 1.89726 -0.80791 0.72671 La deuxi√®me ligne de Importance of components, Proportion Explained, indique la proportion de la variance totale capt√©e successivement par les axes principaux. Le premier axe principal comporte 47.68% de la variance. Le deuxi√®me axe principal ajoutant une proportion de 16,51%, une repr√©sentation en deux axes principaux pr√©sentent 64.19 % de la variance. prop_expl &lt;- vare_pca$CA$eig / sum(vare_pca$CA$eig) prop_expl ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 ## 0.4768180610 0.1650859388 0.1408156459 0.0620101490 0.0386511040 0.0325238535 0.0244303815 0.0196215021 0.0131238464 ## PC10 PC11 PC12 PC13 PC14 ## 0.0102890284 0.0071571089 0.0063756951 0.0028163495 0.0002813359 La d√©cision du nombre d‚Äôaxes principaux √† retenir est arbitraire. Elle peut d√©pendre d‚Äôun nombre maximal de param√®tre √† retenir pour √©viter de surdimensionner un mod√®le (curse of dimensionality, section 11) ou d‚Äôun seuil de pourcentage de variance minimal √† retenir, par exemple 75%. Ou bien, vous retiendrez deux composantes principales si vous d√©sirez pr√©senter un seul biplot. L‚Äôapproche de Kaiser-Guttmann (Borcard et al., 2011) consiste √† s√©lectionner les composantes principales dont la valeur propre est sup√©rieure √† leur moyenne. plot(x = 1:length(vare_pca$CA$eig), y = vare_pca$CA$eig, type = &quot;b&quot;, xlab = &quot;Rang de la valeur propre&quot;, ylab = &quot;Valeur propre&quot;) abline(h = mean(vare_pca$CA$eig), col = &quot;red&quot;, lty = 2) L‚Äôapproche du broken stick consiste √† couper un b√¢ton d‚Äôune longueur de 1 en n tranches. La premi√®re tranche est de longueur \\(\\frac{1}{n}\\). La tranche suivante est d‚Äôune longueur de la tranche pr√©c√©dente √† laquelle on aditionne \\(\\frac{1}{longueur~restante}\\). Puis on place les longueurs en ordre d√©croissant. On retient les composantes principales dont les valeurs propres cumul√©es sont plus grandes que le broken stick. broken_stick &lt;- function(x) { bsm &lt;- vector(&quot;numeric&quot;, length = x) bsm[1] &lt;- 1/x for (i in 2:x) { bsm[i] &lt;- bsm[i-1] + 1/(x+1-i) } bsm &lt;- rev(bsm/x) return(bsm) } Le graphique du broken stick: plot(x = 1:length(vare_pca$CA$eig), y = prop_expl, type = &quot;b&quot;, xlab = &quot;Rang de la valeur propre&quot;, ylab = &quot;Valeur propre&quot;) lines(x = 1:length(vare_pca$CA$eig), y = broken_stick(length(vare_pca$CA$eig)), col = &quot;red&quot;, lty = 2) Les approches Kaiser-Guttmann et broken stick sugg√®rent que les trois premi√®res composantes sont suffisantes pour d√©crire la dispersion des donn√©es. Examinons les loadings (vecteurs propres) plus en particulier. Dans le langage du module vegan, les vecteurs propres sont les esp√®ces (species) et les scores sont les sites. vare_eigenvec &lt;- vegan::scores(vare_pca, scaling = 2, display = &quot;species&quot;, choices = 1:(ncol(vareclr)-1)) vare_eigenvec ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 ## N 0.1437343 0.7606006 -0.6792046 0.1983670 0.1128526122 -0.050148980 -0.09111164 -0.06122008 0.315645453 ## P 0.8669892 -0.3213683 -0.2949864 -0.2294036 0.1437959857 -0.042883754 0.26894062 0.34111276 0.021124287 ## K 0.9122089 -0.3857245 0.2356904 0.0346904 0.2737019601 0.075717162 -0.21662612 -0.01641260 0.143099440 ## Ca 0.9648855 -0.3361651 -0.2147486 0.1775746 -0.2188716732 0.008050762 0.03630015 0.04775616 -0.073609828 ## Mg 0.8263327 -0.2723055 0.1035276 0.5213484 -0.1495399242 -0.342213793 0.04617838 -0.12098602 -0.051599273 ## S 0.8824519 -0.3169039 0.3538854 -0.2121562 0.1176278503 -0.191386377 -0.26825994 0.15822845 0.038378858 ## Al -1.0105173 -0.2441785 0.2145614 0.0267422 -0.1005559874 -0.043569364 -0.22737412 0.10598673 0.040586196 ## Fe -1.0337676 -0.2463987 0.1491865 0.1316173 0.1512218115 0.081571443 0.10553041 -0.09254655 -0.079426433 ## Mn 0.9555632 0.1041030 -0.1256178 -0.2130047 0.2565830557 0.275275174 0.20224538 -0.19347804 -0.038859808 ## Zn 0.7763480 -0.1030878 -0.3122919 -0.3649341 -0.5665691228 0.153089144 -0.12332232 -0.14862229 0.024026151 ## Mo -0.2152399 0.8717229 0.4064967 -0.3364279 -0.2134335302 -0.167725160 0.13788948 0.17165900 0.032981311 ## Fv 0.2360040 0.5775863 -0.8111953 0.1273582 0.1280096553 -0.109737235 -0.20911147 0.11289753 -0.281443886 ## Baresoil 0.5147445 0.4209983 0.4472351 0.5497950 -0.1438569673 0.463148072 -0.02103009 0.23028292 0.004554036 ## Humdepth 0.7455213 0.4379436 0.5193895 0.1649306 0.0004756685 -0.273056212 0.17061078 -0.11310394 0.027515405 ## pH -0.5753858 -0.5863743 -0.5957495 0.2340826 -0.1517660977 -0.056640816 0.19890884 0.12152266 0.150118818 ## PC10 PC11 PC12 PC13 PC14 ## N 0.08090232 -0.019251478 0.045420621 0.05020956 0.002340519 ## P 0.08756299 -0.045741546 0.145128883 -0.03337551 -0.010109130 ## K -0.08737113 0.183005607 0.002260341 -0.10566808 0.001169065 ## Ca -0.10601799 0.161460554 0.041210064 0.14341793 0.007419161 ## Mg 0.18373857 -0.009862571 -0.063493608 -0.03782662 -0.023575986 ## S 0.05100717 -0.138785063 -0.117144869 0.06075094 0.025874035 ## Al -0.14473132 -0.089462074 0.058212507 0.01983102 -0.037901576 ## Fe 0.09908706 -0.006376211 0.049837173 -0.01169516 0.036048221 ## Mn -0.07637994 -0.083300112 -0.133353213 0.02679781 -0.021373612 ## Zn 0.02643462 -0.064973307 0.051057277 -0.06538348 0.010896560 ## Mo 0.01419924 0.128814989 -0.114803631 -0.01989539 -0.001335923 ## Fv -0.08391004 -0.012456867 -0.020157331 -0.05448619 0.005707928 ## Baresoil 0.02604286 -0.061147847 -0.019696758 -0.01640490 0.003823725 ## Humdepth -0.23068827 -0.102189307 0.109293684 -0.02485030 0.016559206 ## pH -0.15240317 -0.037691048 -0.153813168 -0.04523353 0.014193061 ## attr(,&quot;const&quot;) ## [1] 4.309777 L‚Äôordre d‚Äôimportance des vecteurs propres est √©tabli en ordre croissant des √©l√©ment des vecteurs propres associ√©es. Un vecteur propre est une combinaison lin√©aire des variables. Par exemple, le premier vecteur propre pointe surtout dans la direction du Fe (-1.497) et de l‚ÄôAl (-1.463). Le deuxi√®me pointe surtout vers le Mo (2.145). Les vecteurs (loadings) d‚Äôun biplot de distance pr√©sentant les des deux premi√®res composantes principales prendront les coordonn√©es des deux premi√®res colonnes. Le vecteur Al aura la coordonn√©e [-1.463 ; -0.601], le vecteur de Fe sera plac√© √† [-1.497 ; -0.606] et le vecteur Mo √† [-0.312 ; 2.145]. Il existe diff√©rentes fonctions d‚Äôaffichage des biplots. Notez que leur longueur peut √™tre magnifi√©e pour am√©liorer la visualisation. Lan√ßons la fonction biplot pour cr√©er un biplot de distance et un autre de corr√©lation. par(mfrow = c(1, 2)) biplot(vare_pca, scaling = 1, main = &quot;Biplot de distance&quot;) biplot(vare_pca, scaling = 2, main = &quot;Biplot de corr√©lation&quot;) Le biplot de distance permet de d√©gager les variables qui expliquent davantage la variabilit√© dans notre tableau: les clr du Fe et de l‚ÄôAl forment en grande partie le premier axe principal, alors que le clr du Mo forme en grande partie le second axe. Le biplot de corr√©lation montre que les clr du Fe et du Al sont corr√©l√©s dans le m√™me sens, mais das le sens contraire du clr du Mn. L‚Äôinformation sur la teneur en Fe et celle de l‚ÄôAl est en grande partie redondante. Toutefois, le clr du Mo est presque ind√©pendant du clr du Fe, ceux-ci √©tant √† angle presque droit (~90¬∞). Ces relations peuvent √™tre explor√©es directement. par(mfrow = c(1, 2)) plot(vareclr$Al, vareclr$Fe) plot(vareclr$Mo, vareclr$Fe) Nous avons mentionn√© que l‚ÄôACP est une rotation. Prenons un second exemple pour bien en saisir les tenants et aboutissants. Le tableau de donn√©es que nous chargerons provient d‚Äôun infographie d‚Äôun dauphin, intitull√©e Bottlenose Dolphin, con√ßu par l‚Äôartiste Tarnyloo. Les points correspondent √† la surface d‚Äôun dauphin. J‚Äôai ajout√© une colonne anatomy, qui indique √† quelle partie anatomique le point appartient. dolphin &lt;- read_csv(&quot;data/07_dolphin.csv&quot;) ## Parsed with column specification: ## cols( ## x = col_double(), ## y = col_double(), ## z = col_double(), ## anatomy = col_character() ## ) dolphin %&gt;% sample_n(5) ## # A tibble: 5 x 4 ## x y z anatomy ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 -0.440 1.06 1.24 Caudal fin ## 2 0.0360 -0.494 -0.198 Head ## 3 -0.177 -0.324 0.00479 Head ## 4 0.0921 -0.436 -0.0795 Head ## 5 -0.333 1.07 1.23 Caudal fin Voici en vue isom√©trique ce en quoi consiste ce nuage de points. library(&quot;scatterplot3d&quot;) scatterplot3d(x = dolphin$x, y = dolphin$y, z = dolphin$z, pch = 16, cex.symbols = 0.2) Effectuons l‚ÄôACP sur le dauphin. dolph_pca &lt;- rda(dolphin %&gt;% select(x, y, z), scale = FALSE) biplot(dolph_pca, scaling = 2) On n‚Äôy voit pas grand chose, mais si l‚Äôon extrait les scores et que l‚Äôon raccourcit les vecteurs: dolph_scores &lt;- vegan::scores(dolph_pca, display = &quot;sites&quot;) dolph_loads &lt;- vegan::scores(dolph_pca, display = &quot;species&quot;) dolph_loads ## PC1 PC2 ## x -0.02990131 0.01608095 ## y -7.13731672 -1.43221776 ## z -4.56612084 2.23859843 ## attr(,&quot;const&quot;) ## [1] 9.089026 plot(dolph_scores, pch = 16, cex = 0.24, asp = 1, col = factor(dolphin$anatomy)) segments(x0 = rep(0, 3), y0 = rep(0, 3), x = dolph_loads[, 1]/50, y = dolph_loads[, 2]/50, col = &quot;chocolate&quot;, lwd = 4) La meilleure repr√©sentation du dauphin en 2D, selon la variance, est son profil - en effet, il est plus long et haut que large. Note. Une ACP effectue seulement une rotation des points. Les distances euclidiennes entre les points sont maintenues. Note. L‚ÄôACP a √©t√© con√ßue pour projetter en un nombre moindre de dimensions des observations dont les distributions sont multinormales (ce n‚Äôest √©videmment pas le cas du dauphin). Note. Les axes principaux d‚Äôune ACP sont des variables al√©atoires. Elles peuvent √™tre assujetties √† des tests ststistiques, des mod√®les, du partitionnement de donn√©es, etc. Excercice. Effectuez maintenant une ACP avec les donn√©es d‚Äôiris. 9.4.1.2 Analyse de correspondance (AC) L‚Äôanalyse de correspondance (AC) est particuli√®rement appropri√©e pour traiter des donn√©es d‚Äôabondance et d‚Äôoccurence. Tout comme l‚Äôanalyse en composantes principales, les donn√©es apport√©s vers une AC doivent √™tre dimensionnellement homog√®nes, c‚Äôest-√†-dire que chaque variable doit √™tre de m√™me m√©trique: pour des donn√©es d‚Äôabondance, cela signifie que les d√©comptes r√©f√®rent tous au m√™me concept: individus, colonies, surfaces occup√©es, etc. Alors que la distance euclidienne est pr√©serv√©e avec l‚ÄôACP, l‚ÄôAC pr√©serve la distance du \\(\\chi^2\\), qui est insensible aux double-z√©ros. L‚ÄôAC produit \\(min(n,p)-1\\) axes principaux orthogonaux qui captent non pas le maximum de variance, mais la proportion de mesures aux carr√© par rapport √† la somme des carr√©s de la matrice. Le biplot obtenu peut √™tre pr√©sent√© sous forme de biplot de site (scaling 1), o√π la distance du \\(\\chi^2\\) est pr√©serv√©e entre les sites ou biplot d‚Äôesp√®ces (scaling 2), ou la distance du \\(\\chi^2\\) est pr√©serv√©e entre les esp√®ces. L‚ÄôAC h√©rite du coup une propri√©t√© importate de la distance du \\(\\chi^2\\), qui accorde davantage de distance entre un compte de 0 et de 1 qu‚Äôentre 1 et 2, et davantage entre 1 et 2 qu‚Äôentre 2 et 3. Par exemple, sur ces trois sites, on a compt√© un individu A de moins que d‚Äôindividu B. abundance_0123 = tibble(Site = c(&quot;Site 1&quot;, &quot;Site 2&quot;, &quot;Site 3&quot;), A = c(0, 1, 9), B = c(1, 2, 10)) abundance_0123 ## # A tibble: 3 x 3 ## Site A B ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Site 1 0 1 ## 2 Site 2 1 2 ## 3 Site 3 9 10 Pourtant, la distance du \\(\\chi^2\\) est plus √©lev√©e entre le site 1 et le site 2 qu‚Äôentre le site 2 et le site 3. dist(decostand(abundance_0123 %&gt;% select(-Site), method=&quot;chi.square&quot;)) ## 1 2 ## 2 0.6724111 ## 3 0.9555316 0.2831205 La distance du \\(\\chi^2\\) donne davantage d‚Äôimportance aux esp√®ces rares, ce dont une analyse doit tenir compte. Il pourrait √™tre envisageable de retirer d‚Äôun tableau des esp√®ces rare, ou bien pr√©transformer des donn√©es d‚Äôabondance par une transformation de chord ou de Hellinger (tel que discut√© au chapitre 6), puis proc√©der √† une ACP sur ces donn√©es (Legendre et Gallagher, 2001). 9.4.1.2.1 Application Le tableau varespec comprend des donn√©es de surface de couverture de 44 esp√®ces de plantes en lien avec les donn√©es environnementales du tableau varechem. Ces donn√©es ont √©t√© publi√©es dans V√§re et al. (1995) et export√©es du module vegan. data(&quot;varespec&quot;) varespec %&gt;%sample_n(5) ## Callvulg Empenigr Rhodtome Vaccmyrt Vaccviti Pinusylv Descflex Betupube Vacculig Diphcomp Dicrsp Dicrfusc Dicrpoly Hylosple ## 1 0.03 3.65 0.00 0.00 4.43 0.00 0.00 0 1.65 0.50 0.0 0.55 0.00 0.00 ## 2 0.00 1.63 0.35 18.27 7.13 0.05 0.40 0 0.20 0.00 0.3 0.52 0.20 9.97 ## 3 3.75 5.65 0.00 0.08 5.30 0.10 0.00 0 0.00 0.00 0.0 11.32 0.00 0.00 ## 4 3.40 0.63 0.00 0.00 1.98 0.05 0.05 0 0.03 0.00 0.0 0.20 0.00 0.00 ## 5 0.00 5.30 0.00 0.00 8.20 0.00 0.05 0 8.10 0.28 0.0 0.45 0.03 0.00 ## Pleuschr Polypili Polyjuni Polycomm Pohlnuta Ptilcili Barbhatc Cladarbu Cladrang Cladstel Cladunci Cladcocc Cladcorn ## 1 0.05 0 0.00 0.00 0.03 0.03 0 8.80 29.50 55.60 0.25 0.08 0.25 ## 2 70.03 0 0.08 0.00 0.07 0.03 0 0.17 0.87 0.00 0.05 0.02 0.03 ## 3 7.75 0 0.30 0.02 0.07 0.00 0 17.45 1.32 0.12 23.68 0.22 0.50 ## 4 1.53 0 0.10 0.00 0.05 0.00 0 15.73 20.03 28.20 0.73 0.10 0.15 ## 5 0.10 0 0.25 0.00 0.03 0.00 0 35.00 42.50 0.28 0.35 0.08 0.20 ## Cladgrac Cladfimb Cladcris Cladchlo Cladbotr Cladamau Cladsp Cetreric Cetrisla Flavniva Nepharct Stersp Peltapht Icmaeric ## 1 0.25 0.15 0.10 0.03 0.00 0.03 0.00 0.05 0.00 0.15 0.15 0.28 0 0.00 ## 2 0.07 0.10 0.02 0.00 0.02 0.00 0.00 0.00 0.02 0.00 0.00 0.02 0 0.00 ## 3 0.15 0.23 0.97 0.00 0.00 0.00 0.00 0.68 0.02 0.00 0.00 0.33 0 0.02 ## 4 0.13 0.10 0.15 0.00 0.00 0.00 0.05 0.28 0.05 10.03 0.00 0.95 0 0.00 ## 5 0.25 0.18 0.13 0.08 0.00 0.00 0.00 0.05 0.00 0.23 0.20 0.93 0 0.03 ## Cladcerv Claddefo Cladphyl ## 1 0.00 0.08 0.00 ## 2 0.00 0.08 0.00 ## 3 0.00 1.57 0.05 ## 4 0.05 0.08 0.00 ## 5 0.00 0.10 0.00 Pour effectuer l‚ÄôAC, nous utiliserons, comme pour l‚ÄôACP, le module vegan mais cette fois-ci avec la fonction cca. L‚ÄôAC en scaling 1 est effectu√©e sur le tableau des abondances avec les esp√®ces comme colonnes et les sites comme lignes. Les matrices d‚Äôabondance transpos√©es indique les sites o√π chque esp√®ce ont √©t√© d√©nombr√©es: pour une analyse en scaling 2, on effectue une analyse de correspondance sur la matrice d‚Äôabondance (ou d‚Äôoccurence) transpos√©e. Pour chacune des AC, je filtre pour m‚Äôassurer que toutes les lignes contiennent au moins une observation. Ce n‚Äôest pas n√©cessaire dans notre cas, mais je le laisse pour l‚Äôexemple. vare_cca &lt;- cca(varespec %&gt;% filter(rowSums(.) &gt; 0)) summary(vare_cca, scaling = 1) ## ## Call: ## cca(X = varespec %&gt;% filter(rowSums(.) &gt; 0)) ## ## Partitioning of scaled Chi-square: ## Inertia Proportion ## Total 2.083 1 ## Unconstrained 2.083 1 ## ## Eigenvalues, and their contribution to the scaled Chi-square ## ## Importance of components: ## CA1 CA2 CA3 CA4 CA5 CA6 CA7 CA8 CA9 CA10 CA11 CA12 CA13 ## Eigenvalue 0.5249 0.3568 0.2344 0.19546 0.17762 0.12156 0.11549 0.08894 0.07318 0.05752 0.04434 0.02546 0.01710 ## Proportion Explained 0.2520 0.1713 0.1125 0.09383 0.08526 0.05835 0.05544 0.04269 0.03513 0.02761 0.02129 0.01222 0.00821 ## Cumulative Proportion 0.2520 0.4233 0.5358 0.62962 0.71489 0.77324 0.82868 0.87137 0.90650 0.93411 0.95539 0.96762 0.97583 ## CA14 CA15 CA16 CA17 CA18 CA19 CA20 CA21 CA22 CA23 ## Eigenvalue 0.014896 0.010160 0.007830 0.006032 0.004008 0.002865 0.0019275 0.0018074 0.0005864 0.0002434 ## Proportion Explained 0.007151 0.004877 0.003759 0.002896 0.001924 0.001375 0.0009253 0.0008676 0.0002815 0.0001168 ## Cumulative Proportion 0.982978 0.987855 0.991614 0.994510 0.996434 0.997809 0.9987341 0.9996017 0.9998832 1.0000000 ## ## Scaling 1 for species and site scores ## * Sites are scaled proportional to eigenvalues ## * Species are unscaled: weighted dispersion equal on all dimensions ## ## ## Species scores ## ## CA1 CA2 CA3 CA4 CA5 CA6 ## Callvulg 0.0303167 -1.597460 0.11455 -2.894569 0.1376073 2.291129 ## Empenigr 0.0751030 0.379305 0.39303 0.023675 0.8568729 -0.400964 ## Rhodtome 1.1052309 1.499299 3.04284 0.120106 3.2324306 -0.283510 ## Vaccmyrt 1.4614812 1.622935 2.72375 0.231688 0.4604556 0.712538 ## Vaccviti 0.1468014 0.313436 0.14696 0.243505 0.6868371 -0.147815 ## Pinusylv -0.4820096 0.588517 -0.36020 -0.127094 0.4064754 0.386604 ## Descflex 1.5348239 1.218806 1.87562 -0.001340 -1.3136979 -0.070731 ## Betupube 0.6694503 1.951826 3.84017 1.389423 7.5959115 -0.244478 ## Vacculig -0.0830789 -1.629259 1.05063 0.802648 -0.3058811 -1.625341 ## Diphcomp -0.5446464 -1.037570 0.52282 0.940275 0.3682126 -1.082929 ## Dicrsp 1.8120408 0.360290 -4.92082 3.088562 1.3867372 0.157815 ## Dicrfusc 1.2704743 -0.562978 -0.39718 -2.929542 0.3848272 -2.408710 ## Dicrpoly 0.7248118 1.409347 0.80341 1.915549 4.5674148 1.295447 ## Hylosple 2.0062408 1.743883 2.27549 0.928884 -3.7648428 2.254851 ## Pleuschr 1.3102086 0.583036 -0.01004 0.137298 -1.1216144 0.200422 ## Polypili -0.3805097 -1.243904 0.54593 1.477188 -0.7276341 -0.387641 ## Polyjuni 1.0133795 0.099043 -2.24697 1.510641 0.7729714 -3.062378 ## Polycomm 0.8468241 1.321773 1.13585 1.140723 2.6836594 -0.605038 ## Pohlnuta -0.0136453 0.589290 -0.35542 0.135481 0.9369707 0.397246 ## Ptilcili 0.4223631 1.598584 3.43474 1.400065 6.3209491 0.198935 ## Barbhatc 0.5018348 2.119334 4.57303 1.693188 8.1101807 0.645995 ## Cladarbu -0.1531729 -1.483884 0.20024 0.193680 0.0734141 0.358926 ## Cladrang -0.5502561 -1.084008 0.40552 0.724060 -0.3357992 -0.335924 ## Cladstel -1.4373146 1.077753 -0.44397 -0.375926 -0.2421525 0.004212 ## Cladunci 0.8151727 -1.006186 -1.82587 -1.389523 1.6046713 3.675908 ## Cladcocc -0.2133215 -0.584429 -0.21434 -0.567886 -0.0003788 -0.145303 ## Cladcorn 0.2631227 -0.177858 -0.44464 0.272422 0.3992282 -0.306738 ## Cladgrac 0.1956947 -0.311167 -0.23894 0.379013 0.4933026 0.037581 ## Cladfimb 0.0009213 -0.161418 0.18463 -0.435908 0.4831233 -0.143751 ## Cladcris 0.3373031 -0.470369 -0.05093 -0.823855 0.7182250 0.636140 ## Cladchlo -0.6200021 1.207278 0.21889 0.426447 1.9506082 0.120722 ## Cladbotr 0.5647242 1.047333 2.65330 0.907734 4.4946805 1.201655 ## Cladamau -0.6598144 -1.512880 0.83251 1.577699 -0.0407227 -1.419139 ## Cladsp -0.8209003 0.476164 -0.49752 -0.998241 -0.2393208 0.390785 ## Cetreric 0.2458192 -0.689228 -1.68427 -0.131681 0.7439412 2.374535 ## Cetrisla -0.3465221 1.362693 0.85897 0.396752 2.7526968 0.396591 ## Flavniva -1.4391907 -0.833589 -0.12919 0.007071 -1.4841375 2.956977 ## Nepharct 1.6813309 0.199484 -4.33509 2.229917 0.9561223 -5.472858 ## Stersp -0.5172793 -2.280900 0.99775 2.377013 -0.8892757 -1.441228 ## Peltapht 0.4035858 -0.043265 0.04538 0.711040 0.1824679 -0.841227 ## Icmaeric 0.0378754 -2.419595 0.72135 0.361302 -0.3736424 -2.092136 ## Cladcerv -0.9232858 -0.005233 -1.22058 0.305290 -0.8142627 0.414135 ## Claddefo 0.5190399 -0.496632 -0.15271 -0.695927 0.9042143 0.909191 ## Cladphyl -1.2836161 1.155872 -0.79912 -0.741170 -0.1608002 0.490526 ## ## ## Site scores (weighted averages of species scores) ## ## CA1 CA2 CA3 CA4 CA5 CA6 ## sit1 -0.108122 -0.53705 0.229574 0.24412 0.1405624 -0.14253 ## sit2 0.697118 -0.14441 -0.031788 -0.21743 -0.2738522 -0.08146 ## sit3 0.987603 0.15042 -1.348447 0.80472 0.3095168 0.46773 ## sit4 0.851765 0.49901 0.443559 0.12277 -0.4814871 0.07589 ## sit5 0.359881 -0.05608 0.145813 0.15087 0.2405263 -0.17770 ## sit6 0.003545 0.37017 0.027760 0.06168 -0.1158930 -0.03413 ## sit7 0.860732 -0.11504 0.110869 -1.02169 0.0772348 -0.60530 ## sit8 0.636936 -0.33250 0.001120 -0.79797 0.0130769 -0.54049 ## sit9 1.279352 0.81557 0.670053 0.23137 -0.8929976 0.41783 ## sit10 -0.195009 -0.80564 0.117686 -0.58286 -0.0007212 0.53071 ## sit11 0.528532 -0.70420 -0.517771 -0.86836 0.5713441 0.91671 ## sit12 0.382866 -0.18686 -0.004789 0.10156 0.0458125 0.21087 ## sit13 0.990715 0.11967 -1.110040 0.44929 0.1885902 -0.70694 ## sit14 -0.264704 -1.06013 0.334900 0.45973 -0.0326631 -0.19945 ## sit15 -0.428410 -1.20765 0.374344 0.74970 -0.2596294 -0.30467 ## sit16 -0.330534 -0.77498 0.130760 0.22391 0.0632686 0.09060 ## sit17 -0.899601 0.12075 -0.075742 0.03842 -0.1489585 -0.12031 ## sit18 -0.770294 -0.35351 -0.033779 -0.01795 -0.3007839 0.44303 ## sit19 -0.992193 0.50319 -0.157505 -0.07070 -0.1065172 -0.09928 ## sit20 -0.937173 0.78688 -0.258119 -0.19377 -0.0343535 -0.01259 ## sit21 -0.726413 0.49163 -0.157235 -0.08698 -0.0105774 -0.02801 ## sit22 -1.002083 0.71239 -0.236526 -0.18643 -0.0231666 -0.04928 ## sit23 -0.322647 -0.03871 -0.001297 0.09029 -0.1481448 0.06934 ## sit24 0.259527 0.80746 1.124258 0.36083 1.5437866 0.07051 varespec_eigenval &lt;- eigenvals(vare_cca, scaling = 1) prop_expl &lt;- varespec_eigenval / sum(varespec_eigenval) par(mfrow = c(1, 2)) plot(x = 1:length(varespec_eigenval), y = vare_cca$CA$eig, type = &quot;b&quot;, xlab = &quot;Rang de la valeur propre&quot;, ylab = &quot;Valeur propre&quot;) abline(h = mean(varespec_eigenval), col = &quot;red&quot;, lty = 2) plot(x = 1:length(varespec_eigenval), y = prop_expl, type = &quot;b&quot;, xlab = &quot;Rang de la valeur propre&quot;, ylab = &quot;Valeur propre&quot;) lines(x = 1:length(varespec_eigenval), y = broken_stick(length(varespec_eigenval)), col = &quot;red&quot;, lty = 2) Cr√©ons les biplots. par(mfrow = c(1, 2)) plot(vare_cca, scaling = 1, main = &quot;Biplot des esp√®ces&quot;) plot(vare_cca, scaling = 2, main = &quot;Biplot des sites&quot;) Le biplot des esp√®ces, √† gauche (scaling = 1), montre la distribution des sites selon les esp√®ces. Les emplacements des scores (en noir) montrent les contrastes entre sites selon les esp√®ces qui les recouvrent. Les sites 14 et 15, par exemple, contrastent les sites 19, 20, 21 et 22 selon le 2i√®me axe principal. Par ailleurs, les axes principaux sont form√© de plusieurs esp√®ces dont aucune ne domine clairement. Le biplot des sites, √† droite (scaling = 2), montre la distribution des recouvrements d‚Äôesp√®ces selon les sites. Par exemple, les esp√®ces Betupube (Betula pubescens) et Barbhatc (Barbilophozia hatcheri ) se recouvrent en particulier le site 24. Le site 1 est difficile √† identifier, car il est couvert par plusieurs noms d‚Äôesp√®ces, au bas au centre. Les sites 3 et 13 se confondent avec Dicrsp (une esp√®ce de Dicranum) qui le recouvre amplement. Pour les deux types de biplot, les sites o√π les esp√®ces situ√©s pr√®s de l‚Äôorigine, car ils peuvent √™tre soit pr√®s de la moyenne, soit distribu√©s uniform√©ment. Le nombre de composantes √† retenir peut √™tre √©valu√© par les approches Kaiser-Guttmann et broken-stick. scaling &lt;- 1 varespec_eigenval &lt;- eigenvals(vare_cca, scaling = scaling) # peut √™tre effectu√© sur les deux types de scaling prop_expl &lt;- varespec_eigenval / sum(varespec_eigenval) par(mfrow = c(1, 2)) plot(x = 1:length(varespec_eigenval), y = vare_cca$CA$eig, type = &quot;b&quot;, xlab = &quot;Rang de la valeur propre&quot;, ylab = &quot;Valeur propre&quot;, main = paste(&quot;Eigenvalue - Kaiser-Guttmann, scaling =&quot;, scaling)) abline(h = mean(varespec_eigenval), col = &quot;red&quot;, lty = 2) plot(x = 1:length(varespec_eigenval), y = prop_expl, type = &quot;b&quot;, xlab = &quot;Rang de la valeur propre&quot;, ylab = &quot;Valeur propre&quot;, main = paste(&quot;Proportion - broken stick, scaling =&quot;, scaling)) lines(x = 1:length(varespec_eigenval), y = broken_stick(length(varespec_eigenval)), col = &quot;red&quot;, lty = 2) Pour les deux scalings, l‚Äôapproche Kaiser-Guttmann propose 7 axes, tandis que l‚Äôapproche broken-stick en propose 5. Les repr√©sentations biplot d‚Äôanalyse de correspondance peuvent prendre la forme d‚Äôun boomerang, en particulier celles qui sont bas√©es sur des donn√©es d‚Äôoccurence. Le tableau suivant initialement de Chessel et al. (1987) et est distribu√© dans le module ade4. library(&quot;ade4&quot;) data(&quot;doubs&quot;) fish &lt;- doubs$fish doubs_cca &lt;- cca(fish %&gt;% filter(rowSums(.) &gt; 0)) plot(doubs_cca, scaling = 2) Les num√©ros de sites correspondent √† la position dans une rivi√®re, 1 √©tant en amont et 30 en aval. Le premier axe discrimine l‚Äôamont et l‚Äôaval, tandis que le deuxi√®me montre deux niches en amont. Bien que l‚Äôon observe une discontinuit√© dans le cours d‚Äôeau, il y a une continuit√© dans les abondances. Cet effet peut √™tre corrig√© en retirant la tendance de l‚Äôanalyse de correspondance par une detrended correspondance analysis. Pour cela, il faudra utiliser la fonction decorana, ce qui ne sera pas couvert ici. L‚Äôanalyse des correspondances multiples (ACM) est utile pour l‚Äôordination des donn√©es cat√©gorielles. Le module ade4 est en mesure d‚Äôeffectuer des AMC, mais n‚Äôest pas couvert dans ce manuel. Excercice. Effectuez et analysez une AC avec les donn√©es de recouvrement varespec. 9.4.1.3 Positionnement multidimensionnel (PoMd) Le positionnement multidimensionnel (PoMd), ou manifold analysis, se base sur les assiciations entre les objets (mode Q) ou les variables (mode R) pour en r√©duire les dimensions. Alors que l‚Äôanalyse en composantes principales conserve la distance euclidienne et que l‚Äôanalyse de correspondance conserve la distance du \\(\\chi^2\\), le PoMd conserve l‚Äôassociation que vous s√©lectionnerez √† votre convenance. Le PoMd vise √† repr√©senter en un nombre limit√© de dimensions (souvent 2) la distance (ou dissimilarit√©) qu‚Äôont les objets (ou des variables) les uns par rapport aux autres dans l‚Äôespace multidimensionnel. Il existe deux types d‚ÄôAEM. Le PoMd-m√©trique (metric multidimentional scaling MMDS, parfois le metric est retir√©, MDS, et parfois l‚Äôon parle de classic MDS) vise √† repr√©senter fid√®lement la distance entre les objets ou les variables. Le PoMd-m√©trique ne devrait √™tre utilis√©e que lorsque la m√©trique n‚Äôest ni euclidienne, ni de \\(\\chi^2\\) et que l‚Äôon d√©sire pr√©server les distances entre les objets. L‚ÄôPoMd-m√©trique aussi appel√©e analyse en coordonn√©es principales (ACoP ou de l‚Äôanglais PCoA) . Le PoMd-non-m√©trique (nonmetric multidimentional scaling, NMDS) vise quant √† lui √† repr√©senter l‚Äôordre des distances entre les objets ou les variables. C‚Äôest une approche par rang: le PoMd-non-m√©trique vise repr√©senter les objets sont plus proches ou plus √©loign√©es les uns des autres plut√¥t que de repr√©senter leur similarit√© dans l‚Äôespace multidimentionnelle. L‚ÄôIsoMap, pour isometric feature mapping, est une extension du PoMd qui recontruit les distances selon les points retrouv√©s dans le voisinage. Les isomaps sont en mesure d‚Äôapplatir des donn√©es ayant des formes complexes. Nous ne traitons pour l‚Äôinstant que de l‚ÄôPoMd-m√©trique (fonction vegan::cmdscale) et des PoMd-non-m√©trique (fonction vegan::metaMDS). 9.4.1.3.1 Application Utilisons les donn√©es d‚Äôabondance que nous avions au tout d√©but de ce chapitre. La matrice d‚Äôassociation de Bray-Curtis sera utilis√©e. assoc_mat &lt;- vegdist(abundance, method = &quot;bray&quot;) pheatmap(assoc_mat %&gt;% as.matrix(), cluster_rows = FALSE, cluster_cols = FALSE, display_numbers = round(assoc_mat %&gt;% as.matrix(), 2)) Les sites 2 et 3 devraient √™tre plus pr√®s l‚Äôun et l‚Äôautre, puis les sites 3 et 4. Les autres associations sont √©loign√©s d‚Äôenviron la m√™me distance. Lan√ßons le calcul de la PoMd-m√©trique. pcoa &lt;- cmdscale(assoc_mat, k = nrow(abundance)-1, eig = TRUE) spec_scores &lt;- wascores(pcoa$points, abundance) ordiplot(vegan::scores(pcoa), type = &#39;t&#39;, cex = 1.5) ## species scores not available text(spec_scores, row.names(spec_scores), col = &quot;red&quot;, cex = 0.75) On observe en effet que les sites 2 et 3 sont les plus pr√®s. Les sites 3 et 4sont plus √©loign√©s. Les sites 1, 2 et 4 font √† peu pr√®s un triangle √©quilat√©ral, ce qui correspond √† ce √† quoi on devrait s‚Äôattendre. Les wa-scores permettent de juxtaposer les esp√®ces sur les sites, pour r√©f√©rence. Le colibri n‚Äôest pr√©sent que sur le site 2. Le site 1 est popul√© par des jaseurs et des m√©sanges, et c‚Äôest le seul site o√π l‚Äôon a observ√© une citelle. On a observ√© des chardonnerets sur les sites 2 et 3. Sur le site 4, on n‚Äôa observ√© que des bruants, que l‚Äôon a aussi observ√© ailleurs, sauf au site 2. Le PoMd-non-m√©trique (non metric dimensional scaling, NMDS) fonctionne de la m√™me mani√®re que la PoMd-m√©trique, √† la diff√©rence que la distance est bas√©e sur les rangs. √Ä cet √©gard, le site 4 √† une distance de 0.76 du site 3, mais plut√¥t le deuxi√®me plus loin, apr√®s le site 2 et avant le site 1. Utilisons la fonction metaMDS. nmds &lt;- metaMDS(assoc_mat, k = nrow(abundance)-1, eig = TRUE) ## Run 0 stress 0 ## Run 1 stress 0 ## ... Procrustes: rmse 0.1712075 max resid 0.2311738 ## Run 2 stress 0 ## ... Procrustes: rmse 0.1038622 max resid 0.1447403 ## Run 3 stress 0 ## ... Procrustes: rmse 0.1355982 max resid 0.1807723 ## Run 4 stress 0 ## ... Procrustes: rmse 0.1149604 max resid 0.1456473 ## Run 5 stress 0 ## ... Procrustes: rmse 0.08167542 max resid 0.1146596 ## Run 6 stress 0 ## ... Procrustes: rmse 0.1284115 max resid 0.1569041 ## Run 7 stress 0 ## ... Procrustes: rmse 0.1834823 max resid 0.2366359 ## Run 8 stress 0 ## ... Procrustes: rmse 0.1367236 max resid 0.1848335 ## Run 9 stress 0 ## ... Procrustes: rmse 0.09492646 max resid 0.1233517 ## Run 10 stress 0 ## ... Procrustes: rmse 0.12845 max resid 0.1738103 ## Run 11 stress 8.215235e-05 ## ... Procrustes: rmse 0.07684147 max resid 0.1148002 ## Run 12 stress 0 ## ... Procrustes: rmse 0.06054277 max resid 0.07388255 ## Run 13 stress 0 ## ... Procrustes: rmse 0.1239983 max resid 0.1544609 ## Run 14 stress 0 ## ... Procrustes: rmse 0.09964013 max resid 0.1300011 ## Run 15 stress 0 ## ... Procrustes: rmse 0.06571301 max resid 0.08654521 ## Run 16 stress 0 ## ... Procrustes: rmse 0.1092868 max resid 0.1707372 ## Run 17 stress 0 ## ... Procrustes: rmse 0.1520543 max resid 0.202858 ## Run 18 stress 0 ## ... Procrustes: rmse 0.158808 max resid 0.2101473 ## Run 19 stress 0 ## ... Procrustes: rmse 0.1124983 max resid 0.1450684 ## Run 20 stress 0 ## ... Procrustes: rmse 0.09344561 max resid 0.1301929 ## *** No convergence -- monoMDS stopping criteria: ## 20: stress &lt; smin ## Warning in metaMDS(assoc_mat, k = nrow(abundance) - 1, eig = TRUE): stress is (nearly) zero: you may have insufficient data spec_scores &lt;- wascores(nmds$points, abundance) ordiplot(vegan::scores(nmds), type = &#39;t&#39;, cex = 1.5) ## species scores not available text(spec_scores, row.names(spec_scores), col = &quot;red&quot;, cex = 0.75) Dans ce cas, entre PoMd-m√©trique et non-m√©trique, les r√©sultats peuvent √™tre interpr√©t√©s de mani√®re similaire. En ce qui a trait au dauphin, Pour plus de d√©tails, je vous invite √† vous r√©f√©rer √† Borcard et al. (2011)) ou de consulter l‚Äôexcellent site GUSTA ME. 9.4.1.4 Conclusion sur l‚Äôordination non contraignante Lorsque les donn√©es sont euclidiennes, l‚Äôanalyse en composantes principales (ACP) dervait √™tre utilis√©e. Lorsque la m√©trique est celle du \\(\\chi^2\\), on pr√©f√©rera l‚Äôanalyse de correspondance (AC). Si la m√©trique est autre, le positionnement multidimensionel (PoMd) est pr√©f√©rable. Dans ce dernier cas, si l‚Äôon recherche une repr√©sentation simplifi√©e de la distance entre les objets ou variables, on utilisera un PoMd-m√©trique. √Ä l‚Äôinverse, si l‚Äôon d√©sire une repr√©sentation plus fid√®le au rang des distances, on pr√©f√©rera l‚ÄôPoMd-non-m√©trique. 9.4.2 Ordination contraignante Alors que l‚Äôordination non contraignante vous permet de dresser un protrait de vos variables, l‚Äôordination contraignante (ou canonique) permet de tester statistiquement ainsi que de repr√©senter la relation entre plusieurs variables explicatives (par exemple, des conditions environnementales) et une ou plusieurs variables r√©ponses (par exemple, les esp√®ces observ√©es). L‚Äôanalyse discriminante n‚Äôa fondamentalement qu‚Äôune seulement variable r√©ponse, et celle-ci doit d√©crire l‚Äôappartenance √† une cat√©gorie. L‚Äôanalyse de redondance sera pr√©f√©r√©e lorsque le nombre de variable est plus restreint (variables ionomiques et indicateurs de performance des cultures). Les d√©tails, ainsi que les tenants et aboutissants de ces m√©thodes, sont pr√©sent√©s dans Numerical Ecology (Legendre et Legendre, 2012). L‚Äôanalyse canonique des corr√©lations sera pr√©f√©r√©e lorsque les variables sont parsem√©es (beaucoup de colonnes avec beaucoup de z√©ros, comme les variables d‚Äôabondance). 9.4.2.1 Analyse discriminante Alors que l‚Äôanalyse en composante principale vise √† pr√©senter la perspective (les axes) selon laquelle les points sont les plus √©clat√©es, l‚Äôanalyse discriminante, le plus souvent utilis√© dans sa forme lin√©aire (ADL) et quadratique (ADQ), vise √† pr√©senter la perspective selon laquelle les groupes sont les plus √©clat√©s, les groupes formant la variable contraignante. Ces groupes peuvent √™tre connus (e.g.¬†cultivar, r√©gion g√©ographique) ou attribu√©s (exemple: par partitionnement). L‚ÄôADL est parfois nomm√©e analyse canonique de la variance. L‚ÄôAD vise √† repr√©senter des diff√©rences entre des groupes aux moyens de combinaisons lin√©aires (ADL) ou quadratique (ADQ) de variables mesur√©es. Sa repr√©sentation sous forme de biplot permet d‚Äôappr√©cier les diff√©rences entre les groupes d‚Äôidentifier les variables qui sont responsables de la discrimination. Biplot de distance de l‚Äôanalyse discriminante des ionomes d‚Äôesp√®ces de plantes √† fruits cultiv√©es sauvages et domestiqu√©es, Source: Parent et al. (2013) L‚ÄôADL a √©t√© d√©velopp√©e par Fisher (1936), qui √† titre d‚Äôexemple d‚Äôapplication a utilis√© un jeu de donn√©es de dimensions d‚Äôiris collect√©es par Edgar Anderson, du Jardin botanique du Missouri, sur 150 sp√©cimens d‚Äôiris collect√©s en Gasp√©sie (Est du Qu√©bec), ma r√©gion natale (suis-je assez chauvin?). Ce jeu de donn√©es est amplement utilis√© √† titre d‚Äôexemple en analyse multivari√©e. Williams (1983) a pr√©sent√© les tenants et aboutissants de l‚ÄôADL en √©cologie. Tout comme les donn√©es passant pas une ACP doivent suivre une distribution multinormale pour √™tre statistiquement valide, les distributions des groupes dans une ADL doivent √™tre multinormales et les variances des points par groupe doivent √™tre homog√®nes‚Ä¶ ce qui est rarement le cas en science. N√©anmoins: Heureusement, il y a des √©vidences dans la litt√©rature que certaines d‚Äôentre [ces r√®gles] peuvent √™tre transgress√©es mod√©r√©ment sans de grands changement dans les taux de classification. Cette conclusion d√©pends, toutefois, de la s√©v√©rit√© des transgressions, et de facteurs structueaux comme la position relative des moyennes des populations et de la nature des dispersions. - Williams (1983) L‚ÄôADL peut servir autant d‚Äôoutil d‚Äôinterpr√©tation que d‚Äôoutil de classification, c‚Äôest √† dire de pr√©dire une cat√©gorie selon les variables (chapitre 12). Dans les deux cas, lorsque le nombre de variables approchent le nombre d‚Äôobservation, les r√©sultats d‚Äôune ADL risque d‚Äô√™tre difficilement interpr√©tables. Le test appropri√© pour √©valuer l‚Äôhomod√©n√©it√© de la covariance est le M-test de Box. Ce test est peu document√© dans la litt√©rature, est rarement utilis√© mais a la r√©putation d‚Äô√™tre particuli√®rement s√©v√®re. Il est rare que des donn√©es √©cologiques aient des dispersions (covariances) homog√®nes. Contrairement √† l‚ÄôADL, l‚ÄôADQ ne demande pas √† ce que les dispersions (covariances) soient homog√®nes. N√©anmoins, l‚ÄôADQ ne g√©n√®re ni de scores, ni de loadings: il s‚Äôagit d‚Äôun outil pour pr√©dire des cat√©gories (classification), non pas d‚Äôun outil d‚Äôordination. 9.4.2.1.1 Application Utilisons les donn√©es d‚Äôiris. data(&quot;iris&quot;) Testons la multinormalit√© par groupe. Rappelons-nous que pour consid√©rer la distribution comme multinormale, la p-value de la distortion ainsi que la statistique de Kurtosis doivent √™tre √©gale ou plus √©lev√©e que 0.05. La fonction split s√©pare le tableau en listes et la fonction map applique la fonction sp√©cifi√©e √† chaque √©l√©ment de la liste. Cela permet d‚Äôeffectuer des tests de multinormalit√© sur chacune des esp√®ces d‚Äôiris. iris %&gt;% split(.$Species) %&gt;% map(~ mvn(.x %&gt;% select(-Species), mvnTest = &quot;mardia&quot;)$multivariateNormality) ## $setosa ## Test Statistic p value Result ## 1 Mardia Skewness 25.6643445196298 0.177185884467652 YES ## 2 Mardia Kurtosis 1.29499223711605 0.195322907441935 YES ## 3 MVN &lt;NA&gt; &lt;NA&gt; YES ## ## $versicolor ## Test Statistic p value Result ## 1 Mardia Skewness 25.1850115362466 0.194444483140265 YES ## 2 Mardia Kurtosis -0.57186635893429 0.567412516528727 YES ## 3 MVN &lt;NA&gt; &lt;NA&gt; YES ## ## $virginica ## Test Statistic p value Result ## 1 Mardia Skewness 26.2705981752915 0.157059707690356 YES ## 2 Mardia Kurtosis 0.152614173978342 0.878702546726567 YES ## 3 MVN &lt;NA&gt; &lt;NA&gt; YES Le test est pass√© pour toutes les esp√®ces. Voyons maintenant l‚Äôhomog√©n√©it√© de la covariance. Pour ce faire, nous aurons besoin de la fonction boxM, disponible avec le module biotools. Pour que les covariances soient consid√©r√©es comme √©gales, la p-vaule doit √™tre sup√©rieure √† 0.05. library(&quot;heplots&quot;) ## ## Attaching package: &#39;heplots&#39; ## The following object is masked from &#39;package:pls&#39;: ## ## coefplot boxM(iris %&gt;% select(-Species), group = iris$Species) ## ## Box&#39;s M-test for Homogeneity of Covariance Matrices ## ## data: iris %&gt;% select(-Species) ## Chi-Sq (approx.) = 140.94, df = 20, p-value &lt; 2.2e-16 On est loin d‚Äôun cas o√π les distributions sont homog√®nes. Nous allons n√©anmoins proc√©der √† l‚Äôanalyse discriminante avec le module ade4. Nous aurons d‚Äôabord besoin d‚Äôeffectuer une ACP avec la fonction dudi.pca de ade4 (en sp√©cifiant une mise √† l‚Äô√©chelle), que nous projeterons en ADL avec discrimin. library(&quot;ade4&quot;) iris_pca &lt;- dudi.pca(df = iris %&gt;% select(-Species), scannf = FALSE, # ne pas g√©n√©rer de graphique scale = TRUE) iris_lda &lt;- discrimin(dudi = iris_pca, fac = iris$Species, scannf = FALSE) La visualisation peut √™tre effectu√©e directement sur l‚Äôobjet issu de la fonction discrimin. plot(iris_lda) Il s‚Äôagit toutefois d‚Äôune visualisation pour le diagnostic davantage que pour la publication. Si l‚Äôobjectif est la pubilcation, vous pourriez utiliser la fonction plotDA que j‚Äôai con√ßue √† cet effet. J‚Äôai aussi con√ßu une fonction similaire qui utilise le module graphique de base de R. source(&quot;https://raw.githubusercontent.com/essicolo/AgFun/master/plotDA_gg.R&quot;) plotDA(scores = iris_lda$li, loadings = iris_lda$fa, fac = iris$Species, level=0.95, facname = &quot;Species&quot;, propLoadings = 1) ## Loading required package: ellipse ## ## Attaching package: &#39;ellipse&#39; ## The following object is masked from &#39;package:car&#39;: ## ## ellipse ## The following object is masked from &#39;package:graphics&#39;: ## ## pairs ## Loading required package: grid ## Loading required package: plyr ## ---------------------------------------------------------------------------------------------------------------------------- ## You have loaded plyr after dplyr - this is likely to cause problems. ## If you need functions from both plyr and dplyr, please load plyr first, then dplyr: ## library(plyr); library(dplyr) ## ---------------------------------------------------------------------------------------------------------------------------- ## ## Attaching package: &#39;plyr&#39; ## The following objects are masked from &#39;package:plotly&#39;: ## ## arrange, mutate, rename, summarise ## The following objects are masked from &#39;package:dplyr&#39;: ## ## arrange, count, desc, failwith, id, mutate, rename, summarise, summarize ## The following object is masked from &#39;package:purrr&#39;: ## ## compact √Ä la diff√©rence de l‚ÄôACP, l‚ÄôADL maximise la s√©patation des groupes. Nous avions not√© avec l‚ÄôACP que les dimensions des p√©tales distingaient les groupes. Puisque nous avions justement des informations sur les groupes, nous aurions pu proc√©der directement √† un ADL pour obtenir des conclusions plus directes. Si la longueur des p√©tales permet de distinguer l‚Äôesp√®ce setosa des deux autres, la largeur des p√©tales permet de distinguer virginica et versicolor, bien que les nuages de points se superposent. De mani√®re bivari√©e, les r√©gions de confiance des moyennes des scores discriminants (petites ellipses) montrent des diff√©rence significatives au seuil 0.05. Excercice. Si l‚Äôon effectuait l‚ÄôADL sur notre dauphin, avec la colonne anatomy comme variable de regroupement, qu‚Äôobtiendrions-nous? Si l‚Äôon consi√®re la nageoire codale (queue) comme faisant partie du corps? Quelles sont les limitations? 9.4.2.2 Analyse de redondance (RDA) En anglais, on la nomme redundancy analysis, souvent abr√©g√©e RDA. Elle est utilis√©e pour r√©sumer les relations lin√©aires entre des variables r√©ponse et des variables explicatives. La ‚Äúredondance‚Äù se situe dans l‚Äôutilisation de deux tableaux de donn√©es contenant de l‚Äôinformation concordante. L‚Äôanalyse de redondance est une mani√®re √©l√©gante d‚Äôeffectuer une r√©gresssion lin√©aire multiple, o√π la matrice de valeurs pr√©dites par la r√©gression est assujettie √† une analyse en composantes principales. Il est ainsi possible de superposer les scores des variables explicatives √† ceux des variables r√©ponse. Plus pr√©cis√©ment, une RDA effectue les √©tapes suivantes (Borcard et al. (2011)) entre une matrice de variables ind√©pendantes (explicatives) \\(X\\) et une matrice de variables d√©pendantes (r√©ponse) \\(Y\\). 9.4.2.2.1 1. R√©gression entre \\(Y\\) et \\(X\\) Pour chacune des variables r√©ponse de \\(Y\\) (\\(y_1\\), \\(y_2\\), , \\(y_j\\)), effectuer une r√©gression lin√©aire sur les variables explicatives \\(X\\). \\[\\hat{y}_j = b_j + m_{1, j} \\times x_1 + m_{2, j} \\times x_2 + ... + m_{i, j} \\times x_i\\] \\[\\hat{y}_j = y_j + y_{res, j}\\] Pour chaque observation (\\(n\\)), nous obtenons une s√©rie de valeurs de \\(\\hat{y}_j\\) et de \\(y_{res, j}\\). Donc chaque cellule de la matrice \\(Y\\) a ses pendant \\(\\hat{y}\\) et \\(y_{res}\\). Nous obtenons ainsi une matrice de pr√©diction \\(\\hat{Y}\\) et une matrice des r√©sidus \\(Y_{res} = Y - \\hat{Y}\\). 9.4.2.2.2 2. Analyse en composantes principales Ensuite, on effectue une analyse en composantes principales (ACP) sur la matrice des pr√©dictions \\(\\hat{Y}\\). On obtient ainsi ses valeurs et vecteurs propres. Nommons \\(U\\) ses vecteurs propres. Les fonctions de RDA mettent souvent ces veceturs √† l‚Äô√©chelle avant de les retourner √† l‚Äôutilisateur. En ordination √©cologique, ces vecteurs mis √† l‚Äô√©chelle sont souvent appel√©s les scores des esp√®ces, bien qu‚Äôil ne s‚Äôagisse pas n√©cessairement d‚Äôesp√®ces, mais plus g√©n√©ralement des variables de la matrice d√©pendante \\(Y\\). Il est aussi possible d‚Äôeffectuer une ACP sur \\(Y_{res}\\). 9.4.2.2.3 3. Calculer les scores Les vecteurs propres \\(U\\) sont utilis√©s pour calculer les scores des sites, \\(Y \\times U\\), ainsi que les contraintes de site \\(\\hat{Y} \\times U\\). 9.4.2.2.4 Application Nous allons utiliser la fonction rda du module vegan. En ce qui a trait aux donn√©es, utilisons les donn√©es varespec (matrice Y) et varechem (matrice X). La fonction rda peut fonctionner avec l‚Äôinterface-formule de R, o√π √† gauche du ~ on retrouve le Y (la matrice de la communaut√© √©cologique, i.e.¬†les abondances d‚Äôesp√®ces) contre le X (l), √† gauche, ce qui peut √™tre pratique pour l‚Äôanalyse d‚Äôint√©ractions. Mais pour comparer deux matrices, nous pouvons d√©finir X et Y. Ce qui est m√©langeant, c‚Äôest que vegan, contrairement aux conventions, d√©fini X comme √©tant la matrice r√©ponse et Y comme √©tant la matrice explicative. vare_rda &lt;- rda(X = varespec, Y = vareclr, scale = FALSE) par(mfrow = c(1, 2)) ordiplot(vare_rda, scaling = 1, type = &quot;text&quot;, main = &quot;Scaling 1: triplot de distance&quot;) ordiplot(vare_rda, scaling = 2, type = &quot;text&quot;, main = &quot;Scaling 2: triplot de corr√©lation&quot;) La fonction ordiplot permet de cr√©er un triplot de base. La repr√©sentation des wascores est r√©put√©e plus robuste (moins susceptible d‚Äô√™tre bruit√©e), mais leur interpr√©tation porte √† confusion (Borcard et al. (2011)). Triplot de distance (scaling 1). Les angles entre les variables explicatives repr√©sentent leur corr√©lation (non pas les variables r√©ponse). Triplot de corr√©lation (scaling 2). Les angles entre les variables repr√©sentent leurs corr√©lation, que les variables soient r√©ponse ou explicative, ou entre variables r√©ponses et variables explicatives. Les distances entre les objets sur le triplot ne sont pas des approximation de leur distance euclidienne. Les triplots montrent que les variables ont toutes un r√¥le important sur la dispersion des sites autours des axeds principaux. Le premier axe principal est compos√© de mani√®re plus marqu√©e par le clr de l‚ÄôAl et celui du Fe. Le deuxi√®me axe principal est compos√© de mani√®re plus marqu√©e par le clr du S, du P et du K. Le triplot de corr√©lation ne pr√©sente pas de tendance appr√©ciable pour la plupart des esp√®ces, qui ne poss√®dent pas de niche particuli√®re. Toutefois, l‚Äôesp√®ce Cladstel, pr√©sente surtout dans les sites 9 et 10, est li√©e √† de basses teneurs en N et √† de faibles valeurs de Baresoil (sol nu). L‚Äôesp√®ce Pleuschr est li√©e √† des sols o√π l‚Äôon retrouve une grande √©paisseur d‚Äôhumus, ainsi que des teneurs √©lev√©es en nutriment K, P, S, Ca, Mg et Zn. Elle semble appr√©cier les sols √† bas pH, mais √† faible teneur en Fe et Al. La teneur en N lui semble plus indiff√©rente (son vecteur √©tant presque perpendiculaire). On pourra personnaliser les graphiques en extrayant les scores. scaling &lt;- 2 sites &lt;- vegan::scores(vare_rda, display = &quot;wa&quot;, scaling = scaling) species &lt;- vegan::scores(vare_rda, display = &quot;species&quot;, scaling = scaling) env &lt;- vegan::scores(vare_rda, display = &quot;reg&quot;, scaling = scaling) plot(0, 0, type = &quot;n&quot;, xlim = c(-3, 5), ylim = c(-3, 4), asp = 1) abline(h=0, v = 0, col = &quot;grey80&quot;) text(sites/2, labels = rownames(sites), cex = 0.7, col = &quot;grey50&quot;) text(species/2, labels = rownames(species), col = &quot;green&quot;, cex = 0.7) segments(x0 = 0, y0 = 0, x = env[, 1], y = env[, 2], col = &quot;blue&quot;) text(env, labels = rownames(env), col = &quot;blue&quot;, cex = 1) On pourra effectuer une analyse de Kaiser-Guttmann ou de broken-stick de la m√™me mani√®re que pr√©c√©demment. √âtant une collection de r√©gressions, une RDA est en mesure d‚Äôeffectuer des tests statistiques sur les coefficients de la r√©gression en utilisant des permutations pour tester la signification des coefficients et des axes d‚Äôune RDA. On doit n√©anmoins obligatoirement effectuer la RDA avec l‚Äôinterface formule. L‚Äôa variable de gauche‚Äôobjet √† gauche du ~ peut √™tre une matrice ou un tableau, et celui de droite est d√©fini dans data. Le . dans l‚Äôinterface formule signifie ‚Äúune combinaison lin√©aire de toutes les variables, sans int√©raction‚Äù. vare_rda &lt;- rda(varespec ~ ., data = vareclr, scale = FALSE) perm_test_term &lt;- anova(vare_rda, by = &quot;term&quot;) #perm_test_axis &lt;- anova(vare_rda, by = &quot;axis&quot;) La signification des axes est difficile √† interpr√©ter. Toutefois, celui des variables pr√©sente un int√©r√™t. perm_test_term ## Permutation test for rda under reduced model ## Terms added sequentially (first to last) ## Permutation: free ## Number of permutations: 999 ## ## Model: rda(formula = varespec ~ N + P + K + Ca + Mg + S + Al + Fe + Mn + Zn + Mo + Fv + Baresoil + Humdepth + pH, data = vareclr, scale = FALSE) ## Df Variance F Pr(&gt;F) ## N 1 216.13 4.8470 0.009 ** ## P 1 272.71 6.1159 0.003 ** ## K 1 194.97 4.3724 0.016 * ## Ca 1 24.92 0.5589 0.686 ## Mg 1 52.61 1.1799 0.303 ## S 1 100.07 2.2441 0.095 . ## Al 1 177.91 3.9900 0.024 * ## Fe 1 118.59 2.6595 0.053 . ## Mn 1 25.96 0.5822 0.621 ## Zn 1 35.81 0.8030 0.483 ## Mo 1 23.51 0.5273 0.668 ## Baresoil 1 98.64 2.2122 0.100 . ## Humdepth 1 43.59 0.9777 0.402 ## pH 1 38.93 0.8730 0.460 ## Residual 9 401.31 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 La p-value est la probabilit√© que les pentes calcul√©es pour les variables √©mergent de distributions dont la moyenne est nulle. Au seuil 0.05, les variables significatives sont (les clr de) l‚Äôazote, le phosphore, le potassium et l‚Äôaluminium. Dans le cas des matrices d‚Äôabondance (ce n‚Äôest pas le cas de varespec, constitu√©e de donn√©es de recouvrement), il est pr√©f√©rable avec les RDA de les transformer pr√©alablement avec la transformation compositionnelle, de chord ou de Hellinger (chapitre 8). Une autre option est d‚Äôeffectuer une RDA sur des matrices d‚Äôassociation en passant par une analyse en coordonn√©es principales (Legendre et Anderson, 1999). Enfin, les donn√©es d‚Äôabondance √† l‚Äô√©tat brutes devraient plut√¥t passer utiliser une analyse canonique des corr√©lations. 9.4.2.3 Analyse canonique des correspondances (ACC) L‚Äôanalyse canonique des correspondances (Canonical correspondance analysis), ACC, a √©t√© √† l‚Äôorigine con√ßue pour √©tudier les liens entre des variables environnementales et l‚Äôabondance (d√©compte) ou l‚Äôoccurence (pr√©sence-absence) d‚Äôesp√®ces (ter Braak, 1986). L‚ÄôACC est √† la RDA ce que la CA est √† l‚ÄôACP. Alors que la RDA pr√©serve les distance euclidiennes entre variables d√©pendantes et indpendantes, l‚ÄôACC pr√©serve les distances du \\(\\chi^2\\). Tout comme l‚ÄôAC, elle h√©rite du coup une propri√©t√© importate de la distance du \\(\\chi^2\\): il y a davantage davantage d‚Äôimportance aux esp√®ces rares. L‚Äôanalyse des correspondances canoniques est souvent utilis√©e dans la litt√©rature, mais dans bien des cas une RDA sur des donn√©es d‚Äôabondance transform√©es donnera des r√©sultats davantage int√©rpr√©tables (Legendre et Gallagher, 2001). 9.4.2.3.1 Application Cet exemple d‚Äôapplication concerne des donn√©es d‚Äôabondance. Nous allons cons√©quemment utiliser une CCA avec la fonction cca, toujours avec le module vegan. Les tableaux doubs_fish et doubs_env comprennent respectivement des donn√©es d‚Äôabondance d‚Äôesp√®ces de poissons et dans diff√©rents environnements de la rivi√®re Doubs (Europe) publi√©es dans Verneaux. (1973) et export√©es du module ade4. data(&quot;doubs&quot;) doubs_fish &lt;- doubs$fish doubs_env &lt;- doubs$env Sur le site no 8, aucun poisson n‚Äôa pas √©t√© observ√©. Les observations ne comprenant que des z√©ro doivent √™tre pr√©alablement retir√©es. tot_spec &lt;- doubs_fish %&gt;% transmute(tot_spec = apply(., 1, sum)) doubs_fish &lt;- doubs_fish %&gt;% filter(tot_spec != 0) doubs_env &lt;- doubs_env %&gt;% filter(tot_spec != 0) De la m√™me mani√®re qu‚Äôavec la fonction rda de vegan, nous utilisons cca pour l‚ÄôACC. doubs_cca &lt;- cca(doubs_fish ~ ., data = doubs_env, scale = FALSE) Comparons les r√©sultats par(mfrow = c(1, 2)) ordiplot(doubs_cca, scaling = 1, type = &quot;text&quot;, main = &quot;CCA - Scaling 1 - Triplot de distance&quot;) ordiplot(doubs_cca, scaling = 2, type = &quot;text&quot;, main = &quot;CCA - Scaling 2 - Triplot de corr√©lation&quot;) Triplot de distance (scaling 1). La projection des variables r√©ponse √† angle droit sur les variables explicatives est une approximation de la r√©ponse sur l‚Äôexplication. (2) Un objet (site ou r√©ponse) situ√© pr√®s d‚Äôune variable explicative est plus susceptible d‚Äôavoir le d√©compte 1. (3) Les distances entre les variables (r√©ponse et explicatives) approximent la distance du \\(\\chi^2\\) (traduction adapt√©e de Borcard et al. (2011)). Triplot de corr√©lation (scaling 2). La valeur optmiale de l‚Äôesp√®ce sur une variable environnementale quantitative peut √™tre obtenue en projetant l‚Äôesp√®ce √† angle droit sur la variable. (2) Une esp√®ce se trouvant pr√®s d‚Äôune variable environnementale est susceptible de se trouver en plus grande abondance aux sites de statut 1 pour cette variable. (3) Les distances n‚Äôapproximent pas la distance du \\(\\chi^2\\) (traduction adapt√©e de Borcard et al. (2011)). "],
["chapitre-outliers.html", "10 D√©tection de valeurs aberrantes et imputation de donn√©es manquantes 10.1 Donn√©es manquantes: d√©finition, origine, typologie et traitement 10.2 Valeurs et √©chantillons aberrants: d√©finition, origines, m√©thodes de d√©tection et traitement", " 10 D√©tection de valeurs aberrantes et imputation de donn√©es manquantes Ô∏è¬†Objectifs sp√©cifiques: √Ä la fin de ce chapitre, vous saurez comment proc√©der √† l‚Äôimputation de valeurs manquantes en mode univari√© et multivari√© saurez comment d√©tecter des valeurs aberrantes en mode univari√© et multivari√© Note. Ce chapitre a √©t√© initialement r√©dig√© par Zonlehoua Coulibali, qui a gracieusement accept√© de contribuer √† ces notes de cours. Le texte a √©t√© adapt√© au format du manuel par Serge-√âtienne Parent. Les donn√©es √©cologiques sont g√©n√©ralement recueillies √† diff√©rentes √©chelles, concernent plusieurs sites et plusieurs variables (corr√©l√©es ou non), impliquent diff√©rents individus de diff√©rentes agences et peuvent s‚Äô√©tendre sur plusieurs ann√©es (Alameddine et al., 2010; Lokupitiya et al., 2006). De ce fait, la plupart de ces bases de donn√©es contiennent des valeurs manquantes et/ou aberrantes li√©es √† diff√©rentes sources d‚Äôerreurs, pouvant parfois limiter l‚Äôutilit√© des inf√©rences statistiques (Collins et al., 2001; Glasson-Cicognani et Berchtold, 2010). Il convient alors de les traiter correctement avant d‚Äôeffectuer les analyses statistiques car les ignorer peut entra√Æner, outre une perte de pr√©cision, de forts biais dans les mod√®les d‚Äôanalyse (Alameddine et al., 2010; Filzmoser et al., 2008; Glasson-Cicognani et Berchtold, 2010). 10.1 Donn√©es manquantes: d√©finition, origine, typologie et traitement 10.1.1 D√©finition Les tableaux de donn√©es sont organis√©s en lignes et colonnes. Les lignes repr√©sentent les observations, les unit√©s, les sujets ou les cas √©tudi√©s selon le contexte, et les colonnes repr√©sentent les variables mesur√©es pour chaque observation. Les entr√©es qui sont les valeurs (ou contenus) des cellules ou encore les valeurs observ√©es, peuvent √™tre des valeurs continues, ou des valeurs cat√©goriales (Little et Rubin, 2002). Consid√©rant une variable al√©atoire \\(X\\) quelconque, une donn√©e manquante \\(x_m\\), est une donn√©e pour laquelle la valeur de la variable \\(X\\) est inconnue (ou absente). En d‚Äôautres termes, on ne dispose pas de la valeur de \\(X\\) pour le sujet \\(i\\) donn√©. C‚Äôest une donn√©e non disponible qui serait utile pour l‚Äôanalyse si elle √©tait observ√©e (Ware et al., 2012). La litt√©rature sur les donn√©es manquantes est plus abondante dans les domaines des sciences sociales sur les donn√©es d‚Äôenqu√™tes, et des sciences m√©dicales (Davey et al., 2001; Graham, 2012). Pour repr√©senter leur r√©partition dans la table de donn√©es, une matrice indicatrice des valeurs manquantes \\(M = (m_{ij})\\) est g√©n√©ralement utilis√©e o√π \\(m_{ij}\\) est une variable binaire qui prend la valeur 1 si la valeur de la variable (\\(X\\)) est observ√©e et 0 si \\(x\\) est absent (Collins et al., 2001; Graham, 2012; Little et Rubin, 2002). 10.1.2 Origines des donn√©es manquantes Les donn√©es manquantes ont des origines mat√©rielles diverses. Des valeurs peuvent √™tre absentes soit parce qu‚Äôelles n‚Äôont pas √©t√© observ√©es, ou qu‚Äôelles ont √©t√© perdues ou √©taient incoh√©rentes (Glasson-Cicognani et Berchtold, 2010. La donn√©e peut avoir √©t√© perdue lors de la collecte ou du processus d‚Äôenregistrement des donn√©es, non mesur√©e en raison du dysfonctionnement d‚Äôun √©quipement, non mesurable en raison de la disparition du sujet d‚Äô√©tude (mort, fugue, champ non r√©colt√©, etc.), √©cart√©e en raison d‚Äôune contamination, oubli√©e, non √©tudi√©e, etc. 10.1.3 Profils des donn√©es manquantes Les auteurs traitant des donn√©es manquantes distinguent des formes de r√©partition des donn√©es manquantes et des m√©canismes conduisant √† ces derni√®res. La r√©partition des donn√©es manquantes d√©crit les dispositions des valeurs pr√©sentes et celles qui sont manquantes dans la matrice indicatrice. Les m√©canismes √† l‚Äôorigine des donn√©es manquantes d√©crivent la relation probabiliste entre les valeurs observ√©es et les valeurs manquantes de la table de donn√©es. 10.1.3.1 R√©partition des donn√©es manquantes Les donn√©es manquantes se r√©partissent selon diff√©rents cas de figures (Graham, 2012; Little et Rubin, 2002) dont les trois principaux sont les valeurs manquantes univari√©es, les valeurs manquantes monotones et celles non monotones ou arbitraires. Cette distinction est fonction de la matrice indicatrice des valeurs manquantes. Cette matrice est dite √† valeurs manquantes univari√©es ou de non-r√©ponse univari√©e, lorsque pour une variable donn√©e, si une observation est absente, alors toutes les observations suivantes pour cette variable sont absentes (figure 10.1a). En exp√©rimentation agricole, ce cas de figure est qualifi√© de probl√®me de la parcelle manquante o√π, pour une raison quelconque (par exemple : une absence de germination, une destruction accidentelle d‚Äôune parcelle ou des enregistrements incorrects), un facteur √† l‚Äô√©tude est non disponible. Les valeurs manquantes monotones surviennent lorsque la valeur d‚Äôune variable \\(Y_j\\) manquante pour un individu \\(i\\) implique que toutes les variables suivantes \\(Y_k\\) (\\(k &gt; j\\)) sont manquantes pour cet individu (figure 10.1b). Les valeurs manquantes arbitraires ou non monotones ou encore g√©n√©rales, surviennent lorsque la matrice ne dessine sp√©cifiquement aucune des formes pr√©c√©dentes (figure 10.1c). Figure 10.1: Exemple de profils de donn√©es manquantes Le module VIM permet de visualiser la structure des donn√©es manquantes. ## Loading required package: colorspace ## Loading required package: data.table ## data.table 1.12.8 using 2 threads (see ?getDTthreads). Latest news: r-datatable.com ## ## Attaching package: &#39;data.table&#39; ## The following objects are masked from &#39;package:dplyr&#39;: ## ## between, first, last ## The following object is masked from &#39;package:purrr&#39;: ## ## transpose ## VIM is ready to use. ## Since version 4.0.0 the GUI is in its own package VIMGUI. ## ## Please use the package to use the new (and old) GUI. ## Suggestions and bug-reports can be submitted at: https://github.com/alexkowa/VIM/issues ## ## Attaching package: &#39;VIM&#39; ## The following object is masked from &#39;package:dbscan&#39;: ## ## kNN ## The following object is masked from &#39;package:datasets&#39;: ## ## sleep Pour l‚Äôexemple, prenons le tableau iris puis rempla√ßons au hasard des donn√©es par des valeurs manquantes (NA), puis v√©rifions les proportions de donn√©es manquantes et les proportions de combinaisons de donn√©es manquantes. set.seed(2868374) data(&quot;iris&quot;) iris_NA &lt;- iris n_NA &lt;- 20 row_NA &lt;- sample(1:nrow(iris), n_NA, replace = TRUE) col_NA &lt;- sample(1:ncol(iris), n_NA, replace = TRUE) for (i in 1:n_NA) iris_NA[row_NA[i], col_NA[i]] &lt;- NA summary(aggr(iris_NA, sortVar = TRUE)) ## ## Variables sorted by number of missings: ## Variable Count ## Sepal.Width 0.046666667 ## Species 0.040000000 ## Petal.Length 0.020000000 ## Petal.Width 0.020000000 ## Sepal.Length 0.006666667 ## ## Missings per variable: ## Variable Count ## Sepal.Length 1 ## Sepal.Width 7 ## Petal.Length 3 ## Petal.Width 3 ## Species 6 ## ## Missings in combinations of variables: ## Combinations Count Percent ## 0:0:0:0:0 132 88.0000000 ## 0:0:0:0:1 4 2.6666667 ## 0:0:0:1:0 2 1.3333333 ## 0:0:0:1:1 1 0.6666667 ## 0:0:1:0:0 3 2.0000000 ## 0:1:0:0:0 6 4.0000000 ## 0:1:0:0:1 1 0.6666667 ## 1:0:0:0:0 1 0.6666667 Avec la fonction matrixplot, il est possible de visualiser les donn√©es manquantes en rouge, tandis que les donn√©es pr√©sentes prennent un niveau de gris selon leur valeur. matrixplot(iris_NA) 10.1.3.2 M√©canismes conduisant aux donn√©es manquantes Les m√©canismes conduisant aux donn√©es manquantes d√©crivent la relation entre les valeurs manquantes et celles observ√©es des variables de la table (Collins et al., 2001; Graham, 2012; Little et Rubin, 2002). En consid√©rant la table de donn√©e \\(Y = \\{O,M\\}\\) o√π \\(O = \\left[ o_{i, j} \\right]\\) repr√©sente les donn√©es observ√©es et \\(M = \\left[ m_{i, j} \\right]\\) la matrice indicatrice des donn√©es manquantes, le m√©canisme √† l‚Äôorigine des donn√©es manquantes est d√©fini par la distribution conditionnelle de \\(M\\) sachant \\(Y\\). Lorsque la probabilit√© qu‚Äôune valeur soit manquante ne d√©pend ni des valeurs observ√©es, ni de celles manquantes, les donn√©es sont dites manquantes compl√®tement au hasard (* MCAR, missing completely at random*). La probabilit√© d‚Äôabsence est donc la m√™me pour toutes les observations et elle ne d√©pend que de param√®tres ext√©rieurs ind√©pendants de cette variable (Collins et al., 2001; Graham, 2012; Heitjan, 1997; Little et Rubin, 2002; Rubin, 1976). Avec de telles donn√©es (MCAR), les r√©gressions qui n‚Äôutilisent que les enregistrements complets, les moyennes des cas disponibles, les tests non-param√©triques et les m√©thodes bas√©es sur les ‚Äúmoments‚Äù, sont toutes valides (Heitjan, 1997). Toutefois, une perte de pr√©cision est √† pr√©voir dans les r√©sultats (Collins et al., 2001). Selon les m√™mes auteurs, lorsque la probabilit√© qu‚Äôune valeur soit manquante d√©pend uniquement de la composante observ√©e ‚ÄúO‚Äù (une ou plusieurs variables observ√©es) mais pas des valeurs manquantes elles-m√™mes, les donn√©es sont dites manquantes au hasard (* MAR: missing at random*). Dans ce cas, les m√©thodes du maximum de vraisemblance sont valides pour estimer les param√®tres du mod√®le. Les proc√©dures d‚Äôimputation multiples utilisent implicitement le m√©canisme MAR (Collins et al., 2001; Heitjan, 1997). Lorsque la probabilit√© qu‚Äôune valeur manque d√©pend de la valeur non observ√©e de la variable elle-m√™me (\\(M\\)), les donn√©es ne manquent pas au hasard (* MNAR: missing not at random*). Ce type de donn√©es ne doit pas √™tre ignor√© dans l‚Äôajustement de mod√®les car elles induisent une perte de pr√©cision (inh√©rente √† tout cas de donn√©es manquantes) mais aussi un biais dans l‚Äôestimation des param√®tres (Collins et al., 2001; Heitjan, 1997). 10.1.4 Traitement des donn√©es manquantes La pr√©sence de donn√©es manquantes dans une analyse peut conduire √† des estim√©s de param√®tres biais√©s, gonfler les erreurs de type I et II, baisser les performances des intervalles de confiance (Collins et al., 2001) et entacher la g√©n√©ralisation des r√©sultats (Taylor et al., 2002). Plusieurs m√©thodes existent pour calculer des estim√©s de param√®tres de mod√®les approximativement sans biais, en pr√©sence de donn√©es manquantes. 10.1.4.1 L‚Äôanalyse des cas complets Cette m√©thode consiste √† exclure du fichier de donn√©es tous les individus ayant au moins une donn√©e manquante (Glasson-Cicognani et Berchtold, 2010. Elle serait la plus utilis√©e pour traiter les valeurs manquantes mais n‚Äôest efficace que pour les cas de donn√©es manquant compl√®tement au hasard (MCAR) lorsque le nombre de d‚Äôobservations √† √©liminer n‚Äôest pas trop important (Davey et al., 2001). En R, de mani√®re g√©n√©rique, il est possible d‚Äôidentifier une donn√©e manquante dans un tableau, une matrice ou un vecteur avec is.na, qui retourne un objet bool√©en (TRUE / FALSE). La fonction any permet d‚Äôidentifier si au moins une valeur est vraie ou fausse dans un objet, alors que la fonction all permet d‚Äôidentifier si toutes les valeurs sont vraies. On pourra v√©rifier si une ligne contient une valeur manquante avec la fonction apply, dans l‚Äôaxe des lignes. Il faudra toutefois inverser le r√©sultat bool√©en avec un ! pour faire en sorte que l‚Äôon √©carte les valeurs manquantes. row_missing &lt;- iris_NA %&gt;% filter(apply(., 1, function(x) any(is.na(x)))) row_complete &lt;- iris_NA %&gt;% filter(!apply(., 1, function(x) any(is.na(x)))) row_missing ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 4.6 3.4 1.4 0.3 &lt;NA&gt; ## 2 5.7 NA 1.5 0.4 setosa ## 3 4.4 NA 1.3 0.2 setosa ## 4 5.1 NA 1.9 0.4 setosa ## 5 6.7 3.1 4.4 1.4 &lt;NA&gt; ## 6 6.2 2.2 4.5 NA versicolor ## 7 6.7 NA 5.0 1.7 versicolor ## 8 7.1 NA 5.9 2.1 virginica ## 9 6.7 NA 5.8 1.8 virginica ## 10 6.4 2.7 NA 1.9 virginica ## 11 6.5 NA 5.5 1.8 &lt;NA&gt; ## 12 5.6 2.8 4.9 NA virginica ## 13 7.7 2.8 6.7 2.0 &lt;NA&gt; ## 14 6.4 2.8 5.6 NA &lt;NA&gt; ## 15 6.3 2.8 NA 1.5 virginica ## 16 6.1 2.6 5.6 1.4 &lt;NA&gt; ## 17 NA 3.1 5.5 1.8 virginica ## 18 6.7 3.0 NA 2.3 virginica Au lieu de apply, R fournit la fontion raccourci complete.cases. row_missing &lt;- iris_NA %&gt;% filter(complete.cases(.)) Le module tidyr (inclus dans tidyverse) nous facilite la vie avec la fonction tidyr::drop_na, qui retire toutes les lignes contenant au moins une valeur manquante. row_complete &lt;- iris_NA %&gt;% drop_na() De m√™me, on pourra √©valuer la proportion de donn√©es manquantes. nrow(row_complete) / nrow(iris) ## [1] 0.88 Ou bien, √©valuer la proportion de donn√©e manquante par groupe. iris_NA %&gt;% group_by(Species) %&gt;% summarise_each(funs(sum(is.na(.))/length(.))) ## Warning: Factor `Species` contains implicit NA, consider using `forcats::fct_explicit_na` ## Warning: funs() is soft deprecated as of dplyr 0.8.0 ## Please use a list of either functions or lambdas: ## ## # Simple named list: ## list(mean = mean, median = median) ## ## # Auto named with `tibble::lst()`: ## tibble::lst(mean, median) ## ## # Using lambdas ## list(~ mean(., trim = .2), ~ median(., na.rm = TRUE)) ## This warning is displayed once per session. ## # A tibble: 4 x 5 ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 0 0.0612 0 0 ## 2 versicolor 0 0.0204 0 0.0204 ## 3 virginica 0.0217 0.0435 0.0652 0.0217 ## 4 &lt;NA&gt; 0 0.167 0 0.167 Pour terminer cette section, il est possible que certaines variables soient peu mesur√©es dans une √©tude. Au jugement, on pourra sacrifier une colonne contenant plusieurs donn√©es manquantes en vue de conserver des lignes. 10.1.4.2 L‚Äôimputation L‚Äôimputation permet de cr√©er des bases de donn√©es compl√®tes (Donz√©, 2001). Elle corrige la non-r√©ponse partielle en substituant une ‚Äúvaleur artificielle‚Äù √† la valeur manquante. Les auteurs distinguent l‚Äôimputation unique et l‚Äôimputation multiple. 10.1.4.2.1 L‚Äôimputation unique L‚Äôimputation unique consiste √† remplacer chaque donn√©e manquante par une seule valeur plausible telle que la moyenne calcul√©e sur les donn√©es r√©ellement observ√©es, l‚Äôimputation par le ou les plus proche(s) voisin(s) (la technique des plus proches voisins est couverte au chapitre 12). Cette derni√®re remplace les donn√©es manquantes par des valeurs provenant d‚Äôindividus similaires pour lesquels toute l‚Äôinformation a √©t√© observ√©e. L‚Äôimputation peut aussi se faire par r√©gression en rempla√ßant les valeurs manquantes par des valeurs pr√©dites selon un mod√®le de r√©gression ou des m√©thodes bay√©siennes plus sophistiqu√©es. L‚Äôimputation unique est valide en pr√©sence de donn√©es manquantes de type MAR (Davey et al., 2001; Donz√©, 2001; Glasson-Cicognani et Berchtold, 2010. Selon Heitjan (1997), il n‚Äôexiste pas de r√®gles strictes pour d√©cider quand il faut entreprendre une imputation multiple. N√©anmoins, si la fraction des observations avec des donn√©es manquantes est inf√©rieure √† par exemple 5%, et le m√©canisme est ignorable (MCAR ou MAR), les analyses les plus simples sont satisfaisantes. Bien que con√ßu principalement pour l‚Äôimputation multiple (on y arrive bient√¥t), le module mice permet l‚Äôimputation univari√©e. Nous allons tester l‚Äôimputation par la moyenne. Voyons par exemple la moyenne des longueurs des s√©pales. mean(iris_NA$Sepal.Length[!complete.cases(iris_NA)], na.rm = TRUE) ## [1] 6.170588 Lan√ßons l‚Äôimputation par la fonction mice, puis la pr√©diction du tableau imput√© par la fonction complete. library(&quot;mice&quot;) iris_mice &lt;- mice(iris_NA, method = &quot;mean&quot;) iris_imp &lt;- complete(iris_mice) Le tableau original peut √™tre compar√© au tableau imput√©. iris_NA[!complete.cases(iris_NA), ] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 7 4.6 3.4 1.4 0.3 &lt;NA&gt; ## 16 5.7 NA 1.5 0.4 setosa ## 39 4.4 NA 1.3 0.2 setosa ## 45 5.1 NA 1.9 0.4 setosa ## 66 6.7 3.1 4.4 1.4 &lt;NA&gt; ## 69 6.2 2.2 4.5 NA versicolor ## 78 6.7 NA 5.0 1.7 versicolor ## 103 7.1 NA 5.9 2.1 virginica ## 109 6.7 NA 5.8 1.8 virginica ## 112 6.4 2.7 NA 1.9 virginica ## 117 6.5 NA 5.5 1.8 &lt;NA&gt; ## 122 5.6 2.8 4.9 NA virginica ## 123 7.7 2.8 6.7 2.0 &lt;NA&gt; ## 133 6.4 2.8 5.6 NA &lt;NA&gt; ## 134 6.3 2.8 NA 1.5 virginica ## 135 6.1 2.6 5.6 1.4 &lt;NA&gt; ## 138 NA 3.1 5.5 1.8 virginica ## 146 6.7 3.0 NA 2.3 virginica iris[!complete.cases(iris_NA), ] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 7 4.6 3.4 1.4 0.3 setosa ## 16 5.7 4.4 1.5 0.4 setosa ## 39 4.4 3.0 1.3 0.2 setosa ## 45 5.1 3.8 1.9 0.4 setosa ## 66 6.7 3.1 4.4 1.4 versicolor ## 69 6.2 2.2 4.5 1.5 versicolor ## 78 6.7 3.0 5.0 1.7 versicolor ## 103 7.1 3.0 5.9 2.1 virginica ## 109 6.7 2.5 5.8 1.8 virginica ## 112 6.4 2.7 5.3 1.9 virginica ## 117 6.5 3.0 5.5 1.8 virginica ## 122 5.6 2.8 4.9 2.0 virginica ## 123 7.7 2.8 6.7 2.0 virginica ## 133 6.4 2.8 5.6 2.2 virginica ## 134 6.3 2.8 5.1 1.5 virginica ## 135 6.1 2.6 5.6 1.4 virginica ## 138 6.4 3.1 5.5 1.8 virginica ## 146 6.7 3.0 5.2 2.3 virginica iris_imp[!complete.cases(iris_NA), ] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 7 4.600000 3.400000 1.400000 0.300000 &lt;NA&gt; ## 16 5.700000 3.052174 1.500000 0.400000 setosa ## 39 4.400000 3.052174 1.300000 0.200000 setosa ## 45 5.100000 3.052174 1.900000 0.400000 setosa ## 66 6.700000 3.100000 4.400000 1.400000 &lt;NA&gt; ## 69 6.200000 2.200000 4.500000 1.178169 versicolor ## 78 6.700000 3.052174 5.000000 1.700000 versicolor ## 103 7.100000 3.052174 5.900000 2.100000 virginica ## 109 6.700000 3.052174 5.800000 1.800000 virginica ## 112 6.400000 2.700000 3.680142 1.900000 virginica ## 117 6.500000 NA 5.500000 1.800000 &lt;NA&gt; ## 122 5.600000 2.800000 4.900000 1.178169 virginica ## 123 7.700000 2.800000 6.700000 2.000000 &lt;NA&gt; ## 133 6.400000 2.800000 5.600000 NA &lt;NA&gt; ## 134 6.300000 2.800000 3.680142 1.500000 virginica ## 135 6.100000 2.600000 5.600000 1.400000 &lt;NA&gt; ## 138 5.818881 3.100000 5.500000 1.800000 virginica ## 146 6.700000 3.000000 3.680142 2.300000 virginica Dans la colonne Sepal.Length, toutes les valeurs manquantes ont √©t√© remplac√©es par ~5.862. Exercice. Pourquoi la pr√©diction diff√®re-t-elle de la moyenne? üò± Attention. Lorsque les valeurs sont syst√©matiquement manquantes chez une cat√©gorie, les estimateurs seront biais√©s. iris_NA_biais_1 &lt;- tibble(Sepal.Length = c(5.3, NA, 4.9, NA, 4.7, NA), Species = c(&quot;setosa&quot;, &quot;versicolor&quot;, &quot;setosa&quot;, &quot;versicolor&quot;, &quot;setosa&quot;, &quot;versicolor&quot;)) mean(iris_NA_biais_1$Sepal.Length, na.rm = TRUE) ## [1] 4.966667 iris_NA_biais_2 &lt;- tibble(Sepal.Length = c(5.3, 7.0, 4.6, 6.4, 4.8, 6.9), Species = c(&quot;setosa&quot;, &quot;versicolor&quot;, &quot;setosa&quot;, &quot;versicolor&quot;, &quot;setosa&quot;, &quot;versicolor&quot;)) mean(iris_NA_biais_2$Sepal.Length, na.rm = TRUE) ## [1] 5.833333 Dans l‚Äôexemple pr√©c√©dent, les donn√©es sont syst√©matiquement manquantes chez l‚Äôesp√®ce versicolor. La moyenne de la longueur des s√©pales est donc biais√©e, et l‚Äôimputation par la moyenne de sera tout autant. L‚Äôimputation par la moyenne est jug√©e non recommandable par plusieurs statisticiens. Dans la mesure du possible, l‚Äôimputation multiple devrait √™tre favoris√©e √† l‚Äôimputation univari√©e. 10.1.4.2.2 L‚Äôimputation multiple L‚Äôimputation multiple consiste √† imputer plusieurs fois les valeurs manquantes et √† combiner les r√©sultats pour diminuer l‚Äôerreur caus√©e par la compl√©tion (Davey et al., 2001). Les valeurs manquantes sont remplac√©es par \\(M\\) (\\(M &gt; 1\\)) ensembles de valeurs simul√©es donnant lieu √† \\(M\\) versions plausibles mais diff√©rentes des donn√©es compl√®tes (Collins et al., 2001; Taylor et al., 2002). En pratique, seulement \\(M\\) allant de 5 √† 10 (imputations) est suffisant pour produire des bonnes inf√©rences (Collins et al., 2001; Donz√©, 2001). Chacun des \\(M\\) ensembles de donn√©es est analys√© de la m√™me mani√®re par des m√©thodes standards d‚Äôanalyse de donn√©es compl√®tes, et les r√©sultats sont combin√©s en utilisant une arithm√©tique simple: les moyennes des param√®tres estim√©s sont calcul√©es, les erreurs standards sont combin√©es pour refleter l‚Äôincertitude des donn√©es manquantes et l‚Äôerreur d‚Äô√©chantillonnage. L‚Äôimputation multiple est une proc√©dure bas√©e sur un mod√®le (model-based). L‚Äôutilisateur doit sp√©cifier un mod√®le de probabilit√© conjointe pour les donn√©es observ√©es et manquantes (Collins et al., 2001; Taylor et al., 2002). Le module mice donne acc√®s √† plusieurs types de mod√®les (argument method). Les mod√®les cart et rf tombent la la cat√©gorie de l‚Äôautoapprentissage (couvert au chapitre 12). Ils ont l‚Äôavantage important d‚Äô√™tre applicables autant pour tout type de variable. iris_mice &lt;- mice(iris_NA, method = &quot;rf&quot;) iris_imp &lt;- complete(iris_mice) De m√™me que pr√©c√©demment, le tableau original peut √™tre compar√© au tableau imput√©. iris_NA[!complete.cases(iris_NA), ] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 7 4.6 3.4 1.4 0.3 &lt;NA&gt; ## 16 5.7 NA 1.5 0.4 setosa ## 39 4.4 NA 1.3 0.2 setosa ## 45 5.1 NA 1.9 0.4 setosa ## 66 6.7 3.1 4.4 1.4 &lt;NA&gt; ## 69 6.2 2.2 4.5 NA versicolor ## 78 6.7 NA 5.0 1.7 versicolor ## 103 7.1 NA 5.9 2.1 virginica ## 109 6.7 NA 5.8 1.8 virginica ## 112 6.4 2.7 NA 1.9 virginica ## 117 6.5 NA 5.5 1.8 &lt;NA&gt; ## 122 5.6 2.8 4.9 NA virginica ## 123 7.7 2.8 6.7 2.0 &lt;NA&gt; ## 133 6.4 2.8 5.6 NA &lt;NA&gt; ## 134 6.3 2.8 NA 1.5 virginica ## 135 6.1 2.6 5.6 1.4 &lt;NA&gt; ## 138 NA 3.1 5.5 1.8 virginica ## 146 6.7 3.0 NA 2.3 virginica iris[!complete.cases(iris_NA), ] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 7 4.6 3.4 1.4 0.3 setosa ## 16 5.7 4.4 1.5 0.4 setosa ## 39 4.4 3.0 1.3 0.2 setosa ## 45 5.1 3.8 1.9 0.4 setosa ## 66 6.7 3.1 4.4 1.4 versicolor ## 69 6.2 2.2 4.5 1.5 versicolor ## 78 6.7 3.0 5.0 1.7 versicolor ## 103 7.1 3.0 5.9 2.1 virginica ## 109 6.7 2.5 5.8 1.8 virginica ## 112 6.4 2.7 5.3 1.9 virginica ## 117 6.5 3.0 5.5 1.8 virginica ## 122 5.6 2.8 4.9 2.0 virginica ## 123 7.7 2.8 6.7 2.0 virginica ## 133 6.4 2.8 5.6 2.2 virginica ## 134 6.3 2.8 5.1 1.5 virginica ## 135 6.1 2.6 5.6 1.4 virginica ## 138 6.4 3.1 5.5 1.8 virginica ## 146 6.7 3.0 5.2 2.3 virginica iris_imp[!complete.cases(iris_NA), ] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 7 4.6 3.4 1.4 0.3 setosa ## 16 5.7 3.8 1.5 0.4 setosa ## 39 4.4 3.3 1.3 0.2 setosa ## 45 5.1 2.8 1.9 0.4 setosa ## 66 6.7 3.1 4.4 1.4 versicolor ## 69 6.2 2.2 4.5 1.4 versicolor ## 78 6.7 3.0 5.0 1.7 versicolor ## 103 7.1 3.3 5.9 2.1 virginica ## 109 6.7 3.3 5.8 1.8 virginica ## 112 6.4 2.7 4.9 1.9 virginica ## 117 6.5 3.0 5.5 1.8 virginica ## 122 5.6 2.8 4.9 1.7 virginica ## 123 7.7 2.8 6.7 2.0 versicolor ## 133 6.4 2.8 5.6 1.9 virginica ## 134 6.3 2.8 5.1 1.5 virginica ## 135 6.1 2.6 5.6 1.4 versicolor ## 138 6.3 3.1 5.5 1.8 virginica ## 146 6.7 3.0 5.5 2.3 virginica Mieux vauit √©viter d‚Äôimputer des donn√©es compositionnelles transform√©es (alr, clr ou ilr), car l‚Äôimputation d‚Äôune dimension transform√©e aura un impact sur tout le vecteur. Dans ce cas, vous pourriez pr√©f√©rablemen utiliser la fonction robCompositions::impCoda. 10.2 Valeurs et √©chantillons aberrants: d√©finition, origines, m√©thodes de d√©tection et traitement 10.2.1 D√©finitions En analyse univari√©e, une valeur aberrante est une ‚Äúdonn√©e observ√©e‚Äù pour une variable qui semble anormale au regard des valeurs dont on dispose pour les autres observations de l‚Äô√©chantillon (Planchon, 2005). En analyse multivari√©e, l‚Äô√©chantillon aberrant r√©sulte d‚Äôune erreur importante se trouvant dans un des composants du vecteur de r√©ponse, ou de petites erreurs syst√©matiques dans chacun de ses composants, et qui de ce fait, ne partage pas les relations entre les variables de la population (Planchon, 2005). La valeur ou l‚Äôobservation aberrante est statistiquement discordante dans le contexte d‚Äôun mod√®le de probabilit√© suppos√© connu (Barnett et Lewis, 1994; Grubbs, 1969; Munoz-Garcia et al., 1990; Pires et Santos-Pereira, 2005). Leur pr√©sence dans les donn√©es peut conduire √† des estimateurs de param√®tres biais√©s et, suite √† la r√©alisation de tests statistiques, √† une interpr√©tation des r√©sultats erron√©e (Planchon, 2005). 10.2.2 Origines Dans une collecte de donn√©es, plusieurs sources de variabilit√© peuvent mener √† des donn√©es aberrantes: la variabilit√© inh√©rente mais inusit√©e ou erreur syst√©matique, l‚Äôerreur de mesure et l‚Äôerreur d‚Äôex√©cution (figure 10.2) (Barnett et Lewis, 1994; Planchon, 2005). Figure 10.2: Sch√©ma g√©n√©ral de traitement des valeurs aberrantes - adapt√© de Barnett et Lewis, 1994 La variabilit√© inh√©rente est celle par laquelle les observations varient naturellement de mani√®re al√©atoire √† travers la population. L‚Äôerreur de mesure renferme les inad√©quations au niveau de la m√©thode de mesure, des instruments de mesure, l‚Äôarrondi des valeurs obtenues ou les erreurs d‚Äôenregistrement. Cette erreur est donc li√©e √† des circonstances bien d√©termin√©es. Les erreurs d‚Äôex√©cution interviennent √©galement dans des circonstances bien d√©termin√©es. Ce sont les erreurs de manipulation, les erreurs commises dans l‚Äôassemblage des donn√©es, ou lors du traitement informatique. L‚Äôexamen des valeurs aberrantes dans une base de donn√©es a pour objectif de les identifier pour soit les supprimer, soit les conserver, ou les corriger avant d‚Äôajuster des mod√®les non robustes (Filzmoser et al., 2008; Planchon, 2005). La valeur extr√™me peut √™tre li√©e √† un √©v√©nement atypique, mais n√©anmoins connu et int√©ressant √† √©tudier. Dans ce cas elle est importante √† conserver. La correction (ou accommodation) √©vite le rejet des observations aberrantes et consiste √† estimer les valeurs des param√®tres de la distribution de base de fa√ßon relativement libre sans d√©formation des r√©sultats li√©s √† leur pr√©sence (Barnett et Lewis, 1994). 10.2.3 D√©tection et traitement des √©chantillons aberrants multivari√©s L‚Äôapproche d‚Äôidentification des observations aberrantes selon Davies et Gather (1993) est de supposer qu‚Äôelles ont une distribution diff√©rente de celle du reste des observations. Reimann et al. (2005) les distinguent ainsi des valeurs extr√™mes qui, bien qu‚Äô√©loign√©es du centre du nuage, appartiennent √† la m√™me distribution que les autres observations. En analyse univari√©e, les m√©thodes graphiques telles que le diagramme de dispersion des observations class√©es en fonction de leur rang, les boxplots, les graphiques des quantiles de valeurs brutes ou des r√©sidus, permettent de signaler la pr√©sence de valeurs aberrantes (Planchon, 2005). En analyse multivari√©e, il existe deux approches fondamentales d‚Äôidentification des valeurs aberrantes: celles bas√©es sur le calcul de distances et les m√©thodes par projection (Filzmoser et al., 2008; Hadi et al., 2009). 10.2.3.1 Approches bas√©es sur les distances 10.2.3.1.1 La distance de Mahalanobis Les m√©thodes bas√©es sur la distance d√©tectent les valeurs aberrantes en calculant la distance, g√©n√©ralement la distance de Mahalanobis (vue au chapitre 9) entre un point particulier et le centre des donn√©es (Filzmoser et al., 2008; Pires et Santos-Pereira, 2005). Pour un √©chantillon \\(x\\) multivari√©, la distance de Mahalanobis est calcul√©e comme: \\[ \\mathscr{M} = \\sqrt{(\\vec{x}-\\vec{\\mu})^T S^{-1} (\\vec{x}-\\vec{\\mu})}.\\ \\] o√π \\(\\vec{\\mu}\\) est la moyenne arithm√©tique multivari√©e (le centro√Øde) et \\(S\\) la matrice de variance-covariances de l‚Äô√©chantillon, qui doit √™tre invers√©e. Cette distance indique √† quel point chaque observation est √©loign√©e du centre du nuage multivari√© cr√©√© par les donn√©es (Alameddine et al., 2010; Davies et Gather, 1993). D‚Äôapr√®s Alameddine et al. (2010), lorsque les donn√©es sont suppos√©es suivre une distribution normale, les carr√©s des distances \\(\\mathscr{M}\\) calcul√©es peuvent √™tre consid√©r√©s comme suivant une distribution du \\(\\chi^2\\). Par convention, tout point qui a une d√©passant un quantile donn√© de la distribution du \\(\\chi^2\\) (par exemple, \\(\\chi^2_{df = p ; 0.975}\\), le quantile 97,5% avec \\(p\\) (le nombre de variables) degr√©s de libert√©), est consid√©r√© comme atypique et identifi√© comme une valeur aberrante (Filzmoser et al., 2005). Les observations aberrantes multivari√©es peuvent ainsi √™tre d√©finies comme des observations ayant une grande distance de Mahalanobis (\\(\\mathscr{M}^2\\)). L‚Äôinconv√©nient avec les m√©thodes bas√©es sur les distances r√©side dans la difficult√© d‚Äôobtenir des estim√©s robuste de la moyenne \\(\\mu\\) et de la matrice de variance-covariances \\(S\\), puisque la distance de Mahalanobis est elle-m√™me sensible aux donn√©es extr√™mes. De plus, il serait difficile de fixer la valeur critique id√©ale de \\(\\mathscr{M}\\) permettant de s√©parer les valeurs aberrantes des points r√©guliers (Filzmoser et al., 2005; Filzmoser et al., 2008). La fonction sign1 du module mvoutlier d√©tecte les valeurs aberrantes selon un seuil du \\(\\chi^2_{df = 3 ; 0.975}\\) pour les transformations en log-ratio isom√©triques de Al, Fe et K dans un humus (l‚Äôinverse de la matrice de covariance des les log-ratio centr√©s est singuli√®re). library(&quot;mvoutlier&quot;) library(&quot;compositions&quot;) data(&quot;humus&quot;) sbp &lt;- matrix(c(1, 1,-1,-1, 1,-1, 0, 0, 0, 0, 1,-1), ncol = 4, byrow = TRUE) ilr_elements &lt;- humus %&gt;% dplyr::select(Al, Fe, K, Na) %&gt;% ilr(., V = gsi.buildilrBase(t(sbp))) %&gt;% as_tibble(.) %&gt;% dplyr::rename(AlFe_KNa = V1, Al_Fe = V2, K_Na = V3) is_out &lt;- sign1(ilr_elements, qcrit = 0.975)$wfinal01 plot(ilr_elements, col = is_out + 2) La proportion de valeurs aberrantes: sum(is_out == 0) / length(is_out) ## [1] 0.089141 Diff√©rentes m√©thodes robustes (qui s‚Äôaccommodent de la pr√©sence de points extr√™mes) de d√©tection des valeurs aberrantes sont pr√©sent√©es dans la litt√©rature telles que la m√©thode du volume minimum de l‚Äôellipso√Øde (MVE, minimum volume ellipsoid), du d√©terminant minimum de la matrice de covariance (MCD, minimum Covariance matrix determinant), et les estimateurs de type maximum de vraisemblance (M-estimators) (Alameddine et al., 2010; Filzmoser et al., 2008). Ces m√©thodes calculent des distances robustes similaires aux distances de Mahalanobis, mais remplacent les matrices des moyennes et des covariances respectivement par un seuil critique multivari√© robuste (sur \\(\\mu\\)) et un estimateur d‚Äô√©chelle (sur \\(S\\)) qui ne sont pas influenc√©s par les valeurs aberrantes (Alameddine et al., 2010). 10.2.3.1.2 La m√©thode du volume minimum de l‚Äôellipso√Øde (MVE) Le volume minimum de l‚Äôellipso√Øde est le plus petit ellipso√Øde r√©gulier couvrant au moins \\(h\\) √©l√©ments de l‚Äôensemble des donn√©es \\(X = \\{x_1, x_2, ..., x_n \\}\\) o√π l‚Äôestimateur de localisation est le centre de cet ellipso√Øde et l‚Äôestimateur de dispersion correspond √† sa matrice de covariance. \\(h\\) est fix√© √† priori sup√©rieur ou √©gal √† \\(\\frac{n}{2}+1\\), o√π \\(n\\) est le nombre total de points du nuage de donn√©es. Le seuil de d√©tection qui est la fraction des valeurs aberrantes qui, lorsqu‚Äôelle est d√©pass√©e entra√Æne des estim√©s totalement biais√©s est de l‚Äôordre de 50% √† mesure que \\(n\\) augmente (Alameddine et al., 2010; Croux et al., 2002; Filzmoser et al., 2005; Van Aelst et Rousseeuw, 2009). L‚Äôalgorithme MVE est initi√© en choisissant au hasard un ensemble de \\(p+1\\) points de donn√©es pour estimer le mod√®le majoritaire, o√π \\(p\\) est le nombre de variables. Cet ensemble initial est alors augment√© pour contenir les \\(h\\) points de donn√©es. L‚Äôalgorithme passe par plusieurs it√©rations avant de converger sur l‚Äôensemble des points les plus rapproch√©s qui auront le plus petit volume d‚Äôellipso√Øde (Alameddine et al., 2010). Le module MASS comprend la fonction cov.mve √† cet effet. Cette fonction demande le nombre minimal de points que l‚Äôon d√©sire conserver, en absolu. Il s‚Äôagit d‚Äôun nombre entier, alors si l‚Äôon d√©sire en utiliser une fraction (ici, 90%), il faut l‚Äôarrondir. Parmi les sorties de la fonction cov.mve, on retrouve les num√©ros de ligne qui se trouvent √† l‚Äôint√©rieur de l‚Äôellipsoide. library(&quot;MASS&quot;) select &lt;- dplyr::select # pour √©viter que la fonction select du module MASS remplace celle de dplyr min_in &lt;- round(0.9 * nrow(ilr_elements)) # le minimum de points √† garder, 90% du total id_in &lt;- cov.mve(ilr_elements, quantile.used = min_in)$best is_in &lt;- 1:nrow(ilr_elements) %in% id_in plot(ilr_elements, col = is_in + 2) La proportion de valeurs aberrantes: sum(!is_in) / length(is_in) ## [1] 0.1004862 10.2.3.1.3 La m√©thode du d√©terminant minimum de la matrice de covariance (MCD) La m√©thode du d√©terminant minimum de la matrice de covariance a pour objectif de trouver \\(h\\) (\\(h &gt; n\\)) observations de l‚Äôensemble de donn√©es \\(X = \\{x_1, x_2, ..., x_n \\}\\), dont la matrice de covariance a le plus petit d√©terminant. Comme avec la m√©thode MVE, l‚Äôestimateur de localisation est la moyenne de ces \\(h\\) points et celui de la dispersion est proportionnel √† la matrice de covariance (Filzmoser et al., 2005; Hubert et al., 2018; Rousseeuw et Van Driessen, 1999). id_in &lt;- cov.mcd(ilr_elements, quantile.used = min_in)$best is_in &lt;- 1:nrow(ilr_elements) %in% id_in plot(ilr_elements, col = is_in + 2) La proportion de valeurs aberrantes: sum(!is_in) / length(is_in) ## [1] 0.1004862 Mais en cas de dissym√©trie des donn√©es, ces tests (MVE, MCD) ne seraient pas applicables (Planchon, 2005). 10.2.3.2 Les m√©thodes par projection Ces m√©thodes de d√©tection des observations aberrantes trouvent des projections appropri√©es des donn√©es dans lesquelles les observations aberrantes sont facilement apparentes. Ces observations sont ensuite pond√©r√©s pour produire un estimateur robuste pouvant √™tre utilis√© pour identifier les observations aberrantes (Filzmoser et al., 2008). Ces m√©thodes n‚Äôassument pas une distribution particuli√®re des donn√©es mais cherchent des projections utiles. Elles ne sont donc pas affect√©es par la non-normalit√© et s‚Äôappliquent sur divers types de distributions (Filzmoser et al., 2008; Hadi et al., 2009). Le but de cette projection exploratoire est d‚Äôutiliser les donn√©es pour trouver des projections minimales (√† une, deux ou trois dimensions) qui fournissent les vues les plus r√©v√©latrices des donn√©es compl√®tes (Friedman, 1987). La m√©thode attribue un indice num√©rique √† chaque projection en fonction de la densit√© des donn√©es projet√©e pour capturer le degr√© de structure non lin√©aire pr√©sent dans la distribution projet√©e (Friedman, 1987; Hadi et al., 2009). En R, nous revenons au module mvoutlier, mais cette fois-ci avec la fonction sign2. is_out &lt;- sign2(ilr_elements, qcrit = 0.975)$wfinal01 plot(ilr_elements, col = is_out + 2) La proportion de valeurs aberrantes: sum(is_out == 0) / length(is_out) ## [1] 0.102107 "],
["chapitre-temps.html", "11 Les s√©ries temporelles 11.1 Op√©rations sur les donn√©es temporelles 11.2 Analyse de s√©ries temporelles 11.3 Mod√©lisation de s√©ries temporelles 11.4 Pour terminer‚Ä¶", " 11 Les s√©ries temporelles Ô∏è¬†Objectifs sp√©cifiques: √Ä la fin de ce chapitre, vous saurez comment importer et manipuler des donn√©es temporelles (utiliser le format de date, filtrer, effectuer des sommaires, agr√©ger des donn√©es, etc.) effectuer une r√©gression sur une s√©rie temporelle Les s√©ries temporelles (ou chronologiques) sont des donn√©es associ√©es √† des indices temporels de tout ordre de grandeur: seconde, minute, heure, jour, mois, ann√©e, etc. En analyse de s√©rie temporelle, le temps est une variable explicative (ou d√©pendante) incontournable. L‚Äô√©mergence de cycles est une particularit√© des s√©ries temporelles. Ceux-ci peuvent √™tre analys√©s en vue d‚Äôen d√©terminer la tendance. Les s√©ries temporelles peuvent √©galement √™tre mod√©lis√©s en vue d‚Äôeffectuer des pr√©visions. Source: Sc√®ne de Back to the future, Robert Zemeckis et and Bob Gale, 1985 Nous allons couvrir les concepts de base en analyse et mod√©lisation de s√©ries temporelles. Mais avant cela, voyons comment les donn√©es temporelles sont manipul√©es en R. Cette section est bas√©e sur le livre Forecasting: Principles and Practice, de Rob J. Hyndman et George Athanasopoulos, qui peut √™tre enti√®rement consult√© gratuitement en ligne, ainsi que le cours associ√© sur la plateforme d‚Äôapprentissage DataCamp. Figure 11.1: Forecasting: Principles and Practice, de Rob J. Hyndman et George Athanasopoulos. 11.1 Op√©rations sur les donn√©es temporelles Le d√©bit de la rivi√®re Chaudi√®re, dont l‚Äôexutoire se situe pr√®s de Qu√©bec, sur la rive Sud du fleuve Saint-Laurent, est mesur√© depuis 1915. library(&quot;tidyverse&quot;) hydro &lt;- read_csv(&quot;data/023402_Q.csv&quot;) ## Parsed with column specification: ## cols( ## Station = col_double(), ## Date = col_date(format = &quot;&quot;), ## D√©bit = col_double(), ## Remarque = col_character() ## ) La fonction read_csv() d√©tecte automatiquement que la colonne Date est une date. glimpse(hydro) ## Observations: 34,700 ## Variables: 4 ## $ Station &lt;dbl&gt; 23402, 23402, 23402, 23402, 23402, 23402, 23402, 23402, 23402, 23402, 23402, 23402, 23402, 23402, 23402, 2‚Ä¶ ## $ Date &lt;date&gt; 1915-02-27, 1915-02-28, 1915-03-01, 1915-03-02, 1915-03-03, 1915-03-04, 1915-03-05, 1915-03-06, 1915-03-0‚Ä¶ ## $ D√©bit &lt;dbl&gt; 538.0, 377.0, 269.0, 345.0, 269.0, 334.0, 269.0, 269.0, 269.0, 269.0, 269.0, 248.0, 215.0, 193.0, 193.0, 2‚Ä¶ ## $ Remarque &lt;chr&gt; &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;‚Ä¶ Le d√©bit de la rivi√®re Chaudi√®re peut √™tre explor√© graphiquement. hydro %&gt;% ggplot(aes(x = Date, y = `D√©bit`)) + geom_line() On observe des donn√©es sont manquantes de la fin des ann√©es 1920 √† la fin des ann√©es 1930. Autrement, il est difficile de visualiser la structure du d√©bit en fonction du temps, notamment si le d√©bit suit des cycles r√©guliers. On pourra isoler les donn√©es depuis 2014. hydro %&gt;% filter(Date &gt;= as.Date(&quot;2014-01-01&quot;)) %&gt;% ggplot(aes(x = Date, y = `D√©bit`)) + geom_line() R comprend la fonction as.Date(), o√π l‚Äôargument format d√©crit la mani√®re avec laquelle la date est exprim√©e. as.Date(x = &quot;1999/03/29&quot;, format = &quot;%Y/%m/%d&quot;) ## [1] &quot;1999-03-29&quot; L‚Äôargument x peut aussi bien √™tre une cha√Æne de caract√®res qu‚Äôun vecteur o√π l‚Äôon retrouve plusieurs cha√Ænes de caract√®res exprimant un format de date commun. La fonction as.Date() permet ainsi de transformer des caract√®res en date si read_csv() ne le d√©tecte pas automatiquement. Ce format peut prendre la forme d√©sir√©e, dont les param√®tres sont list√©s sur la page d‚Äôaide de la fonction strptime(). Toutefois, le plus petit incr√©ment de temps accept√© par as.Date() est le jour: as.Date() exclut les heures, minutes et secondes. Le module lubridate, issu du tidyverse, permet quant √† lui de manipuler avec plus de gr√¢ce les formats de date standards, incluant les dates et les heures: lubridate sera pr√©f√©r√© dans ce chapitre. ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:data.table&#39;: ## ## hour, isoweek, mday, minute, month, quarter, second, wday, week, yday, year ## The following object is masked from &#39;package:plyr&#39;: ## ## here ## The following object is masked from &#39;package:base&#39;: ## ## date ## [1] &quot;2011-02-19 09:14:00 UTC&quot; Plusieurs autres formats standards sont pr√©sent√©s sur un aide-m√©noire de lubridate. Si vos donn√©es comprennent des formats de date non standard, vous pourrez utiliser la fonction as.POSIXlt(), mais il pourrait √™tre pr√©f√©rable de standardiser les dates a priori. Figure 11.2: Aide-m√©moire du module lubridate. Le module lubridate rend possible l‚Äôextraction de la date (date()), l‚Äôann√©e (year()), le mois (month()), le jour de la semaine (wday()), le jour julien (yday()), etc. pour plus d‚Äôoptions, voir [l‚Äôaide-m√©moire de lubridate])(https://rawgit.com/rstudio/cheatsheets/master/lubridate.pdf). date_1 &lt;- ymd_hms(&quot;2019-03-14 09:14:00&quot;) date_1 %&gt;% date() ## [1] &quot;2019-03-14&quot; date_1 %&gt;% month() ## [1] 3 date_1 %&gt;% yday() ## [1] 73 date_1 %&gt;% wday() ## [1] 5 date_1 %&gt;% seconds() ## [1] &quot;1552554840S&quot; Ces extractions peuvent √™tre utilis√©es dans des suites d‚Äôop√©ration (pipelines). Par exemple, si nous d√©sirons obtenir le d√©bit mensuel moyen de la rivi√®re Chaudi√®re depuis 1990, nous pouvons cr√©er une nouvelle colonne Year et une autre Month avec la fonction mutate(), effectuer un filtre sur l‚Äôann√©e, regrouper par mois pour obtenir le sommaire en terme de moyenne, puis lancer le graphique. hydro_month &lt;- hydro %&gt;% mutate(Year = Date %&gt;% year(), Month = Date %&gt;% month()) %&gt;% filter(Year &gt;= 1990) %&gt;% group_by(Month) %&gt;% dplyr::summarise(MeanFlow = mean(`D√©bit`, na.rm = TRUE)) hydro_month %&gt;% ggplot(aes(x=Month, y=MeanFlow)) + geom_line() + scale_x_continuous(breaks = 1:12) + expand_limits(y = 0) On pourra aussi agr√©ger par moyenne mensuelle en gardant l‚Äôann√©e respective en cr√©ant une nouvelle colonne de date YearMonth qui permettra le regroupement avec group_by(), puis cr√©er plusieurs facettes. hydro %&gt;% mutate(Year = Date %&gt;% year(), Month = Date %&gt;% month(), YearMonth = ymd(paste0(Year, &quot;-&quot;, Month, &quot;-01&quot;))) %&gt;% filter(Year &gt;= 2010 &amp; Year &lt; 2018) %&gt;% group_by(Year, YearMonth) %&gt;% dplyr::summarise(`D√©bit` = mean(`D√©bit`, na.rm = TRUE)) %&gt;% ggplot(aes(x=YearMonth, y=`D√©bit`)) + facet_wrap(~Year, scales = &quot;free_x&quot;, ncol = 4) + geom_line() + expand_limits(y = 0) Il est possible d‚Äôeffectuer des op√©rations math√©matiques sur des donn√©es temporelles. Par exemple, ajouter 10 jours √† chaque date. hydro %&gt;% head(5) %&gt;% mutate(DateOffset = Date + days(10)) ## # A tibble: 5 x 5 ## Station Date D√©bit Remarque DateOffset ## &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; &lt;date&gt; ## 1 23402 1915-02-27 538 MC 1915-03-09 ## 2 23402 1915-02-28 377 MC 1915-03-10 ## 3 23402 1915-03-01 269 MC 1915-03-11 ## 4 23402 1915-03-02 345 MC 1915-03-12 ## 5 23402 1915-03-03 269 MC 1915-03-13 Pour effectuer des op√©rations sur des incr√©ments inf√©rieurs aux jours, il faut s‚Äôassurer que le type des donn√©es temporelles soit bien POSIXct, et non pas Date. hydro %&gt;% pull(Date) %&gt;% class() ## [1] &quot;Date&quot; hydro &lt;- hydro %&gt;% mutate(Date = as_datetime(Date)) hydro %&gt;% pull(Date) %&gt;% class() ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; hydro %&gt;% head(5) %&gt;% mutate(DateOffset = Date + seconds(10)) ## # A tibble: 5 x 5 ## Station Date D√©bit Remarque DateOffset ## &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dttm&gt; ## 1 23402 1915-02-27 00:00:00 538 MC 1915-02-27 00:00:10 ## 2 23402 1915-02-28 00:00:00 377 MC 1915-02-28 00:00:10 ## 3 23402 1915-03-01 00:00:00 269 MC 1915-03-01 00:00:10 ## 4 23402 1915-03-02 00:00:00 345 MC 1915-03-02 00:00:10 ## 5 23402 1915-03-03 00:00:00 269 MC 1915-03-03 00:00:10 11.2 Analyse de s√©ries temporelles Tout comme c‚Äôest le cas de nombreux sujet couverts lors de ce cours, l‚Äôanalyse et mod√©lisation de s√©ries temporelles est un domaine d‚Äô√©tude en soi. Nous allons nous restreindre ici aux s√©ries temporelles consign√©es √† fr√©quence r√©guli√®re. Les exemples d‚Äôanalyses et mod√©lisation de s√©ries temporelles sont typiquement des donn√©es √©conomiques, bien que les principes qui les guident sont les m√™mes qu‚Äôen d‚Äôautres domaines. Cette section est vou√©e √† l‚Äôanalyse, alors que la prochaine est vou√©e √† la mod√©lisation. Par exemple, voici une s√©rie temporelle √©conomique typique, qui exprime les d√©penses mensuelles en restauration en Australie. library(&quot;forecast&quot;) ## Registered S3 method overwritten by &#39;quantmod&#39;: ## method from ## as.zoo.data.frame zoo ## ## Attaching package: &#39;forecast&#39; ## The following object is masked from &#39;package:nlme&#39;: ## ## getResponse library(&quot;fpp2&quot;) ## Loading required package: fma ## ## Attaching package: &#39;fma&#39; ## The following object is masked from &#39;package:plyr&#39;: ## ## ozone ## The following objects are masked from &#39;package:MASS&#39;: ## ## cement, housing, petrol ## The following object is masked from &#39;package:robustbase&#39;: ## ## milk ## Loading required package: expsmooth ## ## Attaching package: &#39;fpp2&#39; ## The following object is masked from &#39;package:pls&#39;: ## ## gasoline data(&quot;auscafe&quot;) autoplot(auscafe) On y d√©tecte une tendance g√©n√©rale, probablement propuls√©e par la croissance de la d√©mographie et des revenus, ainsi que des tendances cycliques. On verra plus loin comment pr√©dire des occurrences futures, ainsi que l‚Äôincertitude de ces pr√©dictions, √† partir des donn√©es consign√©es. Jusqu‚Äô√† pr√©sent, nous avons travaill√© avec des tableaux de donn√©es incluant une colonne en format date. Nous allons maintenant travailler avec des s√©ries temporelles telles que repr√©sent√©es en R. 11.2.1 Cr√©er et visualiser des s√©ries temporelles L‚Äôinformation consign√©e dans une s√©rie temporelle inclut n√©cessairement un indice temporel associ√© √† au moins une variable. En R, cette information est consign√©e dans un objet de type ts, pour time series. Prenons une mesure quelconque prise √† chaque trimestre de l‚Äôann√©e 2018. set.seed(96683) date &lt;- ymd(c(&quot;2018-01-01&quot;, &quot;2018-04-01&quot;, &quot;2018-07-01&quot;, &quot;2018-10-01&quot;)) mesure &lt;- runif(length(date), 1, 10) mesure_ts &lt;- ts(mesure, start = date[1], frequency = 4) mesure_ts ## Qtr1 Qtr2 Qtr3 Qtr4 ## 17532 7.175836 3.646285 6.631606 8.648371 L‚Äôargument start est la date de la premi√®re observation et frequency est le nombre d‚Äôobservations par unit√© temporelle, ici l‚Äôann√©e. J‚Äôai auparavant recueilli des donn√©es m√©t√©o avec weathercan (dispobibles seulement depuis 1998) et fusionn√© avec le tableau hydro. Pour acc√©l√©rer la proc√©dure, j‚Äôai enregistr√© les donn√©es dans un fichier RData. De facto, ne gardons que les donn√©es disponibles entre 1998 et 2008, ainsi que les colonnes d√©signant la date, le d√©bit, les pr√©cipitations totales et la temp√©rature. ## Observations: 3,653 ## Variables: 4 ## $ Date &lt;date&gt; 1998-01-01, 1998-01-02, 1998-01-03, 1998-01-04, 1998-01-05, 1998-01-06, 1998-01-07, 1998-01-08, 1998-‚Ä¶ ## $ D√©bit &lt;dbl&gt; 15.70, 16.00, 17.40, 19.30, 23.20, 29.00, 58.85, 65.80, 73.40, 76.40, 76.70, 74.20, 69.80, 65.30, 61.0‚Ä¶ ## $ total_precip &lt;dbl&gt; 1.6, 2.8, 2.2, 0.0, 5.8, 11.8, 2.4, 19.2, 11.6, 2.6, 0.0, 0.0, 11.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.2, 1.2‚Ä¶ ## $ mean_temp &lt;dbl&gt; -21.1, -8.9, 1.9, -3.2, -8.7, -8.0, -7.4, -6.3, -5.4, -1.8, -6.4, -12.1, -7.9, -16.5, -17.2, -12.1, -9‚Ä¶ Pour cr√©er une s√©rie temporelle de type ts, j‚Äôenl√®ve la date, je d√©marre au premier √©v√©nement de 1998, et chaque incr√©ment a une fr√©quence de 1/365.25 unit√©s depuis 1998 (il y a en moyenne 365.25 jours par an). hydrometeo_ts &lt;- ts(hydrometeo %&gt;% dplyr::select(-Date), start = c(hydrometeo$Date[1] %&gt;% year(), 1), frequency = 365.25) Le module ggplot2 comprend la fonction autoplot(), pratique pour visualiser les s√©ries temporelles. autoplot(hydrometeo_ts, facets = TRUE) + scale_x_continuous(breaks = 1998:2008) Il est possible de filtrer des s√©ries temporelles en mode tidyverse. Toutefois, il est plus simple d‚Äôutiliser la fonction de base windows(). Disons, les 10 premiers jours de l‚Äôan 2000. ## Time Series: ## Start = 2000.00136892539 ## End = 2000.02600958248 ## Frequency = 365.25 ## D√©bit total_precip mean_temp ## 2000.001 42.40 9.4 -5.6 ## 2000.004 40.70 0.0 -5.5 ## 2000.007 43.60 23.5 -0.9 ## 2000.010 49.04 0.0 -8.8 ## 2000.012 58.90 0.0 -12.9 ## 2000.015 49.10 1.2 -4.6 ## 2000.018 44.40 3.8 -10.5 ## 2000.021 40.60 6.8 -4.9 ## 2000.023 38.10 7.0 -2.3 ## 2000.026 36.50 12.9 -0.3 Voyons l‚Äô√©volution des d√©bits mensuelles. hydrometeo_monthly &lt;- hydrometeo %&gt;% mutate(Year = Date %&gt;% year(), Month = Date %&gt;% month(), YearMonth = ymd(paste0(Year, &quot;-&quot;, Month, &quot;-01&quot;))) %&gt;% group_by(Year, YearMonth) %&gt;% dplyr::summarise(`D√©bit` = mean(`D√©bit`, na.rm = TRUE), total_precip = sum(total_precip, na.rm = TRUE), # somme mean_temp = mean(mean_temp, na.rm = TRUE)) # moyenne hydrometeo_monthly_ts &lt;- ts(hydrometeo_monthly %&gt;% ungroup() %&gt;% dplyr::select(`D√©bit`, total_precip, mean_temp), start = c(1998, 1), frequency = 12) Contraignons la p√©riode gr√¢ce √† window(), puis visualisons les tendances cycliques avec forecast::ggseasonplot() et forecast::ggsubseriesplot(). Notez que j‚Äôutilise la fonction cowplot::plot_grid() pour arranger diff√©rents graphiques ggplot2 en une grille. library(&quot;cowplot&quot;) ## ## ******************************************************** ## Note: As of version 1.0.0, cowplot does not change the ## default ggplot2 theme anymore. To recover the previous ## behavior, execute: ## theme_set(theme_cowplot()) ## ******************************************************** ## ## Attaching package: &#39;cowplot&#39; ## The following object is masked from &#39;package:lubridate&#39;: ## ## stamp theme_set(theme_grey()) # cowplot change le theme ggA &lt;- ggseasonplot(window(hydrometeo_monthly_ts[, 1], 1998, 2004-1/365.25)) + ggtitle(&quot;&quot;) ggB &lt;- ggseasonplot(window(hydrometeo_monthly_ts[, 1], 1998, 2004-1/365.25), polar = TRUE) + ggtitle(&quot;&quot;) ggC &lt;- ggsubseriesplot(window(hydrometeo_monthly_ts[, 1], 1998, 2004-1/365.25), polar = TRUE) + ggtitle(&quot;&quot;) + labs(y=&quot;Flow&quot;) plot_grid(ggA, ggB, ggC, ncol = 3, labels = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;)) 11.2.2 Structures dans les s√©ries temporelles Les s√©ries temporelles sont susceptibles d‚Äô√™tre caract√©ris√©es par des structures commun√©ment observ√©es. La tendance est une structure d√©crivant la hausse ou la baisse √† long terme d‚Äôune variable num√©rique. La fluctuation saisonni√®re est une structure p√©riodique, qui oscille autour de la tendance g√©n√©rale de mani√®re r√©guli√®re selon le calendrier. La fluctuation cyclique est aussi une structure p√©riodique, mais irr√©guli√®re (par exemple, les oscillations peuvent durer parfois 2 ans, parfois 3). Les fluctuations cycliques sont souvent de plus longue fr√©quence que les fluctuations saisonni√®res, et leur irr√©gularit√© rend les pr√©dictions plus difficiles. Note. Une tendance d√©tect√©e sur une p√©riode de temps trop courte peut s‚Äôav√©rer √™tre une fluctuation. La figure 11.3 montre diff√©rentes structures. La figure 11.3A montre une tendance croissante des d√©penses mensuelles en restauration en Australie, ainsi que des fluctuations saisonni√®res. La figure 11.3B montre des fluctuations saisonni√®res des temp√©ratures quotidiennes moyennes √† l‚ÄôUniversit√© Laval, sans pr√©senter de tendance claire. La figure 11.3C montre des fluctuations cycliques du nombre de lynx trapp√©s par ann√©e au Canada de 1821 √† 1934, sans non plus pr√©senter de tendance claire. Les cycles sont cons√©quents des m√©canismes de dynamique des populations (plus de proie entra√Æne plus de pr√©dateur, plus de pr√©dateur entra√Æne moins de proie, moins de proie entra√Æne moins de pr√©dateur, moins de pr√©dateur entra√Æne plus de proie, etc.), que nous couvrirons au chapitre 14. data(&quot;lynx&quot;) plot_grid(autoplot(auscafe), autoplot(hydrometeo_ts[, 3]) + labs(y=&quot;Mean temperature&quot;), autoplot(lynx), ncol = 3, labels = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;)) Figure 11.3: Identification des tendances et fluctuations dans des s√©ries temporelles Il est possible que l‚Äôon retrouve une hi√©rarchie dans les fluctuations, c‚Äôest-√†-dire que de grandes fluctuations (saisonni√®res ou cycliques) peuvent contenir des fluctuations sur des incr√©ments de temps plus petits. 11.2.3 L‚Äôautocorr√©lation Lorsque les donn√©es pr√©sentes des fluctuations (saisonni√®res ou cycliques), le graphique d‚Äôautocorr√©lation montrera un sommet aux √©tapes des cycles ou des saisons. Le graphique d‚Äôautocorr√©lation de donn√©es al√©atoires (aussi appel√©es bruit blanc) montera des sommets sans signification. Un graphique de retardement (lag plot) met successivement en relation \\(y_t\\) avec \\(y_{t-p}\\). Un graphique d‚Äôautocorr√©lation est la corr√©lation entre \\(y_t\\), \\(y_{t-1}\\), \\(y_{t-2}\\), etc. Une graphique de retardement donne un aper√ßu de la d√©pendance d‚Äôune variable selon ses valeurs pass√©es. Les graphiques de retardement de donn√©es ayant une forte tendance pr√©senteront des points pr√®s de la diagonale, tandis que ceux montrant des donn√©es fluctuantes de type sinuso√Ødal pr√©senteront des points dispos√©s de mani√®re circulaire. Des donn√©es al√©atoires, quant √† elles, ne pr√©senteront pas de structure de retardement facilement identifiable. set.seed(64301) bruit_blanc &lt;- ts(runif(114, 0, 6000), start = c(1821, 1), frequency = 1) plot_grid(autoplot(lynx) + ggtitle(&quot;Lynx: S√©rie temporelle&quot;), ggAcf(lynx) + ggtitle(&quot;Lynx: Autocorr√©lation&quot;), gglagplot(lynx) + ggtitle(&quot;Lynx: Lag plot&quot;), autoplot(bruit_blanc) + ggtitle(&quot;Bruit blanc: S√©rie temporelle&quot;), ggAcf(bruit_blanc) + ggtitle(&quot;Bruit blanc: Autocorr√©lation&quot;), gglagplot(bruit_blanc) + ggtitle(&quot;Bruit blanc: Lag plot&quot;), ncol = 3) Exercice. Cr√©ez, puis interpr√©tez des graphiques autoplot(), ggAcf() et gglagplot() pour les donn√©es auscafe. Exercice. Trouvez le graphique d‚Äôautocorr√©lation et le graphique de retardement correspondant √† chaque s√©rie temporelle. Figure 11.4: Exercice: Trouvez le graphique d‚Äôautocorr√©lation et le graphique de retardement correspondant √† chaque s√©rie temporelle. R√©ponse, voir source(&quot;lib/09_exercice-hydrometeo.R&quot;): - D√©bit: A-B-C - total_precip: B-A-A - mean_temp: C-C-B 11.2.4 Signification statistique d‚Äôune s√©rie temporelle J‚Äôai pr√©c√©demment introduit la notion de bruit blanc, qui est un signal ne contenant pas de structure, comme le gr√©sillement d‚Äôune radio mal syntonis√©e. Nous avons vu au chapitre 6 que les tests d‚Äôhypoth√®se en statistiques fr√©quentielles visent entre autre √† d√©tecter la probabilit√© que les donn√©es soient g√©n√©r√©es par une distribution dont la tendance centrale est nulle. De m√™me, pour les s√©ries temporelles, il est possible de calculer la probabilit√© qu‚Äôun signal soit un bruit blanc. Deux outils peuvent nous aider √† effectuer ce test: l‚Äôun visuel, l‚Äôautre sous forme de calcul. Le graphique d‚Äôautocorr√©lation est √† m√™me d‚Äôinclure des seuils pour lesquels la corr√©lation est significative (lignes pointill√©es bleues). ggAcf(lynx, ci = 0.95) + ggtitle(&quot;Lynx: Autocorr√©lation&quot;) L‚Äôanalyse des seuils de signification de l‚Äôautocorr√©lation indique sur la possibilit√© de conduire la s√©rie temporelle vers un processus de mod√©lisation pr√©dictive. Dans l‚Äôexemple ci-dessus, on remarque qu‚Äôil existe des corr√©lations significatives pour un d√©calage de 4 √† 6 donn√©es, mais que les donn√©es situ√©es pr√®s les unes des autres pourraient √™tre plus difficiles √† mod√©liser. Le test de Ljung-Box permet quant √† lui de tester si la s√©rie temporelle enti√®re peut √™tre diff√©renci√©e d‚Äôun bruit blanc. Box.test(lynx, lag = 20, type = &quot;Ljung-Box&quot;) ## ## Box-Ljung test ## ## data: lynx ## X-squared = 365.54, df = 20, p-value &lt; 2.2e-16 La probabilit√© que la s√©rie soit un bruit blanc est presque nulle. Notons que les tests statistiques sont aussi valides sur les d√©riv√©es des s√©ries temporelles. En outre, une d√©riv√©e premi√®re de la s√©rie temporelle sur les d√©penses devient une s√©rie temporelle de la variation des d√©penses en restauration. plot_grid(autoplot(diff(auscafe)) + ggtitle(&quot;Restauration: S√©rie temporelle&quot;), ggAcf(diff(auscafe)) + ggtitle(&quot;Restauration: Autocorr√©lation&quot;), gglagplot(diff(auscafe)) + ggtitle(&quot;Restauration: Lag plot&quot;), ncol = 3) Box.test(diff(auscafe), lag = 16, type = &quot;Ljung-Box&quot;) ## ## Box-Ljung test ## ## data: diff(auscafe) ## X-squared = 647.11, df = 16, p-value &lt; 2.2e-16 Jusqu‚Äô√† pr√©sent, nous nous sommes content√©s d‚Äôobserver des s√©ries temporelles. Lan√ßons-nous maintenant dans un domaine plus excitant. Source: Sc√®ne de Back to the future, Robert Zemeckis et and Bob Gale, 1985 11.3 Mod√©lisation de s√©ries temporelles L‚Äôobjectif g√©n√©ral de la mod√©lisation de s√©rie temporelle est la pr√©vision (forecast). La majorit√© des mod√®les se base sur des simulations de futurs possibles, desquels on pourra d√©duire une tendance centrale (point forecast) ainsi que des intervalles pr√©visionnels. Il est important d‚Äôinsister sur le fait que la tendance centrale ne signifie pas que les donn√©es futures suivront cette tendance, mais que, selon les donn√©es et le mod√®le, la moiti√© des donn√©es devrait se retrouver sous la ligne, et l‚Äôautre moiti√© au-dessus. De plus, la r√©gion de confiance d√©finie par les intervalles pr√©visionnels signifient que par exemple 95% des points devraient se situer dans cette r√©gion. Une mani√®re d‚Äô√©valuer la performance d‚Äôune pr√©vision est de pr√©voir des donn√©es auparavant observ√©es √† partir des donn√©es qui les pr√©c√®dent. Ces valeurs sont dites liss√©es. Tout comme c‚Äôest le cas en r√©gression statistique, il est possible de d√©duire les r√©sidus du mod√®le. Pour les r√©gressions couvertes au chapitre 6, nous v√©rifions la validit√© du mod√®le en v√©rifiant si les r√©sidus √©taient distribu√©es normalement. Pour une s√©rie temporelle, on tend plut√¥t √† v√©rifier si les r√©sidus forment un bruit blanc, c‚Äôest-√†-dire qu‚Äôils ne sont pas corr√©l√©s. De plus, pour √©viter d‚Äô√™tre biais√©es, leur moyenne doit √™tre de 0. De mani√®re compl√©mentaire pour la validit√© des intervalles pr√©visionnels, mais non essentielle √† la validit√© du mod√®le, les r√©sidus devraient √™tre distribu√©s normalement et leur variance devrait √™tre constante (Hyndman et Athanasopoulos, 2018). Il est possible qu‚Äôun mod√®le remplisse toutes ces conditions, mais que sa pr√©vision soit m√©diocre. Comme nous le verrons √©galement au chapitre 12, une pr√©diction ou une pr√©vision issue d‚Äôun mod√®le ne peut pas √™tre √©valu√©e sur des donn√©es qui ont servies √† lisser le mod√®le. Pour v√©rifier une pr√©vision temporelle, il faut s√©parer les donn√©es en deux s√©ries: une s√©rie d‚Äôentra√Ænement et une s√©rie de test (figure 11.5). Figure 11.5: Les points bleus d√©signe la s√©rie d‚Äôentra√Ænement et les points rouges, la s√©rie de test. Source de l‚Äôimage: Hyndman et Athanasopoulos, 1998. La s√©paration dans le temps entre la s√©rie d‚Äôentra√Ænement et la s√©rie de test se fait √† votre convenance, selon la disponibilit√© des donn√©es. Vous aurez toutefois avantage √† conserver davantage de donn√©es en entra√Ænement (typiquement, 70%), et √† tout le moins, s√©parer au moins une fluctuation saisonni√®re ou cyclique. La s√©rie d‚Äôentra√Ænement servira √† lisser le mod√®le pour en d√©couvrir les possibles structures. La s√©rie de test servira √† √©valuer sa performance sur des donn√©es obtenues, mais inconnues du mod√®le pour v√©rifier les structures d√©couvertes par le mod√®le. L‚Äôerreur pr√©visionnelle est la diff√©rence entre une donn√©e observ√©e en test et sa pr√©vision (l‚Äô√©quivalent des r√©sidus, mais appliqu√©s sur des donn√©es ind√©pendantes du mod√®le). La performance d‚Äôune pr√©vision peut √™tre √©valu√©e de diff√©rentes mani√®res, mais l‚Äôerreur moyenne absolue √©chelonn√©e (mean absolute scaled error, MASE) est conseill√©e puisqu‚Äôelle ne d√©pend pas de la m√©trique de la quantit√© produite: plus la MASE se rapproche de z√©ro, meilleure est la pr√©vision. Plusieurs m√©thodes de pr√©vision sont possibles. Nous en couvrirons 3 dans ce chapitre: la m√©thode na√Øve, la m√©thode SES et la m√©thode ARIMA. Nous allons couvrir les diff√©rents aspects de la mod√©lisation des s√©ries temporelles √† travers l‚Äôutilisation de ces m√©thodes. 11.3.1 M√©thode na√Øve La m√©thode na√Øve d√©finit la valeur suivante selon la valeur pr√©c√©dente (fonction forecast::naive()), ou la valeur de la saison pr√©c√©dente (fonction forecast::snaive()). Ces fonctions du module forecast incluent un composante al√©atoire pour simuler des occurrences futures selon des marches al√©atoires (random walks), o√π chaque valeur suivante est simul√©e al√©atoirement, consid√©rant la valeur pr√©c√©dente. Nous tenterons de pr√©voir les d√©bits de la rivi√®re Chaudi√®re. Ceux-ci √©tant caract√©ris√© par des fluctuations saisonni√®res, mieux vaut utiliser snaive(). Mais auparavant, s√©parons la s√©rie en s√©rie d‚Äôentra√Ænement et s√©rie de test. flow_ts &lt;- hydrometeo_monthly_ts[, 1] flow_ts_train &lt;- window(flow_ts, start = 1998, end = 2005.999) flow_ts_test &lt;- window(flow_ts, start = 2006) Lan√ßons la mod√©lisation sur les donn√©es d‚Äôentra√Ænement. hm_naive &lt;- snaive(flow_ts_train, h = 24) autoplot(hm_naive) + autolayer(fitted(hm_naive)) + autolayer(flow_ts_test, color = rgb(0, 0, 0, 0.6)) + labs(x = &quot;Ann√©e&quot;, y = &quot;D√©bit&quot;) ## Warning: Removed 12 rows containing missing values (geom_path). Le graphique pr√©c√©dent montre que la pr√©vision na√Øve (en rose) prend bien la valeur observ√©e au cycle pr√©c√©dent (en noir). Les donn√©es de test sont en gris transparent. Notons que la pr√©sence de d√©bit n√©gatifs pourrait √™tre √©vit√©e en utilisant une transformation logarithmique du d√©bit pr√©alablement √† la mod√©lisation. Voyons maintenant l‚Äôanalyse des r√©sidus avec la fonction forecast::checkresiduals(). checkresiduals(hm_naive) ## ## Ljung-Box test ## ## data: Residuals from Seasonal naive method ## Q* = 34.903, df = 19, p-value = 0.01435 ## ## Model df: 0. Total lags used: 19 La p-value √©tant de 0.01546, il est peu probable que les r√©sidus forment un bruit blanc. Les r√©sidus contiennent de l‚Äôautocorr√©lation, ce qui devrait √™tre √©vit√©. Ceci est toutefois d√ª √† un seul point allant au-del√† du seuil de 0.05, que l‚Äôon peut observer sur le graphique d‚Äôautocorr√©lation. Le graphique de la distribution des r√©sidus montre des valeurs aberrantes, ainsi qu‚Äôune distribution plut√¥t pointue, qui donnerait un test de Kurtosis probablement √©lev√©. shapiro.test(residuals(hm_naive)) # non-normal si p-value &lt; seuil (0.05) ## ## Shapiro-Wilk normality test ## ## data: residuals(hm_naive) ## W = 0.93698, p-value = 0.0004733 library(&quot;e1071&quot;) kurtosis(residuals(hm_naive), na.rm = TRUE) # le r√©sultat d&#39;un test de kurtosis sur une distribution normale devrait √™tre de 0. ## [1] 2.909277 Pas de panique, les pr√©dictions peuvent n√©anmoins √™tre valides: seulement, les intervalles pr√©visionnels pourraient √™tre trop vagues ou trop restreintes: √† prendre avec des pincettes. L‚Äô√©valuation du mod√®le peut √™tre effectu√©e avec la fonction forecast::accuracy(), qui d√©tecte automatiquement la s√©rie d‚Äôentra√Ænement et la s√©rie de test si on lui fournit la s√©rie enti√®re (ici l‚Äôobjet flow_ts). accuracy(hm_naive, flow_ts) ## ME RMSE MAE MPE MAPE MASE ACF1 Theil&#39;s U ## Training set 0.5952819 80.24256 54.53431 -57.045415 95.60936 1.000000 0.1669526053 NA ## Test set -1.0165543 75.18877 57.10499 -2.645333 59.10844 1.047139 -0.0006850245 0.3877041 La m√©thode na√Øve est rarement utilis√©e en pratique autrement que comme standard par rapport auquel la performance d‚Äôautres mod√®les est √©valu√©e. 11.3.2 M√©thode SES Alors que la m√©thode na√Øve donne une cr√©dibilit√© compl√®te √† la valeur pr√©c√©dente (ou au cycle pr√©c√©dent), la m√©thode SES (simple exponential smoothing) donne aux valeurs pr√©c√©dentes des poids exponentiellement d√©croissants selon leur anciennet√©. La pr√©vision par SES sera une moyenne pond√©r√©e des derni√®res observations, en donnant plus de poids sur les observations plus rapproch√©es. Math√©matiquement, la m√©thode SES est d√©crite ainsi. \\[\\hat{y}_{t + h|t} = \\alpha y_t + \\alpha\\left( 1-\\alpha \\right) y_{t-1} + \\alpha\\left( 1-\\alpha \\right)^2 y_{t-2} + ...\\] o√π \\(\\hat{y}_{t + h|t}\\) est la pr√©vision de \\(y\\) au temps \\(t + h|t\\), qui est le d√©calage de \\(h\\) √† partir de la derni√®re mesure au temps \\(t\\). Le param√®tre \\(\\alpha\\) prend une valeur de 0 √† 1, et d√©crit la distribution des poids. Une valeur de \\(\\alpha\\) √©lev√© donnera davantage de poids aux √©v√©nements r√©cents. La somme de tous poids \\(\\alpha\\) tend vers 1 lorsque les pas de temps pr√©c√©dents tendent vers l‚Äô\\(\\infty\\). Une autre mani√®re d‚Äôexprimer l‚Äô√©quation est de la segmenter en deux: une pour la pr√©vision en fonction du niveau (level, le mod√®le), une autre pour d√©crire comment le niveau change au fil du temps. Description √âquation Pr√©vision \\(\\hat{y}_{t + h|t} = l_t\\) Niveau \\(l_t = \\alpha y_t + \\alpha\\left( 1-\\alpha \\right) l_{t-1}\\) Exprim√©e ainsi, la pr√©vision n‚Äôexprimera aucune tendance ni fluctuation. Il s‚Äôagira d‚Äôune projection jusqu‚Äô√† l‚Äôinfini de la moyenne des observations pr√©c√©dentes pond√©r√©e par leur d√©calage. 11.3.2.1 SES de base Prenons les donn√©es de la NASA sur l‚Äôindice de temp√©rature terre-oc√©an, qui d√©crit un d√©calage par rapport √† la moyenne des temp√©ratures globales observ√©es entre de 1951 √† 1980. La m√©thode SES est appel√©e par la fonction forecast::ses(), de la m√™me mani√®re qu‚Äôon l‚Äôa fait pr√©c√©demment avec la m√©thode na√Øve. loti_ts &lt;- read_csv(&quot;data/09_nasa.csv&quot;) %&gt;% pull(LOTI) %&gt;% ts(., start = 1880, frequency = 1) #loti_ts &lt;- window(loti_ts, start = 1950) loti_ts_tr &lt;- window(loti_ts, end = 2004) loti_ses &lt;- ses(loti_ts_tr, h = 20, alpha = 0.5) autoplot(loti_ses) + autolayer(fitted(loti_ses)) Note. Les pr√©visions climatiques sont effectu√©es par des mod√®les bien plus complexes que ce que nous voyons ici. Les pr√©visions du GIEC agr√®gent des tendances localis√©es et incluent une batterie de covariables, dont la plus √©vidente est la concentration en CO2 dans l‚Äôatmosph√®re. Il s‚Äôagit seulement d‚Äôun exemple d‚Äôapplication. 11.3.2.2 SES avec tendance La pr√©vision a peu d‚Äôint√©r√™t, √©tant donn√©e qu‚Äôelle n‚Äôinclut pas de tendance. Or, nous pouvons en ajouter une √† l‚Äô√©quation. Ainsi exprim√©e, la tendance changera aussi au fil du temps. Description √âquation Pr√©vision \\(\\hat{y}_{t + h|t} = l_t + \\left( \\phi + \\phi^2 + ... + \\phi^h \\right) \\times b_t\\) Niveau $l_t = y_t + ( 1-) ( l_{t-1} + b_{t-1} ) $ Tendance \\(b_t = \\beta^* \\left( l_t - l_{t-1} \\right) + (1-\\beta^*) \\phi b_{t-1}\\) Le param√®tre \\(\\beta^*\\) d√©crit la vitesse √† laquelle la tendance peut changer, de 0 o√π la pente ne change pas √† 1 o√π la pente change rapidement. Le param√®tre \\(\\phi\\) adouci la pente en s‚Äô√©loignant de la derni√®re mesure. Un tendant vers 0 g√©n√©rera un fort adoucissement, alors qu‚Äôun tendant vers 1 ne g√©n√©rera pas d‚Äôadoucissement. Il peut √™tre difficile de d√©terminer les param√®tres de lissage \\(\\alpha\\), \\(\\beta^*\\) et \\(\\phi\\), ainsi que les param√®tres d‚Äô√©tat \\(l_0\\) et \\(b_0\\). La fonction de forecast::holt() permet de les estimer automatiquement. loti_holt_dF &lt;- holt(loti_ts_tr, damped = FALSE, h = 100) loti_holt_dT &lt;- holt(loti_ts_tr, damped = TRUE, h = 100) plot_grid(autoplot(loti_holt_dF), autoplot(loti_holt_dT)) loti_holt_dF$model$par ## alpha beta l b ## 0.4052862918 0.0001000051 -0.2170967540 0.0052550677 loti_holt_dT$model$par ## alpha beta phi l b ## 0.4843654438 0.0001000061 0.8286096396 -0.1096315698 -0.0332523695 Dans ce cas, l‚Äôoptimisation de \\(\\phi\\) lui donne une valeur de 0.8, une valeur suffisamment faible pour que l‚Äôadoucissement soit fort. Vous obtiendrez une valeur de \\(\\phi\\) plus √©lev√©e en ne consid√©rant que les donn√©es obtenues depuis 1950 (en d√©commentant loti_ts &lt;- window(loti_ts, start = 1950), plus haut). 11.3.2.3 SES avec fluctuation saisonni√®re D‚Äôautres param√®tres peuvent √™tre ajout√©s pour de tenir compte des fluctuations saisonni√®res (les fluctuations cycliques sont plus difficiles √† mod√©liser) de mani√®re additive ou multiplicative. Voici la modification apport√©e pour la mod√©lisation additive, en laissant tomber l‚Äôadoucissement. Description √âquation Pr√©vision \\(\\hat{y}_{t + h|t} = l_t + h \\times b_t + s_{t-m+h_m^+}\\) Niveau $l_t = (y_t - s_{t-m} ) + ( 1-) ( l_{t-1} + b_{t-1} ) $ Tendance \\(b_t = \\beta^* \\left( l_t - l_{t-1} \\right) + (1-\\beta^*) b_{t-1}\\) Saison \\(s_t = \\gamma \\left( y_t - l_{t-1} - b_{t-1} \\right) + (1-\\gamma) s_{t-m}\\) o√π \\(m\\) est la p√©riodicit√© des fluctuations saisonni√®re, par exemple 4 pour quatre saisons annuelles et \\(\\gamma\\) est un param√®tre de la portion saisonni√®re, qui, tout comme un effet al√©atoire en biostatistiques, fluctue autour de z√©ro. La variante multiplicative multiplie la pr√©vision par un facteur plut√¥t que d‚Äôimposer un d√©calage. La math√©matique n‚Äôest pas pr√©sent√©e ici pour plus de simplicit√© (consulter Hyndman et Athanasopoulos (2018), chapitre 7.3 pour plus de d√©tails). Dans le cas multiplicatif, l‚Äôeffet saisonnier fluctue autour de 1. Si l‚Äôamplitude de la fluctuation s‚Äôaccro√Æt au fil de la s√©rie temporelle, la m√©thode multiplicative donnera probablement de meilleurs r√©sultats. La fonction que nous utiliserons pour les SES-saisonniers est forecast::hw(). Les donn√©es de la NASA ne sont pas saisonni√®res (frequency(loti_ts) donne 1). flow_hw &lt;- hw(flow_ts_train, damped = TRUE, h = 12*3, seasonal = &quot;additive&quot;) autoplot(flow_hw) + autolayer(fitted(flow_hw)) 11.3.2.4 Automatiser la pr√©vision avec les SES L‚Äôerreur du mod√®le peut aussi √™tre calcul√©e de sorte qu‚Äôelle soit constante ou augmente selon le niveau (ou d√©calage). Nous avons donc plusieurs types de mod√®les de la famille SES. Tendance: [sans tendance, tendance additive, tendance adoucie] Saison: [sans saison, saison additive, saison multiplicative] Erreur: [erreur additive, erreur multiplicative] Lequel choisir? Encore une fois, on peut laisser R optimiser notre choix avec un mod√®le ETS (error, tend and seasonnal). L‚Äôoptimisation est lanc√©e avec la fonction forecast::ets(). flow_model &lt;- ets(flow_ts_train) flow_model ## ETS(M,N,M) ## ## Call: ## ets(y = flow_ts_train) ## ## Smoothing parameters: ## alpha = 0.0104 ## gamma = 1e-04 ## ## Initial states: ## l = 127.508 ## s = 0.6511 0.9399 0.7981 0.4272 0.6253 0.8872 ## 0.6594 1.2741 3.7317 1.5211 0.1878 0.2971 ## ## sigma: 0.6065 ## ## AIC AICc BIC ## 1222.469 1228.469 1260.935 Le mod√®le retenu est un ETS(M,N,M), d√©finissant dans l‚Äôordre le type d‚Äôerreur, de tendance et de saison selon A pour additif, M pour multiplicatif et N pour l‚Äôabsence. Nous avons une erreur de type M (multiplicative), une tendance de type N (sans tendance) et une saison de type M (multiplicative). L‚Äôabsence de valeur pour phi indique que l‚Äôadoucissement n‚Äôest probablement pas n√©cessaire. Nous pouvons visualiser l‚Äô√©volution des diff√©rentes composantes. autoplot(flow_model) Dans un mod√®le sans tendance, avec saisonnalit√© multiplicative, les donn√©es levels sont multipli√©es par les donn√©es season pour obtenir la pr√©vision. Malgr√© l‚Äôabsence de tendance dans le mod√®le, il semble que le d√©bit a diminu√© de 2000 √† 2003 entre deux √©tats stables de 1998 √† 2000 et de 2003 √† 2006. La fonction forecast::ets() g√©n√®re un mod√®le, mais pas de pr√©diction. Pour obtenir une pr√©diction, nous devons utiliser la fonction forecast::forecast(), que j‚Äôutiliserai ici en mode tidyverse. flow_ets &lt;- flow_ts_train %&gt;% ets() flow_fc &lt;- flow_ets %&gt;% forecast() flow_fc %&gt;% autoplot() L‚Äôanalyse d‚Äôexactitude et celle des r√©sidus sont toutes aussi pertinentes. La premi√®re est effectu√©e sur la pr√©vision, et la seconde sur le mod√®le. accuracy(flow_fc, flow_ts) ## ME RMSE MAE MPE MAPE MASE ACF1 Theil&#39;s U ## Training set -13.346098 59.33195 44.55719 -90.26010 110.68741 0.8170487 0.1612394 NA ## Test set 3.768712 69.95479 55.55668 -24.65164 62.94846 1.0187472 0.2452020 0.6990499 checkresiduals(flow_ets) ## ## Ljung-Box test ## ## data: Residuals from ETS(M,N,M) ## Q* = 27.068, df = 5, p-value = 5.533e-05 ## ## Model df: 14. Total lags used: 19 Il est peu probable que les r√©sidus aient √©t√© g√©n√©r√©s par un bruit blanc, indiquant qu‚Äôil existe une structure dans les donn√©es qui n‚Äôa pas √©t√© captur√©e par le mod√®le. Exercice. Mod√©liser la s√©rie temporelle lynx avec forecast::ets(). Que se passe-t-il? 11.3.2.5 Pr√©traitement des donn√©es J‚Äôai sp√©cifi√© plus haut que les donn√©es de d√©bit pourraient avantageusement √™tre transform√©es avec un logarithme pour √©viter les pr√©dictions de d√©bits n√©gatifs. D‚Äôautres types de transformation peuvent √™tre utilis√©es, comme la racine carr√©e ou cubique, l‚Äôoppos√©e de l‚Äôinverse (\\(-1/x\\)) ou les transformations compositionnelles (chapitre 8). La transformation Box-Cox est aussi largement utilis√©e pour sa polyvalence. \\[ w = \\begin{cases} ln(y_t) &amp;\\text{if } \\lambda = 0 \\\\ \\frac{y_t - 1}{\\lambda} &amp;\\text{if } \\lambda \\neq 0 \\end{cases} \\] \\(\\lambda = 1\\): pas de transformation \\(\\lambda = 1/2\\): ressemble √† \\(\\sqrt{y_t}\\) \\(\\lambda = 1/3\\): ressemble √† \\(\\sqrt[3]{y_t}\\) \\(\\lambda = 0\\): log naturel \\(\\lambda = -1\\): ressemble √† \\(1/y_t\\) La fonction forecast::BoxCox.lambda() estime la valeur optimale de \\(\\lambda\\). BoxCox.lambda(flow_ts_train) ## [1] 0.784101 Cette valeur peut √™tre imput√©e √† l‚Äôargument lambda de la fonction forecast::ets(). Dans notre cas, nous d√©sirions plut√¥t une transformation logarithmique. Cons√©quemment, nous utilisons lambda = 0. flow_ts_train %&gt;% ets(lambda = 0) %&gt;% forecast() %&gt;% autoplot() Les erreurs ne franchissent pas le 0, mais sont vraisemblablement surestim√©es lors des sommets. Notez que R s‚Äôoccupe de la transformation retour. La diff√©renciation est aussi une forme de pr√©traitement. La diff√©renciation (fonction base::diff()) consiste en la soustraction de la valeur pr√©c√©dente √† la valeur suivante. La valeur pr√©c√©dente peut √™tre d√©cal√©e √† la valeur de la p√©riode de l‚Äôunit√© temporelle pr√©c√©dente, par exemple le mois de mars de l‚Äôann√©e pr√©c√©dente. Un objectif de la diff√©renciation est de rendre la s√©rie temporelle stationnaire en termes de tendance et de fluctuation saisonni√®re, de sorte que la s√©rie diff√©renci√©e se comporte comme un bruit blanc. plot_grid(flow_ts_train %&gt;% autoplot() + ggtitle(&quot;D√©bit&quot;), loti_ts_tr %&gt;% autoplot() + ggtitle(&quot;LOTI&quot;), flow_ts_train %&gt;% diff(., lag = 12) %&gt;% autoplot() + ggtitle(&quot;D√©bit avec diff√©renciation saisonni√®re&quot;), loti_ts_tr %&gt;% diff(., lag = 1) %&gt;% autoplot() + ggtitle(&quot;LOTI avec diff√©renciation d&#39;ordre 1&quot;)) 11.3.3 La m√©thode ARIMA Un mod√®le ARIMA, l‚Äôacronyme de l‚Äôanglais auto-regressive integrated moving average, est une combinaison de trois parties: AR-I-MA. L‚Äôautor√©gression consiste en une r√©gression lin√©aire dont la variable r√©ponse \\(y_t\\) est la variable √† l‚Äôinstant \\(t\\) et les variables explicatives sont les variables aux instants pr√©c√©dents. Pour un nombre \\(p\\) de p√©riodes pr√©c√©dentes, nous obtenons une r√©gression lin√©aire typique. \\[y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + ... + \\phi_p y_{t-p} + \\epsilon_t\\] o√π \\(\\epsilon_t\\) est l‚Äôerreur sur la pr√©diction. La partie concernant la moyenne mobile est une r√©gression non pas sur les observations, mais sur les erreurs. Consid√©rant les \\(q\\) erreurs pr√©c√©dentes, nous obtenons \\[y_t = c + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + ... + \\theta_q \\epsilon_{t-q} + \\epsilon_t\\] La somme de l‚Äôautor√©gression et de la moyenne mobile donne un mod√®le ARMA. Le I de ARIMA, mis pour integrated, est le contraire de la diff√©renciation, que j‚Äôai pr√©sent√© √† la fin de la section sur les SES. Puisque la s√©rie temporelle doit √™tre stationnaire pour effectuer l‚ÄôARMA, nous devons diff√©rencier la s√©rie un nom \\(d\\) de fois avant de proc√©der √† l‚Äôautor√©gression et au calcul de la moyenne mobile. Nous obtenons ainsi une ARIMA d‚Äôordres \\(p\\), \\(d\\) et \\(q\\), not√©e \\(ARIMA(p,d,q)\\). Nous devons aussi statuer si \\(c\\) (l‚Äôintercept ou le drift) doit √™tre ou non consid√©r√© come nul. Ces ordres peuvent √™tre sp√©cifi√©s dans la fonction telle que forecast::Arima(order = c(0, 1, 1), include.constant = TRUE). Toutefois, il est possible de les optimiser gr√¢ce √† la fonction forecast::auto.arima(). Tout comme les sorties de forecast::ets(), forecast::auto.arima() fourni le mod√®le, mais pas les pr√©dictions: la fonction forecast::forecast() doit √™tre lanc√©e pour obteir la pr√©diction. loti_arima &lt;- loti_ts_tr %&gt;% auto.arima() loti_arima %&gt;% forecast(h = 30) %&gt;% autoplot() summary(loti_arima) ## Series: . ## ARIMA(1,1,3) with drift ## ## Coefficients: ## ar1 ma1 ma2 ma3 drift ## -0.9405 0.6260 -0.6019 -0.3716 0.0060 ## s.e. 0.0522 0.0976 0.0847 0.0846 0.0031 ## ## sigma^2 estimated as 0.01036: log likelihood=109.53 ## AIC=-207.06 AICc=-206.35 BIC=-190.14 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set -0.0007946749 0.09932376 0.0824145 22.62973 65.41129 0.8925238 -0.0221428 Le sommaire du mod√®le sp√©cifie une \\(ARIMA(1,1,3)\\) en utilisant l‚Äôintercept \\(c\\) (with drift). Lorsque l‚Äôon compte pr√©dire des s√©ries saisonni√®res, nous devons ajouter un nouveau jeu d‚Äôordres \\((P,D,Q)m\\), o√π \\(P\\), \\(D\\) et \\(Q\\) sont √©quivalents √† leurs minuscules, mais portent sur des d√©calages saisonniers et non pas des d√©calages d‚Äôunit√©s de temps. L‚Äôordre \\(m\\) est le nombre de p√©riodes √† consid√©rer par unit√© temporelle, par exemple 12 mois par an. Bonne nouvelle: forecast::auto.arima() automatise le tout. Nous pouvons utiliser lambda = 0 pour effectuer une transformation logarithmique. flow_arima &lt;- flow_ts_train %&gt;% auto.arima(lambda = 0) flow_arima %&gt;% forecast(h = 36) %&gt;% autoplot() summary(flow_arima) ## Series: . ## ARIMA(1,0,0)(2,1,0)[12] ## Box Cox transformation: lambda= 0 ## ## Coefficients: ## ar1 sar1 sar2 ## 0.3029 -0.4913 -0.2687 ## s.e. 0.1074 0.1196 0.1186 ## ## sigma^2 estimated as 0.6561: log likelihood=-101.88 ## AIC=211.77 AICc=212.27 BIC=221.49 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set 9.038874 71.47442 40.46424 -34.75026 64.84494 0.7419961 -0.1823408 Le sommaire du mod√®le, ARIMA(1,0,0)(2,1,0)[12] sous le format \\(ARIMA(p,d,q)(P,D,Q)m\\), retourne automatiquement une p√©riode de 12 mois avec une diff√©renciation saisonni√®re mais sans diff√©renciation ordinaire, excluant la moyenne mobile dans les deux cas. Exercice. Il est toujours pertinent d‚Äôeffectuer l‚Äôanalyse des r√©sidus‚Ä¶ 11.3.4 Les mod√®les dynamiques J‚Äôai not√© pr√©c√©demment que l‚Äô√©volution du climat tient compte d‚Äôune s√©rie de covariables explicatives. De m√™me, le d√©bit dans la rivi√®re Chaudi√®re n‚Äôest pas un effet de la saison, mais de son environnement (climat, changements dans la morphologie du paysage, utilisation de l‚Äôeau, etc.). La pr√©vision du d√©bit aura avantage √† consid√©rer ces covariables. L‚ÄôARIMA peut accueillir des covariables en mod√©lisant le terme d‚Äôerreur, \\(\\epsilon_t\\) en fonction de s√©ries temporelles conjointes. Le d√©bit mensuel de la rivi√®re Chaudi√®re peut √™tre mod√©lis√© en fonction de la temp√©rature moyenne mensuelle et des pr√©cipitations totales mensuelles en ajoutant l‚Äôargument xreg √† la fonction forecast::auto.arima(). L‚Äôargument consiste en la matrice temporelle des variables explicatives. Notez que forecast::auto.arima() ne fonctionne pas (encore?) avec l‚Äôinterface-formule de R, mais que l‚Äôon peut se d√©brouiller en transformant en s√©rie temporelle la sortie de la fonction base::model.matrix(), qui elle peut accueillir une formule. hm_tr &lt;- window(hydrometeo_monthly_ts, end = c(2004, 12)) hm_te &lt;- window(hydrometeo_monthly_ts, start = c(2005, 1)) flow_darima &lt;- auto.arima(y = hm_tr[, &quot;D√©bit&quot;], xreg = hm_tr[, c(&quot;total_precip&quot;, &quot;mean_temp&quot;)], lambda = 0) summary(flow_darima) ## Series: hm_tr[, &quot;D√©bit&quot;] ## Regression with ARIMA(1,0,0)(0,1,1)[12] errors ## Box Cox transformation: lambda= 0 ## ## Coefficients: ## ar1 sma1 total_precip mean_temp ## 0.3213 -0.6236 0.0028 -0.0215 ## s.e. 0.1148 0.1828 0.0017 0.0451 ## ## sigma^2 estimated as 0.5392: log likelihood=-80.87 ## AIC=171.74 AICc=172.65 BIC=183.12 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set 3.046006 64.76333 37.60995 -31.06861 57.3032 0.7149168 -0.3068227 Note. Pour obtenir des r√©sultats plus pr√©cis, mais dont les r√©sultats seront plus longs √† venir, sp√©cifiez l‚Äôargument stepwise = FALSE dans la fonction forecast::auto.arima(). Les coefficients sur les covariables sont interpr√©tables dans l‚Äô√©chelle de la pr√©vision transform√©e. Ainsi, 1 mm de pr√©cipitation par mois augmentera le logarithme naturel du d√©bit augmente de 0.0028. De m√™me, 1 ¬∞C de temp√©rature moyenne diminuera le logarithme naturel du d√©bit augmente de 0.0215: notez que l‚Äôerreur standard sur ce coefficient √©tant tr√®s √©lev√©e, le coefficient n‚Äôest √† premi√®re vue pas diff√©rent de 0. La temp√©rature moyenne aurait avantage √† √™tre remplac√©e par un meilleur indicateur incluant les p√©riodes d‚Äôaccumulation de neige et de leur fonte. Pas mal doc? Source: Sc√®ne de Back to the future, Robert Zemeckis et and Bob Gale, 1985 La pr√©vision d‚Äôun mod√®le dynamique demandera les s√©ries temporelles des covariables, qui peuvent elles-m√™mes √™tre mod√©lis√©es ou √™tre issues de simulations. Dans notre cas, nous pouvons utiliser la s√©rie de test. flow_darima %&gt;% forecast(xreg = hm_te[, c(&quot;total_precip&quot;, &quot;mean_temp&quot;)]) %&gt;% autoplot() checkresiduals(flow_darima) ## ## Ljung-Box test ## ## data: Residuals from Regression with ARIMA(1,0,0)(0,1,1)[12] errors ## Q* = 16.853, df = 13, p-value = 0.2061 ## ## Model df: 4. Total lags used: 17 11.3.5 Les mod√®les TBATS Les mod√®les TBATS (Hyndman et Athanasopoulos, 2018) combinent tout ce que l‚Äôon a vu jusqu‚Äô√† pr√©sent, √† l‚Äôexception notable des covariables, dans une interface automatis√©e. L‚Äôautomatisation a l‚Äôavantage d‚Äôune utilisation rapide, mais donne parfois des pr√©dictions erron√©es. lynx_tbats &lt;- lynx %&gt;% tbats() lynx_tbats_f &lt;- lynx_tbats %&gt;% forecast() lynx_tbats_f %&gt;% autoplot() summary(lynx_tbats_f) ## ## Forecast method: BATS(0.159, {5,1}, 0.833, -) ## ## Model Information: ## BATS(0.159, {5,1}, 0.833, -) ## ## Call: tbats(y = .) ## ## Parameters ## Lambda: 0.159431 ## Alpha: 0.4096962 ## Beta: 0.06308072 ## Damping Parameter: 0.833291 ## AR coefficients: 1.166918 -0.79967 0.178396 -0.165326 -0.174982 ## MA coefficients: -0.391172 ## ## Seed States: ## [,1] ## [1,] 15.0399753 ## [2,] 0.4123534 ## [3,] 0.0000000 ## [4,] 0.0000000 ## [5,] 0.0000000 ## [6,] 0.0000000 ## [7,] 0.0000000 ## [8,] 0.0000000 ## attr(,&quot;lambda&quot;) ## [1] 0.1594313 ## ## Sigma: 1.542751 ## AIC: 1956.137 ## ## Error measures: ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set 52.88873 827.169 496.3222 -21.21278 50.42654 0.5973607 -0.04569254 ## ## Forecasts: ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 1935 3144.2286 1772.9365 5314.924 1279.84707 6900.473 ## 1936 2272.7187 842.6749 5351.691 464.08249 8064.647 ## 1937 1466.3925 410.3827 4224.271 185.16011 6932.120 ## 1938 981.1883 227.2540 3210.040 88.86649 5547.989 ## 1939 807.6476 170.9933 2800.598 62.30711 4954.037 ## 1940 902.9068 196.5571 3075.407 73.22121 5401.634 ## 1941 1286.8205 307.5223 4124.040 123.20099 7068.774 ## 1942 1964.6976 520.9678 5872.628 225.83614 9784.659 ## 1943 2722.5747 765.2794 7819.328 346.39977 12815.672 ## 1944 3100.2748 837.7743 9148.331 368.32463 15162.079 Le sommaire du mod√®le le type de mod√®le s√©lectionn√© ainsi que ses param√®tres. Le titre du graphique en donne aussi un aper√ßu: BATS(0.159, {5,1}, 0.833, -): Le coefficient lambda utilis√© est 0.159. Le {5, 1} signifie que p = 5 et et q = 1. L‚Äôadoucissement est de 0.833 Aucune p√©riode n‚Äôest incluse L‚Äôapproche TBATS performe bien pour les donn√©es √† fluctuations cycliques. Toutefois, les intervalles pr√©visionnels son souvent trop larges et l‚Äôoptimisation peut √™tre longue. 11.4 Pour terminer‚Ä¶ Nous avons vu comment manipuler des s√©ries temporelles avec dplyr et le format de base ts. Le nouveau format de s√©rie temporelle du module tsibble permet des manipulations dans un flux de travail plus conforme au tidyverse. De m√™me, la nouvelle mouture du module forecast nomm√©e fable, r√©√©crite vers l‚Äôapproche tidyverse, offrira des fonctions permettant notamment une hi√©rarchisation dans les fluctuations saisonni√®res, par exemple des cycles journaliers ench√¢ss√©s dans des cycles hebdomadaires, ench√¢ss√©s dans des cycles trimestriels. Le module prophet, distribu√© par Facebook en mode open source, gagne en popularit√©. Bien qu‚Äôil soit r√©put√© pour offrir des pr√©visions fiables, ses bases math√©matiques me semblent insuffisamment document√©es. Fait int√©ressant: prophet est en mesure d‚Äôeffectuer des pr√©visions en mode dynamique. Quoi qu‚Äôil en soit, j‚Äôai favoris√© des modules plus matures et mieux document√©s, qui pourront vous servir de tremplin vers les nouveaux modules. Maintenant, le futur vous appartient. "],
["chapitre-ml.html", "12 Introduction √† l‚Äôautoapprentissage 12.1 Lexique 12.2 D√©marche 12.3 L‚Äôautoapprentissage en R 12.4 Algorithmes", " 12 Introduction √† l‚Äôautoapprentissage Ô∏è¬†Objectifs sp√©cifiques: √Ä la fin de ce chapitre, vous saurez √©tablir un plan de mod√©lisation par autoapprentissage saurez d√©finir le sous-apprentissage et le surapprentissage serez en mesure d‚Äôeffectuer un autoapprentissage avec les techniques des k-proches voisins, les arbres de d√©cision, les for√™ts al√©atoires, les r√©seaux neuronnaux et les processus gaussiens Plusieurs cas d‚Äôesp√®ces en sciences et g√©nies peuvent √™tre approch√©s en liant un variable avec une ou plusieurs autres √† l‚Äôaide de r√©gressions lin√©aires, polynomiales, sinuso√Ødales, exponentielle, sigmo√Ødales, etc. Encore faut-il s‚Äôassurer que ces formes pr√©√©tablies repr√©sentent le ph√©nom√®ne de mani√®re fiable. Lorsque la forme de la r√©ponse est difficile √† envisager, en particulier dans des cas non-lin√©aires ou impliquant plusieurs variables, on pourra faire appel √† des mod√®les dont la structure n‚Äôest pas contr√¥l√©e par une √©quation rigide gouvern√©e par des param√®tres (comme la pente ou l‚Äôintercept). L‚Äôautoapprentissage, apprentissage automatique, ou machine learning, vise √† d√©tecter des structures complexes √©mergeant d‚Äôensembles de donn√©es √† l‚Äôaide des math√©matiques et de processus automatis√©s afin de pr√©dire l‚Äô√©mergence de futures occurrences. Comme ensemble de techniques empiriques, l‚Äôautoapprentissage est un cas particulier de l‚Äôintelligence artificielle, qui elle inclut aussi les m√©canismes d√©terministes et des ensembles d‚Äôop√©rations logiques. Par exemple, les premiers ordinateurs √† comp√©titionner aux √©checs se basaient sur des r√®gles de logique (si la reine noire est positionn√©e en c3 et qu‚Äôun le fou blanc est en position f6 et que ‚Ä¶ alors bouge la tour en g5 - j‚Äô√©cris n‚Äôimporte quoi). Un jeu simple d‚Äôintelligence artificielle consiste √† lancer une marche al√©atoire, par exemple bouger √† chaque pas d‚Äôune distance au hasard en x et y, puis de recalculer le pas s‚Äôil arrive dans une bo√Æte d√©finie (figure 12.1). Dans les deux cas, il s‚Äôagit d‚Äôintelligence artificielle, mais pas d‚Äôautoapprentissage. Figure 12.1: Petite tortue, n‚Äôentre pas dans la bo√Æte! L‚Äôautoapprentissage passera davantage par la simulation de nombreuses parties et d√©gagera la structure optimale pour l‚Äôemporter consid√©rant les positions des pi√®ces sur l‚Äô√©chiquier. 12.1 Lexique L‚Äôautoapprentissage poss√®de son jargon particulier. Puisque certains termes peuvent porter √† confusion, voici quelques d√©finitions de termes que j‚Äôutiliserai dans ce chapitre. R√©ponse. La variable que l‚Äôon cherche √† obtenir. Il peut s‚Äôagir d‚Äôune variable continue comme d‚Äôune variable cat√©gorielle. On la nomme aussi la cible. Pr√©dicteur. Une variable utilis√©e pour pr√©dire une r√©ponse. Les pr√©dicteurs sont des variables continues. Les pr√©dicteurs de type cat√©goriel doivent pr√©alablement √™tre dummifi√©s (voir chapitre 5). On nomme les pr√©dicteurs les entr√©es. Apprentissage supervis√© et non-supervis√©. Si vous avez suivi le cours jusqu‚Äôici, vous avez d√©j√† utilis√© des outils entrant dans la grande famille de l‚Äôapprentissage automatique. La r√©gression lin√©aire, par exemple, vise √† minimiser l‚Äôerreur sur la r√©ponse en optimisant les coefficients de pente et l‚Äôintercept. Un apprentissage supervis√© a une cible, comme c‚Äôest le cas de la r√©gression lin√©aire. En revanche, un apprentissage non supervis√© n‚Äôen a pas: on laisse l‚Äôalgorithme le soin de d√©tecter des structures int√©ressantes. Nous avons d√©j√† utilis√© cette approche. Pensez-y un peu‚Ä¶ l‚Äôanalyse en composante principale ou en coordonn√©es principales, ainsi que le partitionnement hi√©rarchique ou non, couverts au chapitre 9, sont des exemples d‚Äôapprentissage non supervis√©. En revanche, l‚Äôanalyse de redondance a une r√©ponse. L‚Äôanalyse discriminante aussi, bien que sa r√©ponse soit cat√©gorielle. L‚Äôapprentissage non supervis√© ayant d√©j√† √©t√© couvert (sans le nommer) au chapitre 9, ce chapitre ne s‚Äôint√©resse qu‚Äô√† l‚Äôapprentissage supervis√©. R√©gression et Classification. Alors que la r√©gression est un type d‚Äôapprentissage automatique pour les r√©ponses continues, la classification vise √† pr√©dire une r√©ponse cat√©gorielle. Il existe des algorithmes uniquement application √† la r√©gression, uniquement applicables √† la classification, et plusieurs autres adaptable aux deux situations. Donn√©es d‚Äôentra√Ænement et donn√©es de test. Lorsque l‚Äôon g√©n√®re un mod√®le, on d√©sire qu‚Äôil sache comment r√©agir √† ses pr√©dicteurs. Cela se fait avec des donn√©es d‚Äôentra√Ænement, sur lesquelles on calibre et valide le mod√®le. Les donn√©es de test servent √† v√©rifier si le mod√®le est en mesure de pr√©dire des r√©ponses sur lesquelles il n‚Äôa pas √©t√© entra√Æn√©. Fonction de perte. Une fonction qui mesure l‚Äôerreur d‚Äôun mod√®le. 12.2 D√©marche La premi√®re t√¢che est d‚Äôexplorer les donn√©es, ce que nous avons couvert au chapitres 3 et 4. 12.2.1 Pr√©traitement Pour la plupart des techniques d‚Äôautoapprentissage, le choix de l‚Äô√©chelle de mesure est d√©terminant sur la mod√©lisation subs√©quente. Par exemple, un algorithme bas√© sur la distance comme les k plus proches voisins ne mesurera pas les m√™mes distances entre deux observations si l‚Äôon change l‚Äôunit√© de mesure d‚Äôune variable du m√®tre au kilom√®tre. Il est donc important d‚Äôeffectuer, ou d‚Äôenvisager la possibilit√© d‚Äôeffectuer un pr√©traitement sur les donn√©es. Je vous r√©f√®re au chapitre 8 pour plus de d√©tails sur le pr√©traitement. 12.2.2 Entra√Ænement et test Vous connaissez peut-√™tre l‚Äôexpression sportive ‚Äúavoir l‚Äôavantage du terrain‚Äù. Il s‚Äôagit d‚Äôun principe pr√©tendant que les athl√®tes performent mieux en terrain connu. Idem pour les mod√®les ph√©nom√©nologiques. Il est possible qu‚Äôun mod√®le fonctionne tr√®s bien sur les donn√©es avec lesquelles il a √©t√© entra√Æn√©, mais tr√®s mal sur des donn√©es externes. De mauvaises pr√©dictions effectu√©es √† partir d‚Äôun mod√®le qui semblait bien se comporter peut mener √† des d√©cisions qui, pourtant prises de mani√®re confiante, se r√©v√®lent fallacieuses au point d‚Äôaboutir √† de graves cons√©quences. C‚Äôest pourquoi, en mode pr√©dictif, on doit √©valuer la pr√©cision et la justesse d‚Äôun mod√®le sur des donn√©es qui n‚Äôont pas √©t√© utilis√©s dans son entra√Ænement. En pratique, il convient de s√©parer un tableau de donn√©es en deux: un tableau d‚Äôentra√Ænement et un tableau de test. Il n‚Äôexiste pas de standards sur la proportion √† utiliser dans l‚Äôun et l‚Äôautre. Cela d√©pend de la prudence de l‚Äôanalyse et de l‚Äôampleur de son tableau de donn√©es. Dans certains cas, nous pr√©f√©rerons couper le tableau √† 50%. Dand d‚Äôautres, nous pr√©f√©rerons r√©server le deux-tiers des donn√©es pour l‚Äôentra√Ænement, ou 70%, 75%. Rarement, toutefois, r√©servera-t-on moins plus de 50% et moins de 20% √† la phase de test. Si les donn√©es sont peu √©quilibr√©es (par exemple, on retrouve peu de donn√©es de l‚Äôesp√®ce \\(A\\), que l‚Äôon retrouve peu de donn√©es √† un pH inf√©rieur √† 5 ou que l‚Äôon a peu de donn√©es crois√©es de l‚Äôesp√®ce \\(A\\) √† ph inf√©rieur √† 5), il y a un danger qu‚Äôune trop grande part, voire toute les donn√©es, se retrouvent dans le tableau d‚Äôentra√Ænement (certaines situations ne seront ainsi pas test√©es) ou dans le tableau de test (certaines situations ne seront pas couvertes par le mod√®le). L‚Äôanalyste doit s‚Äôassurer de s√©parer le tableau au hasard, mais de mani√®re consciencieuse. 12.2.3 Sousapprentissage et surapprentissage Une difficult√© en mod√©lisation ph√©nom√©nologique est ce qui tient de la structure et ce qui tient du bruit. Lorsque l‚Äôon consid√®re une structure comme du bruit, on est dans un cas de sousapprentissage. Lorsque, au contraire, on interpr√®te du bruit comme une structure, on est en cas de surapprentissage. Les graphiques de la figure 12.2 pr√©sentent ces deux cas, avec au centre un cas d‚Äôapprentissage conforme. Figure 12.2: Cas de figure de m√©sapprentissage. √Ä gauche, sous-apprentissage. Au centre, apprentissage valide. √Ä droite, surapprentissage. Il est n√©anmoins difficile d‚Äôinspecter un mod√®le comprenant plusieurs entr√©es. On d√©tectera le m√©sapprentissage lorsque la pr√©cision d‚Äôun mod√®le est lourdement alt√©r√©e en phase de test. Une mani√®re de limiter le m√©sapprentissage est d‚Äôavoir recours √† la validation crois√©e. 12.2.4 Validation crois√©e Souvent confondue avec le fait de s√©parer le tableau en phases d‚Äôentra√Ænement et de test, la validation crois√©e est un principe incluant plusieurs algorithmes qui consiste √† entra√Æner le mod√®le sur un √©chantillonnage al√©atoire des donn√©es d‚Äôentra√Ænement. La technique la plus utilis√©e est le k-fold, o√π l‚Äôon s√©pare al√©atoirement le tableau d‚Äôentra√Ænement en un nombre k de tableaux. √Ä chaque √©tape de la validation crois√©e, on calibre le mod√®le sur tous les tableaux sauf un, puis on valide le mod√®le sur le tableau exclu. La performance du mod√®le en entra√Ænement est jug√©e sur les validations. 12.2.5 Choix de l‚Äôalgorithme d‚Äôapprentissage Face aux centaines d‚Äôalgorithmes d‚Äôapprentissages qui vous sont offertes, choisir l‚Äôalgorithme (ou les algorithmes) ad√©quats pour votre probl√®me n‚Äôest pas une t√¢che facile. Ce choix sera motiv√© par les tenants et aboutissants des algorithmes, votre exp√©rience, l‚Äôexp√©rience de la litt√©rature, l‚Äôexp√©rience de vos coll√®gues, etc. √Ä moins d‚Äô√™tre particuli√®rement surdou√©.e, il vous sera pratiquement impossible de ma√Ætriser la math√©matique de chacun d‚Äôeux. Une approche raisonnable est de tester plusieurs mod√®les, de retenir les mod√®les qui semblent les plus pertinents, et d‚Äôapprofondir si ce n‚Äôest d√©j√† fait la math√©matique des options retenues. Ajoutons qu‚Äôil existe des algorithmes g√©n√©tiques, qui ne sont pas couverts ici, qui permettent de s√©lectionner des mod√®les d‚Äôautoapprentissages optimaux. Un de ces algorithmes est offert par le module Python tpot. 12.2.6 D√©ploiement Nous ne couvrirons pas la phase de d√©ploiement d‚Äôun mod√®le. Notons seulement qu‚Äôil est possible, en R, d‚Äôexporter un mod√®le dans un fichier .Rdata, qui pourra √™tre charg√© dans un autre environnement R. Cet environnement peut √™tre une feuille de calcul comme une interface visuelle mont√©e, par exemple, avec Shiny (chapitre 8). En r√©sum√©, Explorer les donn√©es S√©lectionner des algorithmes Effectuer un pr√©traitement Cr√©er un ensemble d‚Äôentra√Ænement et un ensemble de test Lisser les donn√©es sur les donn√©es d‚Äôentra√Ænement avec validation crois√©e Tester le mod√®le D√©ployer le mod√®le 12.3 L‚Äôautoapprentissage en R Plusieurs options sont disponibles. Les modules que l‚Äôon retrouve en R pour l‚Äôautoapprentissage sont nombreux, et parfois sp√©cialis√©s. Il est possible de les utiliser individuellement. Chacun de ces modules fonctionne √† sa fa√ßon. Le module caret de R a √©t√© con√ßu pour donner acc√®s √† des centaines de fonctions d‚Äôautoapprentissage via une interface commune. Le module mlr occupe sensiblement le m√™me cr√©neau que caret, mais utilise plut√¥t une approche par objets connect√©s. Au moment d‚Äô√©crire ces lignes, mlr est peu document√©, donc a priori plus complexe √† prendre en main. En Python, le module scikit-learn offre un interface unique pour l‚Äôutilisation de nombreuses techniques d‚Äôautoapprentissage. Il est possible d‚Äôappeler des fonctions de Python √† partir de R gr√¢ce au module reticulate. Dans ce chapitre, nous verrons comment fonctionnent certains algorithmes s√©lectionn√©s, puis nous les appliquerons avec le module respectif qui m‚Äôa sembl√© le plus appropri√©. library(&quot;tidyverse&quot;) # √©videmment library(&quot;caret&quot;) ## ## Attaching package: &#39;caret&#39; ## The following object is masked from &#39;package:compositions&#39;: ## ## R2 ## The following object is masked from &#39;package:pls&#39;: ## ## R2 ## The following object is masked from &#39;package:vegan&#39;: ## ## tolerance ## The following object is masked from &#39;package:purrr&#39;: ## ## lift 12.4 Algorithmes Il existe des centaines d‚Äôalgorithmes d‚Äôapprentissage. Je n‚Äôen couvrirai que quatre, qui me semblent √™tre appropri√©s pour la mod√©lisation ph√©nom√©nologique en agro√©cologie, et utilisables autant pour la r√©gression et la classification. Les k plus proches voisins Les arbres de d√©cision Les r√©seaux neuronaux Les processus gaussiens 12.4.1 Les k plus proches voisins Figure 12.3: &lt;&lt; Le‚Ä¶ l‚Äôid√©e en arri√®re pour √™tre‚Ä¶ euh‚Ä¶ simpliste, l√† c‚Äôest que c‚Äôest un peu de‚Ä¶ euhmm‚Ä¶ de la vitamine de vinyle.&gt;&gt; - Georges (Les voisins, une pi√®ce de Claude Meunier). Pour dire comme Georges, le‚Ä¶ l‚Äôid√©e en arri√®re des KNN pour √™tre‚Ä¶ euh‚Ä¶ simpliste, c‚Äôest qu‚Äôun objet va ressembler √† ce qui se trouve dans son voisinage. Les KNN se basent en effet sur une m√©trique de distance pour rechercher un nombre k de points situ√©s √† proximit√© de la mesure. Les k points les plus proches sont retenus, k √©tant un entier non nul √† optimiser. Un autre param√®tre parfois utilis√© est la distance maximale des voisins √† consid√©rer: un voisin trop √©loign√© pourra √™tre discart√©. La r√©ponse attribu√©e √† la mesure est calcul√©e √† partir de la r√©ponse des k voisins retenus. Dans le cas d‚Äôune r√©gression, on utiliser g√©n√©ralement la moyenne. Dans le cas de la classification, la mesure prendra la cat√©gorie qui sera la plus pr√©sente chez les k plus proches voisins. L‚Äôalgorithme des k plus proches voisins est relativement simple √† comprendre. Certains pi√®ges sont, de m√™me, peuvent √™tre contourn√©s facilement. Imaginez que vous rechercher les points les plus rapproch√©s dans un syst√®me de coordonn√©es g√©ographiques o√π les coordonn√©es \\(x\\) sont exprim√©es en m√®tres et les coordonn√©es \\(y\\), en centim√®tres. Vous y projetez trois points (figure 12.4). Figure 12.4: Distances entre les points pour utilisation avec les KNN Techniquement la distance A-B est 100 plus √©lev√©e que la distance A-C, mais l‚Äôalgorithme ne se soucie pas de la m√©trique que vous utilisez (figure 12.4). Il est primordial dans ce cas d‚Äôutiliser la m√™me m√©trique. Cette strat√©gie est √©vidente lorsque les variables sont comparables. C‚Äôest rarement le cas, que ce soit lorsque l‚Äôon compare des dimensions physionomiques (la longueur d‚Äôune phalange ou celle d‚Äôun f√©mur) mais lorsque les variables incluent des m√©langes de longueurs, des pH, des d√©comptes, etc., il est important de bien identifier la m√©trique et le type de distance qu‚Äôil convient le mieux d‚Äôutiliser. En outre, la standardisation des donn√©es √† une moyenne de z√©ro et √† un √©cart-type de 1 est une approche courrament utilis√©e. 12.4.1.1 Exemple d‚Äôapplication Pour ce premier exemple, je pr√©senterai un cheminement d‚Äôautoapprentissage, du pr√©traitement au test. Nous allons essayer de classer les esp√®ces de dragon selon leurs dimensions. Figure 12.5: Dimensions mesur√©s sur les dragons captur√©s. dragons &lt;- read_csv(&quot;data/11_dragons.csv&quot;) ## Parsed with column specification: ## cols( ## V1 = col_double(), ## V2 = col_double(), ## V3 = col_double(), ## V4 = col_double(), ## V5 = col_double(), ## V6 = col_double(), ## V7 = col_double(), ## V8 = col_double(), ## V9 = col_double(), ## V10 = col_double(), ## V11 = col_double(), ## ID = col_double(), ## Species = col_character() ## ) Assurons-nous que les donn√©es sont toutes √† l‚Äô√©chelle. Nous pourrions utiliser la fonction scale(). Toutefois, si je capture un nouveau dragon, je n‚Äôaurai pas l‚Äôinformation pour convertir mes nouvelles dimensions dans la m√™me m√©trique que celle utilis√©e pour lisser mon mod√®le. Prenez donc soin de conserver la moyenne et l‚Äô√©cart-type pour subs√©quemment calculer des mises √† l‚Äô√©chelle. dim_means &lt;- dragons %&gt;% dplyr::select(starts_with(&quot;V&quot;)) %&gt;% summarise_all(mean, na.rm = TRUE) dim_sds &lt;- dragons %&gt;% dplyr::select(starts_with(&quot;V&quot;)) %&gt;% summarise_all(sd, na.rm = TRUE) dragons_sc &lt;- dragons %&gt;% dplyr::select(starts_with(&quot;V&quot;)) %&gt;% scale(.) %&gt;% as_tibble() %&gt;% mutate(Species = dragons$Species) S√©parons les donn√©es en entra√Ænement (_tr) et en test (_te) avec une proportion 70/30 (p = 0.7). Il est essentiel d‚Äôutiliser set.seed() pour s‚Äôassurer que la partition soit la m√™me √† chaque session de code (pour la reproductibilit√©) - j‚Äôai l‚Äôhabitude de taper n‚Äôimporte quel num√©ro √† environ 6 chiffres, mais lors de publications, je vais sur random.org et je g√©n√®re un num√©ro au hasard, sans biais. set.seed(68017) id_tr &lt;- createDataPartition(dragons_sc$Species, p = 0.7, list = FALSE) dragons_tr &lt;- dragons_sc[id_tr, ] dragons_te &lt;- dragons_sc[-id_tr, ] Avant de lancer nos calculs, allons vois sur la page de caret les modules qui effectuent des KNN pour la classification. Nous trouvons knn et kknn. Si les modules n√©cessaires aux calculs ne sont pas install√©s sur votre ordinateur, caret vous demandera de les installer. Prenons le module kknn, qui demande le param√®tre kmax, soit le nombre de voisins √† consid√©rer, ainsi qu‚Äôun param√®tre de distance (sp√©cifiez 1 pour la distance de Mahattan et 2 pour la distance euclidienne), et un kernel, qui est une fonction pour mesurer la distance. Comment choisir les bons param√®tres? Une mani√®re de proc√©der est de cr√©er une grille de param√®tres. kknn_grid &lt;- expand.grid(kmax = 3:6, distance = 1:2, kernel = c(&quot;rectangular&quot;, &quot;gaussian&quot;, &quot;optimal&quot;)) Les noms des colonnes de la grille doivent correspondre aux noms des param√®tres du mod√®le. Nous allons mod√©liser avec une validation crois√©e √† 5 plis. ctrl &lt;- trainControl(method=&quot;repeatedcv&quot;, repeats = 5) Pour finalement lisser le mod√®le. set.seed(8961704) clf &lt;- train(Species ~ ., data = dragons_tr, method = &quot;kknn&quot;, tuneGrid = kknn_grid, trainControl = ctrl) clf ## k-Nearest Neighbors ## ## 34 samples ## 11 predictors ## 5 classes: &#39;Dragon de caverne&#39;, &#39;Dragon de feu&#39;, &#39;Dragon de mer&#39;, &#39;Dragon de pierre&#39;, &#39;Dragon de saturne&#39; ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 34, 34, 34, 34, 34, 34, ... ## Resampling results across tuning parameters: ## ## kmax distance kernel Accuracy Kappa ## 3 1 rectangular 0.8914566 0.8572135 ## 3 1 gaussian 0.8914566 0.8572135 ## 3 1 optimal 0.8914566 0.8572135 ## 3 2 rectangular 0.8601133 0.8164023 ## 3 2 gaussian 0.8601133 0.8164023 ## 3 2 optimal 0.8601133 0.8164023 ## 4 1 rectangular 0.8914566 0.8572135 ## 4 1 gaussian 0.8914566 0.8572135 ## 4 1 optimal 0.8914566 0.8572135 ## 4 2 rectangular 0.8528406 0.8070909 ## 4 2 gaussian 0.8601133 0.8164023 ## 4 2 optimal 0.8601133 0.8164023 ## 5 1 rectangular 0.8881233 0.8533711 ## 5 1 gaussian 0.8881233 0.8533711 ## 5 1 optimal 0.8914566 0.8572135 ## 5 2 rectangular 0.8528406 0.8071905 ## 5 2 gaussian 0.8601133 0.8165018 ## 5 2 optimal 0.8601133 0.8164023 ## 6 1 rectangular 0.8881233 0.8533711 ## 6 1 gaussian 0.8881233 0.8533711 ## 6 1 optimal 0.8914566 0.8572135 ## 6 2 rectangular 0.8528406 0.8071905 ## 6 2 gaussian 0.8601133 0.8165018 ## 6 2 optimal 0.8601133 0.8164023 ## ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were kmax = 6, distance = 1 and kernel = optimal. Nous obtenons les param√®tres du mod√®le optimal. Pr√©disons l‚Äôesp√®ce de dragons selon ses dimensions pour chacun des tableaux. pred_tr &lt;- predict(clf) pred_te &lt;- predict(clf, newdata = dragons_te) Une mani√®re d‚Äô√©valuer la pr√©diction est d‚Äôafficher un tableau de contingence. table(dragons_tr$Species, pred_tr) ## pred_tr ## Dragon de caverne Dragon de feu Dragon de mer Dragon de pierre Dragon de saturne ## Dragon de caverne 6 0 0 0 0 ## Dragon de feu 0 7 0 0 0 ## Dragon de mer 0 0 7 0 0 ## Dragon de pierre 0 0 0 7 0 ## Dragon de saturne 0 0 0 0 7 table(dragons_te$Species, pred_te) ## pred_te ## Dragon de caverne Dragon de feu Dragon de mer Dragon de pierre Dragon de saturne ## Dragon de caverne 2 0 0 0 0 ## Dragon de feu 0 2 0 0 0 ## Dragon de mer 0 0 3 0 0 ## Dragon de pierre 0 0 0 3 0 ## Dragon de saturne 0 0 0 1 2 Les esp√®ces de dragon sont toutes bien class√©es tant entra√Ænement qu‚Äôen test (c‚Äôest rarement le cas dans les situations r√©elles). 12.4.2 Les arbres d√©cisionnels Figure 12.6: Les Ents, tir√© du film le Seigneur des anneaux, qui prennent trop de temps avant de se d√©cider - paradoxalement, les abrbres de d√©cisions sont dot√©s d‚Äôalgorithmes rapides. Un arbre d√©cisionnel est une collection hi√©rarchis√©e de d√©cisions, le plus souvent binaires. Chaque embranchement est un test √† vrai ou faux sur une variable. La r√©ponse, que ce soit une cat√©gorie ou une valeur num√©rique, se trouve au bout de la derni√®re branche. Les suites de d√©cisions sont organis√©es de mani√®re √† ce que la pr√©cision de la r√©ponse soit optimis√©e. Ils ont l‚Äôavantage de pouvoir √™tre exprim√©s en un sch√©ma simple et imprimable. Figure 12.7: Exemple d‚Äôarbre de d√©cision, tir√© du blogue de Jeremy Jordon. Les arbres sont notamment param√©tr√©s par le nombre maximum d‚Äôembranchements, qui s‚Äôil est trop √©lev√© peut mener √† du surapprentissage. Il existe de nombreux algorithmes d‚Äôarbres de d√©cision. Une collection d‚Äôarbres devient une for√™t. Les for√™ts al√©atoires (random forest) sont une cat√©gorie d‚Äôalgorithmes compos√©s de plusieurs arbres de d√©cision optimis√©s sur des donn√©es r√©pliqu√©es al√©atoirement par bagging. Allons-y par √©tape. √Ä partir des donn√©es existantes compos√©es de n observations (donc n lignes) s√©lectionn√©es pour l‚Äôentra√Ænement, √©chantillonnons au hasard avec remplacement un nombre n de nouvelles observations. Le remplacement implique qu‚Äôon retrouvera fort probablement dans notre nouveau tableau des lignes identiques. Lissons un arbre sur notre tableau al√©atoire. Effectuons un nouveau tirage, puis un autre arbre. Puis encore, et encore, disons 10 fois. Nous obtiendrons une for√™t de 10 arbres. Pour une nouvelle observation √† pr√©dire, nous obtenons donc 10 pr√©dictions, sur lesquelles nous pouvons effectuer un moyenne s‚Äôil s‚Äôagit d‚Äôune variable num√©rique, ou bien prenons la cat√©gorie la plus souvent pr√©dite dans le cas d‚Äôune classification. Les for√™ts al√©atoires peuvent √™tre constitu√©s de 10, 100, 1000 arbres: autant qu‚Äôil en est n√©cessaire. 12.4.2.1 Exemple d‚Äôapplication Utilisons toujours nos donn√©es de dimensions de dragons. Bien qu‚Äôil en existe plusieurs, le module conventionnel pour effectuer un arbre de d√©cision est rpart2. Sur la page de caret, nous trouvons rpart2, apte pour les classifications et les r√©gressions, qui n‚Äôa besoin que du param√®tre maxdepth. rpart2_grid &lt;- expand.grid(maxdepth = 3:10) # expand_grid n&#39;est pas n√©cessaire ici Prenons 5 plis encore une fois. ctrl &lt;- trainControl(method=&quot;repeatedcv&quot;, repeats = 5) Pour finalement lisser le mod√®le. set.seed(3468973) clf &lt;- train(Species ~ ., data = dragons_tr, method = &quot;rpart2&quot;, tuneGrid = rpart2_grid) clf ## CART ## ## 34 samples ## 11 predictors ## 5 classes: &#39;Dragon de caverne&#39;, &#39;Dragon de feu&#39;, &#39;Dragon de mer&#39;, &#39;Dragon de pierre&#39;, &#39;Dragon de saturne&#39; ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 34, 34, 34, 34, 34, 34, ... ## Resampling results across tuning parameters: ## ## maxdepth Accuracy Kappa ## 3 0.4162868 0.2872798 ## 4 0.4162868 0.2872798 ## 5 0.4162868 0.2872798 ## 6 0.4162868 0.2872798 ## 7 0.4162868 0.2872798 ## 8 0.4162868 0.2872798 ## 9 0.4162868 0.2872798 ## 10 0.4162868 0.2872798 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was maxdepth = 3. Nous obtenons les param√®tres du mod√®le optimal: maxdepth = 3 - puisque c‚Äôest √† la limite inf√©rieure de la grille, mieux vaudrait √©tendre la grille, mais passons pour l‚Äôexemple. Comme je l‚Äôai mentionn√©, un arbre de d√©cision est un outil convivial √† visualiser. plot(clf$finalModel) text(clf$finalModel) Ou en plus beau, je vous laisse essayer. library(&quot;rattle&quot;) fancyRpartPlot(clf$finalModel) Tout comme pour les KNN, pr√©disons l‚Äôesp√®ce de dragons selon ses dimensions pour chacun des tableaux. pred_tr &lt;- predict(clf) pred_te &lt;- predict(clf, newdata = dragons_te) En ce qui a trait aux tableaux de contigence‚Ä¶ table(dragons_tr$Species, pred_tr) ## pred_tr ## Dragon de caverne Dragon de feu Dragon de mer Dragon de pierre Dragon de saturne ## Dragon de caverne 0 0 0 0 6 ## Dragon de feu 0 7 0 0 0 ## Dragon de mer 0 0 7 0 0 ## Dragon de pierre 0 0 0 7 0 ## Dragon de saturne 0 0 0 0 7 table(dragons_te$Species, pred_te) ## pred_te ## Dragon de caverne Dragon de feu Dragon de mer Dragon de pierre Dragon de saturne ## Dragon de caverne 0 1 0 0 1 ## Dragon de feu 0 2 0 0 0 ## Dragon de mer 0 0 3 0 0 ## Dragon de pierre 0 0 0 3 0 ## Dragon de saturne 0 0 0 0 3 Les esp√®ces de dragon sont toutes bien class√©es en entra√Ænement et en test‚Ä¶ sauf pour les dragons de caverne, qui (l‚Äôavez-vous remarquez?) n‚Äôapparaissent pas dans l‚Äôarbre de d√©cision! Le module caret vient avec la fonction varImp() qui offre une appr√©ciation de l‚Äôimportance des variables dans le mod√®le final. La notion d‚Äôimportance varie d‚Äôun mod√®le √† l‚Äôautre, et reste √† ce jour mal document√©. Mieux vaut en examiner les tenants et aboutissants avant d‚Äôinterpr√©ter exessivement la sortie de cette fonction. varImp(clf) %&gt;% plot(.) On pourra effectuer de la m√™me mani√®re une for√™t al√©atoire, mais cette fois-ci avec le module rf. set.seed(3468973) ctrl &lt;- trainControl(method=&quot;repeatedcv&quot;, repeats = 5) clf &lt;- train(Species ~ ., data = dragons_tr, method = &quot;rf&quot;) clf ## Random Forest ## ## 34 samples ## 11 predictors ## 5 classes: &#39;Dragon de caverne&#39;, &#39;Dragon de feu&#39;, &#39;Dragon de mer&#39;, &#39;Dragon de pierre&#39;, &#39;Dragon de saturne&#39; ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 34, 34, 34, 34, 34, 34, ... ## Resampling results across tuning parameters: ## ## mtry Accuracy Kappa ## 2 0.9018266 0.8776655 ## 6 0.9555287 0.9441745 ## 11 0.9524518 0.9401765 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was mtry = 6. Et les r√©sultats. pred_tr &lt;- predict(clf) pred_te &lt;- predict(clf, newdata = dragons_te) table(dragons_te$Species, pred_te) ## pred_te ## Dragon de caverne Dragon de feu Dragon de mer Dragon de pierre Dragon de saturne ## Dragon de caverne 2 0 0 0 0 ## Dragon de feu 0 2 0 0 0 ## Dragon de mer 0 0 3 0 0 ## Dragon de pierre 0 0 0 3 0 ## Dragon de saturne 0 0 0 0 3 table(dragons_tr$Species, pred_tr) ## pred_tr ## Dragon de caverne Dragon de feu Dragon de mer Dragon de pierre Dragon de saturne ## Dragon de caverne 6 0 0 0 0 ## Dragon de feu 0 7 0 0 0 ## Dragon de mer 0 0 7 0 0 ## Dragon de pierre 0 0 0 7 0 ## Dragon de saturne 0 0 0 0 7 Notez que les for√™ts al√©atoires ne g√©n√®re par de visuel. 12.4.3 Les r√©seaux neuronaux Apr√®s les KNN et les random forests, nous passons au domaine plus complexe des r√©seaux neuronaux. Le terme r√©seau neuronal est une m√©taphore li√©e √† une perception que l‚Äôon avait du fonctionnement du cerveau humain lorsque la technique des r√©seaux neuronaux a √©t√© d√©velopp√©e dans les ann√©es 1950. Un r√©seau neuronal comprend une s√©rie de bo√Ætes d‚Äôentr√©es li√©e √† des fonctions qui transforment et acheminent successivement l‚Äôinformation jusqu‚Äô√† la sortie d‚Äôune ou plusieurs r√©ponse. Il existe plusieurs formes de r√©seaux neuronnaux, dont la plus simple manifestation est le perceptron multicouche. Dans l‚Äôexemple de la figure 12.8, on retrouve 4 variables d‚Äôentr√©e et trois variables de sortie entre lesquelles on retrouve 5 couches dont le nombre de neurones varient entre 3 et 6. Figure 12.8: R√©seau neuronal sch√©matis√©, Source: Neural designer. Entre la premi√®re couche de neurones (les variables pr√©dictives) et la derni√®re couche (les variables r√©ponse), on retrouve des couches cach√©es. Chaque neurone est reli√© √† tous les neurones de la couche suivante. Les liens sont des poids, qui peuvent prendre des valeurs dans l‚Äôensemble des nombres r√©els. √Ä chaque neurone suivant la premi√®re couche, on fait la somme des poids multipli√©s par la sortie du neurone. Le nombre obtenu entre dans chaque neurone de la couche. Le neurone est une fonction, souvent tr√®s simple, qui transforme le nombre. La fonction plus utilis√©e est probablement la fonction ReLU, pour rectified linear unit, qui expulse le m√™me nombre aux neurones de la prochaine couche s‚Äôil est positif: sinon, il expulse un z√©ro. Exercice. Si tous les neurones sont des fonctions ReLU, calculez la sortie de ce petit r√©seau neuronal. Vous trouverez la r√©ponse sur l‚Äôimage images/11_nn_ex1_R.jpg. Il est aussi possible d‚Äôajouter un biais √† chaque neurone, qui est un nombre r√©el additionn√© √† la somme des neurones pond√©r√©e par les poids. L‚Äôoptimisation les poids pour chaque lien et les biais pour chaque neurone (gr√¢ce √† des algorithmes dont le fonctionnement sort du cadre de ce cours) constitue le processus d‚Äôapprentissage. Avec l‚Äôaide de logiciels et de modules sp√©cialis√©s, la construction de r√©seaux de centaines de neurones organis√©s en centaines de couches vous permettra de capter des patrons complexes dans des ensembles de donn√©es. Vous avez peut-√™tre d√©j√† entendu parler d‚Äôapprentissage profond (ou deep learning). Il s‚Äôagit simplement d‚Äôune appellation des r√©seaux neuronaux modernis√© pour insister sur la pr√©sence de nombreuses couches de neurones. C‚Äôest un terme √† la mode. 12.4.3.1 Les r√©seaux neuronaux sur R avec neuralnet Plusieurs modules sont disponibles sur R pour l‚Äôapprentissage profond. Certains utilisent le module H2O.ia, propuls√© en Java, d‚Äôautres utilisent plut√¥t Keras, propuls√© en Python par l‚Äôinterm√©diaire de Tensorflow. J‚Äôai une pr√©f√©rence pour Keras, puisqu‚Äôil supporte les r√©seaux neuronaux classiques (perceptrons multicouche) autant que convolutifs ou r√©currents. Keras pourrait n√©anmoins √™tre difficile √† installer sur Windows, o√π Python ne vient pas par d√©faut. Sur Windows, Keras ne fonctionne qu‚Äôavec Anaconda: vous devez donc installez Anaconda ou Miniconda (Miniconda offre une installation minimaliste). Donc, pour ce cours, nous utiliserons le module neuralnet. Il est possible de l‚Äôutilser gr√¢ce √† l‚Äôinterface de caret, mais son utilisation directe permet davantage de flexibilit√©. Chargeons les donn√©es d‚Äôiris. library(&quot;neuralnet&quot;) ## ## Attaching package: &#39;neuralnet&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## compute data(&quot;iris&quot;) Prenons soin de segmenter nos donn√©es en entra√Ænement et en test. set.seed(8453668) iris_tr_index &lt;- createDataPartition(y=iris$Species, p = 0.75, list = FALSE) Nous pouvons ainsi cr√©er nos tableaux d‚Äôentra√Ænement et de test pour les variables pr√©dictives. Les r√©seaux neuronnaux sont aptes √† g√©n√©rer des sorties multiples. Nous d√©sirons pr√©dire une cat√©gorie, et neuralnet ne s‚Äôoccupe pas de les transformer de facto. Lors de la pr√©diction d‚Äôune cat√©gorie, nous devons g√©n√©r√©e des sorties multiples qui permettront de d√©cider de l‚Äôappartenance exclusive √† une cat√©gorie ou une autre. Nous avons abord√© l‚Äôencodage cat√©goriel aux chapitres 6 et 8. C‚Äôest ce que nous ferons ici. species_oh &lt;- model.matrix(~ 0 + Species, iris) colnames(species_oh) &lt;- levels(iris$Species) iris_oh &lt;- iris %&gt;% cbind(species_oh) iris_tr &lt;- iris_oh[iris_tr_index, ] iris_te &lt;- iris_oh[-iris_tr_index, ] Lan√ßons le r√©seau neuronnal avec l‚Äôinterface-formule de R (neuralnet n‚Äôaccepte pas le . pour indiquer prend toutes les variables √† l‚Äôexeption de celles utilis√©es en y): nous allons les inclure √† la main. L‚Äôargument hidden est un vecteur qui indique le nombre de neuronnes pour chaque couche. L‚Äôargument linear.input indique si l‚Äôon d√©sire travailler en r√©gression (linear.output = TRUE) ou en classification (linear.output = FALSE). Lorsque les donn√©es sont nombreuses, patience, le calcul prend pas mal de temps. Dans ce cas-ci, nous avons un tout petit tableau. nn &lt;- neuralnet(setosa + versicolor + virginica ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data = iris_tr, hidden = c(5, 5), act.fct = &quot;tanh&quot;, linear.output = FALSE) Un r√©seau neuronnal peu complexe peut √™tre lisible. plot(nn) Il n‚Äôexiste pas de r√®gle stricte sur le nombre de couche et le nombre de noeud par couche. Il est n√©anmoins conseill√© de g√©n√©rer d‚Äôabord un mod√®le simple, puis au besoin de le complexifier graduellement en terme de nombre de noeuds, puis de nombre de couches. Si vous d√©sirez aller plus loin et utiliser keras, le module autokeras, disponible seulement en Python, est con√ßu pour optimiser un mod√®le Keras. La sortie du r√©seau neuronal est une valeur pr√®s de 1 ou une valeur pr√®s de 0. Voici une mani√®re de g√©n√©rer un vecteur cat√©goriel. compute_te &lt;- compute(nn, iris_te) pred_te &lt;- compute_te$net.result %&gt;% as_tibble() %&gt;% apply(., 1, which.max) %&gt;% levels(iris$Species)[.] %&gt;% as.factor() ## Warning: `as_tibble.matrix()` requires a matrix with column names or a `.name_repair` argument. Using compatibility `.name_repair`. ## This warning is displayed once per session. La fonction caret::confusionMatrix() permet de g√©n√©rer les statistiques du mod√®le. confusionMatrix(iris_te$Species, pred_te) ## Confusion Matrix and Statistics ## ## Reference ## Prediction setosa versicolor virginica ## setosa 12 0 0 ## versicolor 1 10 1 ## virginica 0 0 12 ## ## Overall Statistics ## ## Accuracy : 0.9444 ## 95% CI : (0.8134, 0.9932) ## No Information Rate : 0.3611 ## P-Value [Acc &gt; NIR] : 2.421e-13 ## ## Kappa : 0.9167 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: setosa Class: versicolor Class: virginica ## Sensitivity 0.9231 1.0000 0.9231 ## Specificity 1.0000 0.9231 1.0000 ## Pos Pred Value 1.0000 0.8333 1.0000 ## Neg Pred Value 0.9583 1.0000 0.9583 ## Prevalence 0.3611 0.2778 0.3611 ## Detection Rate 0.3333 0.2778 0.3333 ## Detection Prevalence 0.3333 0.3333 0.3333 ## Balanced Accuracy 0.9615 0.9615 0.9615 Encore une fois, c‚Äôest rarement le cas mais nous obtenons une classification parfaite. 12.4.3.2 Pour aller plus loin En une heure divis√©e en 4 vid√©os, Grant Sanderson explique les r√©seaux neuronaux de mani√®re intuitive. En ce qui a trait √† Keras, je recommande le livre Deep learning with R, de Fran√ßois Allaire, auquel vous avez acc√®s avec un IDUL de l‚ÄôUniversit√© Laval. Si vous vous sentez √† l‚Äôaise √† utiliser Keras avec le langage Python, je vous recommande le cours gratuit en ligne Applications of deep neural networks, de Jeff Heaton. Des types de r√©seaux neuronaux sp√©cialis√©s ont √©t√© d√©velopp√©s. Je les pr√©sente sans aller dans les d√©tails. R√©seaux neuronaux convolutif. Ce type de r√©seau neuronal est surtout utilis√© en reconnaissance d‚Äôimage. Les couches de neurones convolutifs poss√®dent, en plus des fonctions des perceptrons classiques, des filtres permettant d‚Äôint√©grer les variables descriptives connexes √† l‚Äôobservation: dans le cas d‚Äôune image, il s‚Äôagit de scanner les pixels au pourtour du pixel trait√©. Une br√®ve introduction sur Youtube. R√©seaux neuronaux r√©currents. Pr√©dire des occurrences futures √† partir de s√©ries temporelles implique que la r√©ponse au temps t d√©pend non seulement de conditions externes, mais aussi le la r√©ponse au temps t-1. Les r√©seaux neuronaux r√©currents. Vous devrez ajouter des neurones particuliers pour cette t√¢che, qui pourra √™tre pris en charge par Keras gr√¢ce aux couches de type Long Short-Term Memory network, ou LSTM. R√©seaux neuronaux probabilistes. Les r√©seaux neuronaux non-probabilistes offre une estimation de la variable r√©ponse. Mais quelle est la cr√©dibilit√© de la r√©ponse selon les variables descriptives? Question qui pourrait se r√©v√©ler cruciale en m√©decine ou en ing√©nierie, √† la laquelle on pourra r√©pondre en mode probabiliste. Pour ce faire, on pose des distributions a priori sur les poids du r√©seau neuronal. Le module edward, programm√© et distribu√© en Python, offre cette possibilit√©. Vous pourrez acc√©der √† edward gr√¢ce au module reticulate, mais √† ce stade mieux vaudra basculer en Python. Pour en savoir davantage, consid√©rez cette conf√©rence de Andrew Rowan. 12.4.4 Les processus gaussiens Les sorties des techniques que sont les KNN, les arbres ou les for√™ts ainsi que les r√©seaux neuronaux sont (classiquement) des nombres r√©els ou des cat√©gories. Dans les cas o√π la cr√©dibilit√© de la r√©ponse est importante, il devient pertinent que la sortie soit probabiliste: les pr√©dictions seront alors pr√©sent√©es sous forme de distributions de probabilit√©. Dans le cas d‚Äôune classification, la sortie du mod√®le sera un vecteur de probabilit√© qu‚Äôune observation appartienne √† une classe ou √† une autre. Dans celui d‚Äôune r√©gression, on obtiendra une distribution continue. Les processus gaussiens tirent profit des statistiques bay√©siennes pour effectuer des pr√©dictions probabilistes. D‚Äôautres techniques peuvent √™tre utilis√©es pour effectuer des pr√©dictions probabilistes, comme les r√©seaux neuronaux probabilistes, que j‚Äôai introduits pr√©c√©demment. Bien que les processus gaussiens peuvent √™tre utilis√©s pour la classification, son fonctionnement s‚Äôexplique favorablement, de mani√®re intuitive, pas la r√©gression. 12.4.4.1 Un approche intuitive Ayant acquis de l‚Äôexp√©rience en enseignement des processus gaussiens, John Cunningham a d√©velopp√© une approche intuitive permettant de saisir les m√©canismes des processus gaussiens. lors de conf√©rences disponible sur YouTube (1, 2), il aborde le sujet par la n√©cessit√© d‚Äôeffectuer une r√©gression non-lin√©aire. G√©n√©rons d‚Äôabord une variable pr√©dictive x, l‚Äôheure, et une variable r√©ponse y, le rythme cardiaque d‚Äôun individu en battements par minute (bpm). x &lt;- c(7, 8, 10, 14, 17) y &lt;- c(61, 74, 69, 67, 78) plot(x, y, xlab=&quot;Heure&quot;, ylab=&quot;Rythme cardiaque (bpm)&quot;) abline(v=12, lty=3, col=&#39;gray50&#39;);text(12, 67, &#39;?&#39;, cex=2) abline(v=16, lty=3, col=&#39;gray50&#39;);text(16, 72, &#39;?&#39;, cex=2) Poser un probl√®me par un processus gaussien, c‚Äôest se demander les valeurs cr√©dibles qui pourraient √™tre obtenues hors du domaine d‚Äôobservations (par exemple, dans la figure ci-dessus, √† x=12 et x=16)? Ou bien, de mani√®re plus g√©n√©rale, quelles fonctions ont pu g√©n√©rer les variables r√©ponse √† partir d‚Äôune structure dans les variables pr√©dictives? Les distributions normales, que nous appellerons gaussiennes dans cette section par concordance avec le terme processus gaussien, sont particuli√®rement utiles pour r√©pondre √† cette question. Nous avons vu pr√©c√©demment ce que sont les distributions de probabilit√©: des outils math√©matiques permettant d‚Äôappr√©hender la structure des processus al√©atoires. Une distribution gaussienne repr√©sente une situation o√π l‚Äôon tire au hasard des valeurs continues. Une distribution gaussienne de la variable al√©atoire \\(X\\) de moyenne \\(0\\) et de variance de \\(1\\) est not√©e ainsi: \\[ X \\sim \\mathcal{N} \\left( 0, 1\\right)\\] Par exemple, une courbe de distribution gaussienne du rythme cardiaque √† 7:00 pourrait prendre la forme suivante. \\[ bpm \\sim \\mathcal{N} \\left( 65, 5\\right)\\] En R: x_sequence &lt;- seq(50, 80, length=100) plot(x_sequence, dnorm(x_sequence, mean=65, sd=5), type=&quot;l&quot;, xlab=&quot;Rythme cardiaque (bpm)&quot;, ylab=&quot;Densit√©&quot;) Une distribution binormale, un cas particulier de la distribution multinormale, comprendra deux vecteurs, \\(x_1\\) et \\(x_2\\). Elle aura donc deux moyennes. Puisqu‚Äôil s‚Äôagit d‚Äôune distribution binormale, et non pas deux distributions normales, les deux variables ne sont pas ind√©pendantes et l‚Äôon utilisera une matrice de covariance au lieu de deux variances ind√©pendantes. \\[ \\binom{x_1}{x_2} \\sim \\mathcal{N} \\Bigg( \\binom{\\mu_1}{\\mu_2}, \\left[ {\\begin{array}{cc} \\Sigma_{x_1} &amp; \\Sigma_{x_1,x_2} \\\\ \\Sigma_{x_1,x_2}^T &amp; \\Sigma_{x_2} \\\\ \\end{array} } \\right] \\Bigg) \\] La matrice \\(\\Sigma\\), dite de variance-covariance, indique sur sa diagonale les variances des variables (\\(\\Sigma_{x_1}\\) et \\(\\Sigma_{x_2}\\)). Les covariances \\(\\Sigma_{x_1,x_2}\\) et \\(\\Sigma_{x_1,x_2}^T\\) sont sym√©triques et indiquent le lien entre les variables. On pourrait supposer que le rythme cardiaque √† 8:00 soit corr√©l√© avec celui √† 7:00. Mises ensembles, les distributions gaussiennes √† 7:00 et √† 8:00 formeraient une distribution gaussienne binormale. \\[ \\binom{bpm_7}{bpm_8} \\sim \\mathcal{N} \\Bigg( \\binom{65}{75}, \\left[ {\\begin{array}{cc} 10 &amp; 6 \\\\ 6 &amp; 15 \\\\ \\end{array} } \\right] \\Bigg) \\] En R: library(&quot;ellipse&quot;) ## ## Attaching package: &#39;ellipse&#39; ## The following object is masked from &#39;package:car&#39;: ## ## ellipse ## The following object is masked from &#39;package:graphics&#39;: ## ## pairs means_vec &lt;- c(65, 75) covariance_mat &lt;- matrix(c(10, 6, 6, 15), ncol=2) par(pty=&#39;s&#39;) plot(ellipse(x=covariance_mat, centre=means_vec, levels=0.95), type=&#39;l&#39;, xlab=&quot;Rythme cardiaque √† 7:00 (bpm)&quot;, ylab=&quot;Rythme cardiaque √† 8:00 (bpm)&quot;) #lines(ellipse(x=covariance_mat, centre=means_vec, level=0.8)) On peut se poser la question: √©tant donn√©e que \\(x_1 = 68\\), quelle serait la distribution de \\(x_2\\)? Dans ce cas bivari√©e, la distribution marginale serait univari√©e, mais dans le cas multivari√© en \\(D\\) dimensions, la distribution marginale o√π l‚Äôon sp√©cifie \\(m\\) variables serait de \\(D-m\\). de Une propri√©t√© fondamentale d‚Äôune distribution gaussienne est que peu importe l‚Äôendroit o√π l‚Äôangle selon lequel on la tranche, la distribution marginale sera aussi gaussienne. Lorsque l‚Äôon retranche une ou plusieurs variables en sp√©cifiant la valeur qu‚Äôelles prennent, on applique un conditionnement √† la distribution. Les points sur l‚Äôaxe (symbole x) conditionn√©s sont des √©chantillons tir√©s au hasard dans la distribution conditionn√©e. Une autre mani√®re de visualiser la distribution gaussienne binormale est de placer \\(x_1\\) et \\(x_2\\) c√¥te √† c√¥te en abscisse, avec leur valeur en ordonn√©e. Le bloc de code suivant peut sembler lourd au premier coup d‚Äô≈ìil: pas de panique, il s‚Äôagit surtout d‚Äôinstructions graphiques. Vous pouvez vous amuser √† changer les param√®tres de la distribution binormale (section 1) ainsi que la valeur de \\(x_1\\) √† laquelle est conditionn√©e la distribution de \\(x_2\\) (section 2). Les valeurs que peuvent prendre le rythme cardiaque en \\(x_2\\) sont tir√©es al√©atoirement d‚Äôune distribution conditionn√©e. Sautons maintenant au cas multinormal, incluant 6 variables (hexanormal!). Afin d‚Äô√©viter de composer une matrice de covariance √† la mitaine, je me permets de la g√©n√©rer avec une fonction. Cette fonction particuli√®re est nomm√©e fonction de base radiale ou exponentiel de la racine. \\[K_{RBF} \\left( x_i, x_j \\right) = \\sigma^2 exp \\left( -\\frac{\\left( x_i - x_j \\right)^2}{2 l^2} \\right) \\] RBF_kernel &lt;- function(x, sigma, l) { n &lt;- length(x) k &lt;- matrix(ncol = n, nrow = n) for (i in 1:n) { for (j in 1:n) { k[i, j] = sigma^2 * exp(-1/(2*l^2) * (x[i] - x[j])^2) } } colnames(k) &lt;- paste0(&#39;x&#39;, 1:n) rownames(k) &lt;- colnames(k) return(k) } Dans la fonction RBF_kernel, x d√©signe les dimensions, sigma d√©signe un √©cart-type commun √† chacune des dimensions et l est la longueur d√©signant l‚Äôamplification de la covariance entre des dimensions √©loign√©es (dans le sens que la premi√®re dimension est √©loign√©e de la derni√®re). Pour 6 dimensions, avec un √©cart-type de 4 et une longueur de 2. covariance_6 &lt;- RBF_kernel(1:6, sigma=4, l=2) round(covariance_6, 2) ## x1 x2 x3 x4 x5 x6 ## x1 16.00 14.12 9.70 5.19 2.17 0.70 ## x2 14.12 16.00 14.12 9.70 5.19 2.17 ## x3 9.70 14.12 16.00 14.12 9.70 5.19 ## x4 5.19 9.70 14.12 16.00 14.12 9.70 ## x5 2.17 5.19 9.70 14.12 16.00 14.12 ## x6 0.70 2.17 5.19 9.70 14.12 16.00 Changez la valeur de l permet de bien saisir son influence sur la matrice de covariance. Avec un l de 1, la covariance entre \\(x_1\\) et \\(x_6\\) est pratiquement nulle: elle est un peut plus √©lev√©e avec l=2. Pour reprendre l‚Äôexemple du rythme cardiaque, on devrait en effet s‚Äôattendre √† retrouver une plus grande corr√©lation entre celles mesur√©es aux temps 4 et 5 qu‚Äôentre les temps 1 et 6. De m√™me que dans la situation o√π nous avions une distribution binormale, nous pouvons conditionner une distribution multinormale. Dans l‚Äôexemple suivant, je conditionne la distribution multinormale de 6 dimensions en sp√©cifiant les valeurs prises par les deux premi√®res dimensions. Le r√©sultat du conditionnement est une distribution en 4 dimensions. Puisqu‚Äôil est difficile de pr√©senter une distribution en 6D, le graphique en haut √† gauche ne comprend que les dimensions 1 et 6. Remarquez que la corr√©lation entre les dimensions 1 et 6 est faible, en concordance avec la matrice de covariance g√©n√©r√©e par la fonction RBF_kernel. Lancez plusieurs fois le code et voyez ce qui advient des √©chantillonnages dans les dimensions 3 √† 6 selon le conditionnement en 1 et 2. La structure de la covariance assure que les dimensions proches prennent des valeurs similaires, assurant une courbe lisse et non en dents de scie. Pourquoi s‚Äôarr√™ter √† 6 dimensions? Prenons-en plusieurs, puis g√©n√©rons plus d‚Äôun √©chantillon. Ensuite, utilisons ces simulations pour de calculer la moyenne et l‚Äô√©cart-type de chacune des dimensions. Revenons au rythme cardiaque. On pourra utiliser le conditionnement aux temps observ√©s, soit 7:00, 8:00, 10:00, 14:00 et 17:00 pour estimer la distribution √† 12:00 et 16:00, o√π √† des dimensions artificielles quelconques ici fix√©es aux demi-heures. Comme on devrait s‚Äôy attendre, la r√©gression r√©sultant de la mise en indices de la distribution est pr√©cise aux mesures, et impr√©cise aux indices peu garnis en mesures. Nous avions utilis√© 21 dimensions. Lorsque l‚Äôon g√©n√©ralise la proc√©dure √† une quantit√© infinie de dimensions, on obtient un processus gaussien. L‚Äôindice de la variable devient ainsi une valeur r√©elle. Un processus gaussien, \\(\\mathcal{GP}\\), est d√©fini par une fonction de la moyenne, \\(m \\left( x \\right)\\), et une autre de la covariance que l‚Äôon nomme noyau (ou kernel), \\(K \\left( x, x&#39; \\right)\\). Un processus gaussien est not√© de la mani√®re suivante: \\[\\mathcal{GP} \\sim \\left( m \\left( x \\right), K \\left( x, x&#39; \\right) \\right)\\] La fonction d√©finissant la moyenne peut √™tre facilement √©cart√©e en s‚Äôassurant de centrer la variable r√©ponse √† z√©ro (\\(y_{centr√©} = y - \\hat{y}\\)). Ainsi, par convention, on sp√©cifie une fonction de moyenne comme retournant toujours un z√©ro. Quant au noyau, il peut prendre diff√©rentes fonctions de covariance ou combinaisons de fonctions de covariance. R√®gle g√©n√©rale, on utilisera un noyau permettant de d√©finir deux param√®tres: la hauteur (\\(\\sigma\\)) et la longueur de l‚Äôondulation (\\(l\\)) (figure 12.9). Figure 12.9: Hyperparam√®tres d‚Äôun noyau RBF. On pourra ajouter √† ce noyau un bruit blanc, c‚Äôest-√†-dire une variation purement al√©atoire, sans covariance (noyau g√©n√©rant une matrice diagonale). Le noyau devient ainsi un a priori, et le processus gaussien conditionn√© aux donn√©es devient un a posteriori probabiliste. Finalement, les processus gaussiens peuvent √™tre extrapol√©s √† plusieurs variables descriptives. 12.4.5 Les processus gaussiens en R Pas de souci, vous n‚Äôaurez pas √† programmer vos propres fonctions pour lancer des processus gaussiens. Vous pourrez passer par caret. Vous pourriez, comme c‚Äôest le cas avec les r√©seaux neuronnaux, obtenir davantage de contr√¥le sur l‚Äôautoapprentissage en utilisant directement la fonction gausspr() du package kernlab. library(&quot;kernlab&quot;) ## ## Attaching package: &#39;kernlab&#39; ## The following object is masked from &#39;package:permute&#39;: ## ## how ## The following object is masked from &#39;package:purrr&#39;: ## ## cross ## The following object is masked from &#39;package:ggplot2&#39;: ## ## alpha x &lt;- c(7, 8, 10, 14, 17) y &lt;- c(61, 74, 69, 67, 78) y_sc &lt;- (y - mean(y)) / sd(y) m &lt;- gausspr(x, y_sc, kernel = &#39;rbfdot&#39;, # le noyau: diff√©rents types disponibles (?gausspr) kpar = list(sigma = 4), # hyperparam√®tre du noyau (l est optimis√©) variance.model = TRUE, # pour pouvoir g√©n√©rer les √©carts-type scaled = TRUE, # mettre √† l&#39;√©chelle des variables var = 0.01, # bruit blanc cross = 2) # nombre de plis de la validation crois√©e xtest &lt;- seq(6, 18, by = 0.1) y_sc_pred_mean &lt;- predict(m, xtest, type=&quot;response&quot;) y_pred_mean &lt;- y_sc_pred_mean * sd(y) + mean(y) y_sc_pred_sd &lt;- predict(m, xtest, type=&quot;sdeviation&quot;) # &quot;sdeviation&quot; en r√©gression et &quot;probabilities&quot; pour la classification y_pred_sd &lt;- y_sc_pred_sd * sd(y) plot(x, y, xlim = c(6, 18), ylim = c(45, 90)) lines(xtest, y_pred_mean) lines(xtest, y_pred_mean + y_pred_sd, col=&quot;red&quot;) lines(xtest, y_pred_mean - y_pred_sd, col=&quot;red&quot;) abline(v=12, lty=3, col=&#39;gray50&#39;);text(12, 67, &#39;?&#39;, cex=2) abline(v=16, lty=3, col=&#39;gray50&#39;);text(16, 72, &#39;?&#39;, cex=2) 12.4.5.1 Application pratique Les processus gaussiens sont utiles pour effectuer des pr√©dictions sur des ph√©nom√®ne sur lesquels on d√©sire √©viter de se commettre sur la structure. Les s√©ries temporelles ou les signaux spectraux en sont des exemples. Aussi, j‚Äôai utilis√© les processus gaussiens pour mod√©liser des courbes de r√©ponse aux fertilisants. Prenons ces donn√©es g√©n√©r√©es au hasard, comprenant l‚Äôidentifiant de la mesure, le bloc du test, la dose de fertilisant, trois variables environnementales ainsi que la performance de la culture en terme de rendement. fert &lt;- read_csv(&quot;data/11_response_fert.csv&quot;) fert %&gt;% ggplot(aes(x = Dose, y = Yield)) + geom_line(aes(group = Block), colour = rgb(0, 0, 0, 0.5)) Les blocs 1 √† 30 serviront d‚Äôentra√Ænement, les autres de test. Le rendement est mis √† l‚Äô√©chelle pour la mod√©lisation. environment &lt;- fert %&gt;% dplyr::select(Dose, var1, var2, var3) yield_sc &lt;- (fert$Yield - mean(fert$Yield)) / sd(fert$Yield) environment_tr &lt;- environment[fert$Block &lt;= 30, ] environment_te &lt;- environment[fert$Block &gt; 30, ] yield_tr &lt;- yield_sc[fert$Block &lt;= 30] yield_te &lt;- yield_sc[fert$Block &gt; 30] Je pourrais optimiser les hyperparam√®tres en cr√©ant une grille puis en lan√ßant plusieurs processus gaussiens en boucle. Mais pour l‚Äôexemple j‚Äôutilise des hyperparam√®tres quelconque. yield_gp &lt;- gausspr(environment_tr, yield_tr, kernel = &#39;rbfdot&#39;, kpar = list(sigma = 0.1), variance.model = TRUE, scaled = TRUE, var = 0.1, cross = 10) # rendements pr√©dits dans l&#39;√©chelle originale gp_pred_tr &lt;- predict(yield_gp, environment_tr, type=&quot;response&quot;) * sd(fert$Yield) + mean(fert$Yield) gp_pred_te &lt;- predict(yield_gp, environment_te, type=&quot;response&quot;) * sd(fert$Yield) + mean(fert$Yield) # rendements r√©els dans l&#39;√©chelle originale yield_tr_os &lt;- yield_tr * sd(fert$Yield) + mean(fert$Yield) yield_te_os &lt;- yield_te * sd(fert$Yield) + mean(fert$Yield) par(mfrow = c(1, 2)) plot(yield_tr_os, gp_pred_tr, main = &quot;train&quot;) abline(0, 1, col = &quot;red&quot;) plot(yield_te_os, gp_pred_te, main = &quot;test&quot;) abline(0, 1, col = &quot;red&quot;) La pr√©diction semble bien fonctionner en entra√Ænement comme en test. Pour une application √† un cas d‚Äô√©tude, disons que pour mon site j‚Äôai des variables environnementales de valeurs du bloc 50, et que je cherche la dose optmale. fert %&gt;% dplyr::filter(Block == 50) %&gt;% dplyr::select(var1, var2, var3) %&gt;% dplyr::slice(1) ## # A tibble: 1 x 3 ## var1 var2 var3 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.57 101. -10.7 Je peux cr√©er un tableau comprenant des environnements √©gaux pour chaque ligne, mais comprenant des incr√©ments de dose, puis pr√©dire la courbe de r√©ponse ainsi que son incertitude. Et puisque c‚Äôest un cas document√©, je peux afficher les r√©sultats de l‚Äôessai pour v√©rifier si le mod√®le est cr√©dible. environment_appl &lt;- data.frame(Dose = seq(0, 200, 5), var1 = 1.57, var2 = 101.5, var3 = -10.7) yield_appl_sc &lt;- predict(yield_gp, environment_appl, type=&quot;response&quot;) y_sc_pred_sd_sc &lt;- predict(yield_gp, environment_appl, type=&quot;sdeviation&quot;) yield_appl &lt;- yield_appl_sc * sd(fert$Yield) + mean(fert$Yield) yield_appl_sd &lt;- y_sc_pred_sd_sc * sd(fert$Yield) plot(environment_appl$Dose, yield_appl, type = &quot;l&quot;, ylim = c(0, 35)) points(x = fert[fert$Block == 50, ]$Dose, y = fert[fert$Block == 50, ]$Yield) lines(environment_appl$Dose, yield_appl + yield_appl_sd, col = &quot;red&quot;) lines(environment_appl$Dose, yield_appl - yield_appl_sd, col = &quot;red&quot;) &lt;our chaque incr√©ment de dose de la courbe de rponse, il est possible de calculer un rendement √©conomique et/ou √©cologique en fonction du prix de la dose pond√©r√© par un co√ªt environnemental, puis de soutirer une performance optimale en terme de fertilisation. Exercice. Changez les valeurs des variables environnementales pour g√©n√©rer le tableau environment_appl avec des valeurs qui sortent du lot (voir figure 12.10). Qu‚Äôobservez-vous? Pourquoi? Figure 12.10: Vairables environnementales du tableau fictif fert. Exercice. Effectuer la pr√©diction du rendement avec d‚Äôautres techniques, comme des r√©seaux neuronaux. Comment les mod√®les se comportent-ils? "],
["chapitre-geo.html", "13 Les donn√©es g√©ospatiales 13.1 Les donn√©es spatiales 13.2 Cartographier avec le module ggmap 13.3 Types g√©n√©riques de donn√©es spatiales 13.4 Les choropl√®the 13.5 Les rasters 13.6 Autoapprentissage spatial 13.7 Les objets spatialis√©s en R 13.8 Les syst√®mes de coordonn√©es 13.9 Manipuler des tableaux sf 13.10 Manipuler des objets raster 13.11 Graphiques d‚Äôobjets spatialis√©s 13.12 Ressources compl√©mentaires", " 13 Les donn√©es g√©ospatiales Ô∏è¬†Objectifs sp√©cifiques: √Ä la fin de ce chapitre, vous saurez cartographier des donn√©es g√©or√©f√©renc√©es avec ggplot serez en mesure d‚Äôeffectuer un autoapprentissage spatial saurez utiliser R comme outil d‚Äôanalyse spatiale (donn√©e associ√©es √† des points, lignes, polygones et rasters) 13.1 Les donn√©es spatiales Des donn√©es associ√©es √† un endroit sont spatiales. Puisque ce cours ne traite pas d‚Äô√©cologie exoplan√©taire, nous traiterons en particulier de donn√©es g√©ospatiales, mot que l‚Äôon utilise pour d√©signer des donn√©es avec r√©f√©rence spatiale sur la plan√®te Terre. Lorsque nous avons abord√© les s√©ries temporelles, j‚Äôai pris pour acquis que nous utilisions le calendrier gr√©gorien comme r√©f√©rence temporelle. Les donn√©es g√©ospatiales, quant √† elles, sont souvent exprim√©es en termes d‚Äôangles donnant une position √† la surface d‚Äôune r√©f√©rence dont la forme est un ellipsoide de r√©volution (le syst√®me g√©od√©sique): la longitude d√©crivant l‚Äôangle de part et d‚Äôautre (entre 0¬∞ et 180¬∞ Ouest ou Est) du m√©ridien de r√©f√©rence (le premier m√©ridien, pr√®s de Greenwich) et la latitude d√©crivant l‚Äôangle entre l‚Äô√©quateur et l‚Äôun des p√¥les (entre 0¬∞ et 90¬∞ Nord ou Sud). Les angles sont parfois exprim√©es sous forme degr√©¬∞ minute' seconde'' Sens cardinal, par exemple 46¬∞ 53' 21.659'' S. Toutefois, il est plus commun (et plus pratique) d‚Äôexprimer les angles de mani√®re d√©cimale, accompagn√©e d‚Äôun signe pour indiquer le sens cardinal (par convention positif au Nord et √† l‚ÄôEst). Par exemple, on exprimerait 46¬∞ 53' 21.659'' S sous forme d√©cimale par les op√©rations suivantes. \\[ secondes = \\frac{21.659&#39;&#39;}{3600&#39;&#39; \\cdot ¬∞^{-1}} = 0.00602¬∞ \\] \\[ minutes = \\frac{53&#39;}{60&#39; \\cdot ¬∞^{-1}} = 0.883¬∞ \\] \\[ - \\left( 46¬∞ + 0.883¬∞ + 0.00602¬∞ \\right) = -46.889¬∞ \\] La r√©f√©rence de l‚Äôaltitude est g√©n√©ralement donn√©e par rapport √† un g√©o√Øde, qui est une √©l√©vation th√©orique du niveau de la mer ainsi qu‚Äôune direction de la gravit√© √©valu√©e sur toute la surface du globe. Toutefois, les angles ne sont pas pratiques pour √©valuer des distances, ce que l‚Äôon fera bien mieux sur une carte. Pour pr√©senter la Terre sous forme de carte, on cr√© des repr√©sentations applaties du globe sous forme de carte avec l‚Äôaide d‚Äô√©quations de projection. Or, il existe diff√©rents syst√®mes g√©od√©siques, diff√©rents g√©oides et de nombreuses mani√®res de calculer les projections. Ainsi, il est important de sp√©cifier les r√©f√©rences utilis√©es lorsque l‚Äôon donne dans la pr√©cision. Pour cette s√©ance, je prendrai pour acquis que vous poss√©dez certaines bases en positionnement, qui sont par ailleurs essentielles pour pratiquer ad√©quatement un m√©tier scientifique. library(&quot;tidyverse&quot;) Dans ce chapitre, j‚Äôutiliserai notamment comme exemple d‚Äôapplication des donn√©es m√©t√©orologiques soutir√©es d‚ÄôEnvironnement Canada gr√¢ce au module weathercan, obtenues entre les longitudes -60¬∞ et -80¬∞ et entre les latitudes 45¬∞ et 50¬∞ en mai 2018. J‚Äôai effectu√© quelques op√©rations pour obtenir des indicateurs m√©t√©o: degr√©s-jour (somme des degr√©s de temp√©rature moyenne &gt; 5 ¬∞C, degree_days), pr√©cipitations totales (cumul_precip) et indice de diversit√© des pr√©cipitations (plus l‚Äôindice sdi s‚Äôapproche de 1, plus les temp√©ratures sont uniform√©ment distribu√©es pendant la p√©riode). Les calculs sont consign√©s dans le fichier lib/12_weather-fetch.R, mais √©tant donn√© que le t√©l√©chargement prend pas mal de temps, j‚Äôai cr√©√© un csv. Les coordonn√©es se trouvent dans les colonnes de latitude (lat) et longitude (lon). source(&quot;lib/12_weather-fetch.R&quot;) weather &lt;- read_csv(&quot;data/12_weather.csv&quot;) weather %&gt;% head() ## # A tibble: 6 x 8 ## station_id station_name prov lat lon degree_days cumul_precip sdi ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10247 ILE AUX GRUES QC 47.1 -70.5 NA NA NA ## 2 10661 NORTHEAST MARGAREE (AUT) NS 46.4 -61.0 372. 222. 0.904 ## 3 10732 NICOLET QC 46.2 -72.7 823. 78.4 0.704 ## 4 10761 MCTAVISH QC 45.5 -73.6 915. 107 0.740 ## 5 10762 STE-CLOTILDE QC 45.2 -73.7 860. 85.2 0.820 ## 6 10763 ILES DE LA MADELEINE QC 47.4 -61.8 326. 167. 0.853 Dans le tableau weather, chaque observation est li√©e √† un point dans l‚Äôespace. Dans ce cas, nous avons tous les outils n√©cessaires pour afficher nos points dans l‚Äôespace (figure 13.1). weather %&gt;% ggplot(mapping = aes(x = lon, y = lat)) + geom_point() Figure 13.1: Position des stations m√©t√©o du tableau weather Si vous avez l‚Äôoeil averti, vous avez peut-√™tre rep√©r√© le Qu√©bec, le Nouveau-Brunswick et la fronti√®re avec les √âtats-Unis. L‚Äôabsence de rep√®re rend n√©anmoins difficle l‚Äôinterpr√©tation de cette carte. 13.2 Cartographier avec le module ggmap Le module ggmap ajoute des couches d‚Äôimages t√©l√©charg√©es depuis des services de cartorgaphie en ligne. Dans cette section, nous allons utiliser le service de carte Stamen, qui ne demande pas de frais d‚Äôutilisation ou d‚Äôenregistrement particulier. La fonction get_stamenmap() demande une bo√Æte de coordonn√©es d√©limitant la carte √† produire, un param√®tre de zoom (plus le zoom est √©lev√©, plus la carte incluera de d√©tails: un zoom de 2 est suffisant pour une carte du monde, mais pour l‚ÄôEst du Canada, on prendra plut√¥t un zoom de 6 - un bon zoom est obtenu par t√¢tonnement) et accessoirement un type de carte. library(&quot;ggmap&quot;) east_canada &lt;- get_stamenmap(bbox = c(left=-81, right = -59, bottom = 44, top = 51), zoom = 6, maptype = &quot;terrain&quot;) Pour afficher la carte, nous ench√¢ssons notre objet dans une fonction ggmap(), √† laquelle nous pouvons ajouter une couche. ggmap(east_canada) + geom_point(data = weather, mapping = aes(x = lon, y = lat)) Une approche plus g√©n√©raliste consiste √† sp√©cifier dans la fonction ggmap() l‚Äôagument de base utilis√© pour lancer un graphique ggplot, comme √† la figure figure 13.2. En outre, l‚Äôutiliation de l‚Äôargument base_layer permet d‚Äôeffectuer des fecettes et d‚Äô√©viter de sp√©cifier la source des donn√©es dans toutes les couches subs√©quentes. ggmap(east_canada, base_layer = ggplot(weather, aes(x = lon, y = lat))) + geom_point() Figure 13.2: Position des stations m√©t√©o du tableau weather superpos√© √† une carte import√©e par ggmap La carte que nous avons cr√©√©e est de type terrain, un type d‚Äôaffichage efficace mais peu appropri√© pour une publication visant √† √™tre imprim√©e. Le type toner-lite est davantage vou√© √† l‚Äôimpression, alors que le type watercolor est plus joli pour le web. Les types offerts sont list√©s dans la ficher d‚Äôaide ?get_stamenmap. maptype = c(&quot;terrain&quot;, &quot;terrain-background&quot;, &quot;terrain-labels&quot;, &quot;terrain-lines&quot;, &quot;toner&quot;, &quot;toner-2010&quot;, &quot;toner-2011&quot;, &quot;toner-background&quot;, &quot;toner-hybrid&quot;, &quot;toner-labels&quot;, &quot;toner-lines&quot;, &quot;toner-lite&quot;, &quot;watercolor&quot;) 13.3 Types g√©n√©riques de donn√©es spatiales Nous avons jusqu‚Äô√† pr√©sent utilis√© des donn√©es spatiales attach√©es √† un point. Ce ne sont pas les seuls. Donn√©es ponctuelles: associ√©es √† un point. Exemple: mesure √† un endroit pr√©cis. Donn√©es lin√©aires: associ√©es √† une s√©rie de point. Exemple: mesure associ√©e √† une route ou une rivi√®re. Donn√©es de polygone: associ√©es √† une aire d√©limit√©e par des points. Exemples: Donn√©es associ√©es √† un champ, une unit√© administrative, un bassin versant, etc. Donn√©es raster: associ√©es √† une grille. Exemple: une image satellite o√π chaque pixel est associ√© √† un recouvrement foliaire. L‚Äôenregistrement des donn√©es ponctuelles ne posent pas de d√©fi particulier. Les donn√©es associ√©es √† un ligne, toutefois posent un probl√®me d‚Äôorganisation, puisqu‚Äôune ligne elle-m√™me contient des informations sur les coordonn√©es de ses points ainsi que l‚Äôordre dans lequel les points sont connect√©s. On pourra soit cr√©er un tableau de donn√©es ayant une colonne o√π l‚Äôidentifiant de la ligne est consign√©, renvoyant √† un autre tableau o√π chaque ligne d√©crit un point en terme d‚Äôidentifiant de ligne √† laquelle il appartient, ses coordonn√©es, ainsi que sont ordre de rattachement dans la ligne. Les informations de la ligne pourraient aussi √™tre ench√¢ss√©es dans une cellule de tableau de donn√©es, en tant que sous-tableau. Ou bien, on pourrait cr√©er un tableau sous forme de jointure entre le tableau des donn√©es et le tableau des lignes. Un d√©fi similaire pourrait subvenir avec des polygones, qui demandent davantage d‚Äôinformation √©tant donn√©e qu‚Äôils peuvent √™tre trou√©s (par exemple un lac) ou s√©par√©s en diff√©rents morceaux (un archipel, par exemple). Enfin, il existe des formats de donn√©es spatiales g√©n√©riques (shapefiles et geojson) ou sp√©cialement con√ßus pour R (module sf), que nous couvrirons plus loin dans ce chapitre. 13.4 Les choropl√®the Les cartes de type choropl√®the se pr√©sentent sous forme de polygones d√©crits par un groupe et un ordre, dont un la couleur de remplissage d√©pend d‚Äôune variable. Les pays, par exemple, forment des polygones. world &lt;- map_data(map = &quot;world&quot;) # jeu de donne√©s de ggplot2 head(world) ## long lat group order region subregion ## 1 -69.89912 12.45200 1 1 Aruba &lt;NA&gt; ## 2 -69.89571 12.42300 1 2 Aruba &lt;NA&gt; ## 3 -69.94219 12.43853 1 3 Aruba &lt;NA&gt; ## 4 -70.00415 12.50049 1 4 Aruba &lt;NA&gt; ## 5 -70.06612 12.54697 1 5 Aruba &lt;NA&gt; ## 6 -70.05088 12.59707 1 6 Aruba &lt;NA&gt; La fonction map_data() de ggplot2 permet de soutirer des polygone de certaines cartes. Pour les zones g√©ographiques pr√©d√©finies, il est pr√©f√©rable de soutirer les polygones d√©sir√©es de donn√©es existantes plut√¥t que les cr√©er soi-m√™me. Souvent, ces polygones ne sont pas directement disponibles en R. Dans ce cas, il faudra trouver des fichiers de carte aupr√®s de Statistique Canada, Donn√©es Qu√©bec, etc., ce que nous verrons plus loin. especes_menacees &lt;- read_csv(&#39;data/WILD_LIFE_14012020030114795.csv&#39;) iucn_oecd &lt;- especes_menacees %&gt;% dplyr::filter(IUCN == &quot;THREATENED&quot;, SPEC == &quot;MAMMAL&quot;) %&gt;% dplyr::select(Country, Value) %&gt;% dplyr::group_by(Country) %&gt;% dplyr::summarise(n_threatened_species = sum(Value)) %&gt;% dplyr::arrange(desc(n_threatened_species)) Les noms des pays doivent correspondre exactement, et la colonne des pays doit porter le m√™me nom (j‚Äôai inspect√© les vecteurs iucn_oecd30$Country et unique(world$region)). iucn_oecd &lt;- iucn_oecd %&gt;% replace(.==&quot;United States&quot;, &quot;USA&quot;) %&gt;% replace(.==&quot;Slovak Republic&quot;, &quot;Slovakia&quot;) %&gt;% replace(.==&quot;United Kingdom&quot;, &quot;UK&quot;) %&gt;% dplyr::rename(&quot;region&quot; = &quot;Country&quot;) Les esp√®ces sont jointes au tableau contenant les polygones. world_iucn &lt;- world %&gt;% left_join(iucn_oecd, by = &quot;region&quot;) Pour le graphique de la figure 13.3, La strat√©gie est de cr√©er des polygones group√©s par groupes de polygones (group = group), dont la couleur de remplissage correspond √† au nombre d‚Äôesp√®ce. J‚Äôajoute coord_map() en sp√©cifiant une projection de type Mercator (essayez projection = &quot;ortho&quot;). Le reste est de la d√©coration. ggplot(world_iucn, aes(long, lat)) + geom_polygon(aes(group = group, fill = n_threatened_species), colour = &quot;grey50&quot;, lwd = 0.1) + coord_map(projection = &quot;mercator&quot;, xlim = c(-180, 180), ylim = c(-90, 90)) + scale_fill_gradient(low = &quot;#8CBFE6&quot;, high = &quot;#FF0099&quot;, na.value = &quot;grey70&quot;) + labs(title = &quot;Number of threatened species in OECD countries&quot;, subtitle = &quot;Source: OCDE, 2019&quot;) + theme(panel.background = element_rect(fill = &quot;grey97&quot;), plot.background = element_rect(fill = &quot;white&quot;), axis.text = element_blank(), axis.ticks = element_blank(), axis.title = element_blank(), panel.grid = element_blank()) + guides(fill = guide_legend(title = &quot;Number of\\nthreatened\\nspecies&quot;)) Figure 13.3: Nombre d‚Äôesp√®ces en danger dans les pays de l‚ÄôOCDE Comme c‚Äôest le cas des points de la figure 13.2, on peut superposer des choropl√®thes √† des cartes t√©l√©charg√©es (figure 13.4). worldmap &lt;- get_stamenmap(bbox = c(left=-170, right = 170, bottom = -80, top = 80), zoom = 2, maptype = &quot;watercolor&quot;) world_iucn_oecd &lt;- world_iucn %&gt;% filter(!is.na(n_threatened_species)) ggmap(worldmap, base_layer = ggplot(world_iucn_oecd, aes(long, lat))) + geom_polygon(aes(group = group, fill = n_threatened_species), colour = &quot;black&quot;, lwd = 0.2) + coord_map(projection = &quot;mercator&quot;, xlim = c(-180, 180), ylim = c(-90, 90)) + scale_fill_gradient(low = &quot;blue&quot;, high = &quot;red&quot;, na.value = &quot;grey80&quot;) + labs(title = &quot;Number of threatened species in OECD countries&quot;, subtitle = &quot;Source: OCDE, 2019&quot;) + theme(axis.text = element_blank(), axis.ticks = element_blank(), axis.title = element_blank(), panel.grid = element_blank()) + guides(fill = guide_legend(title = &quot;Number of\\nthreatened\\nspecies&quot;)) Figure 13.4: Nombre d‚Äôesp√®ces en danger dans les pays de l‚ÄôOCDE superpos√© √† une carte import√©e par ggmap Si vous savez cr√©er des polygones, vous saurez cr√©er des lignes de mani√®re similaire avec la couche graphique geom_path(). Pour les rasters, c‚Äôest moins √©vident. 13.5 Les rasters Les rasters sont des donn√©es associ√©es √† un grille. Nous avons introduit la fonction expand.grid() au chapitre 12 lorsque nous d√©sirions cr√©er un tableau o√π chaque ligne d√©signe une des combinaisons possibles d‚Äôhyperparam√®tres pour ajuster un mod√®le d‚Äôautoapprentissage. De m√™me, nous pouvons cr√©er une grille comprenant les combinaisons de longitudes et de latitudes, puis cr√©er une variable spatialis√©e \\(z = 10\\times sin \\left( xy \\right) - 0.01x^2 + 0.05y^2\\). grid &lt;- expand.grid(lon = seq(from = -80, to = -60, by = 0.25), lat = seq(from = 45, to = 50, by = 0.25)) grid &lt;- grid %&gt;% mutate(z = 10*sin(lon*lat) - 0.01*lon^2 + 0.05*lat^2) # cr√©er une variable spatialis√©e grid %&gt;% head() ## lon lat z ## 1 -80.00 45 39.87084 ## 2 -79.75 45 28.96923 ## 3 -79.50 45 31.05725 ## 4 -79.25 45 43.60578 ## 5 -79.00 45 48.42839 ## 6 -78.75 45 38.89957 Pour visualiser une grille avec ggplot2, on peut avoir recourt √† la couche graphique geom_tile(), dont la couleur remplissage est associ√©e √† la colonne z du tableau. Ce type de graphique est appel√©e heatmap (figure ??). ggplot(grid, aes(lon, lat)) + geom_tile(aes(fill = z)) Remarquez que j‚Äôai cr√©√© une fonction pour g√©n√©rer une variable spatialis√©e. Une telle fonction n‚Äôa pas besoin d‚Äô√™tre invent√©e: on peut en cr√©er une en utilisant les outils que nous avons appris jusqu‚Äô√† pr√©sent, en particulier avec l‚Äôautoapprentissage. 13.6 Autoapprentissage spatial La g√©ostatistique est l‚Äô√©tude statistique des variables spatiales, un sujet complexe qui sort du cadre de ce cours - que vous pourrez creuser dans le livre Applied Spatial Data Analysis with R. Ici, nous allons projeter des variables spatialis√©es √† l‚Äôaide de l‚Äôautoapprentissage, o√π la position (coordonn√©es en longitude et latitude, par exemple) peut servir de variable pr√©dictive (ainsi que, √©ventuellement, des variables spatialis√©es concernant l‚Äôaltitude, l‚Äôhydrologie, la g√©omorphologie, l‚Äô√©cologie, la sociologie, la gestion du territoire, etc.). Pour ce faire, vous pourrez utiliser algorithme qui convient √† vos donn√©es et √† votre domaine d‚Äô√©tude. Nous allons utiliser les processus gaussiens, qui sont particuli√®rement utiles pour √©valuer l‚Äôincertitude des pr√©dictions. Par exemple, nous allons pr√©dire sur une grille les donn√©es de degr√©s-jour du tableau weather avec un processus gaussien. Pour √©valuer la performance d‚Äôune pr√©diction, n‚Äôoublions de s√©parer nos donn√©es en jeux d‚Äôentra√Ænement et de test (avec la fonction caret::createDataPartition())! library(&quot;caret&quot;) weather_dd &lt;- weather %&gt;% dplyr::select(lon, lat, degree_days) %&gt;% drop_na() weather_dd_sc &lt;- weather_dd %&gt;% mutate(degree_days = (degree_days - mean(degree_days))/sd(degree_days)) train_id &lt;- createDataPartition(y = weather_dd_sc$degree_days, p = 0.7, list = FALSE) Utilisons la fonction kernlab::gausspr(), vue au chapitre 12. library(&quot;kernlab&quot;) dd_gp &lt;- gausspr(x = weather_dd_sc[train_id, c(&quot;lon&quot;, &quot;lat&quot;)], y = weather_dd_sc[train_id, &quot;degree_days&quot;], kernel = &quot;rbfdot&quot;, #kpar = list(sigma = 01), # laisser optimiser variance.model = TRUE, scale = TRUE, var = 0.1, cross = 5) ## Using automatic sigma estimation (sigest) for RBF or laplace kernel √âvaluons visuellement al performance de la pr√©diction (figure 13.5). pred_dd_tr &lt;- predict(dd_gp) pred_dd_te &lt;- predict(dd_gp, newdata = weather_dd_sc[-train_id, c(&quot;lon&quot;, &quot;lat&quot;)]) par(mfrow = c(1, 2)) plot(weather_dd_sc$degree_days[train_id], pred_dd_tr, main = &quot;Train prediction&quot;, xlab = &quot;mesur√©&quot;, ylab = &quot;pr√©dit&quot;) abline(0, 1, col=&quot;red&quot;) plot(weather_dd_sc$degree_days[-train_id], pred_dd_te, main = &quot;Test prediction&quot;, xlab = &quot;mesur√©&quot;, ylab = &quot;pr√©dit&quot;) abline(0, 1, col=&quot;red&quot;) Figure 13.5: Performance du processus gaussien en entra√Ænement et en test. La pr√©diction n‚Äôest pas extraordinaire, mais gardons-la pour l‚Äôexemple (j‚Äôai essay√© avec des r√©seaux neuronaux sans plus de succ√®s). La prochaine √©tape est de cr√©er une gille o√π chaque point [longitude, latitude] servira de variable explicative pour calculer les degr√©s-jour. grid &lt;- expand.grid(lon = seq(from = -80, to = -60, by = 0.25), lat = seq(from = 45, to = 50, by = 0.25)) grid &lt;- grid %&gt;% mutate(pred_dd_mean = predict(dd_gp, newdata = ., type = &quot;response&quot;) * sd(weather_dd$degree_days) + mean(weather_dd$degree_days), pred_dd_sd = predict(dd_gp, newdata = ., type = &quot;sdeviation&quot;) * sd(weather_dd$degree_days)) head(grid) ## lon lat pred_dd_mean pred_dd_sd ## 1 -80.00 45 654.5782 70.82743 ## 2 -79.75 45 647.5643 62.33121 ## 3 -79.50 45 637.1303 54.88782 ## 4 -79.25 45 623.4572 48.73097 ## 5 -79.00 45 606.9355 43.98920 ## 6 -78.75 45 588.1704 40.62892 Utilisons les polygones de la carte du monde zoom√©e √† l‚Äôendroit qui nous int√©resse, et ajoutons-y notre pr√©diction superpos√©e par les localisations des stations m√©t√©o. J‚Äôajoute des contours ainsi que des √©tiquettes de contours (ce qui n√©cessite le module metR). Les processus gaussiens permettent de juxtaposer une carte des √©cart-type des pr√©dictions, donnant une appr√©ciation de la pr√©cision du mod√®le (figure 13.6). Cette juxtaposition est effectu√©e avec la fonction plot_grid() le module cowplot. library(&quot;metR&quot;) ## ## Attaching package: &#39;metR&#39; ## The following object is masked from &#39;package:kernlab&#39;: ## ## cross ## The following object is masked from &#39;package:aqp&#39;: ## ## denormalize ## The following object is masked from &#39;package:greta&#39;: ## ## f ## The following object is masked from &#39;package:purrr&#39;: ## ## cross gg_mean &lt;- ggplot(grid, aes(x = lon, y = lat)) + xlim(c(-80, -60)) + ylim(c(45, 50)) + coord_equal() + geom_tile(aes(fill = pred_dd_mean)) + geom_contour(data = grid, mapping = aes(x = lon, y = lat, z = pred_dd_mean), binwidth = 50, colour = &quot;black&quot;, lwd = 0.2) + geom_label_contour(aes(z = pred_dd_mean)) + geom_path(data = world, aes(x = long, y = lat, group = group)) + geom_point(data = weather, mapping = aes(x = lon, y = lat), size = 0.1) + scale_fill_gradient(low = &quot;#8CBFE6&quot;, high = &quot;#FF0099&quot;, na.value = &quot;grey80&quot;) gg_sd &lt;- ggplot(grid, aes(x = lon, y = lat)) + xlim(c(-80, -60)) + ylim(c(45, 50)) + coord_equal() + geom_tile(aes(fill = pred_dd_sd)) + geom_contour(data = grid, mapping = aes(x = lon, y = lat, z = pred_dd_sd), binwidth = 50, colour = &quot;black&quot;, lwd = 0.2) + geom_label_contour(aes(z = pred_dd_sd)) + geom_path(data = world, aes(x = long, y = lat, group = group)) + geom_point(data = weather, mapping = aes(x = lon, y = lat), size = 0.1) + scale_fill_gradient(low = &quot;#8CBFE6&quot;, high = &quot;#FF0099&quot;, na.value = &quot;grey80&quot;) cowplot::plot_grid(gg_mean, gg_sd, labels = c(&quot;A&quot;, &quot;B&quot;), nrow = 2) Figure 13.6: Pr√©diction des degr√©s-jour dans l‚Äôespace avec les processus gaussiens 13.7 Les objets spatialis√©s en R Au chapitre 11, nous avons couvert le type d‚Äôobjet ts, sp√©cialis√© pour les s√©ries temporelles. De m√™me, le type d‚Äôobjet sf est sp√©cialis√© pour les objets geor√©f√©renc√©s. Les formats de donn√©es spatiales conventionnellement utilis√©s en R depuis 2003 sont offerts par le module sp. Ce format h√©ritait de difficult√©s, r√©cemment surmont√©es par le module sf, plus convivial et mieux adapt√© au tidyverse. Bien que sp soit plus largement document√©, sf est suffisamment mature pour une utilisation professionnelle. √âvidemment, un aide-m√©moire a √©t√© cr√©√© (figure 13.7). Figure 13.7: Aide-m√©moire du module sf, cr√©√© par RStudio Nous avons couvert quatre types de donn√©es spatiales. Nous allons maintenant les traiter en deux cat√©gories: les donn√©es vectorielle, comprenant les points, lignes et polygones et les donn√©es raster, comprenant les grilles de donn√©es. 13.7.1 Donn√©es vectorielles (points, lignes et polygones) Un cas typique consiste √† importer un tableau de donn√©es localis√©es en un point, que l‚Äôon d√©sire localiser en format sf avec la fonction st_as_sf(). Le tableau weather, par exemple, comporte une latitude (colonne lat) et une longitude (colonne lon), sp√©cifi√©es dans l‚Äôargument coord. Puisqu‚Äôil s‚Äôagit de donn√©es canadiennes, je suppose que les coordonn√©es sont projet√©es en format NAD83, tel qu‚Äôutilis√© par Statistique Canada et Ressources naturelles Canada (√† d√©faut de trouver la bonne info en ce moment). Le code PROJ4, sp√©cifi√© sous l‚Äôargument crs, d√©crit l‚Äôellipso√Øde utilis√© pour calculer les longitudes et latitudes ainsi que, s‚Äôil y a lieu, la projection (d√©tails plus loin dans la section 13.8). library(&quot;sf&quot;) ## Linking to GEOS 3.6.2, GDAL 2.2.3, PROJ 4.9.3 weather_geo &lt;- weather %&gt;% st_as_sf(coords = c(&quot;lon&quot;, &quot;lat&quot;), crs = &quot;+proj=longlat +datum=NAD83&quot;) weather_geo ## Simple feature collection with 260 features and 6 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: -79.85 ymin: 45.02 xmax: -60.04 ymax: 49.84 ## epsg (SRID): 4269 ## proj4string: +proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs ## # A tibble: 260 x 7 ## station_id station_name prov degree_days cumul_precip sdi geometry ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;POINT [¬∞]&gt; ## 1 10247 ILE AUX GRUES QC NA NA NA (-70.53 47.07) ## 2 10661 NORTHEAST MARGAREE (AUT) NS 372. 222. 0.904 (-60.98 46.37) ## 3 10732 NICOLET QC 823. 78.4 0.704 (-72.66 46.23) ## 4 10761 MCTAVISH QC 915. 107 0.740 (-73.58 45.5) ## 5 10762 STE-CLOTILDE QC 860. 85.2 0.820 (-73.68 45.17) ## 6 10763 ILES DE LA MADELEINE QC 326. 167. 0.853 (-61.77 47.43) ## 7 10764 TROIS-RIVIERES QC 753. 72.2 0.731 (-72.52 46.35) ## 8 10791 MATAGAMI QC 343. 72.4 0.745 (-77.79 49.76) ## 9 10792 GRAND ETANG NS 268. NA NA (-61.05 46.55) ## 10 10797 MISTOOK QC 426. NA NA (-71.72 48.6) ## # ‚Ä¶ with 250 more rows Notre objet sf comprend des m√©tadonn√©es sur le type de g√©om√©trie (geometry type: POINT), les limites des objets (bbox: ...), le syst√®me de r√©f√©rence (epsg ou proj4string: ...) ainsi que le tableau descriptif. En format sf, la colonne geometry (qui elle est de type sfc) comprend dans chacune des cellules, exprim√©e sous forme de liste, toute l‚Äôinformation n√©cessaire √† la construction de la g√©om√©trie, que ce soit un point, une ligne ou un polygone. Pour ce qui est des polygones et des lignes, il est plus commun de les importer depuis des sources institutionnelles. On pourra t√©l√©charger des donn√©es g√©ographiques, puis d√©zipper les fichier manuellement. Mais on peut aussi copier un lien, le coller dans R et d√©zipper automatiquement. Le block de code suivant t√©l√©charge un dossier de shapefiles d√©crivant les polygones des r√©gions administratives du Qu√©bec. download.file(&quot;ftp://ftp.mrnf.gouv.qc.ca/public/dgig/produits/bdga5m/vectoriel/region_admin_SHP.zip&quot;, destfile=&quot;data/12_quebec/12_region_admin_SHP.zip&quot;) unzip(&quot;data/12_quebec/12_region_admin_SHP.zip&quot;, exdir = &quot;data/12_quebec/&quot;) Pour charger dans R des shapefiles en format sf, nous utilisons la fonction st_read() pointant vers le fichier .shp. quebec &lt;- st_read(&quot;data/12_quebec/region_admin_polygone.shp&quot;) ## Reading layer `region_admin_polygone&#39; from data source `/home/essi/Git/ecologie-mathematique-R/data/12_quebec/region_admin_polygone.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 21 features and 10 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: -79.7625 ymin: 44.99136 xmax: -56.93495 ymax: 62.58217 ## epsg (SRID): 4269 ## proj4string: +proj=longlat +datum=NAD83 +no_defs head(quebec) ## Simple feature collection with 6 features and 10 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: -79.7625 ymin: 48.04944 xmax: -56.93495 ymax: 62.58217 ## epsg (SRID): 4269 ## proj4string: +proj=longlat +datum=NAD83 +no_defs ## AREA PERIMETER REGIO_S_ REGIO_S_ID RES_NO_IND RES_DE_IND RES_CO_REG RES_NM_REG RES_CO_REF ## 1 1.240869e+02 94.6754067 2 1 50 02 0100 000 R√©gion administrative 10 Nord-du-Qu√©bec BDGA5M ## 2 7.530857e-04 0.1519356 3 2 50 02 0100 000 R√©gion administrative 09 C√¥te-Nord BDGA5M ## 3 4.460926e+01 55.8047474 4 3 50 02 0100 000 R√©gion administrative 09 C√¥te-Nord BDGA5M ## 4 5.028808e-01 8.8274338 5 4 50 02 0100 000 R√©gion administrative 09 C√¥te-Nord BDGA5M ## 5 8.779790e-02 2.0259021 6 5 50 02 0100 000 R√©gion administrative 09 C√¥te-Nord BDGA5M ## 6 3.813094e+00 22.6943449 7 2 50 02 0100 000 R√©gion administrative 09 C√¥te-Nord BDGA5M ## RES_CO_VER geometry ## 1 V2017-06 POLYGON ((-77.80893 62.4465... ## 2 V2017-06 POLYGON ((-66.6878 55.00005... ## 3 V2017-06 POLYGON ((-70.02987 55.0000... ## 4 V2017-06 POLYGON ((-66.25978 55.0000... ## 5 V2017-06 POLYGON ((-67.2192 55.00003... ## 6 V2017-06 POLYGON ((-63.60572 52.8760... Nous avons vu au chapitre 3 qu‚Äôil est pr√©f√©rable d‚Äô√©viter la r√©p√©tition de l‚Äôinformation. Dans le format tibble que nous avons utilis√© pour d√©crire les polygones, l‚Äôinformation attach√©e √† un polygone est r√©p√©t√©e pour chaque point qui le compose: forcer une information hi√©rarchis√©e √† se conformer √† une structure rectangulaire multiplie la quantit√© d‚Äôinformation. Le format sf √©vite cette mutiplication d‚Äôinformation en hi√©rachisant les polygones dans la colonne geometry. En guise d‚Äôexploration rapide, la fonction plot() affichera les choropl√®thes. plot(quebec) Si nous ne d√©sirons que la g√©om√©trie, quebec %&gt;% st_geometry() %&gt;% plot() # ou bien plot(st_geometry(quebec)), ou bien plot(quebec %&gt;% select(geometry)) Exercice. Explorer l‚Äôobjet quebec, en particulier la colonne geometry, notamment en utilisant la fonction str(). Vous pourrez soutirer les informations du syst√®me de coordonn√©es avec la fonction st_crs(). Il est possible de calculer des attributs des g√©om√©tries √† l‚Äôaide des fonctions st_area() pour les polygones, ou st_length() pour les lignes et les polygones et la fonction st_centroid() pour trouver le centro√Øde d‚Äôun polygone - √† cette √©tape, il se pourrait que R vous demande d‚Äôinstaller le module lwgeom: suivez ses consignes! quebec_point &lt;- quebec %&gt;% mutate(st_area = st_area(quebec), st_length = st_length(quebec)) %&gt;% st_centroid() ## Warning in st_centroid.sf(.): st_centroid assumes attributes are constant over geometries of x ## Warning in st_centroid.sfc(st_geometry(x), of_largest_polygon = of_largest_polygon): st_centroid does not give correct ## centroids for longitude/latitude data plot(quebec_point) ## Warning: plotting the first 9 out of 12 attributes; use max.plot = 12 to plot all Les aires et les p√©rim√®tres calcul√©s ne correspondent pas tout √† fait √† ceux des variables AREA et PERIMETER, probablement calcul√©s sur une autre base. La fonction st_centroid() cr√©e un nouveau tableau dont la g√©om√©trie est le POINT, elle doit donc √™tre pass√©e apr√®s les op√©rations sur les polygones. La fonction st_simplify() permet de simplifier les polygones en un nombre r√©duit de points, ce qui peut √™tre utile pour acc√©l√©rer les calculs. La fonction st_buffer() permet de cr√©er un rayon d‚Äôune longueur donn√©e autour d‚Äôun point, proc√©dure souvent utilis√©e pour visualiser un rayon d‚Äôinfluence. Mais pour calculer des distances, les donn√©es doivent projet√©es. Nous pouvons les projeter avec st_transform() avec le code EPSG d√©sir√©. quebec_point %&gt;% st_transform(3348) %&gt;% st_buffer(50000) %&gt;% # 50 km du centre de la r√©gion plot() ## Warning: plotting the first 10 out of 12 attributes; use max.plot = 12 to plot all D‚Äôautres op√©rations sur les vecteurs sont offertes et document√©es sous la fiche d‚Äôaire sf::geos_unary(). Enfin, pour exporter un tableau sf en format csv incluant la g√©om√©trie, utilisez st_write(obj = tableau,dsn = &quot;tableau.csv&quot;, layer_options = &quot;GEOMETRY=AS_XY&quot;). Toutefois, si la g√©om√©trie n‚Äôest pas consitu√©e de points, il faudra pr√©alablement transformer les polygones en points avec st_cast(). st_write(obj = quebec %&gt;% dplyr::filter(AREA &lt; 1) %&gt;% # ne retenir que quelques r√©gions pour cr√©er un fichier moins volumineux st_cast(&quot;POINT&quot;), dsn = &quot;data/12_quebec_export.csv&quot;, delete_dsn = TRUE, layer_options = &quot;GEOMETRY=AS_XY&quot;) 13.7.2 Donn√©es raster Les donn√©es rasters sont des grilles, souvent ench√¢ss√©es dans des images tif g√©or√©f√©renc√©es. Ces images peuvent comprendre plusieurs variables, que l‚Äôon nomme des bandes, en r√©f√©rence aux bandes spectrales des images sat√©litaires (rouge, vert et bleu). Les donn√©es raster peuvent √™tre import√©es dans votre session gr√¢ce √† deux fonctions du module raster : raster() importera des donn√©es raster √† une bande et brick(), des donn√©es raster √† plusieurs bandes. library(&quot;raster&quot;) canopy &lt;- raster(&quot;data/12_nytrees/canopy.tif&quot;) # source: https://assets.datacamp.com/production/repositories/738/datasets/79cb56df0fa27272e16b366a697aba8ac1d3e923/canopy.zip canopy ## class : RasterLayer ## dimensions : 230, 253, 58190 (nrow, ncol, ncell) ## resolution : 300, 300 (x, y) ## extent : 1793685, 1869585, 2141805, 2210805 (xmin, xmax, ymin, ymax) ## crs : +proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs ## source : /home/essi/Git/ecologie-mathematique-R/data/12_nytrees/canopy.tif ## names : canopy ## values : 0, 255 (min, max) manhattan &lt;- brick(&quot;data/12_nytrees/manhattan/manhattan.tif&quot;) # source: https://assets.datacamp.com/production/repositories/738/datasets/30830f8ba4a60aa1711f41e9a842b22cba3204f3/manhattan.zip manhattan ## class : RasterBrick ## dimensions : 773, 801, 619173, 3 (nrow, ncol, ncell, nlayers) ## resolution : 29.98979, 30.00062 (x, y) ## extent : 575667.9, 599689.7, 4503277, 4526468 (xmin, xmax, ymin, ymax) ## crs : +proj=utm +zone=18 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 ## source : /home/essi/Git/ecologie-mathematique-R/data/12_nytrees/manhattan/manhattan.tif ## names : manhattan.1, manhattan.2, manhattan.3 ## min values : 0, 0, 0 ## max values : 255, 255, 255 Les informations des objets RasterLayer et RasterBrick peuvent √™tre extraites par les fonctions extent(), ncell(), nlayers() et crs(). La fonction plot() permet d‚Äôexplorer les donn√©es en cr√©ant un graphique par bande. plot(manhattan) Les fichiers raster, en format qui viennent souvent en format tif, sont typiquement tr√®s volumineux. Si une plus faible r√©solution convient √† une analyse spatiale, on pourra simplifier un raster avec la fonction raster::aggregate() (j‚Äôutilise la notation module::fonction() pour √©viter la confusion avec la fonction dplyr::aggregate()). L‚Äôargument fact est le facteur de conversion et l‚Äôargument fun est la fonction d‚Äôaggr√©gation (typiquement mean ou median). manhattan_lowres &lt;- raster::aggregate(manhattan, fact = 20, fun = median) manhattan_lowres ## class : RasterBrick ## dimensions : 39, 41, 1599, 3 (nrow, ncol, ncell, nlayers) ## resolution : 599.7958, 600.0124 (x, y) ## extent : 575667.9, 600259.5, 4503067, 4526468 (xmin, xmax, ymin, ymax) ## crs : +proj=utm +zone=18 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 ## source : memory ## names : manhattan.1, manhattan.2, manhattan.3 ## min values : 13, 30, 47 ## max values : 186, 194, 185 plot(manhattan_lowres) Avec un facteur de conversion de 20, nous sommes pass√©s d‚Äôune grille de 773 \\(\\times\\) 801 √† 39 \\(\\times\\) 41. L‚Äôexemple utilis√© est volontairement exag√©r√© pour montrer l‚Äôeffet de la perte de r√©solution, et g√©n√©ralement le facteur de conversion utilis√© est plus faible que 20. La fonction reclassify() est l‚Äô√©quivalent de cut() pour les rasters. L‚Äôargument demand√©, en plus de l‚Äôobjet raster, est une matrice de classification √† trois colonnes. Les deux premi√®res colonnes sp√©cifient la plage de valeur √† classifier et la troisi√®me colonne sp√©cifie la valeur de remplacement (qui peut √™tre NA). La classification s‚Äôapplique √† toutes les couches s‚Äôil s‚Äôagit d‚Äôun RasterBrick. manhattan_rcl &lt;- reclassify(manhattan_lowres, rcl = matrix(c(0, 50, 1, 50, 100, 2, 100, 1000, NA), ncol = 3, byrow = TRUE)) plot(manhattan_rcl) 13.8 Les syst√®mes de coordonn√©es Les longitudes et latitudes sont des angles sur un ellipso√Øde de r√©volution. Diff√©rentes institutions utilisent diff√©rentes formes d‚Äôellipso√Øde portant leur nom particulier: NAD83, WGS84, ETRS89, etc. Les projections servent √† aplanir des coordonn√©es g√©od√©siques obtenues selon un ellipso√Øde donn√© en vue de cr√©er des repr√©sentations 2D, comme la projection Mercator universelle ou de nombreuses autres. Le syst√®me de coordonn√©es peut √™tre projet√© ou non. Syst√®me de coordonn√©es non-projet√©es: caract√©ris√© par des angles de longitude et de latitudes sur un syst√®me g√©od√©sique 3D repr√©sent√© par un ellipso√Øde de r√©volution. Syst√®me de coordonn√©es projet√©es: caract√©ris√© par des distances X et Y sur une syst√®me g√©od√©sique repr√©sent√© en 2D. Lorsque vous utilisez des shapefiles, les informations du syst√®me de coordonn√©es seront incluses dans le fichier ayant une extension prj. Examinons le syst√®me de coordonn√©es du tableau quebec avec la fonction st_crs(). st_crs(quebec) ## Coordinate Reference System: ## EPSG: 4269 ## proj4string: &quot;+proj=longlat +datum=NAD83 +no_defs&quot; D‚Äôembl√©e, la mention +proj=longlat retrouv√©e dans proj4string (une repr√©sentation de PROJ4) indique que le syst√®me n‚Äôest pas projet√©, et que le syst√®me g√©od√©sique est le GRS80, pratiquement identique au WGS84. Le code EPSG contient la m√™me information que le proj4string, traduite de mani√®re succincte par un code √† 4 chiffres. Dans certains cas, les informations du syst√®me de coordonn√©es ne sont pas disponibles: il vous faudra creuser si elles sont essentielles √† vos travaux. Pour assigner un syst√®me de coordonn√©es, vous pourrez soit assigner l‚ÄôEPSG par st_crs(objet_sg) &lt;- 4269 ou objet_sg &lt;- objet_sg %&gt;% st_set_crs(), ou bien le PROJ4 par st_crs(objet_sg) &lt;- &quot;+proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs&quot;. La proc√©dure est la m√™me pour les rasters, mais avec la fonction crs() au lieu de st_crs(). Pour passer d‚Äôun syst√®me de coordonn√©es √† un autre, utilisez st_transform() pour les vecteurs et projectRasters() pour les rasters. Pour les rasters, si vos donn√©es sont cat√©gorielles, et non pas num√©rique, utilisez method = &quot;ngb&quot; plut√¥t que la valeur par d√©faut, method = &quot;bilinear&quot;, con√ßue pour les variables num√©riques. 13.9 Manipuler des tableaux sf Vous avez peut-√™tre remarqu√© que j‚Äôai pr√©c√©demment effectu√© une op√©ration mutate() en mode pipeline (%&gt;%) sur un tableau sf. Eh oui, les sf sont compatibles avec le mode tidyverse. Vous pourrez filtrer avec filter(), s√©lectionner avec select() et manipuler des colonnes avec mutate(). Reprenons les donn√©es des esp√®ces en danger, mais cette fois-ci nous allons travailler avec des donn√©es spatialis√©es avec sf. D‚Äôabord, allons chercher une carte du monde: le format geojson peut √™tre import√© de la m√™me mani√®re que des shape files. Puisqu‚Äôun geojson consigne l‚Äôinformation g√©ographique en un seul fichier (les shapefiles en contiennent plusieurs), on peut l‚Äôimporter directement d‚ÄôInternet. world_gj &lt;- st_read(&quot;https://raw.githubusercontent.com/johan/world.geo.json/master/countries.geo.json&quot;) ## Reading layer `countries.geo&#39; from data source `https://raw.githubusercontent.com/johan/world.geo.json/master/countries.geo.json&#39; using driver `GeoJSON&#39; ## Simple feature collection with 180 features and 2 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -180 ymin: -85.60904 xmax: 180 ymax: 83.64513 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs world_gj ## Simple feature collection with 180 features and 2 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -180 ymin: -85.60904 xmax: 180 ymax: 83.64513 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs ## First 10 features: ## id name geometry ## 1 AFG Afghanistan MULTIPOLYGON (((61.21082 35... ## 2 AGO Angola MULTIPOLYGON (((16.32653 -5... ## 3 ALB Albania MULTIPOLYGON (((20.59025 41... ## 4 ARE United Arab Emirates MULTIPOLYGON (((51.57952 24... ## 5 ARG Argentina MULTIPOLYGON (((-65.5 -55.2... ## 6 ARM Armenia MULTIPOLYGON (((43.58275 41... ## 7 ATA Antarctica MULTIPOLYGON (((-59.57209 -... ## 8 ATF French Southern and Antarctic Lands MULTIPOLYGON (((68.935 -48.... ## 9 AUS Australia MULTIPOLYGON (((145.398 -40... ## 10 AUT Austria MULTIPOLYGON (((16.97967 48... Des multipolygones sont form√©s lorsque plusieurs polygones forment une seule entit√©, par exemple un pays constitu√© de plusieurs √Æles. Nous allons effectu√© la jointure entre le tableau world_gj et les donn√©es de l‚ÄôIUCN. De m√™me que pr√©c√©demment, les noms des pays doivent correspondre exactement. iucn_oecd &lt;- iucn_oecd %&gt;% replace(.==&quot;USA&quot;, &quot;United States of America&quot;) %&gt;% replace(.==&quot;UK&quot;, &quot;United Kingdom&quot;) %&gt;% dplyr::rename(&quot;name&quot; = &quot;region&quot;) Pour cette jointure, je d√©sire ne conserver que les donn√©es g√©ographiques des pays de l‚ÄôOCDE. Je peux effectuer une jointure √† gauche sur le tableau iucn_oecd ou une joiture √† droite sur le tableau world_gj. oecd_gj &lt;- world_gj %&gt;% right_join(iucn_oecd, by = &quot;name&quot;) ## Warning: Column `name` joining factor and character vector, coercing into character vector Contrairement aux tableaux tibble, le format sf conserve la g√©om√©trie. oecd_gj %&gt;% dplyr::select(name, n_threatened_species) ## Simple feature collection with 36 features and 2 fields (with 1 geometry empty) ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -180 ymin: -55.61183 xmax: 180 ymax: 83.23324 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs ## First 10 features: ## name n_threatened_species geometry ## 1 Mexico 151 MULTIPOLYGON (((-97.14001 2... ## 2 Brazil 110 MULTIPOLYGON (((-57.62513 -... ## 3 Australia 109 MULTIPOLYGON (((145.398 -40... ## 4 United States of America 78 MULTIPOLYGON (((-155.5421 1... ## 5 Canada 52 MULTIPOLYGON (((-63.6645 46... ## 6 Colombia 42 MULTIPOLYGON (((-75.37322 -... ## 7 Russia 40 MULTIPOLYGON (((143.648 50.... ## 8 Chile 36 MULTIPOLYGON (((-68.63401 -... ## 9 Slovenia 34 MULTIPOLYGON (((13.80648 46... ## 10 Switzerland 34 MULTIPOLYGON (((9.594226 47... Pour une raison ou une autre, si vous d√©sirex retirer la g√©om√©trie, oecd_gj %&gt;% dplyr::select(name, n_threatened_species) %&gt;% st_set_geometry(NULL) %&gt;% top_n(4) ## Selecting by n_threatened_species ## name n_threatened_species ## 1 Mexico 151 ## 2 Brazil 110 ## 3 Australia 109 ## 4 United States of America 78 On pourra explorer notre tableau avec la fonction plot(). plot(oecd_gj) Tout comme on effectue des jointures entre des tableaux, on peut effectuerd es jointures spatiales sur des sf. On pourra trouver des intersections entre polygones, effectuer des unions, des diff√©rences, etc. Par exemple, en mod√©lisation, il est commun d‚Äôextrapoler des r√©sultats sur une grille. Ici, nous cr√©ons une grille couvrant tout le qu√©bec (figure ??). Nous cr√©ons une grille constitu√©e des centro√Ødes, %&gt;% # (par d√©faut, il s‚Äôagit d‚Äôune grille de polygones rectangulaires) nous transformons le r√©sultat en format sf (au lieu de sfc), %&gt;% nous effectuons une jointure sous forme d‚Äôintersection et %&gt;% nous retirons les occurences hors de la jointure. quebec_grid &lt;- quebec %&gt;% st_make_grid(n = 80, what = &quot;centers&quot;) %&gt;% st_sf() %&gt;% # transformer en objet sf st_join(quebec, join = st_intersects) %&gt;% drop_na() ## although coordinates are longitude/latitude, st_intersects assumes that they are planar ## although coordinates are longitude/latitude, st_intersects assumes that they are planar par(mfrow = c(1, 2)) plot(quebec_grid %&gt;% st_geometry(), pch = 16, cex = 0.2, main = &quot;Grille Qu√©bec&quot;) plot(quebec_grid %&gt;% filter(RES_NM_REG == &quot;Mont√©r√©gie&quot;) %&gt;% st_geometry(), main = &quot;Grille Mont√©r√©gie&quot;) Pour soutirer la grille en vue de mod√©liser, quebec_grid %&gt;% st_coordinates() %&gt;% as_tibble() %&gt;% dplyr::rename(&quot;lon&quot; = &quot;X&quot;, &quot;lat&quot; = &quot;Y&quot;) ## # A tibble: 3,636 x 2 ## lon lat ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -74.2 45.1 ## 2 -73.9 45.1 ## 3 -73.6 45.1 ## 4 -73.3 45.1 ## 5 -73.1 45.1 ## 6 -72.8 45.1 ## 7 -72.5 45.1 ## 8 -72.2 45.1 ## 9 -71.9 45.1 ## 10 -71.6 45.1 ## # ‚Ä¶ with 3,626 more rows 13.10 Manipuler des objets raster Comme les objets vectoriels, les objets raster peuvent subir diff√©rents types d‚Äôop√©rations. Nous en couvrirons trois. Masque (mask()): l‚Äôintersection entre un polygone et un raster. D√©couper (crop()): d√©coupe rectangulaire selon les limites de l‚Äôobjet. Extraction (extract()): extrait, et accessoirement effectue un sommaire, des rasters dans un polygone donn√© Cr√©ons d‚Äôabord un polygone ayant le m√™me syst√®me de coordonn√©es que le raster canopy. poly &lt;- st_sfc(st_polygon(list(cbind(c(1800000, 1830000, 1820000, 1800000), c(2160000, 2200000, 2150000, 2160000))))) %&gt;% st_set_crs(as.character(crs(canopy))) %&gt;% st_cast(&quot;POLYGON&quot;) En ce moment, le module raster n‚Äôest pas adapt√© au format sf, qu‚Äôil faudrait pr√©alablement convertir vers l‚Äôancien format sp. poly_sp &lt;- as(poly, &quot;Spatial&quot;) Appliquons un masque, puis un crop, puis les deux. canopy_mask &lt;- mask(canopy, mask = poly_sp) canopy_crop &lt;- crop(canopy, y = poly_sp) canopy_mc &lt;- crop(canopy_mask, y = poly_sp) par(mfrow = c(1, 4)) plot(canopy, main = &quot;Original&quot;) plot(poly_sp, add = TRUE) plot(canopy_mask, main = &quot;mask()&quot;) plot(poly_sp, add = TRUE) plot(canopy_crop, main = &quot;crop()&quot;) plot(poly_sp, add = TRUE) plot(canopy_mc, main = &quot;mask() &amp; crop()&quot;) plot(poly_sp, add = TRUE) Pour effectuer un calcul sur l‚Äôint√©rieur du polygone avec extract()‚Ä¶ on sp√©cifie le raster, le polygone et la fonction! extract(canopy, poly_sp, fun = mean) ## [,1] ## [1,] 10.51405 13.11 Graphiques d‚Äôobjets spatialis√©s Pour afficher les objets sf et raster, nous avons utilis√© les fonctions de base √† titre exploratoire. Mais lorsque vient le temps de publier une carte, la trousse de ggplot2 est toute indiqu√©e, en y ajoutant l‚Äôoutil geom_sf(). ggplot(quebec) + geom_sf(aes(fill = RES_NM_REG)) + geom_sf(data = quebec_point) Les coordonn√©es peuvent √™tre manipul√©es avec coord_sf(). world_gj_iucn &lt;- world_gj %&gt;% full_join(iucn_oecd, by = &quot;name&quot;) ## Warning: Column `name` joining factor and character vector, coercing into character vector ggplot(world_gj_iucn) + geom_sf(aes(fill = n_threatened_species), colour = &quot;gray50&quot;) + coord_sf(xlim = c(-170, -40), ylim = c(10, 80)) + scale_fill_gradient(low = &quot;#8CBFE6&quot;, high = &quot;#FF0099&quot;, na.value = &quot;grey80&quot;) + labs(title = &quot;Number of threatened species in OECD countries&quot;, subtitle = &quot;Source: OCDE, 2019&quot;) + guides(fill = guide_legend(title = &quot;Number of\\nthreatened\\nspecies&quot;)) Les cartes th√©matiques de tmap (thematic maps) sont construites sensiblement de la m√™me mani√®re que ggplot2. Diff√©rents types de projections sont disponibles avec diff√©rentes palettes de couleurs (palette_explorer()). library(&quot;tmap&quot;) library(&quot;tmaptools&quot;) tm_shape(set_projection(world_gj_iucn, &quot;+proj=wintri&quot;)) + tm_polygons(&quot;n_threatened_species&quot;, palette = &quot;viridis&quot;) ## Warning in CPL_crs_from_proj4string(x): GDAL cannot import PROJ.4 string `+proj=wintri&#39;: returning missing CRS ## Warning: The shape set_projection(world_gj_iucn, &quot;+proj=wintri&quot;) is invalid. See sf::st_is_valid ## Warning: The shape set_projection(world_gj_iucn, &quot;+proj=wintri&quot;) contains empty units. Si vous d√©sirez cr√©er des cartes int√©ractives, passez en mode leaflet en sp√©cifiant tmap_mode(&quot;view&quot;) (pour revenir en mode statique, tmap_mode(&quot;plot&quot;)) tmap_mode(&quot;view&quot;) ## tmap mode set to interactive viewing tm_shape(world_gj_iucn) + tm_polygons(&quot;n_threatened_species&quot;, palette = &quot;viridis&quot;) ## Warning: The shape world_gj_iucn is invalid. See sf::st_is_valid ## Warning: The shape world_gj_iucn contains empty units. Petit exemple avec des facettes synchronis√©es. quebec_tmap &lt;- tm_shape(quebec) + tm_polygons(c(&quot;AREA&quot;, &quot;PERIMETER&quot;)) + tm_facets(sync = TRUE, ncol = 2) quebec_tmap Les cartes tmap peuvent √™tre export√©es sous forme d‚Äôimage. tmap_save(tm = quebec_tmap, filename = &quot;images/12_quebec_tmap.png&quot;, height=7) ## Map saved to /home/essi/Git/ecologie-mathematique-R/images/12_quebec_tmap.png ## Resolution: 3220.011 by 2100 pixels ## Size: 10.73337 by 7 inches (300 dpi) Toutefois, au moment d‚Äô√©crire ces lignes, l‚Äôexportation en format dynamique ne fonctionne pas pour les facettes. De plus, il semble y avoir un bogue avec les chemins relatifs. Il faudra donc coller (paste0) le r√©pertoire de travail (getwd()) au chemin relatif (&quot;/images/12_quebec_tmap_widget/12_quebec_tmap.html&quot;). Enfin, les fichiers html font souvent r√©f√©rence √† des fichiers externes: mieux vaut les enregistrer dans des dossiers ind√©pendants (dans ce cas dans &quot;12_quebec_tmap_widget&quot;, que vous pourrez zipper avant de partager, ou placer sur un site web) plut√¥t que dans un dossier comprenant plusieurs images. Pas si compliqu√©‚Ä¶ quand on le sait. quebec_tmap_area &lt;- tm_shape(quebec) + tm_polygons(&quot;AREA&quot;) tmap_save(tm = quebec_tmap_area, filename = paste0(getwd(),&quot;/images/12_quebec_tmap_widget/12_quebec_tmap.html&quot;)) ## Interactive map saved to /home/essi/Git/ecologie-mathematique-R/images/12_quebec_tmap_widget/12_quebec_tmap.html 13.12 Ressources compl√©mentaires Geocomputation with R, de Robin Lovelace, Jakub Nowosad et Jannes Muenchow (2019). Spatial Data Science, de Robert J. Hijmans, du Geospatial and Farming Systems Research Consortium (GFC), University of California (2019) Simple Features for R, de Edzer Pebesma (2019) "],
["chapitre-ode.html", "14 Mod√©lisation de m√©canismes √©cologiques 14.1 √âquations diff√©rentielles 14.2 Les √©quations diff√©rentielles ordinaires en mod√©lisation √©cologique", " 14 Mod√©lisation de m√©canismes √©cologiques Ô∏è¬†Objectifs sp√©cifiques: √Ä la fin de ce chapitre, vous saurez d√©finir une √©quation diff√©rentielle ordinaire et une √©quation diff√©rentielle partielle saurez aptes √† d√©tecter un probl√®me impliquant le besoin d‚Äôutiliser des √©quations diff√©rentielles serez en mesure d‚Äôeffectuer une mod√©lisation impliquant un syst√®me d‚Äô√âDO en contexte √©cologique On se r√©f√®re √† la mod√©lisation m√©canistique lorsque des principes th√©oriques guident une mod√©lisation, √† l‚Äôinverse de la mod√©lisation ph√©nom√©nologique, qui est guid√©e par les donn√©es. Il existe de nombreuses techniques de mod√©lisation m√©canistique, mais la plupart sont guid√©es par les √©quations diff√©rentielles. 14.1 √âquations diff√©rentielles Les √©quations diff√©rentielles permettent la r√©solution de probl√®mes impliquant des gradients dans le temps et dans l‚Äôespace. On les utilise pour mod√©liser la dynamique des populations, la thermodynamique, l‚Äô√©coulement de l‚Äôeau dans les sols, le transport des solut√©s, etc. On en distingue deux grandes cat√©gories: les √©quations diff√©rentielles ordinaires et partielles. √âquations diff√©rentielles ordinaires (√âDO). Les √©quations diff√©rentielles ordinaires s‚Äôappliquent sur des fonctions s‚Äôappliquant √† une seule variables, qui est souvent le temps. On pourra suivre, par exemple, l‚Äô√©volution de la temp√©rature en un point, en fonction du temps √† partir d‚Äôune condition initiale. Parfois, plusieurs √âDO sont utilis√©es conjointement pour cr√©er un syst√®me d‚Äô√âDO que l‚Äôon pourra nomm√© un syst√®me dynamique. Les solutions analytiques des √âDO sont parfois relativement faciles √† r√©soudre, mais les ordinateurs permettent des r√©solutions num√©riques en quelques lignes de code. √âquations diff√©rentielles partielles (√âDP). Dans ce cas, ce sont plusieurs variables qui sont diff√©renci√©es dans la m√™me fonction. Il peut s‚Äôagir des coordonn√©es dans l‚Äôespace \\([x, y, z]\\) (r√©gime permanent), qui peuvent aussi √™tre appliqu√©s √† diff√©rents pas de temps (r√©gime transitoire). Le probl√®me sera d√©limit√© non pas seulement par des conditions initiales, mais aussi par des conditions aux fronti√®res du mod√®le. Puisque que les solutions analytiques des EDP peuvent rarement √™tre d√©velopp√©es, on utilisera pratiquement toujours des approches num√©riques que sont principalement les m√©thodes de r√©solution par diff√©rences finies ou par √©l√©ments finis. La pr√©sente mouture de ce manuel ne comprend pas la r√©solution d‚Äô√âDP. 14.2 Les √©quations diff√©rentielles ordinaires en mod√©lisation √©cologique L‚Äô√©volution des populations dans le temps peut √™tre abord√©e √† l‚Äôaide de syst√®mes d‚Äô√©quations diff√©rentielles. Une simple √©quation d√©crivant la croissance d‚Äôune population peut √™tre coupl√©e √† des sch√©mas d‚Äôexploitation de cette population, que ce soit une exploitation foresti√®re, une terre fourrag√®re ou un territoire de chasse. On pourra aussi faire interagir des populations dans des sch√©mas de relations biologiques. Ces processus peuvent √™tre impl√©ment√©s avec des processus al√©atoires pour g√©n√©rer des sch√©mas probabilistes. De plus, les biostatistiques et l‚Äôautoapprentissage peuvent √™tre mis √† contribution afin de calibrer les mod√®les. 14.2.1 √âvolution d‚Äôune seule population en fonction du temps La croissance d‚Äôune population (ou de sa densit√©) isol√©e en fonction du temps d√©pend des conditions qui lui offre son environnement. Dans le cas de la biomasse d‚Äôune culture √† croissance constante, le taux de croissance est toujours le m√™me. \\[ \\frac{d üåø }{dt} = c \\] \\[ \\int_0^t c dt = \\int_{üåø_0}^{üåø(t)} ~düåø \\] \\[ ct = üåø(t) - üåø_0\\] \\[ üåø(t) = üåø_0 + ct \\] library(&quot;tidyverse&quot;) y0 &lt;- 2 c &lt;- 2 # exprim√© en individu / pas de temps time &lt;- seq(0, 6, 0.1) y &lt;- y0 + c * time tibble(time, y) %&gt;% ggplot(aes(x = time, y = y)) + geom_line() + geom_label(x = max(time), y = max(y), label = round(max(y))) + expand_limits(y = 0) Dans le cas d‚Äôune population qui se reproduit, une formulation simple mod√©lise une √©volution lin√©aire associ√©e √† un taux de natalit√© \\(n\\) et un taux de mortalit√© \\(m\\), o√π \\(r = n-m\\) est le taux de croissance de la population d‚Äôune population de lapins üê∞ en fonction du temps \\(t\\). \\[ \\frac{düê∞}{dt} = nüê∞ - müê∞ = rüê∞ \\] \\[ \\int_0^t dt = \\int_{üê∞_0}^{üê∞(t)} \\frac{1}{rüê∞} ~düê∞ \\] \\[ t = \\frac{1}{r} ln(üê∞) \\bigg\\rvert_{üê∞_0}^{üê∞(t)} \\] \\[ rt = ln \\left( \\frac{üê∞(t)}{üê∞_0} \\right) \\] \\[ üê∞(t) = üê∞_0 exp(rt) \\] La vitesse de croissance est constante pour une population constante, mais la croissance de la population est exponentielle √©tant donn√©e que chaque nouvel individu se reproduit. y0 &lt;- 10 r &lt;- 0.2 # exprim√© en individu / pas de temps time &lt;- seq(0, 10, 0.1) y &lt;- y0 * exp(r*time) tibble(time, y) %&gt;% ggplot(aes(x = time, y = y)) + geom_line() + geom_label(x = max(time), y = max(y), label = round(max(y))) + expand_limits(y = 0) De 10 lapins au d√©part, nous en avons un peu plus de 75 apr√®s 10 ans‚Ä¶ et pr√®s de 5 milliards apr√®s 100 ans! En fait, la capacit√© de support d‚Äôune population √©tant g√©n√©ralement limit√©e, on peut supposer que le taux de natalit√© d√©croit et que le taux de mortalit√© croit lin√©airement avec l‚Äôeffectif. \\[ n(üê∞) = \\alpha - \\beta üê∞ \\] \\[ m(üê∞) = \\gamma + \\delta üê∞ \\] On aura donc \\[ \\frac{düê∞}{dt} = üê∞ \\left( \\alpha - \\beta üê∞ \\right) - üê∞ \\left( \\gamma + \\delta üê∞ \\right) = rüê∞ \\left( 1 - \\frac{üê∞}{K} \\right) \\] o√π \\(r = \\alpha - \\gamma\\) est l‚Äôordonn√©e √† l‚Äôorigine du taux de croissance (th√©orique, lorsque la population est nulle) et \\(K = \\frac{\\alpha-\\gamma}{\\beta + \\delta}\\) est la capacit√© limite du milieu de subsistance. On pourra s‚Äôaider d‚Äôun logiciel de calcul symbolique comme sympy ou maxima pour en tirer une solution analytique. Mais √† ce point, nous utiliserons une approximation num√©rique. Nous utiliserons le module deSolve. library(&quot;deSolve&quot;) deSolve demande de d√©finir les param√®tres de l‚Äô√âDO ou du syst√®me d‚Äô√âDO. Nous devons d‚Äôabord sp√©cifier √† quels pas de temps notre √âDO doit √™tre approxim√©e. J‚Äô√©tends la plage de temps √† 30 ans pour bien visualiser la courbe de croissance. time &lt;- seq(0, 30, by = 0.5) Les conditions initiales du syst√®me d‚Äô√âDO sont aussi d√©finies dans un vecteur. La seule condition initiale de notre √âDO est le nombre initial de lapin. y0 &lt;- c(lapin = 10) On d√©finira les param√®tres dans un vecteur p. Dans notre cas, nous avons \\(r\\), le taux de croissance √† l‚Äôorigine et \\(K\\), la capacit√© de support de l‚Äô√©cosyst√®me. Il est pr√©f√©rable de nommer les param√®tres du vecteur pour √©viter les erreurs. p &lt;- c(r = 0.2, K = 40) Enfin, une fonction d√©finit l‚Äô√âDO avec, comme entr√©es, les pas de temps, les conditions initiales et les param√®tres. La sortie de la fonction est un vecteur des d√©riv√©es embo√Æt√©s dans une liste (lisez le fichier d‚Äôaide de la fonction ode pour les d√©tails en lan√ßant ?ode). model_logistic &lt;- function(t, y, p) { lapin &lt;- y[1] dlapin_dt &lt;- p[1] * lapin * (1 - lapin/p[2]) return(list(c(dlapin_dt))) } Une fois que les pas de temps, les conditions initiales, les param√®tres et le mod√®le sont d√©finis, on les sp√©cifie comme arguments dans la fonction ode. La sortie de la fonction ode est une matrice dont la premi√®re colonne comprend les pas de temps impos√©s, et les autres colonnes sont les d√©riv√©es sp√©cifi√©es √† la sortie de la fonction ode. lapin_t &lt;- ode(y = y0, times = time, model_logistic, p) head(lapin_t) ## time lapin ## [1,] 0.0 10.00000 ## [2,] 0.5 10.76856 ## [3,] 1.0 11.57342 ## [4,] 1.5 12.41288 ## [5,] 2.0 13.28478 ## [6,] 2.5 14.18643 lapin_t %&gt;% as_tibble() %&gt;% ggplot(aes(x = time, y = lapin)) + geom_line() + expand_limits(y = 0) Exercice. Que ce passerait-il si le taux de croissance √©tait n√©gatif? Profitez-en pour changer les param√®tres r et K. Exercice. D‚Äôautres formulations existent pour exprimer des taux de croissance (Gompertz, Allee, etc.). En outre la formulation de Gompertz s‚Äô√©crit comme suit. \\[ \\frac{düê∞}{dt} = rüê∞ \\left( ln \\frac{K}{üê∞} \\right) \\] Entrer cet √âDO dans R avec deSolve. 14.2.2 Population exploit√©e L‚Äôexploitation d‚Äôune population peut √™tre effectu√©e de diff√©rentes mani√®res. D‚Äôabord, le pr√©l√®vement peut √™tre effectu√© de mani√®re constante, par exemple dans un √©levage ou par la chasse ou la cueillette. Ajoutons un pr√©l√®vement constant dans une courbe de croissance logistique. \\[ \\frac{düê∞}{dt} = rüê∞ \\left( 1 - \\frac{üê∞}{K} \\right) - Q \\] o√π \\(Q\\) est le quota, ou le pr√©l√®vement constant. On pourra aussi effectuer un pr√©l√®vement proportionnel √† la population. \\[ \\frac{düê∞}{dt} = rüê∞ \\left( 1 - \\frac{üê∞}{K} \\right) - Eüê∞ \\] o√π \\(E\\) est l‚Äôeffort d‚Äôexploitation. Ou bien effectuer une s√©rie de pr√©l√®vement ponctuels, comme la r√©colte de plantes fourrag√®res. \\[ \\frac{düåø}{dt} = c - \\left[ üåø - \\gamma \\right] \\bigg\\rvert_{t=a, b, c, d, e, ...} \\] o√π \\(\\gamma\\) est le reste de la biomasse apr√®s la r√©colte et \\(t=a, b, c, d, e, ...\\) sont les pas de temps o√π le bloc entre les crochets est actif, c‚Äôest-√†-dire la p√©riode de r√©colte. La solution analytique d‚Äôune culture √† croissance constante est plut√¥t facile √† d√©duire. Les fonctions de pr√©l√®vement peuvent √™tre modul√©es √† votre guise. Prenons pour l‚Äôexemple un pr√©l√®vement constant et une croissance logistique. p &lt;- c(r = 0.2, K = 40, Q = 1) model_logistic_expl &lt;- function(t, y, p) { lapin &lt;- y[1] dlapin_dt &lt;- p[1] * lapin * (1 - lapin/p[2]) - p[3] return(list(c(dlapin_dt))) } lapin_t &lt;- ode(y = y0, times = time, model_logistic_expl, p) lapin_t %&gt;% as_tibble() %&gt;% ggplot(aes(x = time, y = lapin)) + geom_line() + expand_limits(y = 0) Exercice. Mod√©liser avec un pr√©l√®vement proportionnel. Qu‚Äôarrive-t-il lorsque le pr√©l√®vement est trop √©lev√©? L‚Äôexploitation ponctuelle, comme la r√©colte ou l‚Äôadministration d‚Äôune s√©rie de traitements, implique l‚Äôutilisation d‚Äôapproches intermittentes. Bien que deSolve ignore les changements dans les variables d‚Äô√©tat (y) tels que d√©finis dans les d√©riv√©s, nous pouvons avoir recours √† des √©v√®nements dans le jargon de deSolve. Ces √©v√®nements doivent √™tre sp√©cifi√©s dans un data.frame ou une liste. Il est difficile de trouver un exemple g√©n√©rique pour mod√©liser des √©v√®nements. Pour en savoir davantage, je vous invite donc √† consulter la fiche d‚Äôaide ?events. Dans notre cas, nous allons mod√©liser une r√©colte de plantes fourrag√®res. La r√©colte est d√©clench√©e lorsque le rendement atteint 2 t/ha, et laisser 0.3 t/ha au sol pour assurer le renouvellement pour les coupes subs√©quentes. D√©finissons d‚Äôabord les entr√©es du mod√®les. time &lt;- seq(0, 120, 0.1) p &lt;- c(r = 0.1, K = 2.5) y0 &lt;- c(champ = 0.1) Nous devons d√©finir une fonction root (racine), comprenant tous les arguments de la fonction d‚Äô√âDO, dont la sortie est une valeur qui d√©clenchera un √©v√®nement lorsque la valeur sera nulle. Dans notre cas, la valeur correspond simplement au rendement moins 2, la quantit√© au champ y[1]. Notez que d‚Äôautres strat√©gies peuvent √™tre utilis√©es pour d√©clencher une r√©colte, par exemple le pourcentage de floraison qui demanderait des simulations plus pouss√©es. recolte_root &lt;- function(t, y, p) y[1]-2 Puis, lorsque la fonction root est d√©clench√©e, l‚Äô√©v√®nement ram√®ne la quantit√© au champs √† 0.3 t/ha, une quantit√© qui permet de relancer la croissance. recolte_event &lt;- function(t, y, p) { y[1] &lt;- 0.3 return(y) } La fonction du mod√®le est telle qu‚Äôutilis√©e auparavant: une fonction logistique. recolte &lt;- function(t, y, p) { champ &lt;- y[1] dchamp_dt &lt;- p[1] * champ * (1 - champ/p[2]) return(list(c(dchamp_dt))) } La fonction ode est lanc√©e en entrant les fonction root et events. out &lt;- ode(times = time, y = y0, func = recolte, parms = p, rootfun = recolte_root, events = list(func = recolte_event, root = TRUE), method=&quot;impAdams&quot;) plot(out) Nous pourrons organiser deux r√©coltes de 1.7 t/ha et une de 2 t/ha pour terminer la saison. Exercice. Qu‚Äôadviendrait-il si vous laissiez 0.15 t/ha au champ au lieu de 0.3? Ou si vous laissiez 1 t/ha? Ou si vous d√©clenchiez une r√©colte √† 2.3 t/ha? D√©fi. Pouvez-vous mod√©liser l‚Äôensilage? 14.2.3 Interactions biologiques Les interactions biologiques entre deux esp√®ces √† un stade de croissance d√©fini peuvent prendre diff√©rentes formes, du mutualisme (les deux esp√®ces b√©n√©ficient de la relation) √† la comp√©tition (les deux esp√®ces se nuisent) en passant par la pr√©dation ou le parasitisme (une esp√®ce b√©n√©ficie de l‚Äôautre en lui nuisant) ou le neutralisme (aucun effet). Ces effets sont d√©crits dans Pringle (2016) en un tableau synth√®se. Figure 14.1: Interactions biologiques, Pringle, E.G. 2016. Orienting the Interaction Compass: Resource Availability as a Major Driver of Context Dependence. Plos Biology. https://doi.org/10.1371/journal.pbio.2000891. Ces interactions peuvent √™tre d√©crite math√©matiquement dans des syst√®mes d‚Äô√âDO, ou √âDO coupl√©es. Le cas d‚Äô√©tude le plus courant reprend le syst√®me d‚Äô√©quation pr√©dateur-proie de Lotka-Volterra, deux auteurs ayant d√©velopp√© de mani√®re ind√©pendante des √©quations similaires respectivement en 1925 et 1926. Les √©quations de Lotka-Volterra supposent une croissance illimit√©e des deux esp√®ces: les proies üê∞ se reproduisent par elles-m√™mes (\\(\\alpha üê∞\\)), tandis que les pr√©dateurs ü¶ä croissent selon la disponibilit√© des proies (\\(\\delta üê∞ü¶ä\\)). √Ä l‚Äôinverse, la mortalit√© des proies d√©pend du nombre de pr√©dateurs (\\(- \\beta üê∞ü¶ä\\)), mais la mortalit√© des pr√©dateurs est ind√©pendante des proies (\\(- \\gamma ü¶ä\\)). On obtient ainsi un syst√®me d‚Äô√©quation. \\[\\frac{düê∞}{dt} = \\alpha üê∞ - \\beta üê∞ü¶ä = üê∞ \\left( \\alpha - \\beta ü¶ä \\right)\\] \\[\\frac{dü¶ä}{dt} = \\delta üê∞ü¶ä - \\gamma ü¶ä = ü¶ä \\left( \\delta üê∞ - \\gamma \\right) \\] √Ä l‚Äô√©quilibre de üê∞, c‚Äôest-√†-dire o√π \\(\\frac{düê∞}{dt} = 0\\), on retrouve \\(üê∞=0\\) ou \\(ü¶ä = \\frac{\\alpha}{\\beta}\\). De m√™me, √† l‚Äô√©quilibre de ü¶ä, on retrouve \\(ü¶ä=0\\) ou \\(üê∞ = \\frac{\\gamma}{\\delta}\\). En termes math√©matiques, ces √©quilibre sont des isoclines, des points d‚Äôinflexion dans le syst√®me d‚Äô√âDO. Nous allons r√©soudre les √©quations de Lotka-Volterra avec deSolve. Rappelons-nous que nous devons d√©finir des pas de temps o√π approximer les populations (times), des conditions initiales (y0) et des param√®tres (p). time &lt;- seq(0, 30, by = 0.1) y0 &lt;- c(lapin = 3, renard = 1) p &lt;- c(alpha = 2, # taux de croissance des lapins (naissance - mortalit√©, 1/an) beta = 0.8, # taux de pr√©dation des lapins (renard / an) delta = 0.1, # taux de conversion lors de la pr√©dation (lapin / renard) gamma = 0.2) # mortalit√© naturelle des renards (1/an) On peut calculer d‚Äôembl√©e les isoclines. lapin_iso &lt;- p[4]/p[3] renard_iso &lt;- p[1]/p[2] Nous devons ensuite cr√©er notre mod√®le. modele_LV &lt;- function(t, y, p) { lapin = y[1] renard = y[2] dlapin_dt = p[1] * lapin - p[2] * lapin * renard drenard_dt = p[3] * lapin * renard - p[4] * renard return(list(c(dlapin_dt, drenard_dt))) } Lan√ßons l‚Äôapproximation. effectifs_t = ode(y = y0, times = time, modele_LV, p) head(effectifs_t) ## time lapin renard ## [1,] 0.0 3.000000 1.000000 ## [2,] 0.1 3.380961 1.011940 ## [3,] 0.2 3.806028 1.028156 ## [4,] 0.3 4.278154 1.049326 ## [5,] 0.4 4.799633 1.076263 ## [6,] 0.5 5.371673 1.109943 effectifs_t %&gt;% as_tibble() %&gt;% gather(key=&quot;espece&quot;, value = &quot;value&quot;, -time) %&gt;% ggplot(aes(x=time, y=value)) + geom_line(aes(colour=espece)) + expand_limits(y = 0) Lorsque la population de lapins croit, celle des renards croit √† retardement jusqu‚Äô√† ce que la population de lapin diminue jusqu‚Äô√† √™tre presque √©teinte. Dans ces conditions, la population de renard ne peut plus √™tre soutenue, et d√©croit, ce qui en retour donne l‚Äôopportunit√© de la population de lapins de resurgir. effectifs_t %&gt;% as_tibble() %&gt;% ggplot(aes(x = lapin, y = renard)) + geom_path() + geom_hline(yintercept = lapin_iso, linetype = 2) + geom_vline(xintercept = renard_iso, linetype = 2) Les conditions initiales sont responsables de l‚Äôamplitude des cycles. Excercice. V√©rifier l‚Äôeffet des param√®tres sur les cycles. Qu‚Äôadviendrait-il des populations si l‚Äôon prenait plut√¥t un profil de croissance logistique chez les lapins? \\[\\frac{düê∞}{dt} = \\alphaüê∞ \\left( 1-\\frac{üê∞}{K} \\right) - \\beta üê∞ü¶ä \\] \\[\\frac{dü¶ä}{dt} = \\delta üê∞ü¶ä - \\gamma ü¶ä \\] Pour les isoclines, √† l‚Äô√©quilibre o√π \\(\\frac{düê∞}{dt} = 0\\), on retrouve \\(ü¶ä=\\frac{\\alpha}{\\beta} \\left( 1-\\frac{üê∞}{K} \\right)\\) ou \\(üê∞=0\\). De m√™me que pr√©c√©demment, √† l‚Äô√©quilibre de ü¶ä, on retrouve \\(ü¶ä=0\\) ou \\(üê∞ = \\frac{\\gamma}{\\delta}\\). Reprenons nos param√®tres, mais en ajoutant la capacit√© de support des lapins, √† \\(K = 40\\). time &lt;- seq(0, 60, by = 0.1) y0 &lt;- c(lapin = 3, renard = 1) p &lt;- c(alpha = 2, # taux de croissance des lapins (naissance - mortalit√©, 1/an) beta = 0.8, # taux de pr√©dation des lapins (renard / an) delta = 0.1, # taux de conversion lors de la pr√©dation (lapin / renard) gamma = 0.2, # mortalit√© naturelle des renards (1/an) K = 40) # capacit√© de support de l&#39;√©cosyst√®me Calculons les isoclines, en tenant compte que, cette fois-ci, l‚Äôisocline des renards est une fonction du nombre de lapins. lapin_iso &lt;- p[4] / p[3] renard_iso &lt;- tibble(lapin = seq(from = 0, to = 40, by = 1)) %&gt;% # acec une s√©quence de lapins ... mutate(renard = p[1] / p[2] * (1 - lapin/p[5])) # ... calculer les renards Le mod√®le logistique diff√®re peu du mod√®le classique de Lotka-Volterra. modele_LV_logist &lt;- function(t, y, p) { lapin = y[1] renard = y[2] dlapin_dt = p[1] * lapin * (1-y[1]/p[5]) - p[2] * lapin * renard drenard_dt = p[3] * lapin * renard - p[4] * renard return(list(c(dlapin_dt, drenard_dt))) } Lan√ßons la mod√©lisation, puis affichons les r√©sultats. effectifs_t &lt;- ode(y = y0, times = time, modele_LV_logist, p) gg_time &lt;- effectifs_t %&gt;% as_tibble() %&gt;% gather(key=&quot;espece&quot;, value = &quot;value&quot;, -time) %&gt;% ggplot(aes(x=time, y=value)) + geom_line(aes(colour=espece)) + expand_limits(y = 0) gg_cycle &lt;- effectifs_t %&gt;% as_tibble() %&gt;% ggplot(aes(x = lapin, y = renard)) + geom_path() + geom_vline(xintercept = lapin_iso, linetype = 2) + geom_line(data = renard_iso, linetype = 2) + xlim(c(0, 10)) cowplot::plot_grid(gg_time, gg_cycle) Ainsi con√ßu, le syst√®me tant vers des effectifs constants aux isoclines. Dans les cycles √©tudi√©s jusqu‚Äôici, les effectifs atteignent syst√©matiquement un √©tat critique, mais se recouvrent sans cesse. Il serait toutefois √©tonnant que les param√®tres des √©quations (reproduction, mortalit√©, pr√©dation, support des √©cosyst√®mes) soient constants. On peut admettre que les param√®tres peuvent varier en fonction de d‚Äôautres param√®tres, ou simplement au hasard. Justement, il est possible d‚Äôajouter de la stochastique (processus al√©atoire) dans nos fonctions. En outre, plusieurs simulations pourront nous indiquer un risque d‚Äôeffondrement d‚Äôun √©cosyst√®me. Mais adviendra la possibilit√© que les effectifs des populations prennent des valeurs n√©gatives, ce qui n‚Äôest pas admissible. Une solution est de reformuler nos √©quations pour faire en sorte de mod√©liser le logarithme des effectifs, qui pourront √™tre recalcul√©es par l‚Äôexponentielle dans la base du log. Un log n√©gatif retransform√© par l‚Äôexponentiel devient une fraction de 1 (si \\(log_{10}(x) = -1\\), \\(x = 0.1\\)). Une autre approche est d‚Äôutiliser un √©v√©nement ramenant l‚Äôeffectif n√©gatif √† z√©ro, d√©clanch√© lorsqu‚Äôun des effectifs est infrieur ou √©gal √† z√©ro. C‚Äôest ce que nous allons faire, avec les m√™mes time, y0 et p que pr√©c√©demment. La fonction root est un moyen de d√©clencher l‚Äô√©v√©nement. Elle prend la valeur de z√©ro si l‚Äôun des deux effectifs est nul. zero_root &lt;- function(t, y, p) { x1 &lt;- y[1] &gt;= 0 x2 &lt;- y[2] &gt;= 0 xnum &lt;- as.numeric(x1 &amp; x2) return(xnum) } zero_event &lt;- function(t, y, p) { if (y[1] &lt;= 0) y[1] &lt;- 0 if (y[2] &lt;= 0) y[2] &lt;- 0 return(y) } Reprenons la fonction logistique, mais en ajoutant un effet al√©atoire √† chacun des param√®tres. modele_LV_alea &lt;- function(t, y, p) { lapin = y[1] renard = y[2] alpha &lt;- rnorm(1, p[1], 0.0005) beta &lt;- rnorm(1, p[2], 0.0005) delta &lt;- rnorm(1, p[3], 0.001) gamma &lt;- rnorm(1, p[4], 0.001) K &lt;- rnorm(1, p[5], 1) dlapin_dt &lt;- alpha * lapin * (1-lapin/K) - beta * lapin * renard drenard_dt &lt;- delta * lapin * renard - gamma * renard return(list(c(dlapin_dt, drenard_dt))) } La mod√©lisation prend en compte l‚Äô√©v√©nement. set.seed(14389) effectifs_t = ode(y = y0, times = time, func = modele_LV_alea, parms = p, rootfun = zero_root, events = list(func = zero_event, root = TRUE), method=&quot;impAdams&quot;) effectifs_tibble &lt;- effectifs_t %&gt;% unclass() %&gt;% as_tibble() On lance ensuite les m√™mes fonctions de visualisation que pr√©c√©demment. gg_time &lt;- effectifs_tibble %&gt;% gather(key=&quot;espece&quot;, value = &quot;value&quot;, -time) %&gt;% ggplot(aes(x=time, y=value)) + geom_line(aes(colour=espece)) + expand_limits(y = 0) gg_cycle &lt;- effectifs_tibble %&gt;% ggplot(aes(x = lapin, y = renard)) + geom_path(aes(colour = time)) + geom_vline(xintercept = lapin_iso, linetype = 2) + geom_line(data = renard_iso, linetype = 2) + expand_limits(x = 0, y = 0) cowplot::plot_grid(gg_time, gg_cycle) Une tr√®s faible variance sur les param√®tres peu grandement perturber le syst√®me. Il est possible, en effectuant plusieur simulations en boucle, d‚Äô√©valuer le risque d‚Äôeffondrement des effectifs d‚Äôune esp√®ce, ce qui arrive pour le cas simul√© pour les lapins, puis pour les renards. Nous avons mod√©lis√© une relation biologique de pr√©dation. Il existe dans la litt√©rature une panoplie de mod√®les d‚Äô√âDO pour d√©crire les relations biologiques, qui peuvent √™tre mod√©lis√©s entre plusieurs esp√®ces pour cr√©er des r√©seaux trophiques complexes. Toutefois, la difficult√© de collecter des donn√©es en quantit√© et en qualit√© suffisante rendent ces mod√®les difficiles √† appr√©hender. Exercice. Mod√©liser une comp√©tition intersp√©cifique o√π chaque population croit de mani√®re logistique. \\[\\frac{düêÅ}{dt} = r_1 üêÅ \\left( 1-\\frac{üêÅ}{K_1} -\\alpha \\frac{üêÄ}{K_2} \\right) \\] \\[\\frac{düêÄ}{dt} = r_2 üêÄ \\left( 1-\\frac{üêÄ}{K_2} -\\beta \\frac{üêÅ}{K_1} \\right) \\] o√π \\(r_1\\) et \\(r_2\\) sont les taux de croissances respectifs des üêÅ et des üêÄ, ainsi que \\(K_1\\) et que \\(K_2\\) sont les capacit√©s de support des üêÅ et des üêÄ. Le coefficient \\(\\alpha\\) d√©crit l‚Äôampleur de la comp√©tition de üêÄ sur üêÅ et le coefficient \\(\\beta\\) d√©crit l‚Äôampleur de la comp√©tition de üêÅ sur üêÄ (\\(\\alpha\\) et \\(\\beta\\) sont &gt;= 0). Exercice. Les interactions biologiques forment une bonne introduction aux syst√®mes d‚Äô√©quations diff√©rentielles ordinaires. On fait n√©anmoins souvent r√©f√©rence aux √©quations de Lorenz (1963), qui a d√©velopp√© un syst√®me d‚Äô√âDO chaotique depuis trois √©quations, \\[ X&#39; = aX + YZ, \\] \\[ Y&#39; = b \\left(Y-Z\\right), \\] \\[ Z&#39; = -XY + cY - Z, \\] o√π \\(X\\) est la temp√©rature horizontale, \\(Y\\) est la temp√©rature verticale, \\(Z\\) est le flux de chaleur convectif, et o√π l‚Äôon retrouve les param√®tres \\(a = -8/3\\), \\(b=-10\\) et \\(c=28\\). R√©soudre les √©quations de Lorents avec deSolve. Porter graphiquement les relations entre X, Y et Z. "]
]
