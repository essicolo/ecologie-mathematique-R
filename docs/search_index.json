[
["index.html", "Analyse et modÃ©lisation dâ€™agroÃ©cosystÃ¨mes 1 Introduction 1.1 DÃ©finitions 1.2 Ã€ qui sâ€™adresse ce manuel? 1.3 Les logiciels libres 1.4 Langage de programmation 1.5 Contenu du manuel 1.6 Objectifs gÃ©nÃ©raux 1.7 Lectures complÃ©mentaires 1.8 Besoin dâ€™aide? 1.9 Ã€ propos de lâ€™auteur 1.10 Un cours complÃ©mentaire Ã  dâ€™autres cours 1.11 Contribuer au manuel", " Analyse et modÃ©lisation dâ€™agroÃ©cosystÃ¨mes Serge-Ã‰tienne Parent 2020-01-13 1 Introduction En dÃ©veloppant son jeu de la vie (game of life) en 1970, John Horton Connway a prÃ©sentÃ© un exemple percutant que des rÃ¨gles simples peuvent mener Ã  des rÃ©sultats inattendus. Le jeu consiste Ã  placer des jetons sur les cases dâ€™un plateau de jeu consistant en une simple grille orthogonale. Le jeu Ã©volue en fonction du nombre de jetons prÃ©sents parmi les huit cases du voisinage des jetons ou des cases vides. Les jetons ayant 0 ou 1 voisin sont retirÃ©s. Les jetons ayant 2 ou 3 voisins restent intacts Les jetons ayant plus de 3 voisins sont retirÃ©s Un jeton est posÃ© sur les cases ayant exactement 3 voisins Câ€™est tout. Selon la maniÃ¨re dont les jetons sont placÃ©s au dÃ©part, il se peut que la grille se vide de ses jetons, ou que les jetons y prennent beaucoup de place. Il arrive aussi que des cycles rÃ©guliers se dÃ©gagent ou que lâ€™on se retrouve avec des formes rÃ©guliÃ¨res. Vous aurez peut-Ãªtre compris Ã  ce stade pourquoi le jeu est appelÃ© â€œjeu de la vieâ€. La premiÃ¨re rÃ¨gle est une situation localisÃ©e de sous-population, condition dans laquelle la reproduction est difficile. La deuxiÃ¨me rÃ¨gle est une situation localisÃ©e stable. La troisiÃ¨me est une situation de surpopulation, oÃ¹ des individus meurent par un environnement rendu inadÃ©quat par insuffisance de ressource ou surplus de toxicitÃ©. Enfin, la quatriÃ¨me indique une situation favorable Ã  la reproduction. Une grille vidÃ©e correspond Ã  une extinction et une grille remplie correspond Ã  une explosion de population. Une oscillation est un â€œclimaxâ€, un Ã©tat stable en Ã©cologie. Un lÃ©ger changement initial dans la disposition initiale des jetons peut mener Ã  des solutions diffÃ©rentes. Le jeu, qui en fait est une application de la technique des automates cellulaires, se complexifie Ã  mesure que le nombre de jetons grandit. Un humain passera des heures Ã  calculer une itÃ©ration Ã  50 jetons, commettra probablement quelques erreurs et demandera quelques cafÃ©s. Un processeur pourra gÃ©rer des centaines dâ€™itÃ©rations sur des grilles de centaines de jetons en quelques secondes. En Ã©tablissant des rÃ¨gles correspondant aux mÃ©canismes de lâ€™objet Ã©tudiÃ©, il devient possible de modÃ©liser lâ€™Ã©volution des systÃ¨mes vivants, comme lâ€™Ã©mergence dâ€™espÃ¨ces invasives. Simulation avec automates cellulaires. Source: Anonyme, publiÃ© sur Giphy. 1.1 DÃ©finitions Les mathÃ©matiques confÃ¨rent aux humains une capacitÃ© dâ€™abstraction suffisamment complexe pour leur permettre de toucher les Ã©toiles et les atomes, dâ€™assembler la pensÃ©e pour mieux apprÃ©cier lâ€™histoire et de prÃ©dire le futur, de toucher lâ€™infini et de goÃ»ter Ã  lâ€™Ã©ternitÃ©. Ã€ partir des maths, on a pu crÃ©er des outils de calcul qui permettent de projeter des images de lâ€™univers, bien au-delÃ  de la Voie lactÃ©e. Mais apprÃ©hender le vivant demeure nÃ©anmoins une tÃ¢che complexe. Carte des domaines de lâ€™Ã©cologie mathÃ©matique Lâ€™Ã©cologie mathÃ©matique couvre un large spectre de domaines, mais peut Ãªtre divisÃ©e en deux branches: lâ€™Ã©cologie thÃ©orique et lâ€™Ã©cologie quantitative (Legendre et Legendre, 2012). Alors que lâ€™Ã©cologie thÃ©orique sâ€™intÃ©resse Ã  lâ€™expression mathÃ©matique des mÃ©canismes Ã©cologiques, lâ€™Ã©cologie quantitative, plus empirique, en Ã©tudie principalement les phÃ©nomÃ¨nes. La modÃ©lisation Ã©cologique vise Ã  prÃ©voir une situation selon des conditions donnÃ©es. Faisant partie Ã  la fois de lâ€™Ã©cologie thÃ©orique et de lâ€™Ã©cologie quantitative, elle superpose souvent des mÃ©canismes de lâ€™Ã©cologie thÃ©orique et des phÃ©nomÃ¨nes empiriques de lâ€™Ã©cologie quantitative. Lâ€™Ã©cologie numÃ©rique comprend la branche descriptive de lâ€™Ã©cologie quantitative, câ€™est-Ã -dire quâ€™elle sâ€™intÃ©resse Ã  Ã©valuer des effets Ã  partir de donnÃ©es empiriques. Lâ€™exploration des donnÃ©es dans le but dâ€™y dÃ©couvrir des structures passe souvent par des techniques multivariÃ©es comme la classification hiÃ©rarchique ou la rÃ©duction dâ€™axe (par exemple, lâ€™analyse en composantes principales), qui sont davantage heuristiques (dans notre cas, bioheuristique) que statistiques. Les tests dâ€™hypothÃ¨ses et lâ€™analyse des probabilitÃ©s, quant Ã  eux, relÃ¨vent de la biostatistique. Le gÃ©nie Ã©cologique, une discipline intimement liÃ©e Ã  lâ€™Ã©cologie mathÃ©matique, est vouÃ© Ã  lâ€™analyse, la modÃ©lisation, la conception et la construction de systÃ¨mes vivants dans le but de rÃ©soudre de maniÃ¨re efficace des problÃ¨mes liÃ©s Ã  lâ€™Ã©cologie et Ã  une panoplie de domaines qui lui sont raccordÃ©s. Lâ€™agriculture est lâ€™un de ces domaines. Câ€™est dâ€™emblÃ©e la discipline qui sera prisÃ©e dans ce manuel. NÃ©anmoins, les principes qui seront discutÃ©s sont transfÃ©rables Ã  lâ€™Ã©cologie gÃ©nÃ©rale. 1.2 Ã€ qui sâ€™adresse ce manuel? Le cours vise Ã  introduire des Ã©tudiants graduÃ©s en agronomie, biologie, Ã©cologie, sols, gÃ©nie agroenvironnemental, gÃ©nie civil et gÃ©nie Ã©cologique Ã  lâ€™analyse et la modÃ©lisation dans leur domaine, tant pour les appuyer pour leurs travaux de recherche que pour leur fournir une trousse dâ€™outil Ã©mancipatrice pour leur cheminement professionnel. Plus spÃ©cifiquement, vous serez accompagnÃ© Ã  dÃ©couvrir diffÃ©rents outils numÃ©riques qui vous permettront dâ€™apprÃ©hender vos donnÃ©es, dâ€™en faire Ã©merger lâ€™information et de construire des modÃ¨les. En ce sens, câ€™est un cours de pilotage, pas un cours de mÃ©canique. Bien que des connaissances en programmation et en statistiques aideront grandement les Ã©tudiant.e.s Ã  apprÃ©hender ce document, une littÃ©ratie informatique nâ€™est pas requise. Dans tous les cas, quiconque voudra tirer profit de ce manuel devra faire preuve dâ€™autonomie. Vous serez guidÃ©s vers des ressources et des rÃ©fÃ©rences, mais je vous suggÃ¨re vivement de dÃ©velopper votre propre bibliothÃ¨que adaptÃ©e Ã  vos besoins et Ã  votre maniÃ¨re de comprendre. 1.3 Les logiciels libres Tous les outils numÃ©riques qui sont proposÃ©s dans ce cours sont des logiciels libres: Â« Logiciel libre Â» [free software] dÃ©signe des logiciels qui respectent la libertÃ© des utilisateurs. En gros, cela veut dire que les utilisateurs ont la libertÃ© dâ€™exÃ©cuter, copier, distribuer, Ã©tudier, modifier et amÃ©liorer ces logiciels. Ainsi, Â« logiciel libre Â» fait rÃ©fÃ©rence Ã  la libertÃ©, pas au prix1 (pour comprendre ce concept, vous devez penser Ã  Â« libertÃ© dâ€™expression Â», pas Ã  Â« entrÃ©e libre Â»). - Projet GNU Donc: codes sources ouverts, dÃ©veloppement souvent communautaire, gratuitÃ©. Plusieurs raisons Ã©thiques, principalement liÃ©es au contrÃ´le de lâ€™environnement virtuel par les utilisateurs et les communautÃ©s, peuvent justifier lâ€™utilisation de logiciels libres. Plusieurs raisons pratiques justifient aussi cette orientation. Les logiciels libres vous permettent de transporter vos outils avec vous, dâ€™une entreprise Ã  lâ€™autre, au bureau, ou Ã  la maison, et ce, sans vous soucier dâ€™acheter de coÃ»teuses licences. On soulÃ¨ve avec justesse les risques liÃ©s aux possibles erreurs dans les codes des logiciels communautaires. Pour les scientifiques, une erreur peut mener Ã  une Ã©tude retirÃ©e de la littÃ©rature et mÃªme, potentiellement, des politiques publiques mal avisÃ©es. Pour les ingÃ©nieurs, les consÃ©quences pourraient Ãªtre dramatiques. Mais retenez quâ€™en toute circonstance, comme professionnel.le, vous Ãªtes responsable des outils que vous utilisez: vous devez vous assurer de la bonne qualitÃ© dâ€™un logiciel, quâ€™il soit propriÃ©taire ou communautaire. Alors que la qualitÃ© des logiciels propriÃ©taires est gÃ©nÃ©ralement suivie par audits, celle des logiciels libres est plutÃ´t soumise Ã  la vigilance communautaire. Chaque approche a ses avantages et inconvÃ©nients, mais elles ne sont pas exclusives. Ainsi les logiciels libres peuvent Ãªtre auditÃ©s Ã  lâ€™externe par quiconque dÃ©cide de le faire. DiffÃ©rentes entreprises, souvent concurrentes, participent tant Ã  cette vigilance quâ€™au dÃ©veloppement des logiciels libres: elles en sont mÃªme souvent les instigatrices (comme RStudio, Anaconda et Enthought). Par ailleurs, ce manuel est distribuÃ© librement (licence MIT). 1.4 Langage de programmation 1.4.1 R Ce cours est basÃ© sur le langage R. En plus dâ€™Ãªtre libre, R est un langage de programmation dynamique largement utilisÃ© dans le monde universitaire, et dont lâ€™utilisation sâ€™Ã©tend de maniÃ¨re soutenue hors des tours dâ€™ivoire. R is also the name of a popular programming language used by a growing number of data analysts inside corporations and academia. It is becoming their lingua franca partly because data mining has entered a golden age, whether being used to set ad prices, find new drugs more quickly or fine-tune financial models. New York Times, janvier 2019 Son dÃ©veloppement est supportÃ© par la R Foundation for Statistical Computing, basÃ©e Ã  lâ€™UniversitÃ© de Vienne. Ã‰galement, lâ€™Ã©quipe de RStudio contribue largement au dÃ©veloppement de modules gÃ©nÃ©riques. R est principalement utilisÃ© pour le calcul statistique, mais les rÃ©cents dÃ©veloppements le rendent un outil de choix pour tout ce qui entoure la science des donnÃ©es, de lâ€™interaction avec les bases de donnÃ©es au dÃ©ploiement dâ€™outils dâ€™intelligence artificielle en passant par la visualisation. Une fois implÃ©mentÃ© avec des modules de calcul scientifique spÃ©cialisÃ©s en biologie, en Ã©cologie et en agronomie (que nous couvrirons au long du cours), R devient un outil de calcul convivial, rapide et fiable pour le calcul Ã©cologique. 1.4.2 Pourquoi pas Python? La premiÃ¨re mouture de ce cours se fondait sur le langage Python. Tout comme R, Python est un langage de programmation dynamique prisÃ© pour le calcul scientifique. Python est un langage gÃ©nÃ©rique apprÃ©ciÃ© pour sa polyvalence et sa simplicitÃ©. Python est utilisÃ© autant pour crÃ©er des logiciels ou des sites web que pour le calcul scientifique. Ainsi, Python peut Ãªtre utilisÃ© en interopÃ©rabilitÃ© avec une panoplie de logiciels libres, comme QGIS pour la cartographie et FreeCAD pour le dessin technique. Il est particuliÃ¨rement apprÃ©ciÃ© en ingÃ©nierie pour ses modules de calcul par Ã©lÃ©ments finis (FeNICS, SfePy) et en bioinformatique pour ses outils liÃ©s au sÃ©quenÃ§age (scikit-bio), mais ses lacunes en analyse statistique, en particulier en statistiques multivariÃ©es mâ€™ont amenÃ© Ã  favoriser R. Bien que leurs possibilitÃ©s se superposent largement, ce serait une erreur dâ€™aborder R et Python comme des langages rivaux. Les deux langages sâ€™expriment de maniÃ¨re similaire et sâ€™inspirent mutuellement: apprendre Ã  travailler avec lâ€™un revient Ã  apprendre lâ€™autre. Les spÃ©cialistes en calcul scientifique tendent Ã  apprendre Ã  travailler avec plus dâ€™un langage de programmation. Par ailleurs, lâ€™entreprise Ursa labs travaille en ce moment Ã  lâ€™Ã©laboration dâ€™une infrastructure de donnÃ©es permettant de partager des objets R et Python, en vue dâ€™intÃ©grer diffÃ©rents langages de programmation dans un mÃªme flux de travail. 1.4.3 Pourquoi pas Matlab? Parce quâ€™on est en 2019. 1.4.4 Etâ€¦ SAS? Parce quâ€™on est Ã  lâ€™universitÃ©. 1.4.5 Mais pourquoi pas ______ ? Dâ€™autres langages, comme Julia, Scala, Javascript et mÃªme Ruby sont utilisÃ©s en calcul scientifique. Ils sont nÃ©anmoins moins garnis et moins documentÃ©s que R. Des langages de plus bas niveau, comme Fortran et C++, viennent souvent appuyer les fonctions des autres langages: ces langages sont plus ardus Ã  utiliser au jour le jour, mais leur rapiditÃ© de calcul est imbattable. 1.5 Contenu du manuel Le pire angle avec lequel je pourrais aborder le sujet, câ€™est avec du code et des formules mathÃ©matiques. Ã€ travers chacun des chapitres, je tenterai de vous amener Ã  rÃ©soudre des problÃ¨mes de la maniÃ¨re la plus intuitive possible. Nous aborderons lâ€™analyse et la modÃ©lisation infÃ©rentielle, prÃ©dictive et dÃ©terministe appliquÃ©e aux agroÃ©cosystÃ¨mes. Chapitre 2 - Introduction au langage de programmation R. Quâ€™est-ce que R? Comment lâ€™aborder? Quelles sont les fonctionnalitÃ©s de base et comment tirer profit de tout lâ€™Ã©cosystÃ¨me de programmation? Chapitre 3 - Organisation des donnÃ©es et opÃ©rations sur des tableaux. Les tableaux permettent dâ€™enchÃ¢sser lâ€™information dans un format prÃªt-Ã -porter pour R. Comment les importer, les exporter, les filtrer, et en faire des sommaires? Chapitre 4 - Visualisation. Comment prÃ©senter lâ€™information contenue dans un long tableau en un seul coup dâ€™oeil? Chapitre 5 - Le travail collaboratif, le suivi de version et la science ouverte. Ce chapitre offre une introduction Ã  lâ€™utilisation des outils de calcul collaboratif, ainsi quâ€™un aperÃ§u du systÃ¨me de suivi de version git et de son utilisation sur GitHub. Chapitre 6 - Biostatistiques. Il est audacieux de ne consacrer quâ€™un seul chapitre sur ce vaste sujet. Nous irons Ã  lâ€™essentielâ€¦ pour vous donner les outils qui permettront dâ€™approfondir le sujet. Chapitre 7 - Biostatistiques bayÃ©siennes. Une trÃ¨s brÃ¨ve introduction pour qui est intÃ©ressÃ© Ã  lâ€™analyse bayÃ©sienne. Chapitre 8 - Explorer R. La science des donnÃ©es Ã©volue rapidement. Vous gagnerez Ã  vous tenir au courrant de son Ã©volution, et immanquablement vous vous buterez sur des opÃ©rations qui vous sembleront insolubles. Ce chapitre vous accompagnera Ã  rester Ã  jour sur le dÃ©veloppement de R, Ã  poser de bonnes questions et proposera des modules intÃ©ressants en Ã©cologie mathÃ©matique. Chapitre 9 - Association, partitionnement et ordination. Les Ã©cosystÃ¨mes diffÃ¨rent, mais en quoi sont-ils semblables, et en quoi dffÃ¨rent-ils? Ces questions importantes peuvent Ãªtre abordÃ©s par lâ€™Ã©cologie numÃ©rique, domaine dâ€™Ã©tude au sein duquel lâ€™association, le partitionnement et lâ€™ordination sont des outils prÃ©dominants. Chapitre 10 - DÃ©tection de valeurs aberrantes et imputation. Une donnÃ©e aberrante sortira du lot, pour une raison ou pour une autre. Comment les dÃ©tecter de maniÃ¨re systÃ©matique? Dâ€™autre part, que faire lorsquâ€™une donnÃ©e est manquante? Peut-on lâ€™imputer? Comment? Chapitre 11 - Les sÃ©ries temporelles. Les capteurs modernes permettent de gÃ©nÃ©rer des donnÃ©es en fonction du temps. Que ce soit des donnÃ©es mÃ©tÃ©orologiques enregistrÃ©es quotidiennement ou des donnÃ©es de teneur en eau enregistrÃ©es au 5 secondes, les donnÃ©es en fonction du temps forment un signal. Comment analyser ces signaux? Chapitre 12 - Lâ€™autoapprentissage. Les applications de lâ€™intelligence artificielle ne sont limitÃ©es que par votre imagination. Encore faut-il lâ€™utiliser intelligemment. Chapitre 13 - Les donnÃ©es spatiales. Non, nous nâ€™aborderons pas les gÃ©ostatistiques. Ce chapitre porte plutÃ´t sur lâ€™utilisation de R comme systÃ¨me dâ€™information gÃ©ographique de base. Nous utiliserons aussi lâ€™autoapprentissage comme outil dâ€™interpolation spatial. Chapitre 14 - La modÃ©lisation dÃ©terministe. Les modÃ¨les sont des maquettes simplifiÃ©es. Comment utiliser les Ã©quations diffÃ©rentielles ordinaires pour crÃ©er ces maquettes? Si les chapitres 3 Ã  5 peuvent Ãªtre considÃ©rÃ©s comme fondamentaux pour bien maÃ®triser R, les autres peuvent Ãªtre feuilletÃ©s Ã  la piÃ¨ce, bien quâ€™ils forment une suite logique. Chaque chapitre de ce manuel est rÃ©digÃ© en format R notebook, dans un environnement RStudio. Pour exÃ©cuter les commandes, les vous pourrez soit copier-coller les commandes dans R (ou RStudio), soit tÃ©lÃ©charger les fichiers-sources et exÃ©cuter les blocs de code. 1.6 Objectifs gÃ©nÃ©raux Ã€ la fin du cours, lâ€™Ã©tudiant.e sera en mesure: de programmer en langage R dâ€™importer, de manipuler (sÃ©lection des colonnes, filtres, sommaires statistiques) et dâ€™exporter des tableaux de gÃ©nÃ©rer des graphiques dâ€™utilisation commune dâ€™apprÃ©hender des donnÃ©es Ã©cologiques et agronomiques Ã  lâ€™aide de tests statistiques frÃ©quentiels dâ€™explorer par lui.elle-mÃªme les possibilitÃ©s offertes par la communautÃ© de dÃ©veloppement de modules R dâ€™explorer les donnÃ©es Ã  lâ€™aide des outils de lâ€™Ã©cologie numÃ©rique (association, partitionnement et ordination) dâ€™imputer des donnÃ©es manquantes dans un tableau et de dÃ©tecter des valeurs aberrantes dâ€™effectuer une analyse de sÃ©rie temporelle de sâ€™assurer que ses calculs soit auditables et reproductibles dans une perspective de science ouverte de crÃ©er un modÃ¨le dâ€™autoapprentissage dâ€™intrapoler des donnÃ©es spatiales de modÃ©liser des Ã©quations diffÃ©rentielles ordinaires 1.7 Lectures complÃ©mentaires 1.7.1 Ã‰cologie mathÃ©matique How to be a quantitative ecologist. Jason Mathipoulos vous prend par la main pour dÃ©couvrir les notions de mathÃ©matiques fondamentales en Ã©cologie, appliquÃ©es avec le langage R. Numerical ecology. Lâ€™ouvrage hautement dÃ©taillÃ© des frÃ¨res Legendre est non seulement fondamental, mais aussi fondateur dâ€™une science qui Ã©volue encore aujourdâ€™hui: lâ€™analyse des donnÃ©es Ã©cologiques. A practical guide to ecological modelling. Soetaert et Herman portent une attention particuliÃ¨re Ã  la prÃ©sentation des principes de modÃ©lisation dans un langage accessible - ce qui est rarement le cas dans le domaine de la modÃ©lisation. Les modÃ¨les prÃ©sentÃ©s concernent principalement les bilans de masse, en termes de systÃ¨mes de rÃ©actions chimiques et de relations biologiques. ModÃ©lisation mathÃ©matique en Ã©cologie. Rare livre en modÃ©lisation Ã©cologique publiÃ© en franÃ§ais, la premiÃ¨re partie sâ€™attarde aux concepts mathÃ©matiques, alors que la deuxiÃ¨me planche Ã  les appliquer. Si le haut niveau dâ€™abstraction de la premiÃ¨re partie vous rebute, nâ€™hÃ©sitez pas dÃ©buter par la seconde partie et de vous rÃ©fÃ©rer Ã  la premiÃ¨re au besoin. A new ecology: systems perspective. Principalement grÃ¢ce au soleil, la Terre forme un ensemble de gradients dâ€™Ã©nergie qui se dÃ©clinent en des systÃ¨mes dâ€™une Ã©tonnante complexitÃ©. Câ€™est ainsi que le regrettÃ© Sven Erik JÃ¸rgensen (1934-2016) et ses collaborateurs dÃ©crivent les Ã©cosystÃ¨mes dans cet ouvrage qui fait suite aux travaux fondateurs de Howard Thomas Odum. Sven Erik JÃ¸rgensen Ecological engineering. Principle and Practice. Ecological processes handbook. Modeling complex ecological dynamics 1.7.2 Programmation R for data science. Lâ€™analyse de donnÃ©es est une branche importante de lâ€™Ã©cologie mathÃ©matique. Ce manuel traite des matrices et la manipulation de donnÃ©es chapitre 3), de la visualisation (chapitre 4) ainsi que de lâ€™apprentissage automatique (chapitre 11). R for data science repasse ces sujets plus en profondeur. En particulier, lâ€™ouvrage de Garrett Grolemund et Hadley Wickham offre une introduction au module graphique ggplot2. Numerical ecology with R. Daniel Borcard enseigne lâ€™Ã©cologie numÃ©rique Ã  lâ€™UniversitÃ© de MontrÃ©al. Son cours est condensÃ© dans ce livre recettes vouÃ© Ã  lâ€™application des principes lourdement dÃ©crits dans Numerical ecology. 1.7.3 Divers The truthful art. Dans cet ouvrage, Alberto Cairo sâ€™intÃ©resse Ã  lâ€™utilisation des donnÃ©es et de leurs prÃ©sentations pour fournir une information adÃ©quate Ã  diffÃ©rents publics. 1.8 Besoin dâ€™aide? Les ouvrages de rÃ©fÃ©rence reconnus vous offrent des bases solides sur lesquelles vous pouvez vous appuyer dans vos travaux. Mais au-delÃ  des principes, au jour le jour, vous vous buterez immanquablement Ã  toutes sortes de petits problÃ¨mes. Quel module utiliser pour cette tÃ¢che prÃ©cise? Que veut dire ce message dâ€™erreur? Comment interprÃ©ter ce rÃ©sultat? Pour tous les petits accrocs du quotidien en calcul scientifique, internet offre de nombreuses ressources qui sont trÃ¨s hÃ©tÃ©rogÃ¨nes en qualitÃ©. Vous apprendrez Ã  reconnaÃ®tre les ressources fiables Ã  celles qui sont douteuses. Les plateformes basÃ©es sur Stack Exchange, comme Stack Overflow et Cross Validated, mâ€™ont souvent Ã©tÃ© dâ€™une aide prÃ©cieuse. Vous aurez avantage Ã  vous construire une petite banque dâ€™information (Turtl, Notion, Evernote, Google Keep, One Note, etc.) en collectant des liens, en prenant en notes certaines recettes et en suivant des sites dâ€™intÃ©rÃªt avec des flux RSS. 1.9 Ã€ propos de lâ€™auteur Je mâ€™appelle Serge-Ã‰tienne Parent. Je suis ingÃ©nieur Ã©cologue et professeur adjoint au DÃ©partement des sols et de gÃ©nie agroalimentaire de lâ€™UniversitÃ© Laval, QuÃ©bec, Canada. Je crois que la science est le meilleur moyen dâ€™apprÃ©hender le monde pour prendre des dÃ©cisions avisÃ©es. 1.10 Un cours complÃ©mentaire Ã  dâ€™autres cours Ce cours a Ã©tÃ© dÃ©veloppÃ© pour ouvrir des perspectives mathÃ©matiques en Ã©cologie et en agronomie Ã  la FSAA de lâ€™UniversitÃ© Laval. Il est complÃ©mentaire Ã  certains cours offerts dans dâ€™autres institutions acadÃ©miques au QuÃ©bec, dont ceux-ci. BIO2041. Biostatistiques 1, UniversitÃ© de MontrÃ©al BIO2042. Biostatistiques 2, UniversitÃ© de MontrÃ©al BIO109. Introduction Ã  la programmation scientifique, UniversitÃ© de Sherbrooke BIO500. MÃ©thodes en Ã©cologie computationnelle, UniversitÃ© de Sherbrooke. 1.11 Contribuer au manuel Je suis ouvert aux commentaires et suggestions. Pour contribuer directement, dirigez-vous sur le dÃ©pÃ´t du manuel sur GitHub, puis ouvrez une Issue pour en discuter. CrÃ©ez une nouvelle branche (fork), effectuez les modifications, puis lancer une requÃªte de fusion (pull resquest). "],
["chapitre-intro-a-R.html", "2 La science des donnÃ©es avec R 2.1 Statistiques ou science des donnÃ©es? 2.2 DÃ©buter en R 2.3 PrÃ©parer son flux de travail 2.4 Premiers pas avec R 2.5 Enfinâ€¦ 2.6 Extra: Utiliser R avec Jupyter", " 2 La science des donnÃ©es avec R ï¸Â Objectifs spÃ©cifiques: Ã€ la fin de ce chapitre, vous saurez contextualiser la science des donnÃ©es par rapport aux statistiques, serez en mesure de vous lancer dans un environnement de programmation R, serez en mesure dâ€™effectuer des opÃ©rations de base en R, saurez diffÃ©rencier les grands types dâ€™objets de R et saurez installer et charger des modules complÃ©mentaire. Un projet en science des donnÃ©es comprend trois grandes Ã©tapes. Dâ€™abord, vous devez collecter des donnÃ©es et vous les compilez adÃ©quatement. Cela peut consister Ã  tÃ©lÃ©charger des donnÃ©es existantes, exÃ©cuter un dispositif expÃ©rimental ou effectuer une recensement (Ã©tude observationnelle). Compiler les donnÃ©es dans un format qui puisse Ãªtre importÃ© est une tÃ¢che souvent longue et fastidieuse. Puis, vous investiguez les donnÃ©es collectÃ©es, câ€™est-Ã -dire vous les visualisez, vous appliquez des modÃ¨les et testez des hypothÃ¨ses. Enfin, la communication des rÃ©sultats consiste Ã  prÃ©senter les connaissances qui Ã©mergent de votre analyse sous forme visuelle et narrative, avec un langage adaptÃ© Ã  la personne qui vous Ã©coute, quâ€™elle soit experte ou novice, rÃ©viseure de revue savante ou gestionnaire Grolemund et Wickham (2018) propose la structure dâ€™analyse de la figure 2.1, avec de lÃ©gÃ¨res modifications de ma part. Figure 2.1: Flux des donnÃ©es en sciences des donnÃ©es. Le grand cadre spÃ©cifie Programmer. Oui, vous aurez besoin dâ€™Ã©crire du code. Mais comme je lâ€™ai indiquÃ© dans le premier chapitre, ceci nâ€™est pas un cours de programmation et je prÃ©fÃ©rerai les approches intuitives. 2.1 Statistiques ou science des donnÃ©es? Selon Whitlock et Schluter (2015), la statistique est lâ€™Ã©tude des mÃ©thodes pour dÃ©crire et mesurer des aspects de la nature Ã  partir dâ€™Ã©chantillon. Pour Grolemund et Wickham (2018), la science des donnÃ©es est une discipline excitante permettant de transformer des donnÃ©es brutes en comprÃ©hension, perspectives et connaissances. Oui, excitante! La diffÃ©rence entre les deux champs dâ€™expertise est subtile, et certaines personnes nâ€™y voient quâ€™une diffÃ©rence de ton. Data Science is statistics on a Mac. â€” Big Data Borat (@BigDataBorat) 27 aoÃ»t 2013 ConfinÃ©es Ã  ses applications traditionnelles, les statistiques sont davantage vouÃ©es Ã  la dÃ©finition de dispositifs expÃ©rimentaux et Ã  lâ€™exÃ©cution de tests dâ€™hypothÃ¨ses, alors que la science des donnÃ©es est moins linÃ©aire, en particulier dans sa phase dâ€™analyse, oÃ¹ de nouvelles questions (donc de nouvelles hypothÃ¨ses) peuvent Ãªtre posÃ©es au fur et Ã  mesure de lâ€™analyse. Cela arrive gÃ©nÃ©ralement davantage lorsque lâ€™on fait face Ã  de nombreuses observations sur lesquelles de nombreux paramÃ¨tres sont mesurÃ©s. La quantitÃ© de donnÃ©es et de mesures auxquelles nous avons aujourdâ€™hui accÃ¨s grÃ¢ce aux technologies de mesure et de stockage relativement peu dispendieux rend la science des donnÃ©es une discipline particuliÃ¨rement attrayante, pour ne pas dire sexy. 2.2 DÃ©buter en R R est un langage de programmation dÃ©rivÃ© du langage S, qui fut initialement lancÃ© en 1976. Figure 2.2: Logo officiel du language R. R figure parmi les langages de programmation les plus utilisÃ©s au monde. Bien quâ€™il soit basÃ© sur les langages statiques C et Fortran, R est un langage dynamique, câ€™est-Ã -dire que le code peut Ãªtre exÃ©cutÃ© ligne par ligne ou bloc par bloc: un avantage majeur pour des activitÃ©s qui nÃ©cessitent des interactions frÃ©quentes. Bien que R soit surtout utilisÃ© pour le calcul statistique, il sâ€™impose de plus en plus comme outil privilÃ©giÃ© en sciences des donnÃ©es en raison des rÃ©cents dÃ©veloppements de modules dâ€™analyse, de modÃ©lisation et de visualisation, dont plusieurs seront utilisÃ©s dans ce manuel. Un langage de programmation sâ€™apprend un peu comme une langue. Au dÃ©but, un code R peut sembler incomprÃ©hensible. Et face Ã  son clavier, on ne sait pas trop comment exprimer ce que lâ€™on dÃ©sire. Au fur et Ã  mesure de lâ€™apprentissage, les symboles, les fonctions et le style deviennent de plus en plus familiers et on apprend tranquillement Ã  traduire en code ce que lâ€™on dÃ©sire effectuer. Comme une langue sâ€™apprend en la parlant dans la vie de tous les jours, un language de programmation sâ€™apprend avantageusement en solutionnant vos propres problÃ¨mes. Figure 2.3: R avant et maintenant, Illustration de Allison Horst 2.3 PrÃ©parer son flux de travail Il existe de nombreuses maniÃ¨res dâ€™utiliser R. Parmi celles-ci, jâ€™en couvrirai 3: Installation classique (installation suggÃ©rÃ©e) Installation avec Anaconda Utilisation infonuagique 2.3.1 Installation classique Installation suggÃ©rÃ©e. Sur Windows ou Mac, dirigez-vous ici, tÃ©lÃ©chargez et installez. Sur Linux, ouvrez votre gestionnaire dâ€™application, chercher r-base (Ubuntu, Debian), R-base (openSuse) ou R-core (Fedora) et installez-le (assurez-vous que les librairies suivantes sont aussi installÃ©es: gcc, gcc-fortran, gcc-c++ et make), vous aurez peut-Ãªtre besoin dâ€™installer des librairies supplÃ©mentaires pour faire fonctionner certains modules. Note. Les modules prÃ©sentÃ©s dans ce cours devraient Ãªtre disponibles sur Linux, Windows et Mac. Ce nâ€™est pas le cas pour tous les modules R. La plupart fonctionnent nÃ©anmoins sur Linux, dont les systÃ¨mes dâ€™opÃ©ration (je recommande Ubuntu ou lâ€™une de ses dÃ©rivÃ©es comme elementary OS) sont de bonnes options pour le calcul scientifique. Ã€ cette Ã©tape, R devrait fonctionner dans un interprÃ©teur de commande . Si vous lancez R dans un terminal (chercher cmd dans le menu si vous Ãªtes sur Windows), vous obtiendrez quelque chose comme ceci. Figure 2.4: R dans le terminal. Le symbole &gt; indique que R attend que vos instructions. Vous voilÃ  dans un Ã©tat mÃ©ditatif devant lâ€™indÃ©chiffrable vide du terminal ğŸ˜µ. Ne vous en faites pas: nous commencerons bientÃ´t Ã  jaser avec R. Avant cela, installons-nous au salon. Afin de travailler dans un environnement de travail plus confortable, je recommande lâ€™installation de lâ€™interface RStudio, gratuite et open source: tÃ©lÃ©chargez lâ€™installateur et suivez les instructions. RStudio ressemble Ã  ceci. Figure 2.5: FenÃªtre de RStudio. En haut Ã  droite se trouve un menu Project (None). Il sâ€™agit dâ€™un menu de vos projets. Je recommande dâ€™utiliser ces projets avec RStudio, qui vous permettront de mieux gÃ©rer vos sessions de travail, en particulier en lien avec les chemins vers de vos donnÃ©es, graphiques, etc., que vous pouvez gÃ©rer relativement Ã  lâ€™emplacement de votre dossier de projet plutÃ´t quâ€™Ã  lâ€™emplacement des fichiers sur votre machine: nous verrons plus en dÃ©tails au chapitre 5. En haut Ã  gauche, vous avez vos feuilles de calcul, qui apparaÃ®tront en tant quâ€™onglets. Une feuille de calcul est une sÃ©rie de commandes que vous lancez en sÃ©quence. Il peut aussi sâ€™agir dâ€™un livre de calcul (notebook) si vous choisissez de travailler en format R markdown. Ce format vous permettra de dâ€™Ã©crire du texte en format Markdown entre des blocs de code. Il est question du format R markdown au chapitre (chapitre-git). En bas Ã  gauche apparaÃ®t la Console, oÃ¹ vous voyez les commandes envoyÃ©es Ã  R ainsi que ses sorties. En haut Ã  droite, les diffÃ©rents onglets indiquent oÃ¹ vous en Ãªtes dans vos calculs. En particulier, la liste sous Environment indique les objets qui ont Ã©tÃ© gÃ©nÃ©rÃ©s ou chargÃ©s jusquâ€™alors. En bas Ã  droite, on retrouve des onglets de nature variÃ©s. Files contient les sous-dossiers et fichiers du dossier de projets. Plots est lâ€™endroit oÃ¹ apparaÃ®tront vos graphiques. Packages contient la liste des modules dÃ©jÃ  installÃ©s, ainsi quâ€™un outil de gestion des modules pour leur installation, leur dÃ©sinstallation et leur mise Ã  jour. Help affiche les fiches dâ€™aide des fonctions (pour obtenir de lâ€™aide sur une fonction dans RStudio, surlignez la fonction dans votre feuille de calcul, puis appuyez sur F1). Enfin, lâ€™onglet Viewer affichera les sorties HTML, en particulier les graphiques interactifs que vous gÃ©nÃ©rerez par exemple avec le module plotly. Si votre environnement de travail Ã©tait un avion, R serait le moteur et RStudio serait le cockpit! Figure 2.6: ScÃ¨ne de Fifi Brindacier (Astrid Lindgren, 1945). 2.3.2 Installation avec Anaconda Si vous cherchez une trousse complÃ¨te dâ€™analyse de donnÃ©es, comprenant R et Python, vous pourrez prÃ©fÃ©rer Anaconda. Une fois installÃ©e, vous pourrez isoler un environnement de travail sur R, ou mÃªme isoler des environnements de travail particuliers pour vos projets. Une maniÃ¨re conviviale de crÃ©er des environnements de travail est de passer par lâ€™interface Anaconda navigator, que vous lancerez soit dans le menu Windows, soit en ligne de commande anaconda-navigator sous Mac et Linux, puis dâ€™installer r-essentials, rstudio et jupyterlab dans lâ€™onglet Environment. Vous pourrez aussi installer RStudio et Jupyter lab via lâ€™onglet Home de Anaconda navigator. Dans lâ€™environnement de base, installez le package nb_conda_kernels pour vous assurer que tous les noyaux (R, Python, etc.) installÃ©s dans les environnements de travail soient automatiquement accessibles dans Jupyter. Si vous dÃ©sirez utiliser dans Jupyter la version de R installÃ©e avec lâ€™installation classique, rÃ©fÃ©rez-vous au guide prÃ©sentÃ© en extra au bas de la page. Figure 2.7: Anaconda navigator. Jupyter lab est une interface notebook semblable Ã  R markdown - les format Jupyter (*.ipynb) et R markdown (*.Rmd) sont par ailleurs convertibles grÃ¢ce au module jupytext. Lâ€™utilisation de R en Anaconda nâ€™est pas tout Ã  fait au point, et pourrait poser problÃ¨me pour lâ€™installation de certains modules. Si vous optez pour cette option, prÃ©parez-vous Ã  avoir Ã  bidouiller un peu. Plusieurs prÃ©fÃ¨rent Jupyter Ã  RStudio (ce nâ€™est pas mon cas). 2.3.3 Utilisation infonuagique Pas besoin dâ€™avoir une machine super puissante pour travailler en R. Il existe une multitude de services infonuagique (dans le cloud) vous permettant de lancer vos calculs sur des serveurs plutÃ´t que sur votre Chromebook ou votre vieux laptop dÃ©glinguÃ©. Certains services sont gratuits, et dâ€™autres souvent plus Ã©laborÃ©s sont payants. Un service gratuit qui fonctionne bien en R est Azure Notebooks, offert par Microsoft. Vous y aurez accÃ¨s avec un compte Microsoft ou un compte Exchange (par exemeple avec un IDUL de lâ€™UniversitÃ© Laval). Azure notebooks offre des dossiers comportant des notebooks de type Jupyter ainsi que des fichiers de donnÃ©es. Les dossiers peuvent Ãªtre rendus publics et copiÃ© par des collÃ¨gues. Le travail collaboratif nâ€™est pas disponible. Pour travailler en collaboration, vous pourrez utiliser Google Colaboratory, offert gratuitement par Google. Bien que les noyaux de calcul soient seulement offerts pour Python, vous pouvez clÃ´ner ou importer un notebook conÃ§u pour fonctionner en R, et un noyau R sera chargÃ©. Il vous faudra bidouiller un peu pour charger vos donnÃ©es depuis Google drive ou votre ordinateur. Un avantage de Google Colaboratory est que vous pouvez spÃ©cifier que vous dÃ©sirez que vos calculs tournent sur un processeur graphique, utile pour les calculs lourds et parallÃ©lisables comme les rÃ©seaux neuronnaux. Si je viens de vous perdre, pas de problÃ¨me, câ€™est de lâ€™extra. Si vous dÃ©sirer autant que possible rester indÃ©pendant des gÃ©ants du web, CoCalc est une option avec un volet gratuit et un autre payant. 2.4 Premiers pas avec R R ne fonctionne pas avec des menus, en faisant danser une souris sous une musique de clics. Vous devrez donc entrer des commandes avec votre clavier, que vous apprendrez par cÅ“ur au fur et Ã  mesure, ou que vous retrouverez en lanÃ§ant des recherches sur internet. Par expÃ©rience personnelle, lorsque je travaille avec R, jâ€™ai toujours un navigateur ouvert prÃªt Ã  recevoir une question. Les Ã©tapes qui suivent sont des premiers pas. Elles ne feront pas de vous des ceintures noires delÃ  programmation. La plupart des utilisateurs de R ont appris R en se pratiquant sur leurs donnÃ©es, en frappant des murs, en apprenant comment les escalader ou les contournerâ€¦ Pour lâ€™instant, ouvrez seulement un interprÃ©teur de commande, et lancez R. Voyons si R est aussi libre quâ€™on le prÃ©tend. â€œLa libertÃ©, câ€™est la libertÃ© de dire que deux et deux font quatre. Si cela est accordÃ©, tout le reste suit.â€ - George Orwell, 1984 2 + 2 ## [1] 4 Et voilÃ . Les opÃ©rations mathÃ©matiques sont effectuÃ©es telles que lâ€™on devrait sâ€™attendre. 67.1 - 43.3 ## [1] 23.8 2 * 4 ## [1] 8 1 / 2 ## [1] 0.5 Lâ€™exposant peut Ãªtre notÃ© ^, comme câ€™est le cas dans Excel, ou ** comme câ€™est le cas en Python. 2^4 ## [1] 16 2**4 ## [1] 16 1 / 2 # utilisez des espaces de part et d&#39;autre des opÃ©rateurs (sauf pour l&#39;exposant) pour Ã©claircir le code ## [1] 0.5 R ne lit pas ce qui suit le caractÃ¨re #. Cela vous laisse lâ€™opportunitÃ© de commenter un code comprenant une sÃ©quence de plusieurs lignes. Remarquez Ã©galement que la derniÃ¨re opÃ©ration comporte des espaces entre les nombres et lâ€™opÃ©rateur /. Dans ce cas (ce nâ€™est pas toujours le cas), les espaces ne signifient rien: ils aident seulement Ã  Ã©claircir le code. Il existe des guides pour lâ€™Ã©criture de code en R. Je recommande le guide de style de Hadley Wickahm. Assigner des objets Ã  des variables est fondamental en programmation. En R, on assigne traditionnellement avec la flÃ¨che &lt;-, mais vous verrez parfois le =, qui est davantage utilisÃ© comme standard dans dâ€™autres langages de programmation. Par exemple. a &lt;- 3 Techniquement, a pointe vers le nombre entier 3. ConsÃ©quemment, on peut effectuer des opÃ©rations sur a. a * 6 ## [1] 18 #A + 2 Le message dâ€™erreur nous dit que A nâ€™est pas dÃ©fini. Sa version minuscule, a, lâ€™est pourtant. La raison est que R considÃ¨re la case dans la dÃ©finition des objets. Utiliser la mauvaise case mÃ¨ne donc Ã  des erreurs. Note. Les messages dâ€™erreur ne sont pas toujours clairs, mais vous apprendrez Ã  les comprendre. Dans tous les cas, ils sont fait pour vous aider. Lisez-les attentivement! En gÃ©nÃ©ral, le nom dâ€™une variable doit toujours commencer par une lettre, et ne doit pas contenir de caractÃ¨res rÃ©servÃ©s (espaces, +, *). Dans la dÃ©finition des variables, plusieurs utilisent des symboles . pour dÃ©limiter les mots, mais la barre de soulignement _ est Ã  prÃ©fÃ©rer. En effet, dans dâ€™autres langages de programmation comme Python, le . a une autre signification: son utilisation est Ã  Ã©viter autant que possible. Note. Ã€ ce stade, vous serez probablement plus Ã  lâ€™aise de copier-coller ces commandes dans votre terminal. rendement_arbre &lt;- 50 # pomme/arbre nombre_arbre &lt;- 300 # arbre nombre_pomme &lt;- rendement_arbre * nombre_arbre nombre_pomme ## [1] 15000 Comme chez la plupart des langages de programmation, R respecte les conventions des prioritÃ©s des opÃ©rations mathÃ©atiques. 10 - 9^0.5 * 2 ## [1] 4 2.4.1 Types de donnÃ©es Jusquâ€™Ã  maintenant, nous nâ€™avons utilisÃ© que des nombres entiers (integer ou int) et des nombres rÃ©els (numeric ou float64). R inclut dâ€™autres types. La chaÃ®ne de caractÃ¨re (string ou character) contient un ou plusieurs symboles. Elle est dÃ©finie entre des doubles guillemets &quot; &quot; ou des apostrophes ' '. Il nâ€™existe pas de standard sur lâ€™utilisation de lâ€™un ou de lâ€™autre, mais en rÃ¨gle gÃ©nÃ©rale, on utilise les apostrophes pour les expressions courtes, contenant un simple mot ou sÃ©quence de lettres, et les guillemets pour les phrases. Une raison pour cela: les guillemets sont utiles pour insÃ©rer des apostrophes dans une chaÃ®ne de caractÃ¨re. a &lt;- &quot;L&#39;ours&quot; b &lt;- &quot;polaire&quot; paste(a, b) ## [1] &quot;L&#39;ours polaire&quot; On colle a et b avec la fonction paste. Notez que lâ€™objet a a Ã©tÃ© dÃ©fini prÃ©cÃ©demment. Il est possible en R de rÃ©assigner une variable, mais cela peut porter Ã  confusion, jusquâ€™Ã  gÃ©nÃ©rer des erreurs de calcul si une variable nâ€™est pas assignÃ©e Ã  lâ€™objet auquel on voulait rÃ©fÃ©rer. Combien de caractÃ¨res contient la chaÃ®ne &quot;L'ours polaire&quot;? R sait compter. Demandons-lui. c &lt;- paste(a, b) nchar(c) ## [1] 14 Quatorze, câ€™est bien cela (comptez â€œLâ€™ours polaireâ€, en incluant lâ€™espace). Comme paste, nchar est une fonction incluse par dÃ©faut dans lâ€™environnement de travail de R: plus prÃ©cisÃ©ment, ces fonctions sont incluses dans le module base, inclut par dÃ©faut lorsque R est lancÃ©. La fonction est appelÃ©e en Ã©crivant nchar(). Mais une fonction de quoi? Des arguments, qui se trouvent entre les parenthÃ¨ses. Dans ce cas, il y a un seul argument: c. En calcul scientifique, il est courant de lancer des requÃªtes sur si un rÃ©sultat est vrai ou faux. a &lt;- 17 a &lt; 10 ## [1] FALSE a &gt; 10 ## [1] TRUE a == 10 ## [1] FALSE a != 10 ## [1] TRUE a == 17 ## [1] TRUE !(a == 17) ## [1] FALSE Je viens dâ€™introduire un nouveau type de donnÃ©e: les donnÃ©es boolÃ©ennes (boolean, ou logical), qui ne peuvent prendre que deux Ã©tats - TRUE ou FALSE. En mÃªme temps, jâ€™ai utilisÃ© la fonction print parce que dans mon carnet, seule la derniÃ¨re opÃ©ration permet dâ€™afficher le rÃ©sultat. Si lâ€™on veut forcer une sortie, on utilise print. Puis, on a vu plus haut que le symbole = est rÃ©servÃ© pour assigner des objets: pour les tests dâ€™Ã©galitÃ©, on utilise le double Ã©gal, ==, ou != pour la non-Ã©galitÃ©. Enfin, pour inverser une donnÃ©e de type boolÃ©enne, on utilise le point dâ€™exclamation !. 2.4.2 Les collections de donnÃ©es Les exercices prÃ©cÃ©dents ont permis de prÃ©senter les types de donnÃ©es offerts par dÃ©faut sur R qui sont les plus importants pour le calcul scientifique: int (integer, ou nombre entier), numeric (nombre rÃ©el), character (string, ou chaÃ®ne de caractÃ¨re) et logical (boolÃ©en). Dâ€™autres sâ€™ajouteront tout au long du cours, comme les catÃ©gories (factor) et les unitÃ©s de temps (date-heure). Lorsque lâ€™on procÃ¨de Ã  des opÃ©rations de calcul en science, nous utilisons rarement des valeurs uniques. Nous prÃ©fÃ©rons les organiser et les traiter en collections. Par dÃ©faut, R offre quatre types importants de collections: les vecteurs, les matrices, les listes et les tableaux. 2.4.2.1 Vecteurs Dâ€™abord, les vecteurs sont une sÃ©rie de variables de mÃªme type. Un vecteur est dÃ©limitÃ© par la fonction c( ) (c pour concatÃ©nation). Les Ã©lÃ©ments de la liste sont sÃ©parÃ©s par des virgules. espece &lt;- c(&#39;Petromyzon marinus&#39;, &#39;Lepisosteus osseus&#39;, &#39;Amia calva&#39;, &#39;Hiodon tergisus&#39;) espece ## [1] &quot;Petromyzon marinus&quot; &quot;Lepisosteus osseus&quot; &quot;Amia calva&quot; &quot;Hiodon tergisus&quot; Pour accÃ©der aux Ã©lÃ©ments dâ€™une liste, appelle la liste suivie de la position de lâ€™objet dÃ©sirÃ© entre crochets. espece[1] ## [1] &quot;Petromyzon marinus&quot; espece[2] ## [1] &quot;Lepisosteus osseus&quot; espece[1:3] ## [1] &quot;Petromyzon marinus&quot; &quot;Lepisosteus osseus&quot; &quot;Amia calva&quot; espece[c(1, 3)] ## [1] &quot;Petromyzon marinus&quot; &quot;Amia calva&quot; On peut noter que le premier Ã©lÃ©ment de la liste est notÃ© 1, et non 0 comme câ€™est le cas de la plupart de langages. Le raccourcis 1:3 crÃ©e une liste de nombres entiers de 1 Ã  3 inclusivement, câ€™est-Ã -dire lâ€™Ã©quivalent de c(1, 2, 3). En effet, on crÃ©e une liste dâ€™indices pour soutirer des Ã©lÃ©ments dâ€™une liste. On peut utiliser le symbole de soustraction pour retirer un ou plusieurs Ã©lÃ©ments dâ€™un vecteur. espece[-c(1, 3)] ## [1] &quot;Lepisosteus osseus&quot; &quot;Hiodon tergisus&quot; Pour ajouter un Ã©lÃ©ment Ã  notre liste, on peut utiliser la fonction c( ). espece &lt;- c(espece, &quot;Cyprinus carpio&quot;) espece ## [1] &quot;Petromyzon marinus&quot; &quot;Lepisosteus osseus&quot; &quot;Amia calva&quot; &quot;Hiodon tergisus&quot; &quot;Cyprinus carpio&quot; Notez que lâ€™on efface lâ€™objet espece par une concatÃ©nation de lâ€™objet espece, prÃ©cÃ©demment dÃ©finie, et dâ€™un autre Ã©lÃ©ment. En lanÃ§ant espece[3] &lt;- &quot;Lepomis gibbosus&quot;, il est possible de changer un Ã©lÃ©ment de la liste. espece[3] &lt;- &quot;Lepomis gibbosus&quot; espece ## [1] &quot;Petromyzon marinus&quot; &quot;Lepisosteus osseus&quot; &quot;Lepomis gibbosus&quot; &quot;Hiodon tergisus&quot; &quot;Cyprinus carpio&quot; 2.4.2.2 Matrices Une matrice est un vecteur de dimension plus Ã©levÃ©e que 1. En Ã©cologie, on dÃ©passe rarement la deuxiÃ¨me dimension, quoi que les matrices en N dimensions soient courantes en modÃ©lisation mathÃ©matique. Je ne considÃ©rerai pour le moment que des matrices 2D. Comme câ€™est la cas des vecteurs, les matrices contiennent des valeurs de mÃªme type. En R, on peut attribuer aux matrices 2D des noms de ligne et de colonne. mat &lt;- matrix(c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12), ncol=3) mat ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 colnames(mat) &lt;- c(&#39;A&#39;, &#39;B&#39;, &#39;C&#39;) rownames(mat) &lt;- c(&#39;site_1&#39;, &#39;site_2&#39;, &#39;site_3&#39;, &#39;site_4&#39;) mat ## A B C ## site_1 1 5 9 ## site_2 2 6 10 ## site_3 3 7 11 ## site_4 4 8 12 On peut soutirer les noms de colonne et les noms de ligne. Le rÃ©sultat est un vecteur. colnames(mat) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; rownames(mat) ## [1] &quot;site_1&quot; &quot;site_2&quot; &quot;site_3&quot; &quot;site_4&quot; 2.4.2.3 Listes Les listes sont des collections hÃ©tÃ©rogÃ¨nes dans lesquelles on peut placer les objets dÃ©sirÃ©s, sans distinction: elles peuvent mÃªme inclure dâ€™autres listes. Chacun des Ã©lÃ©ments de la liste peut Ãªtre identifiÃ© par une clÃ©. ma_liste &lt;- list(especes = c(&#39;Petromyzon marinus&#39;, &#39;Lepisosteus osseus&#39;, &#39;Amia calva&#39;, &#39;Hiodon tergisus&#39;), site = &#39;A101&#39;, stations_meteos = c(&#39;746583&#39;, &#39;783786&#39;, &#39;856363&#39;)) ma_liste ## $especes ## [1] &quot;Petromyzon marinus&quot; &quot;Lepisosteus osseus&quot; &quot;Amia calva&quot; &quot;Hiodon tergisus&quot; ## ## $site ## [1] &quot;A101&quot; ## ## $stations_meteos ## [1] &quot;746583&quot; &quot;783786&quot; &quot;856363&quot; Les Ã©lÃ©ments de la liste peuvent Ãªtre soutirÃ©s par le nom de la clÃ© ou par lâ€™indice, de cette maniÃ¨re. ma_liste$especes ## [1] &quot;Petromyzon marinus&quot; &quot;Lepisosteus osseus&quot; &quot;Amia calva&quot; &quot;Hiodon tergisus&quot; ma_liste[[1]] ## [1] &quot;Petromyzon marinus&quot; &quot;Lepisosteus osseus&quot; &quot;Amia calva&quot; &quot;Hiodon tergisus&quot; Exercice. AccÃ©der au deuxiÃ¨me Ã©lÃ©ment du vecteur dâ€™espÃ¨ces dans la liste ma_liste. 2.4.2.4 Tableaux Enfin, le type de collection de donnÃ©es le plus important est le tableau, ou data.frame. Techniquement, il sâ€™agit dâ€™une liste composÃ©e de vecteurs de mÃªme longueur. Chaque colonne peut ainsi prendre un type de donnÃ©e indÃ©pendamment des autres colonnes. tableau &lt;- data.frame(espece = c(&#39;Petromyzon marinus&#39;, &#39;Lepisosteus osseus&#39;, &#39;Amia calva&#39;, &#39;Hiodon tergisus&#39;), poids = c(10, 13, 21, 4), longueur = c(35, 44, 50, 8)) tableau ## espece poids longueur ## 1 Petromyzon marinus 10 35 ## 2 Lepisosteus osseus 13 44 ## 3 Amia calva 21 50 ## 4 Hiodon tergisus 4 8 En programmation classique en R (nous verrons plus loin la mÃ©thode tidyverse), les Ã©lÃ©ments dâ€™un tableau se manipulent comme ceux dâ€™une matrice et les colonnes peuvent Ãªtre appelÃ©s comme les Ã©lÃ©ments dâ€™une liste. tableau[, 2:3] ## poids longueur ## 1 10 35 ## 2 13 44 ## 3 21 50 ## 4 4 8 tableau$poids ## [1] 10 13 21 4 Vous verrez aussi, quoi que rarement, ce format, qui Ã  la diffÃ©rence du format $ gÃ©nÃ¨re un tableau. tableau[&#39;poids&#39;] ## poids ## 1 10 ## 2 13 ## 3 21 ## 4 4 Le tableau est le format de collection Ã  privilÃ©gier pour manipuler des donnÃ©es. RÃ©cemment, le format de tableau tibble a Ã©tÃ© crÃ©Ã© par lâ€™Ã©quipe de RStudio pour offrir un format plus moderne. 2.4.3 Les fonctions Lorsque vous Ã©crivez une commande suivit de parenthÃ¨ses, comme data.frame(especes = ...), vous demandez Ã  R de passer Ã  lâ€™action en appelant une fonction. De maniÃ¨re trÃ¨s gÃ©nÃ©rale, une fonction transforme quelque chose en quelque chose dâ€™autre. Par exemple, la fonction mean() prend une collection de nombre comme entrÃ©e, puis en sort vous devinez quoi. mean(tableau$poids) ## [1] 12 Les entrÃ©es sont appelÃ©s les arguments de la fonction. Leur dÃ©finition est toujours disponible dans la documentation. Exercice. Familiarisez-vous avec la documentation de R en lanÃ§ant ?mean. Truc: si vous avez pris de lâ€™avance et que vous travaillez dÃ©jÃ  en RStudio, mettez le terme en surbrillance, puis appuyez sur F1. Vous verrez dans la documentation que la fonction mean() demande trois arguments, x, trim et na.rm. Or nous avons seulement placÃ© un vecteur, sans spÃ©cifier dâ€™argument! En effet. En lâ€™absence dâ€™une dÃ©finition des arguments, R supposera que les arguments dans la parenthÃ¨se, sÃ©parÃ©s par une virgule, sont prÃ©sentÃ©s dans le mÃªme ordre que celui spÃ©cifiÃ© dans la dÃ©finition de la fonction (celle qui est prÃ©sentÃ©e dans le fichier dâ€™aide). Dans le cas qui nous intÃ©resse, mean(tableau$poids) est Ã©quivalent Ã  mean(x = tableau$poids). Maintenant, selon la fiche dâ€™aire lâ€™argument na.rm est un valeur logique spÃ©cifiant si oui (TRUE) ou non (FALSE) les valeurs manquantes doivent Ãªtre considÃ©rÃ©es (une moyenne dâ€™un vecteur comprenant au moins un NA sera de NA). En ne spÃ©cifiant rien, R prend la valeur par dÃ©faut, telle que spÃ©cifiÃ©e dans la documentation. Il en va de mÃªme que lâ€™argument trim, qui permet dâ€™Ã©laguer des valeurs extrÃªmes. Dans la fiche dâ€™aide, mean(x, trim = 0, na.rm = FALSE, ...) signifie que par dÃ©faut, lâ€™argument x est vide (il doit donc Ãªtre spÃ©cifiÃ©), lâ€™argument trim est de 0 et lâ€™argument na.rm est FALSE. mean(c(6, 1, 7, 4, 9, NA, 1)) ## [1] NA mean(c(6, 1, 7, 4, 9, NA, 1), na.rm = TRUE) ## [1] 4.666667 Vous nâ€™Ãªtes pas emprisonnÃ© par les fonctions offertes par R. Vous pouvez installer des modules qui complÃ¨tent les fonctions de base de R: on le verra un peu plus loin dans ce chapitre. Mais pour lâ€™instant, voyons comment vous pouvez crÃ©er vos propres fonctions. Disons que vous voulez crÃ©er une fonction qui calcule la sortie de \\(x^3-2y+a\\). Pour obtenir la rÃ©ponse on a besoin des arguments x, y et a. La sortie de la fonction est ici triviale: la rÃ©ponse de lâ€™Ã©quation. Lâ€™opÃ©ration function permet de prendre Ã§a en charge. operation_f &lt;- function(x, y, a = 10) { return(x^3-2*y+a) } Notez que a a une valeur par dÃ©faut. La sortie de la fonction est ce qui se trouve entre les parenthÃ¨ses de return. Vous pouvez maintenant utiliser la fonction operation_nl au besoin. operation_f(x = 2, y = 3, a = 1) ## [1] 3 Une telle fonction est peu utile. Mais lâ€™utilisation de fonctions personnalisÃ©es vous permettra dâ€™Ã©viter de rÃ©pÃ©ter la mÃªme opÃ©ration plusieurs fois dans un flux de travail, en Ã©vitant de gÃ©nÃ©rer trop de code, donc aussi de potentielles erreurs. Personnellement, jâ€™utilise les fonctions surtout pour gÃ©nÃ©rer des graphiques personnalisÃ©s. Exercice. Afin dâ€™acquÃ©rir de lâ€™autonomie, vous devrez Ãªtre en mesure de trouver le nom des commandes dont vous avez besoin pour effectuer la tÃ¢che que vous dÃ©sirer effectuer. Cela peut causer des frustrations, mais vous vous sentirez toujours plus Ã  lâ€™aise avec R jour aprÃ¨s jour. Lâ€™exercice ici est de trouver par vous-mÃªme la commande qui vous permettra mesurer la longueur dâ€™un vecteur. 2.4.4 Les boucles Les boucles permettent dâ€™effectuer une mÃªme suite dâ€™opÃ©rations sur plusieurs objets. Pour faire suite Ã  notre exemple, nous dÃ©sirons obtenir le rÃ©sultat de lâ€™opÃ©ration f pour des paramÃ¨tres que nous enregistrons dans ce tableau. params &lt;- data.frame(x = c(2, 4, 1, 5, 6), y = c(3, 4, 8, 1, 0), a = c(6, 1, 8, 2, 5)) params ## x y a ## 1 2 3 6 ## 2 4 4 1 ## 3 1 8 8 ## 4 5 1 2 ## 5 6 0 5 Nous crÃ©ons un vecteur vide, puis nous itÃ©rons ligne par ligne en remplissant le vecteur. operation_res &lt;- c() for (i in 1:nrow(params)) { operation_res[i] &lt;- operation_f(x = params[i, 1], y = params[i, 2], a = params[i, 3]) } operation_res ## [1] 8 57 -7 125 221 En faisant varier i sur des valeurs du vecteur donnÃ© par la sÃ©quence de nombre entiers de 1 au nombre de ligne du tableau de paramÃ¨tres, nous demandons Ã  R dâ€™effectuer la suite dâ€™opÃ©ration entre les accolades {}. Ã€ chaque boucle, i prend une valeur de la sÃ©quence. i est utilisÃ© ici comme indice de la ligne Ã  soutirer du tableau params, qui correspond Ã  lâ€™indice dans le vecteur operation_res. Ainsi, chaque rÃ©sultat est calculÃ© dans lâ€™ordre des lignes du tableau de paramÃ¨tres et lâ€™on pourra trÃ¨s bien y coller nos rÃ©sultats: params$resultats &lt;- operation_res params ## x y a resultats ## 1 2 3 6 8 ## 2 4 4 1 57 ## 3 1 8 8 -7 ## 4 5 1 2 125 ## 5 6 0 5 221 Notez que puisque la colonne resultat nâ€™existe pas dans le tableau params, R crÃ©e automatiquement une nouvelle colonne. Les boucles for vous permettront par exemple de gÃ©nÃ©rer en peu de temps 10, 100, 1000 graphiques (autant que vous voulez), chacun issu de simulations obtenues Ã  partir de conditions initiales diffÃ©rentes, et de les enregistrer dans un rÃ©pertoire sur votre ordinateur. Un travail qui pourrait prendre des semaines sur Excel peut Ãªtre effectuÃ© en R en quelques secondes. Un second outil est disponible pour les itÃ©rations: les boucles while. Elles effectuent une opÃ©ration tant quâ€™un critÃ¨re nâ€™est pas atteint. Elles sont utiles pour les opÃ©rations oÃ¹ lâ€™on cherche une convergence. Je les couvre rapidement puisquâ€™elles sont rarement utilisÃ©es dans les flux de travail courants. En voici un petit exemple. x &lt;- 100 while (x &gt; 1.1) { x &lt;- sqrt(x) print(x) } ## [1] 10 ## [1] 3.162278 ## [1] 1.778279 ## [1] 1.333521 ## [1] 1.154782 ## [1] 1.074608 Nous avons initiÃ© x Ã  une valeur de 100. Puis, tant que (while) le test x &gt; 1.1 est vrai, attribuer Ã  x la nouvelle valeur calculÃ©e en extrayant la racine de la valeur prÃ©cÃ©dente de x. Enfin, indiquer la valeur avec print. 2.4.5 Conditions: if, else if, else Si la condition 1 est remplie, effectuer une suite dâ€™instruction 1. Si la condition 1 nâ€™est pas remplie, et si la condition 2 est remplie, effectuer la suite dâ€™instruction 2. Sinon, effectuer la suite dâ€™instruction 3. VoilÃ  comment on exprime une suite de conditions. Prenons lâ€™exemple simple dâ€™une discrÃ©tisation dâ€™une valeur continue. Si \\(x&lt;10\\), il est classÃ© comme faible. Si \\(10 \\leq x &lt;20\\), il est classÃ© comme moyen. Si \\(x \\geq 20\\), il est classÃ© comme Ã©levÃ©. PlaÃ§ons cette classification dans une fonction. classification &lt;- function(x, lim1=10, lim2=20) { if (x &lt; lim1) { categorie &lt;- &quot;faible&quot; } else if (x &lt; lim2) { categorie &lt;- &quot;moyen&quot; } else { categorie &lt;- &quot;Ã©levÃ©&quot; } return(categorie) } classification(-10) ## [1] &quot;faible&quot; classification(15.4) ## [1] &quot;moyen&quot; classification(1000) ## [1] &quot;Ã©levÃ©&quot; Une condition est dÃ©finie avec le if, suivi du test Ã  vrai ou faux entre parenthÃ¨ses. Si le test retourne un vrai (TRUE), lâ€™instruction entre accolades est exÃ©cutÃ©e. Si elle est fausse, on passe au suivant. Exercice. Explorer les commandes ifelse et cut et rÃ©flÃ©chissez Ã  la maniÃ¨re quâ€™elles pourraient Ãªtre utilisÃ©es pour effectuer une discrÃ©tisation plus efficacement quâ€™avec les if et les else. 2.4.6 Installer et charger un module La plupart des opÃ©rations dâ€™ordre gÃ©nÃ©ral (comme les racines carrÃ©es, les tests statistiques, la gestion de matrices et de tableau, les graphiques, etc.) sont accessibles grÃ¢ce aux modules de base de R, qui sont installÃ©s et chargÃ©s par dÃ©faut lors du dÃ©marrage de R. Des Ã©quipes de travail ont nÃ©anmoins dÃ©veloppÃ© plusieurs modules pour rÃ©pondre Ã  leurs besoins spÃ©cialisÃ©s, et les ont laissÃ©es disponibles au grand public dans des modules que vous pouvez installer dâ€™un dÃ©pÃ´t CRAN (le AppStore de R), dâ€™un dÃ©pÃ´t Anaconda (le AppStore de Anaconda, si vous utilisez cette plate-forme), dâ€™un dÃ©pÃ´t Github (dÃ©pÃ´ts dÃ©centralisÃ©s), etc. RStudio possÃ¨de un pratique bouton Install qui vous permet dâ€™y inscrire une liste de modules. Le navigateur anaconda offre aussi une interface dâ€™installation. La commande R pour installer un module est install.packages(&quot;ggplot2&quot;), si par exemple vous dÃ©sirez installer ggplot2, le module graphique par excellence en R. Câ€™est la commande que RStudio lancera tout seul si vous lui demandez dâ€™installer ggplot2. Les modules sont lâ€™Ã©quivalent des applications spÃ©cialisÃ©es que vous installez sur un tÃ©lÃ©phone mobile. Pour les utiliser, il faut les ouvrir. GÃ©nÃ©ralement, jâ€™ouvre toutes les applications nÃ©cessaires Ã  mon flux de travail au tout dÃ©but de ma feuille de calcul (la prochaine cellule retournera un message dâ€™erreur si les packages ne sont pas installÃ©s). library(&quot;tidyverse&quot;) # mÃ©ta-package qui charge entre autres dplyr et ggplot2 ## â”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 1.3.0 â”€â”€ ## âœ“ ggplot2 3.2.1 âœ“ purrr 0.3.3 ## âœ“ tibble 2.1.3 âœ“ dplyr 0.8.3 ## âœ“ tidyr 1.0.0 âœ“ stringr 1.4.0 ## âœ“ readr 1.3.1 âœ“ forcats 0.4.0 ## â”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€ ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() library(&quot;vegan&quot;) ## Loading required package: permute ## Loading required package: lattice ## This is vegan 2.5-6 library(&quot;nlme&quot;) ## ## Attaching package: &#39;nlme&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## collapse Les modules sont installÃ©s sur votre ordinateur Ã  un endroit que vous pourrez retrouver avec la commande .libPaths() Exercice. Ã€ partir dâ€™ici jusquâ€™Ã  la fin du cours, nous utiliserons RStudio. Ouvrez-le et familiarisez-vous avec lâ€™interface! Quelques petits trucs: pour lancer une ligne, placez votre curseur sur la ligne, puis appuyez sur Ctrl+Enter pour lancer une partie de code prÃ©cise, mettez le en surbrillance, puis Ctrl+Enter utilisez toujours le gestionnaire de projets, en haut Ã  droite! installez le module tidyverse lancez data(iris) pour obtenir un tableau dâ€™exercice, puis cliquez sur lâ€™objet dans la fenÃªtre environnement essayer R notebook 2.5 Enfinâ€¦ Comme une langue, on nâ€™apprend Ã  sâ€™exprimer en un langage informatique quâ€™en se mettant Ã  lâ€™Ã©preuve, ce que vous ferez tout au long de ce cours. Pour vous encourager, voici quelques trucs pour apprendre Ã  coder en R. R nâ€™aime pas lâ€™ambiguÃ¯tÃ©. Une simple virgule mal placÃ©e et il ne sait plus quoi faire. Cela peut Ãªtre frustrant au dÃ©but, mais cette rigiditÃ© est nÃ©cessaire pour effectuer du calcul scientifique. Le copier-coller est votre ami. En gardant Ã  lâ€™esprit que vous Ãªtre responsable de votre code et que vous respectez les droits dâ€™auteur, nâ€™ayez pas peur de copier-coller des lignes de code et de personnaliser par la suite. Lâ€™erreur que vous obtenez: dâ€™autres lâ€™ont obtenue avant vous. Le site de question-rÃ©ponse stackoverflow est une ressource inestimable oÃ¹ des gens ayant postÃ© des questions ont reÃ§u des rÃ©ponses dâ€™experts (les meilleures rÃ©ponses et les meilleures questions apparaissent en premier). Apprenez Ã  chercher intelligemment des rÃ©ponses en formulant prÃ©cisÃ©ment vos questions! Ã‰tudiez et pratiquez. Les messages dâ€™erreur en R sont courants, mÃªme chez les personnes expÃ©rimentÃ©es. La meilleure maniÃ¨re dâ€™apprendre une langue est de la parler, dâ€™Ã©tudier ses susceptibilitÃ©s, de les tester dans une conversation, etc. 2.6 Extra: Utiliser R avec Jupyter Pour utiliser R dans Jupyter notebook ou Jupyter lab, vous devez installer le module IRkernel dans la version de R que vous dÃ©sirez utiliser avec Jupyter, puis de lancer la commande IRkernel::installspec(). La prochaine fois que vous ouvrirez Jupyter, le noyau de R devrait apparaÃ®tre. Je nâ€™ai aucune expÃ©rience sur Mac, mais semble-t-il cela fonctionne comme en Linux. Ouvrez R Ã  partir dâ€™un terminal (R + Enter), puis lancez IRkernel::installspec() aprÃ¨s avoir installÃ© IRkernel. Si vous travaillez en Windows, il vous faudra lancer R par son chemin complet dans lâ€™invite de commande de Anaconda (Anaconda Powershell Prompt). Par exemple, ouvrir Anaconda Powershell Prompt, puis, si votre installation de R se trouve dans C:\\Program Files\\R\\R-3.6.2, (base) PS C:\\Users\\fifi&gt; cd &quot;C:\\Program Files\\R\\R-3.6.2\\bin&quot; (base) PS C:\\Program Files\\R\\R-3.6.2\\bin&gt; .\\R.exe R version 3.6.2 (2019-12-12) -- &quot;Dark and Stormy Night&quot; Copyright (C) 2019 The R Foundation for Statistical Computing Platform: x86_64-w64-mingw32/x64 (64-bit) R est un logiciel libre livrÃ© sans AUCUNE GARANTIE. Vous pouvez le redistribuer sous certaines conditions. Tapez &#39;license()&#39; ou &#39;licence()&#39; pour plus de dÃ©tails. R est un projet collaboratif avec de nombreux contributeurs. Tapez &#39;contributors()&#39; pour plus d&#39;information et &#39;citation()&#39; pour la faÃ§on de le citer dans les publications. Tapez &#39;demo()&#39; pour des dÃ©monstrations, &#39;help()&#39; pour l&#39;aide en ligne ou &#39;help.start()&#39; pour obtenir l&#39;aide au format HTML. Tapez &#39;q()&#39; pour quitter R. &gt; install.packages(&quot;IRkernel&quot;) Installation du package dans &#39;C:/Users/fifi/Documents/R/win-library/3.6&#39; (car &#39;lib&#39; n&#39;est pas spÃ©cifiÃ©) --- SVP sÃ©lectionner un miroir CRAN pour cette session --- essai de l&#39;URL &#39;https://cloud.r-project.org/bin/windows/contrib/3.6/IRkernel_1.1.zip&#39; Content type &#39;application/zip&#39; length 138696 bytes (135 KB) downloaded 135 KB le package &#39;IRkernel&#39; a Ã©tÃ© dÃ©compressÃ© et les sommes MD5 ont Ã©tÃ© vÃ©rifiÃ©es avec succÃ©s Les packages binaires tÃ©lÃ©chargÃ©s sont dans C:\\Users\\fifi\\AppData\\Local\\Temp\\Rtmp6xJtB3\\downloaded_packages &gt; IRkernel::installspec() [InstallKernelSpec] Installed kernelspec ir in C:\\Users\\fifi\\AppData\\Roaming\\jupyter\\kernels\\ir &gt; qui() "],
["chapitre-tableaux.html", "3 Organisation des donnÃ©es et opÃ©rations sur des tableaux 3.1 Les collections de donnÃ©es 3.2 Organiser un tableau de donnÃ©es 3.3 Formats de tableau 3.4 Entreposer ses donnÃ©es 3.5 Manipuler des donnÃ©es en mode tidyverse 3.6 RÃ©fÃ©rences", " 3 Organisation des donnÃ©es et opÃ©rations sur des tableaux ï¸Â Objectifs spÃ©cifiques: Ã€ la fin de ce chapitre, vous comprendrez les rÃ¨gles guidant la crÃ©ation et la gestion des tableaux, saurez importer et exporter des donnÃ©es et saurez effectuer des opÃ©rations en cascade avec le module tidyverse, dont des filtres sur les lignes, des sÃ©lections de colonnes, des sommaires statistiques et des jointures entre tableaux. Les donnÃ©es sont utilisÃ©es Ã  chaque Ã©tape dans les flux de travail en sciences. Elles alimentent lâ€™analyse et la modÃ©lisation. Les rÃ©sultats qui en dÃ©coulent sont aussi des donnÃ©es qui peuvent alimenter les travaux subsÃ©quents. Une bonne organisation des donnÃ©es facilitera le flux de travail. Dicton. Proportions de temps vouÃ© aux calcul scientifique: 80% de nettoyage de donnÃ©es mal organisÃ©es, 20% de calcul. Quâ€™est-ce quâ€™une donnÃ©e? De maniÃ¨re abstraite, il sâ€™agit dâ€™une valeur associÃ©e Ã  une variable. Une variable peut Ãªtre une dimension, une date, une couleur, le rÃ©sultat dâ€™un test statistique, Ã  laquelle on attribue la valeur quantitative ou qualitative dâ€™un chiffre, dâ€™une chaÃ®ne de caractÃ¨re, dâ€™un symbole conventionnÃ©, etc. Par exemple, lorsque vous commandez un cafÃ© latte vÃ©gane, au latte est la valeur que vous attribuez Ã  la variable type de cafÃ©, et vÃ©gane est la valeur de la variable type de lait. Lâ€™exemple est peut Ãªtre horrible. Jâ€™ai besoin dâ€™un cafÃ©â€¦ Ce chapitre traite de lâ€™importation, lâ€™utilisation et lâ€™exportation de donnÃ©es structurÃ©es, en R, sous forme de vecteurs, matrices, tableaux et ensemble de tableaux (bases de donnÃ©es). Bien quâ€™il soit toujours prÃ©fÃ©rable dâ€™organiser les structures qui accueilleront les donnÃ©es dâ€™une expÃ©rience avant-mÃªme de procÃ©der Ã  la collecte de donnÃ©es, lâ€™analyste doit sâ€™attendre Ã  rÃ©organiser ses donnÃ©es en cours de route. Or, des donnÃ©es bien organisÃ©es au dÃ©part faciliteront aussi leur rÃ©organisation. Ce chapitre dÃ©bute avec quelques dÃ©finitions: les donnÃ©es, les matrices, les tableaux et les bases de donnÃ©es, ainsi que leur signification en R. Puis nous verrons comment organiser un tableau selon quelques rÃ¨gles simples, mais importantes pour Ã©viter les erreurs et les opÃ©rations fastidieuses pour reconstruire un tableau mal conÃ§u. Ensuite, nous traiterons des formats de tableau courant, pour enfin passer Ã  lâ€™utilisation de dplyr, le module tidyverse pour effectuer des opÃ©rations sur les tableaux. 3.1 Les collections de donnÃ©es Dans le chapitre 2, nous avons survolÃ© diffÃ©rents types dâ€™objets: rÃ©els, entiers, chaÃ®nes de caractÃ¨res et boolÃ©ens. Les donnÃ©es peuvent appartenir Ã  dâ€™autres types: dates, catÃ©gories ordinales (ordonnÃ©es: faible, moyen, Ã©levÃ©) et nominales (non ordonnÃ©es: espÃ¨ces, cultivars, couleurs, unitÃ© pÃ©dologique, etc.). Comme mentionnÃ© en dÃ©but de chapitre, une donnÃ©e est une valeur associÃ©e Ã  une variable. Les donnÃ©es peuvent Ãªtre organisÃ©es en collections. Nous avons aussi vu au chapitre 2 que la maniÃ¨re privilÃ©giÃ©e dâ€™organiser des donnÃ©es Ã©tait sous forme de tableau. De maniÃ¨re gÃ©nÃ©rale, un tableau de donnÃ©es est une organisation de donnÃ©es en deux dimensions, comportant des lignes et des colonnes. Il est prÃ©fÃ©rable de respecter la convention selon laquelle les lignes sont des observations et les colonnes sont des variables. Ainsi, un tableau est une collection de vecteurs de mÃªme longueur, chaque vecteur reprÃ©sentant une variable. Chaque variable est libre de prendre le type de donnÃ©es appropriÃ©. La position dâ€™une donnÃ©e dans le vecteur correspond Ã  une observation. Imaginez que vous consignez des donnÃ©es de diffÃ©rents sites (A, B et C), et que chaque site possÃ¨de ses propres caractÃ©ristiques. Il est redondant de dÃ©crire le site pour chaque observation. Vous prÃ©fÃ©rerez crÃ©er deux tableaux: un pour dÃ©crire vos observations, et un autre pour dÃ©crire les sites. De cette maniÃ¨re, vous crÃ©ez une collection de tableaux intereliÃ©s: une base de donnÃ©es. R peut soutirer des donnÃ©es des bases de donnÃ©es grÃ¢ce au module DBI, qui nâ€™est pas couvert Ã  ce stade de dÃ©veloppement du cours. Dans R, les donnÃ©es structurÃ©es en tableaux, ainsi que les opÃ©rations sur les tableaux, peuvent Ãªtre gÃ©rÃ©s grÃ¢ce aux modules readr, dplyr et tidyr, tous des modules faisant partie du mÃ©ta-module tidyverse, devenu incontoutnable. Mais avant de se lancer dans lâ€™utilisation de ces modules, voyons quelques rÃ¨gles Ã  suivre pour bien structurer ses donnÃ©es en format tidy, un jargon du tidyverse qui signifie proprement organisÃ©. 3.2 Organiser un tableau de donnÃ©es Afin de repÃ©rer chaque cellule dâ€™un tableau, on attribue Ã  chaque lignes et Ã  chaque colonne colonnes un identifiant unique, que lâ€™on nomme indice pour les lignes et entÃªte pour les colonnes. RÃ¨gle no 1. Une variable par colonne, une observation par ligne, une valeur par cellule. Les unitÃ©s expÃ©rimentales sont dÃ©crits par une ou plusieurs variables par des chiffres ou des lettres. Chaque variable devrait Ãªtre prÃ©sente en une seule colonne, et chaque ligne devrait correspondre Ã  une unitÃ© expÃ©rimentale oÃ¹ ces variables ont Ã©tÃ© mesurÃ©es. La rÃ¨gle parait simple, mais elle est rarement respectÃ©e. Prenez par exemple le tableau suivant. Site Traitement A Traitement B Traitement C Sainte-Souris 4.1 8.2 6.8 Sainte-Fourmi 5.8 5.9 NA Saint-Ours 2.9 3.4 4.6 Tableau 1. Rendements obtenus sur les sites expÃ©rimentaux selon les traitements. Quâ€™est-ce qui cloche avec ce tableau? Chaque ligne est une observation, mais contient plusieurs observations dâ€™une mÃªme variable, le rendement, qui devient Ã©talÃ© sur plusieurs colonnes. Ã€ bien y penser, le type de traitement est une variable et le rendement en est une autre: Site Traitement Rendement Sainte-Souris A 4.1 Sainte-Souris B 8.2 Sainte-Souris C 6.8 Sainte-Fourmi A 5.8 Sainte-Fourmi B 5.9 Sainte-Fourmi C NA Saint-Ours A 2.9 Saint-Ours B 3.4 Saint-Ours C 4.6 Tableau 2. Rendements obtenus sur les sites expÃ©rimentaux selon les traitements. Plus prÃ©cisÃ©ment, lâ€™expression Ã  bien y penser suggÃ¨re une rÃ©flexion sur la signification des donnÃ©es. Certaines variables peuvent parfois Ãªtre intÃ©grÃ©es dans une mÃªme colonne, parfois pas. Par exemple, les concentrations en cuivre, zinc et plomb dans un sol contaminÃ© peuvent Ãªtre placÃ©s dans la mÃªme colonne â€œConcentrationâ€ ou dÃ©clinÃ©es en plusieurs colonnes Cu, Zn et Pb. La premiÃ¨re version trouvera son utilitÃ© pour des crÃ©er des graphiques (chapitre 3), alors que la deuxiÃ¨me favorise le traitement statistique (chapitre 5). Il est possible de passer dâ€™un format Ã  lâ€™autre grÃ¢ce Ã  la fonction gather() et spread() du module tidyr. RÃ¨gle no 2. Un tableau par unitÃ© observationnelle: ne pas rÃ©pÃ©ter les informations. Reprenons la mÃªme expÃ©rience. Supposons que vous mesurez la prÃ©cipitation Ã  lâ€™Ã©chelle du site. Site Traitement Rendement PrÃ©cipitations Sainte-Souris A 4.1 813 Sainte-Souris B 8.2 813 Sainte-Souris C 6.8 813 Sainte-Fourmi A 5.8 642 Sainte-Fourmi B 5.9 642 Sainte-Fourmi C NA 642 Saint-Ours A 2.9 1028 Saint-Ours B 3.4 1028 Saint-Ours C 4.6 1028 Tableau 3. Rendements obtenus sur les sites expÃ©rimentaux selon les traitements. Segmenter lâ€™information en deux tableaux serait prÃ©fÃ©rable. Site PrÃ©cipitations Sainte-Souris 813 Sainte-Fourmi 642 Saint-Ours 1028 Tableau 4. PrÃ©cipitations sur les sites expÃ©rimentaux. Les tableaux 2 et 4, ensemble, forment une base de donnÃ©es (collection organisÃ©e de tableaux). Les opÃ©rations de fusion entre les tableaux peuvent Ãªtre effectuÃ©es grÃ¢ce aux fonctions de jointure (left_join(), par exemple) du module tidyr. RÃ¨gle no 3. Ne pas bousiller les donnÃ©es. Par exemple. Ajouter des commentaires dans des cellules. Si une cellule mÃ©rite dâ€™Ãªtre commentÃ©e, il est prÃ©fÃ©rable de placer les commentaires soit dans un fichier dÃ©crivant le tableau de donnÃ©es, soit dans une colonne de commentaire juxtaposÃ©e Ã  la colonne de la variable Ã  commenter. Par exemple, si vous nâ€™avez pas mesure le pH pour une observation, nâ€™Ã©crivez pas â€œÃ©chantillon contaminÃ©â€ dans la cellule, mais annoter dans un fichier dâ€™explication que lâ€™Ã©chantillon no X a Ã©tÃ© contaminÃ©. Si les commentaires sont systÃ©matique, il peut Ãªtre pratique de les inscrire dans une colonne commentaire_pH. Inscription non systÃ©matiques. Il arrive souvent que des catÃ©gories dâ€™une variable ou que des valeurs manquantes soient annotÃ©es diffÃ©remment. Il arrive mÃªme que le sÃ©parateur dÃ©cimal soit non systÃ©matique, parfois notÃ© par un point, parfois par une virgule. Par exemple, une fois importÃ©s dans votre session, les catÃ©gories St-Ours et Saint-Ours seront traitÃ©es comme deux catÃ©gories distinctes. De mÃªme, les cellules correspondant Ã  des valeurs manquantes ne devraient pas Ãªtre inscrite parfois avec une cellule vide, parfois avec un point, parfois avec un tiret ou avec la mention NA. Le plus simple est de laisser systÃ©matiquement ces cellules vides. Inclure des notes dans un tableau. La rÃ¨gle â€œune colonne, une variableâ€ nâ€™est pas respectÃ©e si on ajoute des notes un peu nâ€™importe oÃ¹ sous ou Ã  cÃ´tÃ© du tableau. Ajouter des sommaires. Si vous ajoutez une ligne sous un tableau comprenant la moyenne de chaque colonne, quâ€™est-ce qui arrivera lorsque vous importerez votre tableau dans votre session de travail? La ligne sera considÃ©rÃ©e comme une observation supplÃ©mentaire. Inclure une hiÃ©rarchie dans le entÃªtes. Afin de consigner des donnÃ©es de texture du sol, comprenant la proportion de sable, de limon et dâ€™argile, vous organisez votre entÃªte en plusieurs lignes. Une ligne pour la catÃ©gorie de donnÃ©e, Texture, fusionnÃ©e sur trois colonnes, puis trois colonnes intitulÃ©es Sable, Limon et Argile. Votre tableau est joli, mais il ne pourra pas Ãªtre importÃ© conformÃ©ment dans un votre session de calcul: on recherche une entÃªte unique par colonne. Votre tableau de donnÃ©es devrait plutÃ´t porter les entÃªtes Texture sable, Texture limon et Texture argile. Un conseil: rÃ©server le travail esthÃ©tique Ã  la toute fin dâ€™un flux de travail. 3.3 Formats de tableau Plusieurs outils sont Ã  votre disposition pour crÃ©er des tableaux. Je vous prÃ©sente ici les plus communs. 3.3.1 xls ou xlsx Microsoft Excel est un logiciel de type tableur, ou chiffrier Ã©lectronique. Lâ€™ancien format xls a Ã©tÃ© remplacÃ© par le format xlsx avec lâ€™arrivÃ©e de Microsoft Office 2010. Il sâ€™agit dâ€™un format propriÃ©taire, dont lâ€™alternative libre la plus connue est le format ods, popularisÃ© par la suite bureautique LibreOffice. Les formats xls, xlsx ou ods sont davantage utilisÃ©s comme outils de calcul que dâ€™entreposage de donnÃ©es. Ils contiennent des formules, des graphiques, du formatage de cellule, etc. Je ne les recommande pas pour stocker des donnÃ©es. 3.3.2 csv Le format csv, pour comma separated values, est un fichier texte, que vous pouvez ouvrir avec nâ€™importe quel Ã©diteur de texte brut (Bloc note, Atom, Notepad++, etc.). Chaque colonne doit Ãªtre dÃ©limitÃ©e par un caractÃ¨re cohÃ©rent (conventionnellement une virgule, mais en franÃ§ais un point-virgule ou une tabulation pour Ã©viter la confusion avec le sÃ©parateur dÃ©cimal) et chaque ligne du tableau est un retour de ligne. Il est possible dâ€™ouvrir et dâ€™Ã©diter les fichiers csv dans un Ã©diteur texte, mais il est plus pratique de les ouvrir avec des tableurs (LibreOffice Calc, Microsoft Excel, Google Sheets, etc.). Encodage des fichiers texte. Puisque le format csv est un fichier texte, un souci particulier doit Ãªtre portÃ© sur la maniÃ¨re dont le texte est encodÃ©. Les caractÃ¨res accentuÃ©s pourrait Ãªtre importer incorrectement si vous importez votre tableau en spÃ©cifiant le mauvais encodage. Pour les fichiers en langues occidentales, lâ€™encodage UTF-8 devrait Ãªtre utilisÃ©. Toutefois, par dÃ©faut, Excel utilise un encodage de Microsoft. Si le csv a Ã©tÃ© gÃ©nÃ©rÃ© par Excel, il est prÃ©fÃ©rable de lâ€™ouvrir avec votre Ã©diteur texte et de lâ€™enregistrer dans lâ€™encodage UTF-8. 3.3.3 json Comme le format csv, le format json indique un fichier en texte clair. Il est utilisÃ© davantage pour le partage de donnÃ©es des applications web. En analyse et modÃ©lisation, ce format est surtout utilisÃ© pour les donnÃ©es gÃ©orÃ©fÃ©rencÃ©es. Lâ€™encodage est gÃ©rÃ© de la mÃªme maniÃ¨re quâ€™un fichier csv. 3.3.4 SQLite SQLite est une application pour les bases de donnÃ©es relationnelles de type SQL qui nâ€™a pas besoin de serveur pour fonctionner. Les bases de donnÃ©es SQLite sont encodÃ©s dans des fichiers portant lâ€™extension db, qui peuvent Ãªtre facilement partagÃ©s. 3.3.5 Suggestion En csv pour les petits tableaux, en sqlite pour les bases de donnÃ©es plus complexes. Ce cours se concentre toutefois sur les donnÃ©es de type csv. 3.4 Entreposer ses donnÃ©es La maniÃ¨re la plus sÃ©curisÃ©e pour entreposer ses donnÃ©es est de les confiner dans une base de donnÃ©es sÃ©curisÃ©e sur un serveur sÃ©curisÃ© dans un environnement sÃ©curisÃ© et dâ€™encrypter les communications. Câ€™est aussi la maniÃ¨re la moins accessible. Des espaces de stockage nuagiques, comme Dropbox ou dâ€™autres options similaires, peuvent Ãªtre pratiques pour les backups et le partage des donnÃ©es avec une Ã©quipe de travail (qui risque en retour de bousiller vos donnÃ©es). Le suivi de version est possible chez certains fournisseurs dâ€™espace de stockage. Mais pour un suivi de version plus rigoureux, les espaces de dÃ©veloppement (comme GitHub et GitLab) sont plus appropriÃ©s (couverts au chapitre 5). Dans tous les cas, il est important de garder (1) des copies anciennes pour y revenir en cas dâ€™erreurs et (2) un petit fichier dÃ©crivant les changements effectuÃ©s sur les donnÃ©es. 3.5 Manipuler des donnÃ©es en mode tidyverse Le mÃ©ta-module tidyverse regroupe une collection de prÃ©cieux modules pour lâ€™analyse de donnÃ©es en R. Il permet dâ€™importer des donnÃ©es dans votre session de travail avec readr, de les explorer avec le module de visualisation ggplot2, de les transformer avec tidyr et dplyr et de les exporter avec readr. Les tableaux de classe data.frame, comme ceux de la plus moderne classe tibble, peuvent Ãªtre manipulÃ©s Ã  travers le flux de travail pour lâ€™analyse et la modÃ©lisation (chapitres suivants). Comme câ€™Ã©tait le cas pour le chapitre sur la visualisation, ce chapitre est loin de couvrir les nombreuses fonctionnalitÃ©s qui sont offertes dans le tidyverse. 3.5.1 Importer vos donnÃ©es dans voter session de travail Supposons que vous avec bien organisÃ© vos donnÃ©es en mode tidy. Pour les importer dans votre session et commencer Ã  les inspecter, vous lancerez une des commandes du module readr, dÃ©crites dans la documentation dÃ©diÃ©e. read_csv() si le sÃ©parateur de colonne est une virgule read_csv2() si le sÃ©parateur de colonne est un point-virgule et que le sÃ©parateur dÃ©cimal est une virgule read_tsv() si le sÃ©parateur de colonne est une tabulation read_table() si le sÃ©parateur de colonne est un espace blanc read_delim() si le sÃ©parateur de colonne est un autre caractÃ¨re (comme le point-virgule) que vous spÃ©cifierez dans lâ€™argument delim = &quot;;&quot; Les principaux arguments sont les suivants. file: le chemin vers le fichier. Ce chemin peut aussi bien Ãªtre une adresse locale (data/â€¦) quâ€™une adresse internet (https://â€¦). delim: le symbole dÃ©limitant les colonnes dans le cas de read_delim. col_names: si TRUE, la premiÃ¨re ligne est lâ€™entÃªte du tableau, sinon FALSE. Si vous spÃ©cifiez un vecteur numÃ©rique, ce sont les numÃ©ros des lignes utilisÃ©es pour le nom de lâ€™entÃªte. Si vous utilisez un vecteur de caractÃ¨res, ce sont les noms des colonnes que vous dÃ©sirez donner Ã  votre tableau. na: le symbole spÃ©cifiant une valeur manquante. Lâ€™argument na='' signifie que les cellules vides sont des donnÃ©es manquantes. Si les valeurs manquantes ne sont pas uniformes, vous pouvez les indiquer dans un vecteur, par exemple na = c(&quot;&quot;, &quot;NA&quot;, &quot;NaN&quot;, &quot;.&quot;, &quot;-&quot;). local: cet argument prend une fonction local() qui peut inclure des arguments de format de temps, mais aussi dâ€™encodage (voir documentation) Dâ€™autres arguments peuvent Ãªtre spÃ©cifiÃ©s au besoin, et les rÃ©pÃ©ter ici dupliquerait lâ€™information de la documentation de la fonction read_csv de readr. Je dÃ©conseille dâ€™importer des donnÃ©es en format xls ou xlsx. Si toutefois cela vous convient, je vous rÃ©fÃ¨re au module readxl. Lâ€™aide-mÃ©moire de readr est Ã  afficher prÃ¨s de soi. Aide-mÃ©moire de readr, source: https://www.rstudio.com/resources/cheatsheets/ Nous allons charger des donnÃ©es de culture de la chicoutÃ© (Rubus chamaemorus), un petit fruit nordique, tirÃ© de Parent et al. (2013). Ouvrons dâ€™abord le fichier pour vÃ©rifier les sÃ©parateurs de colonne et de dÃ©cimale. Le sÃ©parateur de colonne est un point-virgule et le dÃ©cimal est une virgule. Avec Atom, mon Ã©diteur texte prÃ©fÃ©rÃ©, je vais dans Edit &gt; Select Encoding et jâ€™obtiens bien le UTF-8. Nous allons donc utiliser read_csv2() avec ses arguments par dÃ©faut. library(&quot;tidyverse&quot;) chicoute &lt;- read_csv2(&#39;data/chicoute.csv&#39;) ## Using &#39;,&#39; as decimal and &#39;.&#39; as grouping mark. Use read_delim() for more control. ## Parsed with column specification: ## cols( ## .default = col_double(), ## CodeTourbiere = col_character(), ## Ordre = col_character(), ## Traitement = col_character(), ## DemiParcelle = col_character(), ## SousTraitement = col_character() ## ) ## See spec(...) for full column specifications. Quelques commandes utiles inspecter le tableau: head() prÃ©sente lâ€™entÃªte du tableau, soit ses 6 premiÃ¨res lignes str() et glimpse() prÃ©sentent les variables du tableau et leur type - glimpse()est la fonction tidyverse et str() est la fonction classique (je prÃ©fÃ¨re str()) summary() prÃ©sente des statistiques de base du tableau names() ou colnames() sort les noms des colonnes sous forme dâ€™un vecteur dim() donne les dimensions du tableau, ncol() son nombre de colonnes et nrow() son nombre de lignes skim est une fonction du module skimr montrant un portrait graphique et numÃ©rique du tableau Extra 1. Plusieurs modules ne se trouvent pas dans les dÃ©pÃ´t CRAN, mais sont disponibles sur GitHub. Pour les installer, installez dâ€™abord le module devtools disponible sur CRAN. Vous pourrez alors installer les packages de GitHub comme on le fait avec le package skimr. Extra 2. Lorsque je dÃ©sire utiliser une fonction, mais sans charger le module dans la session, jâ€™utilise la notation module::fonction. Comme dans ce cas, pour skimr. # devtools::install_github(&quot;ropenscilabs/skimr&quot;) skimr::skim(chicoute) Table 3.1: Data summary Name chicoute Number of rows 90 Number of columns 31 _______________________ Column type frequency: character 5 numeric 26 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace CodeTourbiere 0 1.00 1 4 0 12 0 Ordre 0 1.00 1 2 0 20 0 Traitement 50 0.44 6 11 0 2 0 DemiParcelle 50 0.44 4 5 0 2 0 SousTraitement 50 0.44 1 7 0 3 0 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist ID 0 1.00 45.50 26.12 1.00 23.25 45.50 67.75 90.00 â–‡â–‡â–‡â–‡â–‡ Site 0 1.00 6.33 5.49 1.00 2.00 4.00 9.00 20.00 â–‡â–ƒâ–â–â– Latitude_m 0 1.00 5701839.86 1915.50 5695688.00 5701868.50 5702129.00 5702537.00 5706394.00 â–â–‚â–…â–‡â– Longitude_m 0 1.00 485295.54 6452.33 459873.00 485927.00 486500.00 486544.75 491955.00 â–â–â–â–‚â–‡ Rendement_g_5m2 50 0.44 13.33 21.56 0.00 0.00 0.94 15.63 72.44 â–‡â–â–â–â– TotalRamet_nombre_m2 0 1.00 251.26 156.06 40.74 122.70 212.92 347.80 651.90 â–‡â–‡â–ƒâ–‚â–‚ TotalVegetatif_nombre_m2 4 0.96 199.02 139.13 22.92 86.26 161.25 263.78 580.60 â–‡â–‡â–‚â–‚â– TotalFloral_nombre_m2 4 0.96 52.08 40.41 4.80 22.92 43.00 69.52 198.62 â–‡â–…â–‚â–â– TotalMale_nombre_m2 4 0.96 24.40 26.87 0.00 3.30 15.28 36.51 104.41 â–‡â–‚â–‚â–â– TotalFemelle_nombre_m2 4 0.96 27.53 29.83 2.55 10.34 17.19 31.96 187.17 â–‡â–â–â–â– FemelleFruit_nombre_m2 18 0.80 19.97 23.79 0.40 7.64 11.46 22.83 157.88 â–‡â–‚â–â–â– FemelleAvorte_nombre_m2 4 0.96 8.49 14.52 0.00 1.27 3.07 10.14 76.80 â–‡â–â–â–â– SterileFleur_nombre_m2 4 0.96 0.26 0.71 0.00 0.00 0.00 0.00 3.82 â–‡â–â–â–â– C_pourc 0 1.00 50.28 1.61 46.72 49.14 50.45 51.58 53.83 â–ƒâ–†â–…â–‡â– N_pourc 0 1.00 2.20 0.40 1.53 1.89 2.12 2.58 3.10 â–ƒâ–‡â–ƒâ–ƒâ–‚ P_pourc 0 1.00 0.14 0.04 0.07 0.12 0.14 0.16 0.23 â–ƒâ–†â–‡â–‚â–‚ K_pourc 0 1.00 0.89 0.27 0.35 0.69 0.86 1.13 1.54 â–ƒâ–‡â–‡â–‡â– Ca_pourc 0 1.00 0.39 0.10 0.19 0.32 0.37 0.44 0.88 â–…â–‡â–‚â–â– Mg_pourc 0 1.00 0.50 0.08 0.36 0.45 0.48 0.52 0.86 â–‡â–‡â–‚â–â– S_pourc 0 1.00 0.13 0.04 0.07 0.11 0.13 0.14 0.28 â–…â–‡â–‚â–â– B_pourc 0 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 â–‚â–…â–ƒâ–‡â–ƒ Cu_pourc 0 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 â–‡â–â–â–â– Zn_pourc 0 1.00 0.01 0.00 0.00 0.01 0.01 0.01 0.02 â–‡â–‡â–‚â–â– Mn_pourc 0 1.00 0.03 0.03 0.00 0.01 0.03 0.05 0.10 â–‡â–…â–ƒâ–‚â– Fe_pourc 0 1.00 0.02 0.01 0.01 0.01 0.01 0.02 0.05 â–‡â–‚â–â–â– Al_pourc 0 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 â–‡â–…â–â–â– Exercice. Inspectez le tableau. 3.5.2 Comment sÃ©lectionner et filtrer des donnÃ©es? On utiliser le terme sÃ©lectionner lorsque lâ€™on dÃ©sire choisir une ou plusieurs lignes et colonnes dâ€™un tableau (la plupart du temps des colonnes). Lâ€™action de filtrer signifie de sÃ©lectionner des lignes selon certains critÃ¨res. 3.5.2.1 SÃ©lectionner Voici trois maniÃ¨res de sÃ©lectionner une colonne en R. Une mÃ©thode rapide mais peu expressive consiste Ã  indiquer les valeurs numÃ©riques de lâ€™indice de la colonne entre des crochets. Il sâ€™agit dâ€™appeler le tableau suivit de crochets. Lâ€™intÃ©rieur des crochets comprend deux Ã©lÃ©ments sÃ©parÃ©s par une virgule. Le premier Ã©lÃ©ment sert Ã  filtrer selon lâ€™indice, le deuxiÃ¨me sert Ã  sÃ©lectionner selon lâ€™indice. Ainsi: chicoute[, 1]: sÃ©lectionner la premiÃ¨re colonne chicoute[, 1:10]: sÃ©lectionner les 10 premiÃ¨res colonnes chicoute[, c(2, 4, 5)]: sÃ©lectionner les colonnes 2, 4 et 5 chicoute[c(10, 13, 20), c(2, 4, 5)]: sÃ©lectionner les colonnes 2, 4 et 5 et les lignes 10, 13 et 20. Une autre mÃ©thode rapide, mais plus expressive, consiste Ã  appeler le tableau, suivi du symbole $, puis le nom de la colonne. Truc. La plupart des IDE, comme RStudio, peuvent vous proposer des colonnes dans une liste. AprÃ¨s avoir entrer le $, taper sur la touche de tabulation: vous pourrez sÃ©lectionner la colonne dans une liste dÃ©filante. Une autre option est dâ€™inscrire le nom de la colonne, ou du vecteur des colonnes, entre des crochets suivant le nom du tableau, câ€™est-Ã -dire chicoute[c(&quot;Site&quot;, &quot;Latitude_m&quot;, &quot;Longitude_m&quot;)]. Enfin, dans une sÃ©quence dâ€™opÃ©rations en mode pipeline (chaque opÃ©ration est mise Ã  la suite de la prÃ©cÃ©dente en plaÃ§ant le pipe %&gt;% entre chacune), il peut Ãªtre prÃ©fÃ©rable de sÃ©lectionner des colonnes avec la fonction select(), i.e. chicoute %&gt;% select(Site, Latitude_m, Longitude_m) La fonction select() permet aussi de travailler en exclusion. Ainsi pour enlever des colonnes, on placera un - (signe de soustraction) devant le nom de la colonne. Dâ€™autre arguments de select() permettent une sÃ©lection rapide. Par exemple, pour obtenir les colonnes contenant des pourcentages: chicoute %&gt;% select(ends_with(&quot;pourc&quot;)) %&gt;% head(3) ## # A tibble: 3 x 13 ## C_pourc N_pourc P_pourc K_pourc Ca_pourc Mg_pourc S_pourc B_pourc Cu_pourc Zn_pourc Mn_pourc Fe_pourc Al_pourc ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 51.5 1.72 0.108 1.21 0.435 0.470 0.0976 0.00258 0.000175 0.00470 0.101 0.0133 0.00156 ## 2 51.3 2.18 0.0985 1.22 0.337 0.439 0.0996 0.00258 0.000407 0.00729 0.0783 0.0148 0.00189 ## 3 50.6 2.12 0.0708 1.05 0.373 0.420 0.104 0.00258 0.000037 0.00713 0.0722 0.0148 0.00160 3.5.2.2 Filtrer Comme câ€™est le cas de la sÃ©lection, on pourra filtrer un tableau de plusieurs maniÃ¨res. Jâ€™ai dÃ©jÃ  prÃ©sentÃ© comment filtrer selon les indices des lignes. Les autres maniÃ¨res reposent nÃ©anmoins sur une opÃ©ration logique ==, &lt;, &gt; ou %in% (le %in% signifie se trouve parmi et peut Ãªtre suivi dâ€™un vecteur de valeur que lâ€™on dÃ©sire accepter). Les conditions boolÃ©ennes peuvent Ãªtre combinÃ©es avec les opÃ©rateurs et, &amp;, et ou, |. Pour rappel, OpÃ©ration RÃ©sultat Vrai et Vrai Vrai Vrai et Faux Faux Faux et Faux Faux Vrai ou Vrai Vrai Vrai ou Faux Vrai Faux ou Faux Faux La mÃ©thode classique consiste Ã  appliquer une opÃ©ration logique entre les crochets, par exemple chicoute[chicoute$CodeTourbiere == &quot;BEAU&quot;, ] La mÃ©thode tidyverse, plus pratique en mode pipeline, passe par la fonction filter(), i.e. chicoute %&gt;% filter(CodeTourbiere == &quot;BEAU&quot;) Combiner le tout. chicoute %&gt;% filter(Ca_pourc &lt; 0.4 &amp; CodeTourbiere %in% c(&quot;BEAU&quot;, &quot;MB&quot;, &quot;WTP&quot;)) %&gt;% select(contains(&quot;pourc&quot;)) ## # A tibble: 4 x 13 ## C_pourc N_pourc P_pourc K_pourc Ca_pourc Mg_pourc S_pourc B_pourc Cu_pourc Zn_pourc Mn_pourc Fe_pourc Al_pourc ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 51.3 2.18 0.0985 1.22 0.337 0.439 0.0996 0.00258 0.000407 0.00729 0.0783 0.0148 0.00189 ## 2 50.6 2.12 0.0708 1.05 0.373 0.420 0.104 0.00258 0.000037 0.00713 0.0722 0.0148 0.00160 ## 3 53.8 2.04 0.115 0.947 0.333 0.472 0.106 0.00258 0.000037 0.00510 0.0345 0.0120 0.00102 ## 4 52.6 2.11 0.0847 0.913 0.328 0.376 0.111 0.00296 0.000037 0.00679 0.0491 0.0141 0.00151 3.5.3 Le format long et le format large Dans le tableau chicoute, chaque Ã©lÃ©ment possÃ¨de sa propre colonne. Si lâ€™on voulait mettre en graphique les boxplot des facettes de concentrations dâ€™azote, de phosphore et de potassium dans les diffÃ©rentes tourbiÃ¨res, il faudrait obtenir une seule colonne de concentrations. Pour ce faire, nous utiliserons la fonction gather(). Le premier argument est le nom de la colonne des variables, le deuxiÃ¨me est le nom de la nouvelle colonne des valeurs. La suite consiste Ã  dÃ©crire les colonnes Ã  inclure ou Ã  exclure. Dans le cas qui suit, jâ€™exclue CodeTourbiere de la refonte jâ€™utilise sample_n() pour prÃ©senter un Ã©chantillon du rÃ©sultat. Notez la ligne comprenant la fonction mutate, que lâ€™on verra plus loin. Cette fonction ajoute une colonne au tableau. Dans ce cas-ci, jâ€™ajoute une colonne constituÃ©e dâ€™une sÃ©quence de nombres allant de 1 au nombre de lignes du tableau (il y en a 90). Cet identifiant unique pour chanque ligne permettra de reconstituer par la suite le tableau initial. chicoute_long &lt;- chicoute %&gt;% select(CodeTourbiere, N_pourc, P_pourc, K_pourc) %&gt;% mutate(ID = 1:nrow(.)) %&gt;% # mutate ajoute une colonne au tableau gather(key = element, value = concentration, -CodeTourbiere, -ID) chicoute_long %&gt;% sample_n(10) ## # A tibble: 10 x 4 ## CodeTourbiere ID element concentration ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 66 N_pourc 2.63 ## 2 2 25 K_pourc 1.24 ## 3 BP 8 K_pourc 0.778 ## 4 1 83 N_pourc 1.82 ## 5 BEAU 4 P_pourc 0.0909 ## 6 BP 9 P_pourc 0.156 ## 7 1 84 K_pourc 1.32 ## 8 NBM 49 N_pourc 2.06 ## 9 1 77 P_pourc 0.138 ## 10 2 29 N_pourc 2.70 Lâ€™opÃ©ration inverse est spread(). chicoute_large &lt;- chicoute_long %&gt;% spread(key = &quot;element&quot;, value = &quot;concentration&quot;) %&gt;% select(-ID) # enlever l&#39;identifiant unique chicoute_large %&gt;% sample_n(10) ## # A tibble: 10 x 4 ## CodeTourbiere K_pourc N_pourc P_pourc ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BP 0.778 1.77 0.166 ## 2 2 1.17 2.70 0.190 ## 3 1 0.863 2.36 0.168 ## 4 1 0.988 2.63 0.186 ## 5 BS2 0.632 2.62 0.108 ## 6 2 1.34 2.59 0.158 ## 7 BS2 1.54 1.78 0.118 ## 8 1 0.937 1.95 0.141 ## 9 MB 0.738 2.35 0.115 ## 10 2 1.01 2.90 0.166 3.5.4 Combiner des tableaux Nous avons introduit plus haut la notion de base de donnÃ©es. Nous voudrions peut-Ãªtre utiliser le code des tourbiÃ¨res pour inclure leur nom, le type dâ€™essai menÃ© Ã  ces tourbiÃ¨res, etc. Importons dâ€™abord le tableau des noms liÃ©s aux codes. tourbieres &lt;- read_csv2(&quot;data/chicoute_tourbieres.csv&quot;) ## Using &#39;,&#39; as decimal and &#39;.&#39; as grouping mark. Use read_delim() for more control. ## Parsed with column specification: ## cols( ## Tourbiere = col_character(), ## CodeTourbiere = col_character(), ## Type = col_character(), ## TypeCulture = col_character() ## ) tourbieres ## # A tibble: 11 x 4 ## Tourbiere CodeTourbiere Type TypeCulture ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Beaulieu BEAU calibration naturel ## 2 Brador Path BP calibration naturel ## 3 Lichen (BS2E) 2 validation cultive sec ## 4 Mannys Brook MB calibration naturel ## 5 Middle Bay Road MR calibration naturel ## 6 North Est of Smelt Pond NESP calibration naturel ## 7 North of Blue Moon NBM calibration naturel ## 8 South of Smelt Pond SSP calibration naturel ## 9 Sphaigne (BS2F) BS2 validation cultive sec ## 10 Sphaigne (BS2F) 1 calibration naturel ## 11 West of Trout Pond WTP calibration naturel Notre information est organisÃ©e en deux tableaux, liÃ©s par la colonne CodeTourbiere. Comment fusionner lâ€™information pour quâ€™elle puisse Ãªtre utilisÃ©e dans son ensemble? La fonction left_join effectue cette opÃ©ration typique avec les bases de donnÃ©es. chicoute_merge &lt;- left_join(x = chicoute, y = tourbieres, by = &quot;CodeTourbiere&quot;) # ou bien chicoute %&gt;% left_join(y = tourbieres, by = &quot;CodeTourbiere&quot;) chicoute_merge %&gt;% sample_n(4) ## # A tibble: 4 x 34 ## ID CodeTourbiere Ordre Site Traitement DemiParcelle SousTraitement Latitude_m Longitude_m Rendement_g_5m2 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5 BEAU A 5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 5702445 490654 NA ## 2 37 MR I 2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 5701987 459873 NA ## 3 48 NBM D 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 5696607 485912 NA ## 4 8 BP H 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 5702280 484742 NA ## # â€¦ with 24 more variables: TotalRamet_nombre_m2 &lt;dbl&gt;, TotalVegetatif_nombre_m2 &lt;dbl&gt;, TotalFloral_nombre_m2 &lt;dbl&gt;, ## # TotalMale_nombre_m2 &lt;dbl&gt;, TotalFemelle_nombre_m2 &lt;dbl&gt;, FemelleFruit_nombre_m2 &lt;dbl&gt;, FemelleAvorte_nombre_m2 &lt;dbl&gt;, ## # SterileFleur_nombre_m2 &lt;dbl&gt;, C_pourc &lt;dbl&gt;, N_pourc &lt;dbl&gt;, P_pourc &lt;dbl&gt;, K_pourc &lt;dbl&gt;, Ca_pourc &lt;dbl&gt;, Mg_pourc &lt;dbl&gt;, ## # S_pourc &lt;dbl&gt;, B_pourc &lt;dbl&gt;, Cu_pourc &lt;dbl&gt;, Zn_pourc &lt;dbl&gt;, Mn_pourc &lt;dbl&gt;, Fe_pourc &lt;dbl&gt;, Al_pourc &lt;dbl&gt;, ## # Tourbiere &lt;chr&gt;, Type &lt;chr&gt;, TypeCulture &lt;chr&gt; Dâ€™autres types de jointures sont possibles, et dÃ©crites en dÃ©tails dans la documentation. Garrick Aden-Buie a prÃ©parÃ© de jolies animations pour dÃ©crire les diffÃ©rents types de jointures. left_join(x, y) colle y Ã  x seulement ce qui dans y correspond Ã  ce que lâ€™on trouve dans x. right_join(x, y) colle y Ã  x seulement ce qui dans x correspond Ã  ce que lâ€™on trouve dans y. inner_join(x, y) colle x et y en excluant les lignes oÃ¹ au moins une variable de joint est absente dans x et y. full_join(x, y)garde toutes les lignes et les colonnes de x et y. 3.5.5 OpÃ©rations sur les tableaux Les tableaux peuvent Ãªtre segmentÃ©s en Ã©lÃ©ments sur lesquels on calculera ce qui nous chante. On pourrait vouloir obtenir: la somme avec la function sum() la moyenne avec la function mean() ou la mÃ©diane avec la fonction median() lâ€™Ã©cart-type avec la function sd() les maximum et minimum avec les fonctions min() et max() un dÃ©compte dâ€™occurrence avec la fonction n() ou count() Par exemple, mean(chicoute$Rendement_g_5m2, na.rm = TRUE) ## [1] 13.32851 En mode classique, pour effectuer des opÃ©rations sur des tableaux, on utilisera la fonction apply(). Cette fonction prend, comme arguments, le tableau, lâ€™axe (opÃ©ration par ligne = 1, opÃ©ration par colonne = 2), puis la fonction Ã  appliquer. apply(chicoute %&gt;% select(contains(&quot;pourc&quot;)), 2, mean) ## C_pourc N_pourc P_pourc K_pourc Ca_pourc Mg_pourc S_pourc B_pourc Cu_pourc ## 5.027911e+01 2.199411e+00 1.388959e-01 8.887000e-01 3.884391e-01 4.980142e-01 1.347177e-01 3.090922e-03 4.089891e-04 ## Zn_pourc Mn_pourc Fe_pourc Al_pourc ## 6.662155e-03 3.345239e-02 1.514885e-02 2.694979e-03 Les opÃ©ration peuvent aussi Ãªtre effectuÃ©es par ligne, par exemple une somme (je garde seulement les 10 premiers rÃ©sultats). apply(chicoute %&gt;% select(contains(&quot;pourc&quot;)), 1, sum)[1:10] ## [1] 55.64299 55.76767 54.78856 55.84453 57.89671 55.53603 55.62526 55.10991 55.06295 55.16774 La fonction Ã  appliquer peut Ãªtre personnalisÃ©e, par exemple: apply(chicoute %&gt;% select(contains(&quot;pourc&quot;)), 2, function(x) (prod(x))^(1/length(x))) ## C_pourc N_pourc P_pourc K_pourc Ca_pourc Mg_pourc S_pourc B_pourc Cu_pourc ## 50.253429104 2.165246915 0.133754530 0.846193827 0.376192724 0.491763884 0.129900753 0.003014675 0.000000000 ## Zn_pourc Mn_pourc Fe_pourc Al_pourc ## 0.006408775 0.024140327 0.014351745 0.002450982 Vous reconnaissez cette fonction? Câ€™Ã©tait la moyenne gÃ©omÃ©trique (la fonction prod() Ã©tant le produit dâ€™un vecteur). En mode tidyverse, on aura besoin principalement des fonction suivantes: group_by() pour effectuer des opÃ©rations par groupe, lâ€™opÃ©ration group_by() sÃ©pare le tableau en plusieurs petits tableaux, en attendant de les recombiner. Câ€™est un peu lâ€™Ã©quivalent des facettes avec le module de visualisation ggplot2, que nous explorons au chapitre 4. summarise() pour rÃ©duire plusieurs valeurs en une seule, il applique un calcul sur le tableau ou sâ€™il y a lieu sur chaque petit tableau segmentÃ©. Il en existe quelques variantes. summarise_all() applique la fonction Ã  toutes les colonnes summarise_at() applique la fonction aux colonnes spÃ©cifiÃ©es summarise_if() applique la fonction aux colonnes qui ressortent comme TRUE selon une opÃ©ration boolÃ©enne mutate() pour ajouter une nouvelle colonne Si lâ€™on dÃ©sire ajouter une colonne Ã  un tableau, par exemple le sommaire calculÃ© avec summarise(). Ã€ lâ€™inverse, la fonction transmute() retournera seulement le rÃ©sultat, sans le tableau Ã  partir duquel il a Ã©tÃ© calculÃ©. De mÃªme que summarise(), mutate() et transmute() possÃ¨dent leurs Ã©quivalents _all(), _at() et _if(). arrange() pour rÃ©ordonner le tableau On a dÃ©jÃ  couvert arrange() dans le chapitre 3. Rappelons que cette fonction nâ€™est pas une opÃ©ration sur un tableau, mais plutÃ´t un changement dâ€™affichage en changeant lâ€™ordre dâ€™apparition des donnÃ©es. Ces opÃ©rations sont dÃ©crites dans lâ€™aide-mÃ©moire Data Transformation Cheat Sheet. Aide-mÃ©moire de dplyr, source: https://www.rstudio.com/resources/cheatsheets/ Pour effectuer des statistiques par colonne, on utilisera summarise pour des statistiques effectuÃ©es sur une seule colonne. sumarise peut prendre le nombre dÃ©sirÃ© de statistiques dont la sortie est un scalaire. chicoute %&gt;% summarise(moyenne = mean(TotalFloral_nombre_m2, na.rm = TRUE), ecart_type = sd(TotalFloral_nombre_m2, na.rm = TRUE)) ## # A tibble: 1 x 2 ## moyenne ecart_type ## &lt;dbl&gt; &lt;dbl&gt; ## 1 52.1 40.4 Si lâ€™on dÃ©sire un sommaire sur toutes les variables sÃ©lectionnÃ©es, on utilisera summarise_all(). Pour spÃ©cifier que lâ€™on dÃ©sire la moyenne et lâ€™Ã©cart-type on inscrit les noms des fonctions dans funs(). chicoute %&gt;% select(contains(&quot;pourc&quot;)) %&gt;% summarise_all(funs(mean, sd)) ## Warning: funs() is soft deprecated as of dplyr 0.8.0 ## Please use a list of either functions or lambdas: ## ## # Simple named list: ## list(mean = mean, median = median) ## ## # Auto named with `tibble::lst()`: ## tibble::lst(mean, median) ## ## # Using lambdas ## list(~ mean(., trim = .2), ~ median(., na.rm = TRUE)) ## This warning is displayed once per session. ## # A tibble: 1 x 26 ## C_pourc_mean N_pourc_mean P_pourc_mean K_pourc_mean Ca_pourc_mean Mg_pourc_mean S_pourc_mean B_pourc_mean Cu_pourc_mean ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 50.3 2.20 0.139 0.889 0.388 0.498 0.135 0.00309 0.000409 ## # â€¦ with 17 more variables: Zn_pourc_mean &lt;dbl&gt;, Mn_pourc_mean &lt;dbl&gt;, Fe_pourc_mean &lt;dbl&gt;, Al_pourc_mean &lt;dbl&gt;, ## # C_pourc_sd &lt;dbl&gt;, N_pourc_sd &lt;dbl&gt;, P_pourc_sd &lt;dbl&gt;, K_pourc_sd &lt;dbl&gt;, Ca_pourc_sd &lt;dbl&gt;, Mg_pourc_sd &lt;dbl&gt;, ## # S_pourc_sd &lt;dbl&gt;, B_pourc_sd &lt;dbl&gt;, Cu_pourc_sd &lt;dbl&gt;, Zn_pourc_sd &lt;dbl&gt;, Mn_pourc_sd &lt;dbl&gt;, Fe_pourc_sd &lt;dbl&gt;, ## # Al_pourc_sd &lt;dbl&gt; On utilisera group_by() pour segmenter le tableau, et ainsi obtenir des statistiques pour chaque groupe. chicoute %&gt;% group_by(CodeTourbiere) %&gt;% summarise(moyenne = mean(TotalFloral_nombre_m2, na.rm = TRUE), ecart_type = sd(TotalFloral_nombre_m2, na.rm = TRUE)) ## # A tibble: 12 x 3 ## CodeTourbiere moyenne ecart_type ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 72.1 32.7 ## 2 2 37.1 32.9 ## 3 BEAU 149. 53.2 ## 4 BP 60.4 30.6 ## 5 BS2 27.2 15.5 ## 6 MB 64.7 40.8 ## 7 MR 35.1 10.5 ## 8 NBM 35.1 16.6 ## 9 NESP 21.4 4.88 ## 10 NTP 47.6 15.9 ## 11 SSP 25.7 11.1 ## 12 WTP 50.2 28.3 Dans le cas de summarise_all, les rÃ©sultats sâ€™affichent de la mÃªme maniÃ¨re. chicoute %&gt;% group_by(CodeTourbiere) %&gt;% select(N_pourc, P_pourc, K_pourc) %&gt;% summarise_all(funs(mean, sd)) ## Adding missing grouping variables: `CodeTourbiere` ## # A tibble: 12 x 7 ## CodeTourbiere N_pourc_mean P_pourc_mean K_pourc_mean N_pourc_sd P_pourc_sd K_pourc_sd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2.26 0.156 0.880 0.250 0.0193 0.201 ## 2 2 2.76 0.181 1.12 0.178 0.0283 0.179 ## 3 BEAU 2.00 0.0967 1.12 0.179 0.0172 0.120 ## 4 BP 2.05 0.158 0.747 0.161 0.00625 0.0652 ## 5 BS2 2.08 0.103 1.12 0.420 0.0218 0.333 ## 6 MB 2.15 0.109 0.675 0.114 0.0165 0.183 ## 7 MR 1.99 0.127 0.830 0.0802 0.0131 0.142 ## 8 NBM 2.01 0.127 0.854 0.310 0.0202 0.177 ## 9 NESP 1.76 0.135 0.945 0.149 0.0108 0.152 ## 10 NTP 1.83 0.0873 0.402 0.166 0.0103 0.0404 ## 11 SSP 1.83 0.130 0.700 0.160 0.00383 0.0487 ## 12 WTP 1.79 0.0811 0.578 0.132 0.00587 0.102 Pour obtenir des statistiques Ã  chaque ligne, mieux vaut utiliser apply(), tel que vu prÃ©cÃ©demment. Le point, ., reprÃ©sente le tableau dans la fonction. chicoute %&gt;% select(contains(&quot;pourc&quot;)) %&gt;% apply(., 1, sum) ## [1] 55.64299 55.76767 54.78856 55.84453 57.89671 55.53603 55.62526 55.10991 55.06295 55.16774 56.41123 55.47917 55.43537 ## [14] 55.79175 55.44561 54.85448 54.34262 55.03075 54.40533 51.89319 54.70172 54.62176 54.30250 53.86976 53.44731 53.86244 ## [27] 52.43280 54.34978 53.96756 51.46672 55.44267 54.70350 55.30711 56.16200 56.64710 55.95499 54.76370 54.32775 54.95419 ## [40] 53.37094 53.07855 53.04541 52.09520 52.40456 51.92376 53.33248 56.56405 56.35004 56.27185 55.56986 53.81654 55.39638 ## [53] 55.51961 54.88098 54.74774 51.08921 51.31462 53.46819 53.15640 52.82020 57.78038 57.94636 56.65558 56.28845 55.54463 ## [66] 56.51751 55.36497 56.00594 55.64247 56.56967 56.81674 55.87070 55.72308 56.14116 56.42611 55.35650 54.90469 54.03674 ## [79] 53.42991 53.99334 53.09085 53.23222 53.28212 53.63192 53.48102 52.31131 51.72026 51.10534 51.49055 51.59297 Prenons ce tableau des espÃ¨ces menacÃ©es issu de lâ€™Union internationale pour la conservation de la nature distribuÃ©es par lâ€™OCDE. library(&quot;tidyverse&quot;) especes_menacees &lt;- read_csv(&#39;data/WILD_LIFE_14012020030114795.csv&#39;) ## Parsed with column specification: ## cols( ## IUCN = col_character(), ## `IUCN Category` = col_character(), ## SPEC = col_character(), ## Species = col_character(), ## COU = col_character(), ## Country = col_character(), ## `Unit Code` = col_character(), ## Unit = col_character(), ## `PowerCode Code` = col_double(), ## PowerCode = col_character(), ## `Reference Period Code` = col_logical(), ## `Reference Period` = col_logical(), ## Value = col_double(), ## `Flag Codes` = col_logical(), ## Flags = col_logical() ## ) Nous exÃ©cutons le pipeline suivant. especes_menacees %&gt;% dplyr::filter(IUCN == &#39;CRITICAL&#39;) %&gt;% dplyr::select(Country, Value) %&gt;% dplyr::group_by(Country) %&gt;% dplyr::summarise(n_critical_species = sum(Value)) %&gt;% dplyr::arrange(desc(n_critical_species)) %&gt;% dplyr::top_n(10) ## Selecting by n_critical_species ## # A tibble: 10 x 2 ## Country n_critical_species ## &lt;chr&gt; &lt;dbl&gt; ## 1 Czech Republic 1987 ## 2 United States 1409 ## 3 Germany 1077 ## 4 Canada 727 ## 5 Japan 727 ## 6 Austria 618 ## 7 Slovak Republic 602 ## 8 Poland 485 ## 9 Switzerland 475 ## 10 Brazil 453 Ce pipeline consiste Ã : prendre le tableau especes_menacees, puis filtrer pour n&#39;obtenir que les espÃ¨ces critiques, puis sÃ©lectionner les colonnes des pays et des valeurs (nombre d&#39;espÃ¨ces), puis segmenter le tableaux en plusieurs tableaux selon le pays, puis appliquer la fonction sum pour chacun de ces petits tableaux (puis de recombiner ces sommaires), puis trier les pays en nombre dÃ©croissant de dÃ©compte d&#39;espÃ¨ces, puis afficher le top 10 3.5.6 Exemple (difficile) Pour revenir Ã  notre tableau chicoute, imaginez que vous aviez une station mÃ©tÃ©o (station_A) situÃ©e aux coordonnÃ©es (490640, 5702453) et que vous dÃ©siriez calculer la distance entre lâ€™observation et la station. Prenez du temps pour rÃ©flÃ©chir Ã  la maniÃ¨re dont vous procÃ©derezâ€¦ On pourra crÃ©er une fonction qui mesure la distance entre un point x, y et les coordonnÃ©es de la station Aâ€¦ dist_station_A &lt;- function (x, y) { return(sqrt((x - 490640)^2 + (y - 5702453)^2)) } â€¦ puis ajouter une colonne avec mutate grÃ¢ce Ã  une fonction prenant les arguments x et y spÃ©cifiÃ©s. chicoute %&gt;% mutate(dist = dist_station_A(x = Longitude_m, y= Latitude_m)) %&gt;% select(ID, CodeTourbiere, Longitude_m, Latitude_m, dist) %&gt;% top_n(10) ## Selecting by dist ## # A tibble: 10 x 5 ## ID CodeTourbiere Longitude_m Latitude_m dist ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7 BP 484054 5706307 7631. ## 2 36 MR 459875 5701988 30769. ## 3 37 MR 459873 5701987 30771. ## 4 38 MR 459880 5701971 30764. ## 5 39 MR 459894 5701966 30750. ## 6 40 MR 459915 5701994 30728. ## 7 46 NBM 485975 5695688 8218. ## 8 48 NBM 485912 5696607 7519. ## 9 49 NBM 485903 5696611 7521. ## 10 50 NBM 485884 5696612 7532. Nous pourrions procÃ©der de la mÃªme maniÃ¨re pour fusionner des donnÃ©es climatiques. Le tableau chicoute ne possÃ¨de pas dâ€™indicateurs climatiques, mais il est possible de les soutirer de stations mÃ©tÃ©o placÃ©es prÃ¨s des site. Ces donnÃ©es ne sont pas disponibles pour le tableau de la chicoutÃ©, alors jâ€™utiliserai des donnÃ©es fictives pour lâ€™exemple. Voici ce qui pourrait Ãªtre fait. CrÃ©er un tableau des stations mÃ©tÃ©o ainsi que des indices mÃ©tÃ©o associÃ©s Ã  ces stations. Lier chaque site Ã  une station (Ã  la main oÃ¹ selon la plus petite distance entre le site et la station). Fusionner les indices climatiques aux sites, puis les sites aux mesures de rendement. Ces opÃ©rations demandent habituellement du tÃ¢tonnement. Il serait surprenant que mÃªme une personne expÃ©rimentÃ©e soit en mesure de compiler ces opÃ©rations sans obtenir de message dâ€™erreur, et retravailler jusquâ€™Ã  obtenir le rÃ©sultat souhaitÃ©. Lâ€™objectif de cette section est de vous prÃ©sentÃ© un flux de travail que vous pourriez Ãªtre amenÃ©s Ã  effectuer et de fournir quelques Ã©lÃ©ments nouveau pour mener Ã  bien une opÃ©ration. Il peut Ãªtre frustrant de ne pas saisir toutes les opÃ©rations: passez Ã  travers cette section sans jugement. Si vous devez vous frotter Ã  problÃ¨me semblable, vous saurez que vous trouverez dans ce manuel une recette intÃ©ressante. mes_stations &lt;- data.frame(Station = c(&#39;A&#39;, &#39;B&#39;, &#39;C&#39;), Longitude_m = c(490640, 484870, 485929), Latitude_m = c(5702453, 5701870, 5696421), t_moy_C = c(13.8, 18.2, 16.30), prec_tot_mm = c(687, 714, 732)) mes_stations ## Station Longitude_m Latitude_m t_moy_C prec_tot_mm ## 1 A 490640 5702453 13.8 687 ## 2 B 484870 5701870 18.2 714 ## 3 C 485929 5696421 16.3 732 La fonction suivante calcule la distance entre des coordonnÃ©es x et y et chaque station dâ€™un tableau de stations, puis retourne le nom de la station dont la distance est la moindre. dist_station &lt;- function (x, y, stations_df) { # stations est le tableau des stations Ã  trois colonnes # 1iere: nom de la station # 2ieme: longitude # 3ieme: latitude distance &lt;- c() for (i in 1:nrow(stations_df)) { distance[i] &lt;- sqrt((x - stations_df[i, 2])^2 + (y - stations_df[i, 3])^2) } nom_station &lt;- as.character(stations_df$Station[which.min(distance)]) return(nom_station) } Testons la fonction avec des coordonnÃ©es. dist_station(x = 459875, y = 5701988, stations_df = mes_stations) ## [1] &quot;B&quot; Nous appliquons cette fonction Ã  toutes les lignes du tableau, puis en retournons un Ã©chantillon. chicoute %&gt;% rowwise() %&gt;% mutate(Station = dist_station(x = Longitude_m, y = Latitude_m, stations_df = mes_stations)) %&gt;% select(ID, CodeTourbiere, Longitude_m, Latitude_m, Station) %&gt;% sample_n(10) ## Source: local data frame [10 x 5] ## Groups: &lt;by row&gt; ## ## # A tibble: 10 x 5 ## ID CodeTourbiere Longitude_m Latitude_m Station ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 3 BEAU 490638 5702461 A ## 2 79 1 486499 5702076 B ## 3 53 NTP 487562 5704093 A ## 4 66 1 486544 5702078 B ## 5 6 BP 484865 5706394 B ## 6 54 NTP 487553 5704096 B ## 7 28 2 486353 5702478 B ## 8 65 BS2 486510 5702203 B ## 9 46 NBM 485975 5695688 C ## 10 36 MR 459875 5701988 B Cela semble fonctionner. On peut y ajouter un left_join() pour joindre les donnÃ©es mÃ©tÃ©o au tableau principal. chicoute_weather &lt;- chicoute %&gt;% rowwise() %&gt;% mutate(Station = dist_station(x = Longitude_m, y = Latitude_m, stations_df = mes_stations)) %&gt;% left_join(y = mes_stations, by = &quot;Station&quot;) ## Warning: Column `Station` joining character vector and factor, coercing into character vector chicoute_weather %&gt;% sample_n(10) ## Source: local data frame [10 x 36] ## Groups: &lt;by row&gt; ## ## # A tibble: 10 x 36 ## ID CodeTourbiere Ordre Site Traitement DemiParcelle SousTraitement Latitude_m.x Longitude_m.x Rendement_g_5m2 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 63 BS2 G 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 5702197 486545 NA ## 2 48 NBM D 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 5696607 485912 NA ## 3 68 1 1 2 temoin left Control 5702099 486528 6.42 ## 4 38 MR I 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 5701971 459880 NA ## 5 44 NESP J 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 5701860 484864 NA ## 6 70 1 2 3 fertilisaâ€¦ left Control 5702120 486488 55.6 ## 7 90 WTP E 5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 5700750 487053 NA ## 8 72 1 2 4 temoin left B 5702129 486488 54.1 ## 9 34 MB C 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 5699332 491955 NA ## 10 23 2 9 17 fertilisaâ€¦ left Control 5702544 486327 0 ## # â€¦ with 26 more variables: TotalRamet_nombre_m2 &lt;dbl&gt;, TotalVegetatif_nombre_m2 &lt;dbl&gt;, TotalFloral_nombre_m2 &lt;dbl&gt;, ## # TotalMale_nombre_m2 &lt;dbl&gt;, TotalFemelle_nombre_m2 &lt;dbl&gt;, FemelleFruit_nombre_m2 &lt;dbl&gt;, FemelleAvorte_nombre_m2 &lt;dbl&gt;, ## # SterileFleur_nombre_m2 &lt;dbl&gt;, C_pourc &lt;dbl&gt;, N_pourc &lt;dbl&gt;, P_pourc &lt;dbl&gt;, K_pourc &lt;dbl&gt;, Ca_pourc &lt;dbl&gt;, Mg_pourc &lt;dbl&gt;, ## # S_pourc &lt;dbl&gt;, B_pourc &lt;dbl&gt;, Cu_pourc &lt;dbl&gt;, Zn_pourc &lt;dbl&gt;, Mn_pourc &lt;dbl&gt;, Fe_pourc &lt;dbl&gt;, Al_pourc &lt;dbl&gt;, ## # Station &lt;chr&gt;, Longitude_m.y &lt;dbl&gt;, Latitude_m.y &lt;dbl&gt;, t_moy_C &lt;dbl&gt;, prec_tot_mm &lt;dbl&gt; 3.5.7 Exporter un tableau Simplement avec write_csv(). write_csv(chicoute_weather, &quot;data/chicoute_weather.csv&quot;) 3.5.8 Aller plus loin dans le tidyverse Le livre R for Data Science, de Garrett Grolemund et Hadley Wickham, est un incontournable. 3.6 RÃ©fÃ©rences Parent L.E., Parent, S.Ã‰., Herbert-Gentile, V., Naess, K. et Lapointe, L. 2013. Mineral Balance Plasticity of Cloudberry (Rubus chamaemorus) in Quebec-Labrador Bogs. American Journal of Plant Sciences, 4, 1508-1520. DOI: 10.4236/ajps.2013.47183 "],
["chapitre-visualisation.html", "4 Visualisation 4.1 Pourquoi explorer graphiquement? 4.2 Publier un graphique 4.3 Choisir le type de graphique le plus appropriÃ© 4.4 Choisir son outils de visualisation 4.5 Visualisation en R 4.6 Module de base pour les graphiques 4.7 La grammaire graphique ggplot2 4.8 Mon premier ggplot 4.9 Les graphiques comme outil dâ€™exploration des donnÃ©es 4.10 Choisir les bonnes couleurs 4.11 RÃ¨gles particuliÃ¨res", " 4 Visualisation ï¸Â Objectifs spÃ©cifiques: Ã€ la fin de ce chapitre, vous comprendrez lâ€™importance de lâ€™exploration des donnÃ©es comprendrez les guides gÃ©nÃ©raux pour crÃ©er un graphique appropriÃ© comprendrez la diffÃ©rence entre les modes impÃ©ratifs et dÃ©claratifs pour la crÃ©ation de graphique serez en mesure de crÃ©er des nuages de points, lignes, histogrammes, diagrammes en barres et boxplots en R saurez exporter un graphique en vue dâ€™une publication Lorsque jâ€™aborde un document scientifique, la premiÃ¨re chose que je fais aprÃ¨s avoir lu le rÃ©sumÃ© est de regarder les graphiques. Un graphique bien conÃ§u est dense en information, de sorte quâ€™il met en lumiÃ¨re une information qui pourrait passer inaperÃ§ue dans un tableau. Reconnaissez-vous cette image? Source: GIEC, Bilan 2001 des changements climatiques : Les Ã©lÃ©ments scientifiques Elle a Ã©tÃ© conÃ§ue par Michael E. Mann, Raymond S. Bradley et Malcolm K. Hughes. Le graphique montre lâ€™Ã©volution des tempÃ©ratures en Â°C normalisÃ©es selon la tempÃ©rature moyenne entre 1961 et 1990 sur lâ€™axe des Y en fonction du temps, sur lâ€™axe des X. On le connait aujourdâ€™hui comme le bÃ¢ton de hockey, et on reconnait son rÃ´le clÃ© pour sensibiliser la civilisation entiÃ¨re face au rÃ©chauffement global. On aura recours Ã  la visualisation des donnÃ©es pour plusieurs raison: en particulier, lorsque lâ€™information dâ€™un tableau devient difficile Ã  interprÃ©ter. Ainsi, crÃ©er des graphiques est une tÃ¢che courante dans un flux de travail en science, que ce soit pour explorer les donnÃ©es ou les communiquerâ€¦ ce Ã  quoi cette section est vouÃ©e. 4.1 Pourquoi explorer graphiquement? La plupart des graphiques que vous gÃ©nÃ©rerez ne seront pas destinÃ©s Ã  Ãªtre publiÃ©s. Ils viseront probablement dâ€™abord Ã  explorer des donnÃ©es. Cela vous permettra de mettre en Ã©vidence de nouvelles perspectives. Prenons par exemple deux variables, \\(X\\) et \\(Y\\). Vous calculez leur moyenne, Ã©cart-type et la corrÃ©lation entre les deux variables (nous verrons les statistiques en plus de dÃ©tails dans un prochain chapitre). Pour dÃ©montrer que ces statistiques ne vous apprendront pas grand chose sur la structure des donnÃ©es, Matejka et Fitzmaurice (2017) ont gÃ©nÃ©rÃ© 12 jeux de donnÃ©es \\(X\\) et \\(Y\\), ayant chacun pratiquement les mÃªmes statistiques. Mais avec des structures bien diffÃ©rentes. Animation montrant la progression du jeu de donnÃ©es Datasaurus pour toutes les formes visÃ©es. Source: Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing. 4.2 Publier un graphique Vous voilÃ  sensibilisÃ© Ã  lâ€™importance dâ€™explorer les donnÃ©es graphiquement. Mais ce qui ultimement Ã©manera dâ€™un projet sera le rapport que vous dÃ©poserez, lâ€™article scientifique que vous ferez publier ou le billet de blogue que vous posterez. Les graphiques inclus dans vos publications mÃ©ritent une attention particuliÃ¨re pour que votre audience puisse comprendre les dÃ©couvertes et perspectives offertes par vos travaux. Pour ce faire, un graphique doit Ã©videmment rÃ©pondre honnÃªtement Ã  la question posÃ©e, sans artifices inutiles, mais tout de mÃªme attrayante. 4.2.1 Cinq qualitÃ©s dâ€™un bon graphique Alberto Cairo, chercheur spÃ©cialisÃ© en visualisation de donnÃ©es, a fait paraÃ®tre en 2016 le livre The Truthful art, note cinq qualitÃ©s dâ€™une visualisation bien conÃ§ue (les citations de cette section proviennent de ma traduction de Alberto Cairo, The Truthful Art (2016), p.Â 45.). 1- Elle est vÃ©ritable, puisquâ€™elle est basÃ©e sur une recherche exhaustive et honnÃªte. Cela vaut autant pour les graphiques que pour lâ€™analyse de donnÃ©es. Il sâ€™agit froidement de prÃ©senter les donnÃ©es selon lâ€™interprÃ©tation la plus exacte. Les piÃ¨ges Ã  Ã©viter sont le picorage de cerises et la surinterprÃ©tation des donnÃ©es. Le picorage, câ€™est lorsquâ€™on rÃ©duit les perspectives afin de soutenir un argumentaire. Par exemple, retirer des donnÃ©es dâ€™une rÃ©gion ou dâ€™une dÃ©cennie qui rendraient factice une conclusion fixÃ©e a priori. Ceci vaut autant pour les graphiques que pour les statistiques (nous parlerons du p-hacking au prochain chapitre). La surinterprÃ©tation, câ€™est lorsque lâ€™on saute rapidement aux conclusions: par exemple, que lâ€™on gÃ©nÃ¨re des corrÃ©lations, voire mÃªme des relations de causalitÃ©s Ã  partir de ce qui nâ€™est que du bruit de fond. Ã€ ce titre, lors dâ€™une confÃ©rence, Heather Krause insiste sur lâ€™importance de faire en sorte que les reprÃ©sentations graphiques rÃ©pondent correctement aux questions posÃ©es dans une Ã©tude (Ã  voir!). 2- Elle est fonctionnelle, puisquâ€™elle constitue une reprÃ©sentation prÃ©cise des donnÃ©es, et quâ€™elle est construite de maniÃ¨re Ã  laisser les observateurs.trices prendre des initiatives consÃ©quentes. â€œLa seule chose qui est pire quâ€™un diagramme en pointe de tarte, câ€™est dâ€™en prÃ©senter plusieursâ€ (Edward Tufte, designer, citÃ© par Alberto Cairo, 2016, p.Â 50). Choisir le bon graphique pour reprÃ©senter vos donnÃ©es est beaucoup moins une question de bon goÃ»t quâ€™une question de dÃ©marche rationnelle sur lâ€™objectif visÃ© par la prÃ©sentation dâ€™un graphique. Je prÃ©senterai des lignes guides pour sÃ©lectionner le type de graphique qui prÃ©sentera vos donnÃ©es de maniÃ¨re fonctionnelle en fonction de lâ€™objectif dâ€™un graphique (dâ€™ailleurs, avez-vous vraiment besoin dâ€™un graphique?). 3- Elle est attrayante et intrigante, et mÃªme esthÃ©tiquement plaisante pour lâ€™audience visÃ©e - les scientifiques dâ€™abord, mais aussi le public en gÃ©nÃ©ral. En sciences naturelles, la pensÃ©e rationnelle, la capacitÃ© Ã  organiser la connaissance et crÃ©er de nouvelles avenues sont des qualitÃ©s qui sont privilÃ©giÃ©es au talent artistique. Que vous ayez oÃ¹ non des aptitudes en art visuel, prÃ©sentez de lâ€™information, pas des dÃ©corations. Excel vous permet dâ€™ajouter une perspective 3D Ã  un diagramme en barres. La profondeur contient-elle de lâ€™information? Non. Cette dÃ©coration ne fait quâ€™ajouter de la confusion. Minimalisez, fournissez le plus dâ€™information possible avec le moins dâ€™Ã©lÃ©ments possibles. Câ€™est ce que vous proposent les guides graphiques que jâ€™introduirai plus loin. 4- Elle est pertinente, puisquâ€™elle rÃ©vÃ¨le des Ã©vidences scientifiques autrement difficilement accessibles. Il sâ€™agit de susciter un eurÃªka, dans le sens quâ€™elle gÃ©nÃ¨re une idÃ©e, et parfois une initiative, en un coup dâ€™Å“il. Le graphique en bÃ¢ton de hockey est un exemple oÃ¹ lâ€™on a spontanÃ©ment une idÃ©e de la situation. Cette situation peut Ãªtre la prÃ©sence dâ€™un phÃ©nomÃ¨ne comme lâ€™augmentation de la tempÃ©rature globale, mais aussi lâ€™absence de phÃ©nomÃ¨nes pourtant attendus. 5- Elle est instructive, parce que si lâ€™on saisit et accepte les Ã©vidences scientifiques quâ€™elle dÃ©crit, cela changera notre perception pour le mieux. En prÃ©sentant cette qualitÃ©, Alberto Cairo voulait insister ses lecteurs.trices Ã  choisir des sujets de discussion visuelle de maniÃ¨re Ã  participer Ã  un monde meilleur. En ce qui nous concerne, il sâ€™agit de bien sÃ©lectionner lâ€™information que lâ€™on dÃ©sire transmettre. Imaginez que vous avez travaillÃ© quelques jours pour crÃ©er un graphique, sont vous Ãªtes fier, mais vous (ou un collÃ¨gue hiÃ©rarchiquement favorisÃ©) vous rendez compte que le graphique soutient peu ou pas le propos ou lâ€™objectif de votre thÃ¨se/mÃ©moire/rapport/article. Si câ€™est bien le cas, vous feriez mieux de laisser tomber votre oeuvre et considÃ©rer votre dÃ©marche comme une occasion dâ€™apprentissage. Alberto Cairo rÃ©sume son livre The Truthful Art dans une entrevue avec le National Geographic. 4.3 Choisir le type de graphique le plus appropriÃ© Vous connaissez sans doute les nuages de point, les lignes, les histogrammes, les diagrammes en barre et en pointe de tarte. De nombreuses maniÃ¨res de prÃ©senter les donnÃ©es ont Ã©tÃ© dÃ©veloppÃ©es. Les principaux types de graphique seront couverts dans ce chapitre. Dâ€™autres types spÃ©cialisÃ©s seront couverts dans les chapitres appropriÃ©s (graphiques davantage orientÃ©s vers les statistiques, les biplots, les dendrogrammes, les diagrammes ternaires, les cartes, etc.). La visualisation de donnÃ©es est aujourdâ€™hui devenue une expertise en soi. Plusieurs personnes ayant acquis une expertise dans le domaine partage leurs expÃ©riences. Ã€ ce titre, le site from data to viz est Ã  conserver dans vos marques-page. Il comprend des arbres dÃ©cisionnels qui vous guident vers les options appropriÃ©es pour prÃ©senter vos donnÃ©es, puis fournissent des exemples en R que vous pourrez copier-coller-adapter dans vos feuilles de calcul. Ã‰galement, je suggÃ¨re le site internet de Ann K. Emery, qui prÃ©sente des lignes guide pour prÃ©sentÃ© le graphique adÃ©quat selon les donnÃ©es en main. De nombreuses recettes sont Ã©galement proposÃ©es sur r-graph-gallery.com. En ce qui a trait aux couleurs, le choix nâ€™est pas anodin. Si vous avez le souci des dÃ©tails sur les Ã©lÃ©ments esthÃ©tiques de vos graphiques, je recommande la lecture de ce billet de blog de Lisa Charlotte Rost. Le Financial Times offre Ã©galement ce guide visuel. Cairo (2016) propose de procÃ©der avec ces Ã©tapes: RÃ©flÃ©chissez au message que vous dÃ©sirez transmettre: comparer les catÃ©gories \\(A\\) et \\(B\\), visualiser une transition ou un changement de \\(A\\) vers \\(B\\), prÃ©senter une relation entre \\(A\\) et \\(B\\) ou la distribution de \\(A\\) et \\(B\\) sur une carte. Essayez diffÃ©rentes reprÃ©sentations: si le message que vous dÃ©sirez transmettre a plusieurs volets, il se pourrait que vous ayez besoin de plus dâ€™un graphique. Mettez de lâ€™ordre dans vos donnÃ©es. Câ€™Ã©tait le sujet du chapitre 3. Testez le rÃ©sultat. â€œHÃ©, quâ€™est-ce que tu comprends de cela?â€ Si la personne hausse les Ã©paules, il va falloir rÃ©Ã©valuer votre stratÃ©gie. 4.4 Choisir son outils de visualisation Les modules et logiciels de visualisation sont basÃ©s sur des approches que lâ€™on pourrait placer sur un spectre allant de lâ€™impÃ©ratif au dÃ©claratif. 4.4.1 Approche impÃ©rative Selon cette approche, vous indiquez comment placer lâ€™information dans un espace graphique. Vous indiquer les symboles, les couleurs, les types de ligne, etc. Peu de choses sont automatisÃ©es, ce qui laisse une grande flexibilitÃ©, mais demande de vouer beaucoup dâ€™Ã©nergie Ã  la maniÃ¨re de coder pour obtenir le graphique dÃ©sirÃ©. Le module graphique de Excel, ainsi que le module graphique de base de R, utilisent des approches impÃ©ratives. 4.4.2 Approche dÃ©clarative Les stratÃ©gies dâ€™automatisation graphique se sont grandement amÃ©liorÃ©es au cours des derniÃ¨res annÃ©es. PlutÃ´t que de vouer vos Ã©nergies Ã  crÃ©er un graphique, il est maintenant possible de spÃ©cifier ce que lâ€™on veut prÃ©senter. La visualisation dÃ©clarative vous permet de penser aux donnÃ©es et Ã  leurs relations, plutÃ´t que des dÃ©tails accessoires. Jake Vanderplas, Declarative Statistical Visualization in Python with Altair (ma traduction) Lâ€™approche dÃ©clarative passe souvent par une grammaire graphique, câ€™est-Ã -dire un langage qui explique ce que lâ€™on veut prÃ©senter - en mode impÃ©ratif, on spÃ©cifie plutÃ´t comment on veut prÃ©senter les donnÃ©es. Le module ggplot2 est le module dÃ©claratif par excellence en R. 4.5 Visualisation en R En R, votre trousse dâ€™outils de visualisation mÃ©riterait de comprendre les modules suivants. base. Le module de base de R contient des fonctions graphique trÃ¨s polyvalentes. Les axes sont gÃ©nÃ©rÃ©es automatiquement, on peut y ajouter des titres et des lÃ©gendes, on peut crÃ©er plusieurs graphiques sur une mÃªme figure, on peut y ajouter diffÃ©rentes gÃ©omÃ©tries (points, lignes et polygones), avec diffÃ©rents types de points ou de trait, et diffÃ©rentes couleurs, etc. Les modules spÃ©cialisÃ©s viennent souvent avec leurs graphiques spÃ©cialisÃ©s, construit Ã  partir du module de base. En tant que module graphique impÃ©ratif, on peut tout faire ou presque (pas dâ€™interactivitÃ©), mais lâ€™Ã©criture du code est peut expressive. ggplot2. Câ€™est le module graphique par excellence en R (et jâ€™ose dire: en calcul scientifique). ggplot2 se base sur une grammaire graphique. Ã€ partir dâ€™un tableau de donnÃ©es, une colonne peut dÃ©finir lâ€™axe des x, une autre lâ€™axe des y, une autre la couleur couleur des points ou leur dimension. Une autre colonne dÃ©finissant des catÃ©gories peut segmenter la visualisation en plusieurs graphiques alignÃ©s horizontalement ou verticalement. Des extensions de ggplot2 permettent de gÃ©nÃ©rer des cartes (ggmap), des diagrammes ternaires (ggtern), des animations (gganimate), etc. plotly. plotly offre une fonction toute simple pour rendre interactif un graphique ggplot2. plotly est aussi un module graphique en soit, particuliÃ¨rement utile pour les graphiques interactifs. Nous survolerons rapidement le module de base, irons plus en profondeur avec ggplot2, puis je prÃ©senterai briÃ¨vement les graphiques interactifs avec plotly. 4.6 Module de base pour les graphiques Nous allons dâ€™abord survoler le module de base, en mode impÃ©ratif. La fonction de base pour les graphiques en R est plot(). Pour nous exercer avec cette fonction, chargeons dâ€™abord le tableau de donnÃ©es dâ€™exercice iris, publiÃ© en 1936 par le cÃ©lÃ¨bre biostatisticien Ronald Fisher. data(iris) head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa Le tableau iris contient 5 colonnes, les 4 premiÃ¨res dÃ©crivant les longueurs et largeurs des pÃ©tales et sÃ©pales de diffÃ©rentes espÃ¨ces dâ€™iris dont le nom apparaÃ®t Ã  la 5iÃ¨me colonne. La maniÃ¨re la plus rapide dâ€™extraire une colonne dâ€™un tableau est dâ€™appeler le tableau, suivit du $, puis du nom de la colonne, par exemple iris$Species. Pour gÃ©nÃ©rer un graphique avec la fonction plot(): plot(iris$Sepal.Length, iris$Petal.Length) Par dÃ©faut, le premier argument est le vecteur dÃ©finissant lâ€™axe des x et le deuxiÃ¨me est celui dÃ©finissant lâ€™axe des y. Le graphique prÃ©cÃ©dent peut Ãªtre amplement personnalisÃ© en utilisant diffÃ©rents arguments. Exercice. Utilisez ces arguments dans la cellule de code de la figure plot(iris$Sepal.Length, iris$Petal.Length). Remarquez que la fonction a dÃ©cidÃ© toute seule de crÃ©er un nuage de point. La fonction plot() est conÃ§ue pour crÃ©er le graphique appropriÃ© selon le type des donnÃ©es spÃ©cifiÃ©es: lignes, boxplot, etc. Si lâ€™on spÃ©cifiait les espÃ¨ces comme argument xâ€¦ plot(iris$Species, iris$Petal.Length) # ou bien # iris %&gt;% # select(Species, Petal.Length) %&gt;% # plot() De mÃªme, la fonction plot() appliquÃ©e Ã  un tableau de donnÃ©es gÃ©nÃ©rera une reprÃ©sentation bivariÃ©e. plot(iris) Il est possible dâ€™encoder des attributs grÃ¢ce Ã  des vecteurs de facteurs (catÃ©gories). plot(iris, col = iris$Species) Lâ€™argument type = &quot;&quot; permet de personnaliser lâ€™apparence: type = &quot;p&quot;: points type = &quot;l&quot;: ligne type = &quot;o&quot; et type = &quot;b&quot;: ligne et points type = &quot;n&quot;: ne rien afficher CrÃ©ons un jeu de donnÃ©es. time &lt;- seq(from = 0, to = 100, by = 10) height &lt;- abs(time * 0.1 + rnorm(length(time), 0, 2)) # abs pourvaleur absolue (changement de signe si nÃ©gatif) plot(x = time, y = height, type = &#39;b&#39;, lty = 2, lwd = 1) Le type de ligne est spÃ©cifiÃ© par lâ€™argument lty (qui peut prendre un chiffre ou une chÃ¢ine de caractÃ¨res, i.e. 1 est Ã©quivalent de &quot;solid&quot;, 2 de &quot;dashed&quot;, 3 de &quot;dotted&quot;, etc.) et la largeur du trait (valeur numÃ©rique), par lâ€™argument lwd. La fonction hist() permet quant Ã  elle de crÃ©er des histogrammes. Parmi ses arguments, breaks est particuliÃ¨rement utile, car il permet dâ€™ajuster la segmentation des incrÃ©ments. hist(iris$Petal.Length, breaks = 60) Exercice. Ajustez le titre de lâ€™axe des x, ainsi que les limites de lâ€™axe des x. ÃŠtes-vous en mesure de colorer lâ€™intÃ©rieur des barres en bleu? La fonction plot() peut Ãªtre suivie de plusieurs autres couches comme des lignes (lines() ou abline()), des points (points()), du texte (text()), des polygones (polygon(), des lÃ©gendes (legend())), etc. On peut aussi personnaliser les couleurs, les types de points, les types de lignes, etc. Lâ€™exemple suivant ajoute une ligne au graphique. Ne prÃªtez pas trop attention aux fonctions predict() et lm() pour lâ€™instant: nous les verrons au chapitre 6. plot(time, height) lines(time, predict(lm(height ~ time))) Pour exporter un graphique, vous pouvez passer par le menu Export de RStudio. Mais pour des graphiques destinÃ©s Ã  Ãªtre publiÃ©s, je vous suggÃ¨re dâ€™exporter vos graphiques avec une haute rÃ©solution Ã  la suite de la commande png() (ou jpg() ou svg()). png(filename = &#39;images/mon-graphique.png&#39;, width = 3000, height=2000, res=300) plot(x = iris$Petal.Length, y = iris$Sepal.Length, col = iris$Species, cex=3, # dimension des points pch = 16) # type de points dev.off() ## png ## 2 Ce format crÃ©e une version vectorielle du graphique, câ€™est-Ã -dire que lâ€™image exportÃ©e est un fichier contenant les formes, non pas les pixels. Cela vous permet dâ€™Ã©diter votre graphique dans un logiciel de dessin vectoriel (comme Inkscape). Jâ€™ai utilisÃ© le format dâ€™image png, utile pour les images de type graphique, avec des changements de couleurs drastiques. Pour les photos, vous prÃ©fÃ©rerez le format jpg. Des Ã©diteurs demanderont peut-Ãªtre des formats vectoriels comme pdf ou eps. Si vous ne trouvez pas de moyen de modifiÃ© un aspect du graphique dans le code (bouger des Ã©tiquettes ou des lÃ©gendes, ajouter des Ã©lÃ©ments graphiques), vous pouvez exporter votre graphique en format svg (par la commande svg(). Ce format vectoriel peut Ãªtre ouvert avec des logiciels de dessin vectoriel comme le logiciel libre Inkscape. Le module de base de R comprend une panoplie dâ€™autres particularitÃ©s que je ne couvrirai pas ici, en faveur du module ggplot2. 4.7 La grammaire graphique ggplot2 BriÃ¨vement, une grammaire graphique permet de schÃ©matiser (ma traduction de to map) des donnÃ©es sur des attributs esthÃ©tiques avec des gÃ©omÃ©tries.Cette approche permet de dÃ©gager 5 composantes. Les donnÃ©es. Votre tableau est bien sÃ»r un argument nÃ©cessaire pour gÃ©nÃ©rer le graphique. Les marqueurs. Un terme abstrait pour dÃ©signer les points, les lignes, les polygones, les barres, les flÃ¨ches, etc. Les attributs encodÃ©s. La position, la dimension, la couleur ou la forme que prendront les gÃ©omÃ©tries. En ggplot2, on les nomme les aesthetics. Les attributs globaux. Les attributs sont globaux lorsquâ€™ils sont constant (ils ne dÃ©pendent pas dâ€™une variable). Les valeurs par dÃ©faut conviennent gÃ©nÃ©ralement, mais certains attributs peuvent Ãªtre spÃ©cifiÃ©s: par exemple la forme ou la couleur des points, le type de ligne. Les thÃ¨mes. Le thÃ¨me du graphique peut Ãªtre spÃ©cifiÃ© dans son ensemble, câ€™est-Ã -dire en utilisant un thÃ¨me prÃ©dÃ©fini, mais lâ€™on peut modifier certains dÃ©tails. Le flux de travail pour crÃ©er un graphique Ã  partir dâ€™une grammaire ressemble donc Ã  ceci: Avec mon tableau, CrÃ©er un marqueur ( encoder(position X = colonne A, position Y = colonne B, couleur = colonne C) forme globale = 1) Avec un thÃ¨me noir et blanc Le module tidyverse installera des modules utilisÃ©s de maniÃ¨re rÃ©currente dans ce cours, comme ggplot2, dplyr, tidyr et readr. Je recommande de le charger au dÃ©but de vos sessions de travail. library(&quot;tidyverse&quot;) Lâ€™approche tidyverse est une grammaire des donnÃ©es. Le module ggplot2, qui en fait partie, est une grammaire graphique (dâ€™oÃ¹ le gg de ggplot). 4.8 Mon premier ggplot Pour notre premier exercice, je vais charger un tableau depuis le fichier de donnÃ©es abalone.data depuis un dÃ©pÃ´t sur internet. Je nâ€™irai pas dans les dÃ©tails sur les tableaux de donnÃ©es, puisque câ€™est le sujet du chapitre 3. Le fichier de donnÃ©es porte sur un escargot de mer et comprend le sexe (M: mÃ¢le, F: femelle et I: enfant), des poids et dimensions des individus observÃ©s, et le nombre dâ€™anneaux comptÃ©s dans la coquille. abalone &lt;- read_csv(&quot;data/abalone.csv&quot;) ## Parsed with column specification: ## cols( ## Type = col_character(), ## LongestShell = col_double(), ## Diameter = col_double(), ## Height = col_double(), ## WholeWeight = col_double(), ## ShuckedWeight = col_double(), ## VisceraWeight = col_double(), ## ShellWeight = col_double(), ## Rings = col_double() ## ) Inspectons lâ€™entÃªte du tableau avec la fonction head(). head(abalone) ## # A tibble: 6 x 9 ## Type LongestShell Diameter Height WholeWeight ShuckedWeight VisceraWeight ShellWeight Rings ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 M 0.455 0.365 0.095 0.514 0.224 0.101 0.15 15 ## 2 M 0.35 0.265 0.09 0.226 0.0995 0.0485 0.07 7 ## 3 F 0.53 0.42 0.135 0.677 0.256 0.142 0.21 9 ## 4 M 0.44 0.365 0.125 0.516 0.216 0.114 0.155 10 ## 5 I 0.33 0.255 0.08 0.205 0.0895 0.0395 0.055 7 ## 6 I 0.425 0.3 0.095 0.352 0.141 0.0775 0.12 8 Suivant la grammaire graphique ggplot2, on pourra crÃ©er ce graphique de points comprenant les attributs suivants suivants. data = abalone, le fichier de donnÃ©es. mapping = aes(...), spÃ©cifiÃ© comme attribut de la fonction ggplot(), cet encodage (ou aesthetic) reste lâ€™encodage par dÃ©faut pour tous les marqueurs du graphique. Toutefois, lâ€™encodage mapping = aes() peut aussi Ãªtre spÃ©cifiÃ© dans la fonction du marqueur (par exemple geom_point()). Dans lâ€™encodage global du graphique, on place en x la longueur de la coquille (x = LongestShell) et on place en y le poids de la coquille (y = ShellWeight). Pour ajouter un marqueur, on utilise le +. GÃ©nÃ©ralement, on change aussi de ligne. Le marqueur ajoutÃ© est un point, geom_point(), dans lequel on spÃ©cifie un encodage de couleur sur la variable Type (colour = Type) et un encodage de dimension du point sur la variable rings (size = Rings). Lâ€™attribut alpha = 0.5 se situe hors du mapping et de la fonction aes(): câ€™est un attribut identique pour tous les points. ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) + geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5) Il existe plusieurs types de marqueurs: geom_point pour les points geom_line pour les lignes geom_bar pour les diagrammes en barre en dÃ©compte, geom_col en terme de grandeur et geom_histogram pour les histogrammes geom_boxplot pour les boxplots geom_errorbar, geom_pointrange ou geom_crossbar pour les marges dâ€™erreur geom_map pour les cartes etc. Il existe plusieurs attributs dâ€™encodage: la position x, y et z (z pertinent notamment pour le marqueur geom_tile()) la taille size la forme des points shape la couleur, qui peut Ãªtre discrÃ¨te ou continue : colour, pour la couleur des contours fill, pour la couleur de remplissage le type de ligne linetype la transparence alpha et dâ€™autres types spÃ©cialisÃ©s que vous retrouverez dans la documentation des marqueurs Les types de marqueurs et leurs encodages sont dÃ©crits dans la documentation de ggplot2, qui fournit des feuilles aide-mÃ©moire quâ€™il est commode dâ€™imprimer et dâ€™afficher prÃ¨s de soi. Aide-mÃ©moire de ggplot2, source: https://www.rstudio.com/resources/cheatsheets/ 4.8.0.1 Les facettes Dans ggplot2, les facetttes sont un type spÃ©cial dâ€™encodage utilisÃ©s pour dÃ©finir des grilles de graphique. Elles prennent deux formes: Le collage, facet_wrap(). Une variable catÃ©gorielle est utilisÃ©e pour segmenter les graphiques en plusieurs graphiques, qui sont placÃ©s lâ€™un Ã  la suite de lâ€™autre dans un arrangement spÃ©cifiÃ© par un nombre de colonne ou un nombre de ligne. La grille, facet_grid(). Une ou deux variables segmentent les graphiques selon les colonnes et les lignes. Les facettes peuvent Ãªtre spÃ©cifiÃ©es nâ€™importe oÃ¹ dans la chaÃ®ne de commande de ggplot2, mais conventionnellement, on les place tout de suite aprÃ¨s la fonction ggplot(). ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) + facet_wrap(~Type, ncol=2) + geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5) La fonction cut() permet de discrÃ©tiser des variables continues en catÃ©gories ordonnÃ©es - les fonctions peuvent Ãªtre utilisÃ©es Ã  lâ€™intÃ©rieur de la fonction ggplot. ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) + facet_grid(Type ~ cut(Rings, breaks = seq(0, 30, 5))) + geom_point(mapping = aes(colour = Type), alpha = 0.5) Par dÃ©faut, les axes des facettes, ainsi que leurs dimensions, sont les mÃªmes. Une telle reprÃ©sentation permet de comparer les facets sur une mÃªme Ã©chelle. Les axes peuvent Ãªtre dÃ©finis selon les donnÃ©es avec lâ€™argument scales, tandis que lâ€™espace des facettes peut Ãªtre conditionnÃ© selon lâ€™argument space - pour plus de dÃ©tails, voir la fiche de documentation. Exercice. Personnalisez le graphique avec les donnÃ©es abalone en remplaÃ§ant les variables et en rÃ©organisant les facettes. 4.8.1 Plusieurs sources de donnÃ©es Il peut arriver que les donnÃ©es pour gÃ©nÃ©rer un graphique proviennent de plusieurs tableaux. Lorsquâ€™on ne spÃ©cifie pas la source du tableau dans un marqueur, la valeur par dÃ©faut est le tableau spÃ©cifier dans lâ€™amorce ggplot(). Il est nÃ©anmoins possible de dÃ©finir une source personnalisÃ©e pour chaque marqueur en spÃ©cifiant data = ... comme argument du marqueur. abalone_siteA &lt;- data.frame(LongestShell = c(0.3, 0.8, 0.7), ShellWeight = c(0.05, 0.81, 0.77)) ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) + geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5) + geom_point(data = abalone_siteA, size = 8, shape = 4) 4.8.2 Exporter avec style Le fond gris est une marque distinctive de ggplot2. Il nâ€™est toutefois pas apprÃ©ciÃ© de tout le monde. Dâ€™autres thÃ¨mes dits complets peuvent Ãªtre utilisÃ©s (liste des thÃ¨mes complets). Les thÃ¨mes complets sont appelÃ©s avant la fonction theme(), qui permet dâ€™effectuer des ajustements prÃ©cis dont la liste exhaustive se trouve dans la documentation de ggplot2. Vous pouvez aussi personnaliser le titre des axes (xlab() et ylab()), leur limites (xlim() et ylim()) ou spÃ©cifier un titre global (ggtitle()). Pour exporter un ggplot, on pourra utiliser les commandes de R png(), svg() ou pdf(), ou les outils de RStudio. Toutefois, ggplot2 offre la fonction ggsave(), que lâ€™on place en remorque du graphique, en spÃ©cifiant les dimensions (width et height) ainsi que la rÃ©solution (dpi). La rÃ©solution dâ€™un graphique destinÃ© Ã  la publication est typiquement de plus de 300 dpi. ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) + geom_point(mapping = aes(colour = Type, size = Rings), alpha = 0.5) + xlab(&quot;Length (mm)&quot;) + ylab(&quot;Shell weight (g)&quot;) + ggtitle(&quot;Abalone&quot;) + xlim(c(0, 1)) + theme_classic() + theme(axis.title = element_text(size=20), axis.text = element_text(size=20), axis.text.y = element_text(size=20, angle=90, hjust=0.5), legend.box = &quot;horizontal&quot;) ggsave(&quot;images/abalone.png&quot;, width = 8, height = 8, dpi = 300) Nous allons maintenant couvrir diffÃ©rents types de graphiques, accessibles selon diffÃ©rents marqueurs: les nuages de points les diagrammes en ligne les boxplots les histogrammes les diagrammes en barres 4.8.3 Nuages de points Lâ€™exemple prÃ©cÃ©dent est un nuage de points, que nous avons gÃ©nÃ©rÃ© avec le marqueur geom_point(), qui a dÃ©jÃ  Ã©tÃ© passablement introduit. Lâ€™exploration de ces donnÃ©es a permis de dÃ©tecter une croissance exponentielle du poids de la coquille en fonction de sa longueur. Il est clair que les abalones juvÃ©niles (Type I) sont plus petits et moins lourds, mais nous devrons probablement procÃ©der Ã  des tests statistiques pour vÃ©rifier sâ€™il y a des diffÃ©rences entre mÃ¢les et femelles. Le graphique Ã©tant trÃ¨s chargÃ©, nous avons utilisÃ© des stratÃ©gies pour lâ€™allÃ©ger en utilisant de la transparence et des facettes. Le marqueur geom_jitter() peut permettre de mieux apprÃ©cier la dispersion des points en ajoutant une dispersion randomisÃ©e en x ou en y. ggplot(data = abalone, mapping = aes(x = LongestShell, y = ShellWeight)) + geom_jitter(mapping = aes(colour = Type, size = Rings), alpha = 0.5, width = 0.05, height=0.1) Dans ce cas-ci, Ã§a ne change pas beaucoup, mais retenons-le pour la suite. 4.8.4 Diagrammes en lignes Les lignes sont utilisÃ©es pour exprimer des liens entre une suite dâ€™information. Dans la plupart des cas, il sâ€™agit dâ€™une suite dâ€™information dans le temps que lâ€™on appelle les sÃ©ries temporelles. En lâ€™occurrence, les lignes devraient Ãªtre Ã©vitÃ©es si la sÃ©quence entre les variables nâ€™est pas Ã©vidente. Nous allons utiliser un tableau de donnÃ©es de R portant sur la croissance des orangers. data(Orange) head(Orange) ## Grouped Data: circumference ~ age | Tree ## Tree age circumference ## 1 1 118 30 ## 2 1 484 58 ## 3 1 664 87 ## 4 1 1004 115 ## 5 1 1231 120 ## 6 1 1372 142 La premiÃ¨re colonne spÃ©cifie le numÃ©ro de lâ€™arbre mesurÃ©, la deuxiÃ¨me son Ã¢ge et la troisiÃ¨me sa circonfÃ©rence. Le marqueur geom_line() permet de tracer la tendance de la circonfÃ©rence selon lâ€™Ã¢ge. En encodant la couleur de la ligne Ã  lâ€™arbre, nous pourrons tracer une ligne pour chacun dâ€™entre eux. ggplot(data = Orange, mapping = aes(x = age, y = circumference)) + geom_line(aes(colour = Tree)) La lÃ©gende ne montre pas les numÃ©ros dâ€™arbre en ordre croissance. En effet, la lÃ©gende (tout comme les facettes) classe les catÃ©gories prioritairement selon lâ€™ordre des catÃ©gories si elles sont ordinales, ou par ordre alphabÃ©tique si les catÃ©gories sont nominales. Inspectons la colonne Tree en inspectant le tableau avec la commande str() - la commande glimpse() du tidyverse donne un sommaire moins complet que str(). str(Orange) ## Classes &#39;nfnGroupedData&#39;, &#39;nfGroupedData&#39;, &#39;groupedData&#39; and &#39;data.frame&#39;: 35 obs. of 3 variables: ## $ Tree : Ord.factor w/ 5 levels &quot;3&quot;&lt;&quot;1&quot;&lt;&quot;5&quot;&lt;&quot;2&quot;&lt;..: 2 2 2 2 2 2 2 4 4 4 ... ## $ age : num 118 484 664 1004 1231 ... ## $ circumference: num 30 58 87 115 120 142 145 33 69 111 ... ## - attr(*, &quot;formula&quot;)=Class &#39;formula&#39; language circumference ~ age | Tree ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; ## - attr(*, &quot;labels&quot;)=List of 2 ## ..$ x: chr &quot;Time since December 31, 1968&quot; ## ..$ y: chr &quot;Trunk circumference&quot; ## - attr(*, &quot;units&quot;)=List of 2 ## ..$ x: chr &quot;(days)&quot; ## ..$ y: chr &quot;(mm)&quot; En effet, la colonne Tree est un facteur ordinal dont les niveaux sont dans le mÃªme ordre que celui la lÃ©gende. 4.8.5 Les histogrammes Nous avons vu les histogrammes dans la brÃ¨ve section sur les fonctions graphiques de base dans R: il sâ€™agit de segmenter lâ€™axe des x en incrÃ©ments, puis de prÃ©senter sur lâ€™axe de y le nombre de donnÃ©es que lâ€™on retrouve dans cet incrÃ©ment. Le marqueur Ã  utiliser est geom_histogram(). Revenons Ã  nos escargots. Comment prÃ©senteriez-vous la longueur de la coquille selon la variable Type? Selon des couleurs ou des facettes? La couleur, dans le cas des histogrammes, est celle du pourtour des barres. Pour colorer lâ€™intÃ©rieur des barres, lâ€™argument Ã  utiliser est fill. ggplot(data = abalone, mapping = aes(x = LongestShell)) + geom_histogram(mapping = aes(fill = Type), colour = &#39;black&#39;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. On nâ€™y voit pas grand chose. Essayons plutÃ´t les facettes. ggplot(data = abalone, mapping = aes(x = LongestShell)) + facet_grid(Type ~ .) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Les facettes permettent maintenant de bien distinguer la distribution des longueur des juvÃ©niles. Lâ€™argument bins, tout comme lâ€™argument breaks du module graphique de base, permet de spÃ©cifier le nombre dâ€™incrÃ©ments, ce qui peut Ãªtre trÃ¨s utile en exploration de donnÃ©es. ggplot(data = abalone, mapping = aes(x = LongestShell)) + facet_grid(Type ~ .) + geom_histogram(bins=60, colour = &#39;white&#39;) Le nombre dâ€™incrÃ©ments est un paramÃ¨tre quâ€™il ne faut pas sous-estimer. Ã€ preuve, ce tweet de [@NicholasStrayer](https://twitter.com/NicholasStrayer): Histograms are fantastic, but make sure your bin-width/number is chosen well. This is the exact same data, plotted with different bin-widths. Notice that the pattern doesn't necessarily get clearer as bin num increases. #dataviz pic.twitter.com/3MhSFwTVPH â€” Nick Strayer (@NicholasStrayer) 7 aoÃ»t 2018 4.8.6 Boxplots Les boxplots sont une autre maniÃ¨re de visualiser des distributions. Lâ€™astuce est de crÃ©er une boÃ®te qui sâ€™Ã©tant du premier quartile (valeur oÃ¹ lâ€™on retrouve 25% de donnÃ©es dont la valeur est infÃ©rieure) au troisiÃ¨me quartile (valeur oÃ¹ lâ€™on retrouve 75% de donnÃ©es dont la valeur est infÃ©rieure). Une barre Ã  lâ€™intÃ©rieur de cette boÃ®te est placÃ©e Ã  la mÃ©diane (qui est en fait le second quartile). De part et dâ€™autre de la boÃ®te, on retrouve des lignes spÃ©cifiant lâ€™Ã©tendue hors quartile. Cette Ã©tendue peut Ãªtre dÃ©terminÃ©e de plusieurs maniÃ¨res, mais dans le cas de ggplot2, il sâ€™agit de 1.5 fois lâ€™Ã©tendue de la boÃ®te (lâ€™Ã©cart interquartile). Au-delÃ  de ces lignes, on retrouve les points reprÃ©sentant les valeurs extrÃªmes. Le marqueur Ã  utiliser est geom_boxplot(). Lâ€™encodage x est la variable catÃ©gorielle et lâ€™encodage y est la variable continue. ggplot(data = abalone, mapping = aes(x = Type, y = LongestShell)) + geom_boxplot() Exercice. On suggÃ¨re parfois de prÃ©senter les mesures sur les boxplots. Utiliser geom_jitter avec un bruit horizontal. 4.8.7 Les diagrammes en barre Les diagrammes en barre reprÃ©sente une variable continue associÃ©e Ã  une catÃ©gorie. Les barres sont gÃ©nÃ©ralement horizontales et ordonnÃ©es. Nous y reviendrons Ã  la fin de ce chapitre, mais retenez pour lâ€™instant que dans tous les cas, les diagrammes en barre doivent inclure le zÃ©ro pour Ã©viter les mauvaises interprÃ©tations. Pour les diagrammes en barre, nous allons utiliser les donnÃ©es de lâ€™union internationale pour la conservation de la nature distribuÃ©es par lâ€™OCDE. # Certaines colonnes de caractÃ¨re sont considÃ©rÃ©es comme boolÃ©ennes # mieux vaut dÃ©finir leur type pour s&#39;assurer que le bon type # soit attribuÃ© especes_menacees &lt;- read_csv(&#39;data/WILD_LIFE_14012020030114795.csv&#39;, col_types = list(&quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;d&quot;, &quot;c&quot;, &quot;c&quot;, &quot;c&quot;, &quot;d&quot;, &quot;c&quot;, &quot;c&quot;)) head(especes_menacees) ## # A tibble: 6 x 15 ## IUCN `IUCN Category` SPEC Species COU Country `Unit Code` Unit `PowerCode Code` PowerCode `Reference Periâ€¦ ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 TOT_â€¦ Total number oâ€¦ MAMMâ€¦ Mammals AUS Austraâ€¦ NBR Numbâ€¦ 0 Units &lt;NA&gt; ## 2 ENDAâ€¦ Number of endaâ€¦ MAMMâ€¦ Mammals AUS Austraâ€¦ NBR Numbâ€¦ 0 Units &lt;NA&gt; ## 3 CRITâ€¦ Number of critâ€¦ MAMMâ€¦ Mammals AUS Austraâ€¦ NBR Numbâ€¦ 0 Units &lt;NA&gt; ## 4 VULNâ€¦ Number of vulnâ€¦ MAMMâ€¦ Mammals AUS Austraâ€¦ NBR Numbâ€¦ 0 Units &lt;NA&gt; ## 5 THREâ€¦ Total number oâ€¦ MAMMâ€¦ Mammals AUS Austraâ€¦ NBR Numbâ€¦ 0 Units &lt;NA&gt; ## 6 TOT_â€¦ Total number oâ€¦ MAMMâ€¦ Mammals AUT Austria NBR Numbâ€¦ 0 Units &lt;NA&gt; ## # â€¦ with 4 more variables: `Reference Period` &lt;chr&gt;, Value &lt;dbl&gt;, `Flag Codes` &lt;chr&gt;, Flags &lt;chr&gt; Lâ€™exercice consiste Ã  crÃ©er un diagramme en barres horizontales du nombre de plantes vasculaires menacÃ©es de maniÃ¨re critique pour les 10 pays qui en contiennent le plus. Je vais effectuer quelques opÃ©rations sur ce tableau afin dâ€™en arriver avec un tableau que nous pourrons convenablement mettre en graphique: nâ€™y portez pas trop attention pour lâ€™instant: ces opÃ©rations sont un avant-goÃ»t du prochain chapitre. Nous allons filtrer le tableau pour obtenir le nombre de plantes vascularies critiquement menacÃ©es, sÃ©lectionner seulement le pays et le nombre dâ€™espÃ¨ces, les grouper par pays, additionner toutes les espÃ¨ces pour chaque pays, les placer en ordre descendant et enfin sÃ©lectionner les 10 premiers. Comme vous le voyez, la crÃ©ation de graphique est liÃ©e de prÃ¨s avec la manipulation des tableaux! especes_crit &lt;- especes_menacees %&gt;% filter(IUCN == &#39;CRITICAL&#39;, SPEC == &quot;VASCULAR_PLANT&quot;) %&gt;% dplyr::select(Country, Value) %&gt;% group_by(Country) %&gt;% summarise(n_critical_species = sum(Value)) %&gt;% arrange(desc(n_critical_species)) %&gt;% head(10) especes_crit ## # A tibble: 10 x 2 ## Country n_critical_species ## &lt;chr&gt; &lt;dbl&gt; ## 1 United States 1222 ## 2 Japan 525 ## 3 Canada 315 ## 4 Czech Republic 284 ## 5 Spain 271 ## 6 Belgium 253 ## 7 Austria 172 ## 8 Slovak Republic 155 ## 9 Australia 148 ## 10 Italy 128 Le premier type de diagramme en barre que nous allons couvrir est obtenu par le marqueur geom_col(). ggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) + geom_col() Ce graphique est perfectible. Les barres sont verticales et non ordonnÃ©es. Souvenons-nous que ggplot2 ordonne par ordre alphabÃ©tique si aucun autre ordre est spÃ©cifiÃ©. Nous pouvons changer lâ€™ordre en changeant lâ€™ordre des niveaux de la variable Country selon le nombre dâ€™espÃ¨ces grÃ¢ce Ã  la fonction fct_reorder. especes_crit &lt;- especes_crit %&gt;% mutate(Country = fct_reorder(Country, n_critical_species)) Pour faire pivoter le graphique, nous ajoutons coord_flip() Ã  la sÃ©quence. ggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) + geom_col() + coord_flip() Une autre mÃ©thode, geom_bar(), est un raccourcis permettant de compter le nombre dâ€™occurrence dâ€™une variable unique. Par exemple, dans le tableau abalone, le nombre de fois que chaque niveau de la variable Type ggplot(data = abalone, mapping = aes(x = Type)) + geom_bar() + coord_flip() Personnellement, je prÃ©fÃ¨re passer par un diagramme en lignes avec le marqueur geom_segment(). Cela me donne la flexibilitÃ© pour dÃ©finir un largeur de trait et Ã©ventuellement dâ€™ajouter un point au bout pour en faire un diagramme en suÃ§on. ggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) + geom_segment(mapping = aes(xend=Country, yend = 0), lwd = 2) + geom_point(size=6, colour = &quot;black&quot;) + coord_flip() + theme_bw() Les diagrammes en barre peuvent Ãªtre placÃ©s en relation avec dâ€™autres. Reprenons notre manipulation de donnÃ©es prÃ©cÃ©dente, mais en incluant tous les pays, pour les trois niveaux dâ€™alerte, pour les poissons. especes_pays_iucn &lt;- especes_menacees %&gt;% filter(IUCN %in% c(&#39;ENDANGERED&#39;, &#39;VULNERABLE&#39;,&#39;CRITICAL&#39;), SPEC == &quot;FISH_TOT&quot;) %&gt;% dplyr::select(IUCN, Country, Value) %&gt;% group_by(Country, IUCN) %&gt;% summarise(n_species = sum(Value)) %&gt;% group_by(Country) %&gt;% mutate(n_tot = sum(n_species)) %&gt;% ungroup() %&gt;% # pour pouvoir modifier Country, non modifiable tant qu&#39;elle est une variable de regroupement (voir group_by) mutate(Country = fct_reorder(Country, n_tot)) head(especes_pays_iucn) ## # A tibble: 6 x 4 ## Country IUCN n_species n_tot ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Australia CRITICAL 8 48 ## 2 Australia ENDANGERED 16 48 ## 3 Australia VULNERABLE 24 48 ## 4 Austria CRITICAL 6 39 ## 5 Austria ENDANGERED 18 39 ## 6 Austria VULNERABLE 15 39 Pour placer les barres les unes Ã  cÃ´tÃ© des autres, nous spÃ©cifions position = &quot;dodge&quot;. ggplot(data = especes_pays_iucn, mapping = aes(x = Country, y = n_species)) + geom_col(aes(fill=IUCN), position = &quot;dodge&quot;) + coord_flip() Il est parfois plus pratique dâ€™utiliser les facettes. ggplot(data = especes_pays_iucn, mapping = aes(x = Country, y = n_species)) + facet_grid(IUCN ~ .) + geom_col() + coord_flip() 4.8.8 Exporter un graphique Plus besoin dâ€™utiliser la fonction png() en mode ggplot2. Utilisons plutÃ´t ggsave(). ggplot(data = especes_pays_iucn, mapping = aes(x = Country, y = n_species)) + facet_grid(IUCN ~ .) + geom_col(aes(fill=IUCN)) + coord_flip() ggsave(&quot;images/especes_pays_iucn.png&quot;, width = 6, height = 8, dpi = 300) 4.9 Les graphiques comme outil dâ€™exploration des donnÃ©es La plupart des graphiques que vous crÃ©erez ne seront pas destinÃ©s Ã  Ãªtre publiÃ©s, mais serviront dâ€™outil dâ€™exploration des donnÃ©es. Le jeu de donnÃ©es datasaurus, prÃ©sentÃ© en dÃ©but de chapitre, permet de saisir lâ€™importance des outils graphiques pour bien comprendre les donnÃ©es. datasaurus &lt;- read_tsv(&#39;data/DatasaurusDozen.tsv&#39;) ## Parsed with column specification: ## cols( ## dataset = col_character(), ## x = col_double(), ## y = col_double() ## ) head(datasaurus) ## # A tibble: 6 x 3 ## dataset x y ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 dino 55.4 97.2 ## 2 dino 51.5 96.0 ## 3 dino 46.2 94.5 ## 4 dino 42.8 91.4 ## 5 dino 40.8 88.3 ## 6 dino 38.7 84.9 Projetons dâ€™abord les coordonnÃ©es x et y sur un graphique. Jâ€™utilise FacetGrid ici, sachant que ce sera utile pour lâ€™exploration. ggplot(data = datasaurus, mapping = aes(x = x, y = y)) + geom_point() Ce graphique pourrait ressembler Ã  une distribution binormale, ou un coup de 12 dans une porte de grange. Mais on aperÃ§oit des donnÃ©es alignÃ©es, parfois de maniÃ¨re rectiligne, parfois en forme dâ€™ellipse. Le tableau datasaurus a une colonne dâ€™information supplÃ©mentaire. Utilisons-la comme catÃ©gorie pour gÃ©nÃ©rer des couleurs diffÃ©rente. ggplot(data = datasaurus, mapping = aes(x = x, y = y)) + geom_point(mapping = aes(colour = dataset)) Ce nâ€™est pas vraiment plus clair. Il y a toutefois des formes qui se dÃ©gage, comme des ellipse et des lignes. Et si je regarde bien, jâ€™y vois une Ã©toile. La catÃ©gorisation pourrait-elle Ãªtre mieux utilisÃ©e si on segmentait par facettes au lieu de des couleurs? ggplot(data = datasaurus, mapping = aes(x = x, y = y)) + facet_wrap(~dataset, nrow=2) + geom_point(size = 0.5) + coord_equal() VoilÃ ! Fait intÃ©ressant, ni les statistiques, ni les algorithmes de regroupement ne nous auraient Ã©tÃ© utiles pour diffÃ©rencier les groupes! 4.9.1 Des graphiques interactifs! Les graphiques sont traditionnellement des images statiques. Toutefois, les graphiques nâ€™Ã©tant pas dÃ©pendants de supports papiers peuvent Ãªtre utilisÃ©s de maniÃ¨re diffÃ©rente, en ajoutant une couche dâ€™interaction. ConÃ§ue Ã  MontrÃ©al, plotly est un module graphique interactif en soi. Il peut Ãªtre utilisÃ© grÃ¢ce Ã  son outil web, tout comme il peut Ãªtre interfacÃ© avec R, Python, javascript, etc. Mais ce qui retient notre attention ici est son interface avec ggplot2. Les graphiques ggplot2 peuvent Ãªtre enregistrÃ©s en tant quâ€™objets. Il peuvent consÃ©quemment Ãªtre manipulÃ©s par des fonctions. La fonction ggplotly permet de rendre votre ggplot interactif. library(&quot;plotly&quot;) ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout especes_crit_bar &lt;- ggplot(data = especes_crit, mapping = aes(x = Country, y = n_critical_species)) + geom_segment(mapping = aes(xend=Country, yend = 0), lwd = 2) + geom_point(size=6) + coord_flip() # ggplotly(especes_crit_bar) # erreur en Rmd 4.9.2 Des extensions de ggplot2 ggplot2 est un module graphique Ã©lÃ©gant et polyvalent. Il a pourtant bien des limitations. Justement, le module est conÃ§u pour Ãªtre implÃ©mentÃ© avec des extensions. Vous en trouverez plusieurs sur ggplot2-exts.org, mais en trouverez de nombreuses autres en cherchant avec le terme ggplot2 sur github.com, probablement la plate-forme (voire un rÃ©seau social) de dÃ©veloppement de logiciels la plus utilisÃ©e dans le monde. En voici quelques unes. ggthemr: spÃ©cifier un thÃ¨me graphique une seule fois dans votre session, et tout le reste suit. cowplot permet de crÃ©er des graphiques prÃªts pour la publication, par exemple en crÃ©ant des grilles de plusieurs ggplots, en les numÃ©rotant, etc. Si les thÃ¨mes de base ne vous conviennent pas, vous en trouverez dâ€™autres en installant ggthemes. ggmap et ggspatial sont deux extensions pour crÃ©er des cartes. Un chapitre sur les donnÃ©es spatiales est en dÃ©veloppement. ggtern permet de crÃ©er des diagrammes ternaires, qui sont utiles pour la visualisation de proportions incluant trois composantes. Ce sujet est couvert au chapitre 6, en dÃ©veloppement. 4.9.3 Aller plus loin avec ggplot2 Claus O. Wilke est professeur en biologie intÃ©grative Ã  lâ€™UniversitÃ© du Texas Ã  Austin. Son livre Fundamentals of Data Visualization est un guide thÃ©orique et pratique pour la visualisation de donnÃ©es avec ggplot2. Le site data-to-viz.com vous accompagne dans le choix du graphique Ã  crÃ©er selon vos donnÃ©es. Le site r-graph-gallery.com offre des recettes pour crÃ©er des graphiques avec ggplot2. 4.10 Choisir les bonnes couleurs La couleur est une information. Les couleurs devraient Ãªtre sÃ©lectionnÃ©es dâ€™abord pour Ãªtre lisibles par les personnes ne percevant pas les couleurs, selon le support (apte Ã  Ãªtre photocopiÃ©, lisible Ã  lâ€™Ã©cran, lisible sur des documents imprimÃ©s en noir et blanc) et selon le type de donnÃ©es. - DonnÃ©es continues ou catÃ©gorielles ordinales: gradient (transition graduelle dâ€™une couleur Ã  lâ€™autre), sÃ©quence (transition saccadÃ©e selon des groupes de donnÃ©es continues) ou divergentes (transition saccadÃ©e dâ€™une couleur Ã  lâ€™autre vers des couleurs divergentes, par exemple orange vers blanc vers bleu). - DonnÃ©es catÃ©gorielles nominales: couleurs Ã©loignÃ©es dâ€™une catÃ©gorie Ã  une autre (plus il y a de catÃ©gories, plus les couleurs sont susceptibles de se ressembler). Capture dâ€™Ã©cran de colorbrewer2.org, qui propose des palettes de couleurs pour crÃ©er des cartes, mais lâ€™information est pertinente pour tout type de graphique. 4.11 RÃ¨gles particuliÃ¨res Les mauvais graphiques peuvent survenir Ã  cause de lâ€™ignorance, bien sÃ»r, mais souvent ils existent pour la mÃªme raison que la boeuferie [bullhist] verbale ou Ã©crite. Parfois, les gens ne se soucient pas de la faÃ§on dont ils prÃ©sentent les donnÃ©es aussi longtemps que Ã§a appuie leurs arguments et, parfois, ils ne se soucient pas que Ã§a porte Ã  confusion tant quâ€™ils ont lâ€™air impressionnant. \\(-\\) Carl Bergstorm et Jevin West, Calling Bullshit Read-Along Week 6: Data Visualization Une reprÃ©sentation visuelle est un outil tranchant qui peut autant prÃ©senter un Ã©tat vÃ©ritable des donnÃ©es quâ€™une perspective trompeuse. Bien souvent, une ou plusieurs des 5 qualitÃ©s ne sont pas respectÃ©es. Les occasions dâ€™erreur ne manquent pas - jâ€™en ferai mention dans la section Choisir le bon type de graphique. Pour lâ€™instant, notons quelques rÃ¨gles particuliÃ¨res. 4.11.1 Ne tronquez pas inutilement lâ€™axe des \\(y\\) Tronquer lâ€™axe vertical peut amener Ã  porter de fausses conclusions. Effets sur la perception dâ€™utiliser diffÃ©rentes rÃ©fÃ©rences. Source: Yau (2015), Real Chart Rules to Follow. La rÃ¨gle semble simple: les diagrammes en barre (utilisÃ©s pour reprÃ©senter une grandeur) devraient toujours prÃ©senter le 0 et les diagrammes en ligne (utilisÃ©s pour prÃ©senter des tendances) ne requiert pas nÃ©cessairement le zÃ©ro ((Bergstrom et West, Calling bullshit: Misleading axes on graphs)[http://callingbullshit.org/tools/tools_misleading_axes.html]). Mais le zÃ©ro nâ€™est pas toujours liÃ© Ã  une quantitÃ© particuliÃ¨re, par exemple, la tempÃ©rature ou un log-ratio. De plus, avec un diagramme en ligne on pourra toujours magnifier des tendances en zoomant sur une variation somme toute mineure. On arrive donc moins Ã  une rÃ¨gle quâ€™une qualitÃ© dâ€™un bon graphique, en particulier la qualitÃ© no 1 de Cairo: offrir une reprÃ©sentation honnÃªte des donnÃ©es. Par exemple, Nathan Yau, auteur du blogue Flowing Data, propose de prÃ©senter des rÃ©sultats de maniÃ¨re relative Ã  la mesure initiale. Câ€™est dâ€™ailleurs ce qui a Ã©tÃ© fait pour gÃ©nÃ©rer le graphique de Michael Mann et al., ci-dessus, oÃ¹ le zÃ©ro correspond Ã  la moyenne des tempÃ©ratures enregistrÃ©es entre 1961 et 1990. Il peut Ãªtre tentant de tronquer lâ€™axe des \\(y\\) lorsque lâ€™on dÃ©sire superposer deux axes verticaux. Souvent, lâ€™utilisation de plusieurs axes verticaux amÃ¨ne une perception de causalitÃ© dans des situations de fausses corrÃ©lations. On ne devrait pas utiliser plusieurs axes verticaux. 4.11.2 Utilisez un encrage proportionnel Cette rÃ¨gle a Ã©tÃ© proposÃ©e par Edward Tufte dans Visual Display of Quantitative Information. Une des raisons pour lesquelles on Ã©vite de tronquer lâ€™axe des \\(y\\) en particulier pour les diagrammes en barre est que lâ€™aire reprÃ©sentant une mesure (la quantitÃ© dâ€™â€œencreâ€ nÃ©cessaire pour la dessiner) devrait Ãªtre proportionnelle Ã  sa magnitude. Les diagrammes en barre sont particuliÃ¨rement sensibles Ã  cette rÃ¨gle, Ã©tant donnÃ©e que la largeur des barres peuvent amplifier lâ€™aire occupÃ©e. Deux solutions dans ce cas: (1) utiliser des barres minces ou (2) prÃ©fÃ©rer des â€œdiagrammes de pointsâ€ (dot charts, Ã  ne pas confondre aux nuages de points). Lâ€™encrage a beau Ãªtre proportionnel, la difficultÃ© que les humains Ã©prouvent Ã  comparer la dimension des cercles, et a fortiori la dimension de parties de cercle, donne peu dâ€™avantage Ã  utiliser des diagrammes en pointe de tarte, souvent utilisÃ©s pour illustrer des proportions. Nathan Yau suggÃ¨re de les utiliser avec suspicions et dâ€™explorer dâ€™autres options. Pour comparer deux proportions, une avenue intÃ©ressante est le diagramme en pente, suggÃ©rÃ© notamment par Ann K. Emery. Par extension, le diagramme en pente devient un diagramme en ligne lorsque plusieurs types de proportions sont comparÃ©es, ou lorsque des proportions Ã©voluent selon des donnÃ©es continuent. De la mÃªme maniÃ¨re, les diagrammes en bulles ne devraient pas Ãªtre reprÃ©sentatifs de la quantitÃ©, mais plutÃ´t de contextualiser des donnÃ©es. Justement, le graphique tirÃ© des donnÃ©es de Gap minder prÃ©sentÃ© plus haut est une contextualisation: lâ€™aire dâ€™un cercle ne permet pas de saisir la population dâ€™un pays, mais de comparer grossiÃ¨rement la population dâ€™un pays par rapport aux autres. 4.11.3 Publiez vos donnÃ©es Vous avez peut-Ãªtre dÃ©jÃ  feuilletÃ© un article et voulu avoir accÃ¨s aux donnÃ©es incluses dans un graphique. Il existe des outils pour digitaliser des graphiques pour en extraire les donnÃ©es. Mais le processus est fastidieux, long, souvent peu prÃ©cis. De plus en plus, les chercheurs sont encouragÃ©s Ã  publier leurs donnÃ©es et leurs calculs. Matplotlib et Seaborn sont des outils graphiques classiques qui devraient Ãªtre accompagnÃ©s des donnÃ©es et calculs ayant servi Ã  les gÃ©nÃ©rer. Mais ce nâ€™est pas idÃ©al non plus. En revanche, les outils graphiques modernes comme Plotly et Altair peuvent Ãªtre exportÃ©s en code javascipt, qui contient toutes les informations sur les donnÃ©es et la maniÃ¨re de les reprÃ©senter graphiquement. Ce chapitre a pour objectif de vous familiariser avec les outils de base les plus communÃ©ment utilisÃ©s en calcul scientifique avec Python, mais je vous encourage Ã  explorer la nouvelle gÃ©nÃ©ration dâ€™outils graphiques. 4.11.4 Visitez www.junkcharts.typepad.com de temps Ã  autre Le statisticien et blogueur Kaiser Fung sâ€™affaire quotidiennement Ã  proposer des amÃ©liorations Ã  de mauvais graphiques sur son blogue Junk Charts. "],
["chapitre-git.html", "5 Science ouverte et reproductibilitÃ© 5.1 Un code reproductible 5.2 Introduction Ã  GitHub 5.3 Introduction Ã  Pakrat ğŸ“¦ğŸ€ 5.4 Pour terminer, le reprex", " 5 Science ouverte et reproductibilitÃ© ï¸Â Objectifs spÃ©cifiques: Ã€ la fin de ce chapitre, vous saurez exprimer lâ€™importance et les enjeux de la science ouverte saurez arranger vos donnÃ©es (format csv) et votre code (format notebook) afin de rendre vos recherches reproductibles saurez comment crÃ©er un dÃ©pÃ´t sur GitHub, puis administrer son dÃ©veloppement La science ouverte favorise la diffusion des connaissances Ã  travers plusieurs aspects. MÃ©thodologie ouverte. Ce nâ€™est pas pour rien que les revues scientifiques demandent de la minutie dans la description de la mÃ©thodologie: câ€™est pour sâ€™assurer de bien comprendre la signification des donnÃ©es collectÃ©es et faire en sorte que vos donnÃ©es puissent Ãªtre Ã©chantillonnÃ©es de la mÃªme maniÃ¨re dans une potentielle expÃ©rience subsÃ©quente. Ã€ ce titre, la revue Nature a crÃ©Ã© le site de publication de protocoles expÃ©rimentaux Protocol exchange, â€œoÃ¹ la communautÃ© scientifique met en commun son savoir-faire expÃ©rimental pour accÃ©lÃ©rer la rechercheâ€ (ma traduction). DonnÃ©es ouvertes. En rendant nos donnÃ©es publiques, on permet Ã  la postÃ©ritÃ© de les utiliser pour amÃ©liorer les connaissances, dÃ©couvrir des structures qui nous avaient Ã©chappÃ©es, etc. Dans certains cas, lâ€™ouverture des donnÃ©es peut Ãªtre contrainte par des enjeux lÃ©gaux (donnÃ©es privÃ©es) ou Ã©thiques (donnÃ©es pouvant Ãªtre utilisÃ©es Ã  mauvais escient). Dans la plupart des cas, les avantages surpassent largement les risques encourus par la publication des donnÃ©es, et les informations personnelles peuvent Ãªtre retirÃ©es. Des journaux comme Plos exigent que les donnÃ©es minimales Ã  la reproduction de lâ€™expÃ©rience soient fournies en tant que matÃ©riel supplÃ©mentaire. Code source ouvert. Les logiciels open source, comme R, sont gratuits pour la plupart. Cela permet Ã  quiconque de les utiliser, pourvu que lâ€™on possÃ¨de le support matÃ©riel (un ordinateur) et une connection internet. De la mÃªme maniÃ¨re, le code R qui vous a permis de gÃ©nÃ©rer des rÃ©sultats Ã  partir de vos donnÃ©es peut Ãªtre rendu public sous toutes sortes de licenses open source peu restrictive (GPL, BSD, MIT, etc.). Avec les donnÃ©es et le code, vos travaux pourront Ãªtre reproduits. RÃ©vision ouverte. La rÃ©vision est un travail essentiel en science. Traditionnellement, les publications scientifiques sont rÃ©visÃ©s de maniÃ¨re anonyme, le but Ã©tant dâ€™Ã©viter les conflits. RÃ©cemment, des revues comme Frontiers ont dÃ©ployÃ© des modes de rÃ©vision ouverts, permettant (1) des Ã©changes plus constructifs entre auteurs et rÃ©viseurs et (2) de remercier ouvertement la contribution des rÃ©viseurs Ã  lâ€™article final. AccÃ¨s ouvert. Les Ã©diteurs scientifiques sont largement critiquÃ©s pour demander des frais usuraires aux bibliothÃ¨ques et pour la consultation Ã  la piÃ¨ce, ainsi que des frais de publication dÃ©mesurÃ©s. En rÃ©action Ã  cela, le site Sci-Hub dÃ©bloque gratuitement des millions dâ€™articles scientifiques. Aussi, des journaux sÃ©rieux comme Plos et Frontiers publient de facto les articles sur leur site internet, de sorte quâ€™ils peuvent Ãªtre librement tÃ©lÃ©chargÃ©s. Le manque dâ€™ouverture dans la science a menÃ© plusieurs scientifiques Ã  parler dâ€™une crise de la reproductibilitÃ© (Baker, 2016). Dans ce chapitre, nous verrons quelques astuces pour que R devienne un outil favorisant la science ouverte. Ã€ la fin de ce chapitre, vous devriez Ãªtre en mesure de dÃ©ployer votre code sur une archive en ligne, comme ceci. Figure 5.1: Exemple dâ€™un dossier de code et de donnÃ©es ouvertes, (Jeanne et al.Â 2019) 5.1 Un code reproductible Figure 5.2: A Guide to Reproducible Code in Ecology and Evolution, BES 2017 La British ecological society offre des lignes guide pour crÃ©er un flux de travail reproductible (BES, 2017). En outre, les principes suivants doivent Ãªtre respectÃ©s (ma traduction, avec ajouts). Commencez votre analyse Ã  partir dâ€™une copie des donnÃ©es brutes. Les donnÃ©es doivent Ãªtre fournies dans un format ouvert (csv, json, sqlite, etc.). Ã‰vitez de dÃ©marrer une analyse par un chiffrier Ã©lectronique ou un logiciel propriÃ©taire (qui nâ€™est pas open source). En ce sens, dÃ©marrer avec Excel (xls ou xlsx) est Ã  Ã©viter, tout comme les sont les donnÃ©es encodÃ©es pour SPSS ou SAS. Toute opÃ©ration sur les donnÃ©es, que ce soit du nettoyage, des fusions, des transformations, etc. devrait Ãªtre effectuÃ©e avec du code, non pas manuellement. Sâ€™il sâ€™agit dâ€™une erreur de frappe dans un tableau, on peut dÃ©roger Ã  la rÃ¨gle. Mais sâ€™il sâ€™agit par exemple dâ€™Ã©limier des outliers, ne supprimez pas des entrÃ©es de vos donnÃ©es brutes. De mÃªme, nâ€™effectuez pas de transformation de vos donnÃ©es brutes Ã  lâ€™extÃ©rieur du code. En somme, vos calculs devraient Ãªtre en mesure dâ€™Ãªtre lancÃ©s dâ€™un seul coup, sans opÃ©rations manuelles intermÃ©diaires. SÃ©parez vos opÃ©rations en unitÃ©s logiques thÃ©matiques. Par exemple, vous pourriez sÃ©parer votre code en parties: (i) charger, fusionner et nettoyer les donnÃ©es, (ii) analyser les donnÃ©es, (iii) crÃ©er des fichiers comme des tableaux et des figures. Ã‰liminez la duplication du code en crÃ©ant des fonctions personnalisÃ©es. Assurez-vous de commenter vos fonctions en dÃ©tails, expliquez ce qui est attendu comme entrÃ©es et comme sorties, ce quâ€™elles font et pourquoi. Documentez votre code et vos donnÃ©es Ã  mÃªme les feuilles de calcul ou dans un fichier de documentation sÃ©parÃ©. Tout fichier intermÃ©diaire devrait Ãªtre sÃ©parÃ© de vos donnÃ©es brutes. 5.1.1 Structure dâ€™un projet Un projet de calcul devrait Ãªtre contenu en un seul dossier. Si vous nâ€™avez que quelques projets, il est assez facile de garder lâ€™info en mÃ©moire. Toutefois, en particulier en milieu dâ€™entreprise, il se pourrait fort bien que vous ayez Ã  mener plusieurs projets de front. Certaines entreprises crÃ©ent des numÃ©ros de projet: vous aurez avantage Ã  nommer vos dossiers avec ces numÃ©ros, incluant une brÃ¨ve description. Pour ma part, jâ€™ordonne mes projets chronologiquement par annÃ©e, avec un descriptif. ğŸ“ 2019_abeille-canneberge Notez que je nâ€™utilise ni espace, ni caractÃ¨re spÃ©cial dans le nom du fichier, pour Ã©viter les erreurs potentielles avec des logiciels capricieux. Ã€ lâ€™intÃ©rieur du dossier racine du projet, jâ€™inclus lâ€™information gÃ©nÃ©rale: donnÃ©es source (souvent des fichiers Excel), manuscrit (mÃ©moire, thÃ¨se, article, etc.) documentation particuliÃ¨re (pour les articles, jâ€™utilise Zotero, un gestionnaire de rÃ©fÃ©rence), photos et, Ã©videmment, mon dossier de code (par exemple rstats). ğŸ“ 2019_abeille-canneberge |-ğŸ“ documentation |-ğŸ“ manuscrit |-ğŸ“ photos |-ğŸ“ rstats |-ğŸ“ source Si vous rÃ©digez votre manuscrit Ã  mÃªme votre code (en Latex, Lyx, markdown ou R markdown que nous verrons cela plus loin), vous pouvez trÃ¨s bien lâ€™inclure dans votre fichier de calcul. Ã€ lâ€™intÃ©rieur du fichier de calcul, vous aurez votre projet RStudio et vos feuilles de calcul sÃ©quencÃ©es. Jâ€™utilise 01-, et non pas 1- pour Ã©viter que le 10- suive le 1- dans le classement en ordre alpha-numÃ©rique au cas oÃ¹ jâ€™aurais plus de 10 feuilles de calcul. Jâ€™inclus un fichier README.md (extension md pour markdown), qui contient les informations gÃ©nÃ©rales de mes calculs. Les donnÃ©es brutes (csv) sont placÃ©es dans un dossier data, mes graphiques sont exportÃ©s dans un dossier image, mes tableaux sont exportÃ©s dans un dossier tables et mes fonctions externes sont exportÃ©es dans un dossier lib. ğŸ“ rstats |-ğŸ“ data |-ğŸ“ images |-ğŸ“ lib |-ğŸ“ tables ğŸ“„ bees.Rproj ğŸ“„ 01_clean-data.R ğŸ“„ 02_data-mining.R ğŸ“„ 03_data-analysis.R ğŸ“„ 04_data-modeling.R ğŸ“„ README.md Je dÃ©cris les noms de fichiers dans la langue de communication utile pour le rendu final du projet, souvent en anglais lors de publications acadÃ©miques. Jâ€™Ã©vite les noms de fichier qui ne sont pas informatifs, par exemple 01.R ou Rplot1.png, ainsi que les majuscules, les caractÃ¨res spÃ©ciaux et les espaces comme dans DeuxiÃ¨me essai.R (le README.md est une exception). Pour partager un dossier de projet sur R, on nâ€™a quâ€™Ã  le compresser (zip), puis lâ€™envoyer. Pour que le code fonctionne sur un autre ordinateur, les liens vers les fichiers de donnÃ©es Ã  importer ou les graphiques exportÃ©s doivent Ãªtre relatifs au fichier R ouvert dans votre projet, non pas le chemin complet sur votre ordinateur. Figure 5.3: Retrouvez votre chemin, dessin de Allison Horst Tout comme la BSE, lâ€™organisme sans but lucratif rOpenSci offre un guide sur la reproductibilitÃ©. 5.1.2 Le format R markdown Un code reproductible est un code bien dÃ©crit. La structure de projet prÃ©sentÃ©e prÃ©cÃ©demment propose de segmenter le code en plusieurs fichiers R. Cette maniÃ¨re de procÃ©der est optionnelle. Si le fichier de calcul nâ€™est pas trop encombrant, on pourra nâ€™en utiliser quâ€™un seul, par exemple stats.R. Ã€ lâ€™intÃ©rieur mÃªme des feuilles de calcul R, vous devrez commenter votre code pour en expliquer les Ã©tapes, par exemple: ############# ## Titre 1 ## ############# # Titre 2 ## Titre 3 data &lt;- read_csv(&quot;data/abeilles.csv&quot;) # commentaire particulier RStudio a dÃ©veloppÃ© une approche plus conviviale avec son format R markdown. Le langage markdown permet de formater un texte avec un minimum de dÃ©corations, et R markdown permet dâ€™intÃ©grer du texte et des codes. Ces notes de cours sont par ailleurs entiÃ¨rement Ã©crites en R markdown. 5.1.2.1 Le langage markdown Un fichier portant lâ€™extension .md ou .markdown est un fichier texte clair (que vous pouvez ouvrir et Ã©diter dans nâ€™importe votre Ã©diteur texte prÃ©fÃ©rÃ©), tout comme un fichier .R. Il existe nÃ©anmoins de nombreux Ã©diteurs de texte spÃ©cialisÃ©s en Ã©dition markdown - mon prÃ©fÃ©rÃ© est Zettlr. Les dÃ©corations principales en markdown sont les suivantes (les citations utilisÃ©es ci-aprÃ¨s sont tirÃ©es du roman Dune, de Frank Herbert). Italique. Pour emphaser en italique, balisez le texte avec des astÃ©risques. Par exemple, â€œPourrais-je porter parmi vous le nom de *Paul-Muad'dib*?â€ devient â€œPourrais-je porter parmi vous le nom de Paul-Muadâ€™dib?â€ Gras. Pour emphaser en gras, balisez le texte avec des doubles astÃ©risques. Par exemple, â€œL'espÃ©rance **ternit** l'observation.â€ devient â€œLâ€™espÃ©rance ternit lâ€™observationâ€. Largeur fixe. Pour un texte Ã  largeur fixe (signifiant du code), balisez le texte avec des accents graves. Par exemple, â€œQuel nom donnez-vous Ã  la petite `souris`, celle qui saute ?â€ devient â€œQuel nom donnez-vous Ã  la petite souris, celle qui saute?â€ Listes. Pour effectuer une liste numÃ©rotÃ©e, utilisez le chiffre 1. Par exemple, 1. Paul 1. Leto 1. Alia devient Paul Jessica Alia De mÃªme, pour une liste Ã  puces, changez le 1. par le - ou le *. EntÃªtes. Les titres sont prÃ©cÃ©dÃ©s par des #. Un # pour un titre 1, deux ## pour un titre 2, etc. Par exemple, # Imperium ## Landsraad ### Maison des AtrÃ©ides ### Maison des Harkonnen ## CHOAM # Guilde des navigateurs InsÃ©rera les titres appropriÃ©s (que je nâ€™insÃ¨re pas pour ne pas bousiller la structure de ce texte). Liens. Pour insÃ©rer des liens, le texte est entre crochet directement suivi du lien entre parenthÃ¨ses. Par exemple, â€œLongue vie aux [combattants](https://youtu.be/Cv87NJ2xX0k?t=59)â€ devient â€œLongue vie aux combattantsâ€. Ã‰quations. Les Ã©quations suivent la syntaxe Latex entre deux $$ pour les Ã©quations sur une ligne et entre des doubles $$$$ pour les Ã©quations sur un paragraphe. Par exemple, $c = \\sqrt{a^2 + b^2}$ devient \\(c = \\sqrt{a^2 + b^2}\\). Images. Pour insÃ©rer une image, ![nom de l'image](images/spice-must-flow.png). Une liste exhaustive des balises markdown est disponible sous forme dâ€™aide-mÃ©moire. 5.1.2.2 R markdown Dans RStudio, ouvrez un R markdown par File &gt; New file &gt; R Markdown. Si le module rmarkdown nâ€™est pas installÃ©, RStudio vous demandera de lâ€™installer. Une fenÃªtre apparaÃ®tra. Figure 5.4: Nouveau fichier R markdown Les options dâ€™exportation pourront Ãªtre modifiÃ©es par la suite. Un fichier dâ€™exemple sera crÃ©Ã©, et vous pourrez le modifier. Les parties de texte sont Ã©crits en markdown, et le code R est enchÃ¢ssÃ© entre les balises ```{r} et ```. Je nommerai ces parties de code des cellules de code. Des options de code lâ€™intÃ©rieur peuvent Ãªtre utilisÃ©es Ã  lâ€™intÃ©rieur des accolades {r}. Par exemple {r, filtre-outliers} donne le nom filtre-outliers au bloc de code, qui permet nommÃ©ment de nommer les images crÃ©er dans le bloc de code. {r, eval = FALSE} permet dâ€™activer (TRUE, valeur par dÃ©faut) ou de dÃ©sactiver (FALSE) le calcul de la cellule. {r, echo = FALSE} permet de nâ€™afficher que la sortie de la cellule de code en nâ€™affichant pas le code, par exemple un graphique ou le sommaire dâ€™une rÃ©gression. {r, results = FALSE} permet de nâ€™afficher que le code, mais pas la sortie. {r, warning = FALSE, message = FALSE, error = FALSE} nâ€™affichera pas les avertissements, les messages automatiques et les messages dâ€™erreur. {r, fig.width = 10, fig.height = 5, fig.align = &quot;center&quot;} affichera les graphiques dans les dimensions voulues, alignÃ©e au centre (&quot;center&quot;), Ã  gauche (&quot;left&quot;) ou Ã  droite (&quot;right&quot;). Notez que vous pouvez exÃ©cuter rapidement du code sur une ligne avec la formulation `r `, par exemple la moyenne des nombres `\\r a&lt;-round(runif(4, 0, 10)); a` est de `\\r mean(a)`, en enlevant les \\ devant les r (ajoutÃ©es artificiellement pour Ã©viter que le code soit calculÃ©) sera la moyenne des nombres 7, 2, 2, 8 est de 4.75 Une fois que vous serez satisfait de votre document, cliquer sur Knit et le fichier de sortie sera gÃ©nÃ©rÃ©. Le guide qui permet de gÃ©nÃ©rer le fichier de sortie est tout en haut du fichier. Nous lâ€™appelons le YAML (acronyme rÃ©cursif de YAML Ainâ€™t Markup Language). Prenez le YAML suivant. --- title: &quot;Dune&quot; author: &quot;Frank Herbert&quot; date: &quot;1965-08-01&quot; output: github_document --- Le titre, lâ€™auteur et la date sont spÃ©cifiÃ©es. Pour indiquer la date courante, on peut simplement la gÃ©nÃ©rer avec R en remplaÃ§ant &quot;1965-08-01&quot; par 2020-01-13. La spÃ©cification output indique le type de document Ã  gÃ©nÃ©rer, par exemple html_document pour une page web, pdf_document pour un pdf, ou word_document pour un docx. Dans ce cas-ci, jâ€™indique github_document pour crÃ©er un fichier markdown comprenant nommÃ©ment des liens relatifs vers les images des graphiques gÃ©nÃ©rÃ©s. Pourquoi un github_document? Câ€™est le sujet de la prochaine sous-section. Mais avant cela, je vous rÃ©fÃ¨re Ã  un autre aide-mÃ©moire. Figure 5.5: Aide-mÃ©moire pour R Markdown, Source: RStudio 5.2 Introduction Ã  GitHub Le system de suivi de version git (open source) a Ã©tÃ© crÃ©Ã© par Linus Torvalds, aussi connu pour avoir crÃ©Ã© Linux. git prend une photo de votre rÃ©pertoire de projet Ã  chaque fois que vous commettez un changement. Vous pourrez revenir sans problÃ¨me sur dâ€™anciennes versions si quelque chose tourne mal, et vous pourrez publier le rÃ©sultat final sur un service dâ€™hÃ©bergement utilisant git. Il existe plusieurs services pour rendre git utilisable en ligne, mais GitHub est dÃ©finitivement le plus utilisÃ© dâ€™entre tous. La plateforme GitHub est presque devenue un rÃ©seau social de dÃ©veloppement. GitHub, maintenant la propriÃ©tÃ© de Microsoft, nâ€™est en soi pas open source. Si cela vous pose problÃ¨me, je vous redirige vers la plateforme open source GitLab, qui fonctionne Ã  peu prÃ¨s de la mÃªme maniÃ¨re que GitHub (alors que la plateforme GitHub sera fort probablement toujours vivante dans plusieurs annÃ©es, on en est moins sÃ»r pour GitLab, câ€™est pourquoi jâ€™utilise GitHub Ã  des fins professionnelles mais jâ€™utilise GitLab Ã  des fins personnelles). Pour suivre cette partie, je vous invite Ã  crÃ©er un compte sur GitHub ou GitLab, Ã  votre choix. CrÃ©ez un nouveau dÃ©pÃ´t (New repository). Figure 5.6: Nouveau dÃ©pÃ´t avec GitHub Figure 5.7: Nouveau dÃ©pÃ´t avec GitLab Pour utiliser git, vous pourrez toujours travailler en ligne de commande, mais je vous suggÃ¨re dâ€™utiliser GitHub desktop (qui fonctionne aussi sur GitLab). Github desktop (ou tout auter logiciel de gestion de git) vous permettra dâ€™abord de cloner un rÃ©pertoire en ligne. Le clonage vous permet de crÃ©er une copie locale du rÃ©pertoire. Figure 5.8: Cloner dÃ©pÃ´t avec GitHub Figure 5.9: Cloner dÃ©pÃ´t avec GitLab Une fois que le dÃ©pÃ´t est clonÃ©, il est sur voter ordinateur. Lorsque vous effectuez un changement, vous devez commettre (commit), puis envoyer (push) vos changements vers le dÃ©pÃ´t en ligne. Pour que votre document markdown soit lisible par GitHub et GitLab, il doit Ãªtre exportÃ© sous forme de github_document. Un fichier .md sera crÃ©Ã©, et inclura les dÃ©tails de votre document de calculs. Figure 5.10: Commettre et dÃ©ployer un dÃ©pÃ´t avec GitHub Lâ€™interface de GitHub Desktop vous permet de revenir en arriÃ¨re en Ã©liminant des commits prÃ©cÃ©dents. Figure 5.11: Revenir en arriÃ¨re avec GitHub desktop Vous pourrez ajouter des collaborateurs Ã  votre dÃ©pÃ´t, pour que plusieurs personnes travaillent de front sur un mÃªme dÃ©pÃ´t. Il est aussi possible de crÃ©er une branche dâ€™un dÃ©pÃ´t, fusionner la branche de dÃ©veloppement avec la branche principale, commenter les codes, suggÃ©rer des changements, etc., mais cela sort du cadre dâ€™un cours sur la reproductibilitÃ©. Enfin, pour renvoyer un article vers votre matÃ©riel supplÃ©mentaire, insÃ©rez le lien dans la section mÃ©thodologie. Il peut sâ€™agit du lien complet, ou bien dâ€™un lien raccourci avec git.io. Par exemple, The data and the R code used to compute the results are both available as supplementary material at https://git.io/fhHEj. Notez que RStudio offre une interface pour utiliser git via un onglet afichÃ© en haut Ã  droite dans lâ€™affichage par dÃ©faut. Ne lâ€™ayant jamais utilisÃ©, et je ne me sens pas Ã  lâ€™aise dâ€™en suggÃ©rer lâ€™utilisation, mais libre Ã  vous dâ€™explorer cet outil et de vous lâ€™approprier! Figure 5.12: Lâ€™outil Git de RStudio 5.3 Introduction Ã  Pakrat ğŸ“¦ğŸ€ Alors que les modules sont continuellement mis Ã  jour, on doit sâ€™assurer que lâ€™on sache exactement quelle version a Ã©tÃ© utilisÃ©e si lâ€™on dÃ©sire Ãªtre stricte sur la reproductibilitÃ©. Lorsque je rÃ©vise un article, je demande Ã  ce que le nom des modules utilisÃ©s et leur numÃ©ro de version soient explicitement citÃ©s et rÃ©fÃ©rencÃ©s. Par exemple, dans un article sur lâ€™analyse de compositions foliaires de laitues inoculÃ©es par une bactÃ©rie, jâ€™Ã©crivais: Computations were performed in the R statistical language version 3.4.1 (R Development Core Team, 2017). The main packages used in the data analysis workflow were the vegan package version 2.4-3 (Oksanen et al., 2017) for ordination, the compositions package version 1.40-1 (van den Boogaart and Tolosana-Delgado, 2013) for ilr transformations, the nlme version 3.1-131 (Pinheiro et al., 2017) package to compute the random experimental effect, the mvoutlier package version 2.0.8 (Filzmoser and Gschwandtner, 2017) for multivariate outlier detection, and the ggplot2 package version 2.2.1 (Wickham and Chang, 2017) for data visualization. The data and computations are publicly available at https://github.com/essicolo/Nicolas-et-al_Infected-lettuce-ionomics. Nicolas et al., 2019 De cette maniÃ¨re, une personne (que ce soit vos collÃ¨gues, quiconque voudra auditer ou Ã©valuer votre code ou vous-mÃªme dans le futur) pourra reproduire le code publiÃ© sur GitHub en installant les versions de R et des modules citÃ©s. Mais cela est fastidieux. Câ€™est pourquoi lâ€™Ã©quipe de RStudio (oui, encore ceux-lÃ ) ont dÃ©veloppÃ© le module packrat, qui permet dâ€™installer les modules Ã  mÃªme voter dossier de projet (le dossier contenant le fichier .Rproj). Pour lâ€™utiliser Ã  tout moment en cours de projet, Figure 5.13: Lâ€™outil Packrat de RStudio Le .gitignore contient tous les documents et les types de documents qui sont ignorÃ©s par git. Lâ€™option par dÃ©faut est dâ€™ignorer le dossier lib, qui contient les modules installÃ©s, mais de garder le dossier src, qui contient la source des modules non installÃ©s (qui devront Ãªtre installÃ©s par les autres personnes utilisant votre projet). Mieux vaut garder les options par dÃ©faut. Initialiser Packrat revient Ã  scanner vos documents de projet pour trouver les modules utilisÃ©s et crÃ©er un paquet contenant tout cela Ã  mÃªme votre projet, dans un dossier packrat. ğŸ“ rstats |-ğŸ“ data |-ğŸ“ images |-ğŸ“ lib |-ğŸ“ packrat |-ğŸ“ tables ğŸ“„ sentier-d-or.Rproj ğŸ“„ stats.Rmd ğŸ“„ README.md Ce dossier contiendra tout ce quâ€™il faut pour utiliser les modules du projet dâ€™une personne que lâ€™on nommera Leto. Lorsquâ€™une autre personne, appellons-la Ghanima, utilisera le projet de Leto, RStudio vÃ©rifiera si le module packrat est bien installÃ©, et lâ€™installera sâ€™il ne lâ€™est pas (Leto et Ghanima sont deux personnage de la sÃ©rie de romans Dune). Pour utiliser les modules du projet et non pas les modules de son ordinateur, Ghanima lancera la fonction packrat::restore(). Si Leto dÃ©cide de mettre Ã  jour ses modules en cours de projet, il lancera la fonction packrat::snapshot() pour que ces nouveaux modules soit intÃ©grÃ©s Ã  son projet. Lorsque Leto commettra (commit) ses changements dans git et les publiera (push) sur GitHub, puis lorsque Ghanima mettra Ã  jour (fetch) son dÃ©pÃ´t local git liÃ© au dÃ©pÃ´t GitHub, elle devra Ã  nouveau lancer packrat::restore() pour que les modules soient bel et bien ceux utilisÃ©s par Leto. 5.4 Pour terminer, le reprex Lorsque jâ€™ai dÃ©couvert un bogue dans le module weathercan, jâ€™ai ouvert une issue sur GitHub en indiquant le message dâ€™erreur obtenu, en espÃ©rant que lâ€™origine du bogue puisse Ãªtre facilement dÃ©duit. Un dÃ©veloppeur de weathercan mâ€™a demandÃ© un reprex. Jâ€™ai Ã©tÃ© dÃ©Ã§u lorsque jâ€™ai compris que le reprex nâ€™Ã©tait pas une espÃ¨ce de dinosaure, mais plutÃ´t un exemple reproductible (reproducible example). Jâ€™aissayÃ© dâ€™isoex**r le problÃ¨me pour reproduire lâ€™erreur avec le minimum de code possible. Ã€ partir dâ€™un code de plus de 7000 lignes (les prÃ©sentes notes de cours), jâ€™en suis arrivÃ© Ã  ceci: stations &lt;- data.frame(A = 1) library(&quot;weathercan&quot;) mont_bellevue &lt;- weather_dl(station_ids = c(5397, 48371), start = &quot;2019-02-01&quot;, end = &quot;2019-02-07&quot;, interval = &quot;hour&quot;, verbose = TRUE) , qui me retournait lâ€™erreur Getting station: 5397 Formatting station data: 5397 Error in strptime(xx, f, tz = tz) : valeur &#39;tz&#39; incorrecte Le bogue: la fonction weather_dl() utilisait Ã  lâ€™interne un objet nommÃ© stations, qui entrait en conflit avec un objet stations sâ€™il Ã©tait dÃ©fini hors de la fonction. SynthÃ©tiser une question nâ€™est pas facile (crÃ©er cet exemple reprductible mâ€™a pris prÃ¨s de 2 hoeures de travail). Mais rÃ©pondre Ã  une question non synthÃ©tisÃ©e, câ€™est encore plus difficile. Câ€™est pourquoi on vous demandera systÃ©matiquement un reprex lorsque vous posez une question liÃ©e Ã  une erreur systÃ©matique, le plus souvent en programmation. Un exemple reproductible permet Ã  quelquâ€™un de recrÃ©er lâ€™erreur que vous avez obtenue simplement en copiant-collant voter code. - Hadley Wickham Selon Hadley Wickham (gourou de R), un reprex devrait comprendre quatre Ã©lÃ©ments (je joue Ã  lâ€™hÃ©rÃ©tique en me permettant dâ€™adapter le document du gourou): Les modules devraient Ãªtre chargÃ©s en dÃ©but de code. Puis vous chargez des donnÃ©es, qui peuvent Ãªtre des donnÃ©es dâ€™exemple ou des donnÃ©es incluses Ã  mÃªme le code R (comme des donnÃ©es gÃ©nÃ©rÃ©es au hasard). Assurez-vous que voter code est un exemple minimal (retirer le superflu) et quâ€™il soit facilement lisible. Incluez la sortie de la fonction sessionInfo(), qui indique la plateforme matÃ©rielle et logicielle sur laquelle vous avez gÃ©nÃ©rÃ© lâ€™erreur. Ceci est important en particulier sâ€™il sâ€™agit dâ€™un bogue. Lorsque vous pensez avoir gÃ©nÃ©rÃ© votre reprex, redÃ©marrez R (Session &gt; Restart R dans RStudio), puis lancez votre code pour vous assurer que lâ€™erreur puisse Ãªtre gÃ©nÃ©rÃ©e dans un nouvel environnement tout propre. "],
["chapitre-biostats.html", "6 Biostatistiques 6.1 Populations et Ã©chantillons 6.2 Les variables 6.3 Les probabilitÃ©s 6.4 Les distributions 6.5 Statistiques descriptives 6.6 Tests dâ€™hypothÃ¨ses Ã  un et deux Ã©chantillons 6.7 Lâ€™analyse de variance 6.8 Les modÃ¨les statistiques", " 6 Biostatistiques ï¸Â Objectifs spÃ©cifiques: Ã€ la fin de ce chapitre, vous serez en mesure de dÃ©finir les concepts de base en statistique: population, Ã©chantillon, variable, probabilitÃ© et distribution serez en mesure de calculer des statistiques descriptives de base: moyenne et Ã©cart-type, quartiles, maximum et minimum comprendrez les notions de test dâ€™hypothÃ¨se, dâ€™effet et de p-value, ainsi quâ€™Ã©viter les erreurs communes dans leur interprÃ©tation saurez effectuer une modÃ©lisation statistique linÃ©aire simple, multiple et mixte, entre autre sur des catÃ©gories saurez effectuer une modÃ©lisation statistique non linÃ©aire simple, multiple et mixte Aux chapitres prÃ©cÃ©dents, nous avons vu comment visualiser, organiser et manipuler des tableaux de donnÃ©es. La statistique est une collection de disciplines liÃ©es Ã  la collecte, lâ€™organisation, lâ€™analyse, lâ€™interprÃ©tation et la prÃ©sentation de donnÃ©es. Les biostatistiques est lâ€™application de ces disciplines Ã  la biosphÃ¨re. Dans Principles and procedures of statistics: A biometrical approach, Steel, Torie et Dickey (1997) dÃ©finissent les statistiques ainsi: Les statistiques forment la science, pure et appliquÃ©e, de la crÃ©ation, du dÃ©veloppement, et de lâ€™application de techniques par lesquelles lâ€™incertitude de lâ€™induction infÃ©rentielle peut Ãªtre Ã©valuÃ©e. (ma traduction) Alors que lâ€™infÃ©rence consiste Ã  gÃ©nÃ©raliser des observations sur des Ã©chantillons Ã  lâ€™ensemble dâ€™une population, lâ€™induction est un type de raisonnement qui permet de gÃ©nÃ©raliser des observations en thÃ©ories. Les statistiques permettent dâ€™Ã©valuer lâ€™incertitude dÃ©coulant du processus qui permet dâ€™abord de passer de lâ€™Ã©chantillon Ã  la population reprÃ©sentÃ©e par cet Ã©chantillon, puis de passer de cette reprÃ©sentation dâ€™une population en lois gÃ©nÃ©rales la concernant. La dÃ©finition de Whitlock et Schuluter (2015), dans The Analysis of Biological Data, est plus simple, insistant sur lâ€™infÃ©rence: La statistique est lâ€™Ã©tude des mÃ©thodes pour mesurer des aspects de populations Ã  partir dâ€™Ã©chantillons et pour quantifier lâ€™incertitude des mesures. (ma traduction) Les statistiques consistent Ã  faire du sens (anglicisme assumÃ©) avec des observations dans lâ€™objectif de rÃ©pondre Ã  une question que vous aurez formulÃ©e clairement, prÃ©alablement Ã  votre expÃ©rience. The more time I spend as The Statistician in the room, the more I think the best skill you can cultivate is the ability to remain calm and repeatedly ask â€œWhat question are you trying to answer?â€ â€” Bryan Howie (@bryan_howie) 13 dÃ©cembre 2018 Le flux de travail conventionnel consiste Ã  collecter des Ã©chantillons, transformer les donnÃ©es, effectuer des tests, analyser les rÃ©sultats, les interprÃ©ter et les visualiser. Bien que ces tÃ¢ches soient complexes, en particulier en ce qui a trait aux tests statistiques, la plupart des opÃ©rations statistiques peuvent Ãªtre effectuÃ©es sans lâ€™assistance de statisticien.ne.sâ€¦ Ã  condition de comprendre suffisamment les concepts utilisÃ©s. Ce chapitre Ã  lui seul est trop court pour permettre dâ€™intÃ©grer toutes les connaissances nÃ©cessaires Ã  une utilisation raisonnÃ©e des statistiques, mais fourni les bases pour aller plus loin. Notez que les erreurs dâ€™interprÃ©tation statistiques sont courantes et la consultation de spÃ©cialistes nâ€™est souvent pas un luxe. Dans ce chapitre, nous verrons comment rÃ©pondre correctement Ã  une question valide et adÃ©quate avec lâ€™aide dâ€™outils de calcul scientifique. Nous couvrirons les notions de bases des distributions et des variables alÃ©atoires qui nous permettront dâ€™effectuer des tests statistiques communs avec R. Nous couvrirons aussi les erreurs communÃ©ment commises en recherche acadÃ©mique et les moyens simples de les Ã©viter. Ce chapitre est une introduction aux statistiques avec R, et ne remplacera pas un bon cours de stats. En plus des modules de base de R nous utiliserons les modules de la tidyverse, le module de donnÃ©es agricoles agridat, ainsi que le module nlme spÃ©cialisÃ© pour la modÃ©lisation mixte. Avant de survoler les applications statistiques avec R, je vais dâ€™abord et rapidement prÃ©senter quelques notions importantes en statistiques : populations et Ã©chantillons, variables, probabilitÃ©s et distributions. Nous allons effectuer des tests dâ€™hypothÃ¨se univariÃ©s (notamment les tests de t et les analyses de variance) et dÃ©tailler la notion de p-value. Mais avant tout, je vais mâ€™attarder plus longuement aux modÃ¨les linÃ©aires gÃ©nÃ©ralisÃ©s, incluant en particulier des effets fixes et alÃ©atoires (modÃ¨les mixtes), qui fournissent une trousse dâ€™analyse polyvalente en analyse multivariÃ©e. Je terminerai avec les perspectives multivariÃ©es que sont les matrices de covariance et de corrÃ©lation. 6.1 Populations et Ã©chantillons Le principe dâ€™infÃ©rence consiste Ã  gÃ©nÃ©raliser des conclusions Ã  lâ€™Ã©chelle dâ€™une population Ã  partir dâ€™Ã©chantillons issus de cette population. Alors quâ€™une population contient tous les Ã©lÃ©ments Ã©tudiÃ©s, un Ã©chantillon dâ€™une population est une observation unique. Une expÃ©rience bien conÃ§ue fera en sorte que les Ã©chantillons sont reprÃ©sentatifs de la population qui, la plupart du temps, ne peut Ãªtre observÃ©e entiÃ¨rement pour des raisons pratiques. Les principes dâ€™expÃ©rimentation servant de base Ã  la conception dâ€™une bonne mÃ©thodologie sont prÃ©sentÃ©s dans le cours Dispositifs expÃ©rimentaux (BVG-7002). Ã‰galement, je recommande le livre Principes dâ€™expÃ©rimentation: planification des expÃ©riences et analyse de leurs rÃ©sultats de Pierre Dagnelie (2012), disponible en ligne en format PDF. Un bon aperÃ§u des dispositifs expÃ©rimentaux est aussi prÃ©sentÃ© dans Introductory Statistics with R, de Peter Dalgaard (2008). Une population est Ã©chantillonnÃ©e pour induire des paramÃ¨tres: un rendement typique dans des conditions mÃ©tÃ©orologiques, Ã©daphiques et managÃ©riales donnÃ©es, la masse typique des faucons pÃ¨lerins, mÃ¢les et femelles, le microbiome typique dâ€™un sol agricole ou forestier, etc. Une statistique est une estimation dâ€™un paramÃ¨tre calculÃ©e Ã  partir des donnÃ©es, par exemple une moyenne et un Ã©cart-type. Par exemple, la moyenne (\\(\\mu\\)) et lâ€™Ã©cart-type (\\(\\sigma\\)) dâ€™une population sont estimÃ©s par les moyennes (\\(\\bar{x}\\)) et Ã©carts-types (\\(s\\)) calculÃ©s sur les donnÃ©es issues de lâ€™Ã©chantillonnage. Chaque paramÃ¨tre est liÃ©e Ã  une perspective que lâ€™on dÃ©sire connaÃ®tre chez une population. Ces angles dâ€™observations sont les variables. 6.2 Les variables Nous avons abordÃ© au chapitre 4 la notion de variable par lâ€™intermÃ©diaire dâ€™une donnÃ©e. Une variable est lâ€™observation dâ€™une caractÃ©ristique dÃ©crivant un Ã©chantillon et qui est susceptible de varier dâ€™un Ã©chantillon Ã  un autre. Si les observations varient en effet dâ€™un Ã©chantillon Ã  un autre, on parlera de variable alÃ©atoire. MÃªme le hasard est rÃ©gi par certaines lois: ce qui est alÃ©atoire dans une variable peut Ãªtre dÃ©crit par des lois de probabilitÃ©, que nous verrons plus bas. Mais restons aux variables pour lâ€™instant. Par convention, on peut attribuer aux variables un symbole mathÃ©matique. Par exemple, on peut donner Ã  la masse volumique dâ€™un sol (qui est le rÃ©sultat dâ€™une mÃ©thodologie prÃ©cise) le symbole \\(\\rho\\). Lorsque lâ€™on attribue une valeur Ã  \\(\\rho\\), on parle dâ€™une donnÃ©e. Chaque donnÃ©e dâ€™une observation a un indice qui lui est propre, que lâ€™on dÃ©signe souvent par \\(i\\), que lâ€™on place en indice \\(\\rho_i\\). Pour la premiÃ¨re donnÃ©e, on a \\(i=1\\), donc \\(\\rho_1\\). Pour un nombre \\(n\\) dâ€™Ã©chantillons, on aura \\(\\rho_1\\), \\(\\rho_2\\), \\(\\rho_3\\), â€¦, \\(\\rho_n\\), formant le vecteur \\(\\rho = \\left[\\rho_1, \\rho_2, \\rho_3, ..., \\rho_n \\right]\\). En R, une variable est associÃ©e Ã  un vecteur ou une colonne dâ€™un tableau. rho &lt;- c(1.34, 1.52, 1.26, 1.43, 1.39) # matrice 1D data &lt;- data.frame(rho = rho) # tableau data ## rho ## 1 1.34 ## 2 1.52 ## 3 1.26 ## 4 1.43 ## 5 1.39 Il existe plusieurs types de variables, qui se regroupe en deux grandes catÃ©gories: les variables quantitatives et les variables qualitatives. 6.2.1 Variables quantitatives Ces variables peuvent Ãªtre continues dans un espace Ã©chantillonnal rÃ©el ou discrÃ¨tes dans un espace Ã©chantillonnal ne considÃ©rant que des valeurs fixes. Notons que la notion de nombre rÃ©el est toujours une approximation en sciences expÃ©rimentales comme en calcul numÃ©rique, Ã©tant donnÃ©e que lâ€™on est limitÃ© par la prÃ©cision des appareils comme par le nombre dâ€™octets Ã  utiliser. Bien que les valeurs fixes des distributions discrÃ¨tes ne soient pas toujours des valeurs entiÃ¨res, câ€™est bien souvent le cas en biostatistiques comme en dÃ©mographie, oÃ¹ les dÃ©comptes dâ€™individus sont souvent prÃ©sents (et oÃ¹ la notion de fraction dâ€™individus nâ€™est pas acceptÃ©e). 6.2.2 Variables qualitatives On exprime parfois quâ€™une variable qualitative est une variable impossible Ã  mesurer numÃ©riquement: une couleur, lâ€™appartenance Ã  espÃ¨ce ou Ã  une sÃ©rie de sol. Pourtant, dans bien des cas, les variables qualitatives peut Ãªtre encodÃ©es en variables quantitatives. Par exemple, on peut accoler des pourcentages de sable, limon et argile Ã  un loam sableux, qui autrement est dÃ©crit par la classe texturale dâ€™un sol. Pour une couleur, on peut lui associer des pourcentages de rouge, vert et bleu, ainsi quâ€™un ton. En ce qui a trait aux variables ordonnÃ©es, il est possible de supposer un Ã©talement. Par exemple, une variable dâ€™intensitÃ© faible-moyenne-forte peut Ãªtre transformÃ©e linÃ©airement en valeurs quantitatives -1, 0 et 1. Attention toutefois, lâ€™Ã©talement peut parfois Ãªtre quadratique ou logarithmique. Les sÃ©ries de sol peuvent Ãªtre encodÃ©es par la proportion de gleyfication (Parent et al., 2017). Quant aux catÃ©gories difficilement transformables en quantitÃ©s, on pourra passer par lâ€™encodage catÃ©goriel, souvent appelÃ© dummyfication, qui nous verrons plus loin. 6.3 Les probabilitÃ©s Â« Nous sommes si Ã©loignÃ©s de connaÃ®tre tous les agens de la nature, et leurs divers modes dâ€™action ; quâ€™il ne serait pas philosophique de nier les phÃ©nomÃ¨nes, uniquement parce quâ€™ils sont inexplicables dans lâ€™Ã©tat actuel de nos connaissances. Seulement, nous devons les examiner avec une attention dâ€™autant plus scrupuleuse, quâ€™il paraÃ®t plus difficile de les admettre ; et câ€™est ici que le calcul des probabilitÃ©s devient indispensable, pour dÃ©terminer jusquâ€™Ã  quel point il faut multiplier les observations ou les expÃ©riences, afin dâ€™obtenir en faveur des agens quâ€™elles indiquent, une probabilitÃ© supÃ©rieure aux raisons que lâ€™on peut avoir dâ€™ailleurs, de ne pas les admettre. Â» â€” Pierre-Simon de Laplace Une probabilitÃ© est la vraisemblance quâ€™un Ã©vÃ¨nement se rÃ©alise chez un Ã©chantillon. Les probabilitÃ©s forment le cadre des systÃ¨mes stochastiques, câ€™est-Ã -dire des systÃ¨mes trop complexes pour en connaÃ®tre exactement les aboutissants, auxquels on attribue une part de hasard. Ces systÃ¨mes sont prÃ©dominants dans les processus vivants. On peut dÃ©gager deux perspectives sur les probabilitÃ©s: lâ€™une passe par une interprÃ©tation frÃ©quentielle, lâ€™autre bayÃ©sienne. Lâ€™interprÃ©tation frÃ©quentielle reprÃ©sente la frÃ©quence des occurrences aprÃ¨s un nombre infini dâ€™Ã©vÃ¨nements. Par exemple, si vous jouez Ã  pile ou face un grand nombre de fois, le nombre de pile sera Ã©gal Ã  la moitiÃ© du nombre de lancÃ©s. Il sâ€™agit de lâ€™interprÃ©tation communÃ©ment utilisÃ©e. Lâ€™interprÃ©tation bayÃ©sienne vise Ã  quantifier lâ€™incertitude des phÃ©nomÃ¨nes. Dans cette perspective, plus lâ€™information sâ€™accumule, plus lâ€™incertitude diminue. Cette approche gagne en notoriÃ©tÃ© notamment parce quâ€™elle permet de dÃ©crire des phÃ©nomÃ¨nes qui, intrinsÃ¨quement, ne peuvent Ãªtre rÃ©pÃ©tÃ©s infiniment (absence dâ€™asymptote), comme celles qui sont bien dÃ©finis dans le temps ou sur des populations limitÃ©s. Lâ€™approche frÃ©quentielle teste si les donnÃ©es concordent avec un modÃ¨le du rÃ©el, tandis que lâ€™approche bayÃ©sienne Ã©value la probabilitÃ© que le modÃ¨le soit rÃ©el. Une erreur courante consiste Ã  aborder des statistiques frÃ©quentielles comme des statistiques bayÃ©siennes. Par exemple, si lâ€™on dÃ©sire Ã©valuer la probabilitÃ© de lâ€™existence de vie sur Mars, on devra passer par le bayÃ©sien, car avec les stats frÃ©quentielles, lâ€™on devra plutÃ´t conclure si les donnÃ©es sont conformes ou non avec lâ€™hypothÃ¨se de la vie sur Mars (exemple tirÃ©e du blogue Dynamic Ecology). Des rivalitÃ©s factices sâ€™installent enter les tenants des diffÃ©rentes approches, dont chacune, en rÃ©alitÃ©, rÃ©pond Ã  des questions diffÃ©rentes dont il convient rÃ©flÃ©chir sur les limitations. Bien que les statistiques bayÃ©siennes soient de plus en plus utilisÃ©es, nous ne couvrirons dans ce chapitre que lâ€™approche frÃ©quentielle. Lâ€™approche bayÃ©sienne est nÃ©anmoins traitÃ©e dans le chapitre 7. 6.4 Les distributions Une variable alÃ©atoire peut prendre des valeurs selon des modÃ¨les de distribution des probabilitÃ©s. Une distribution est une fonction mathÃ©matique dÃ©crivant la probabilitÃ© dâ€™observer une sÃ©rie dâ€™Ã©vÃ¨nements. Ces Ã©vÃ¨nements peuvent Ãªtre des valeurs continues, des nombres entiers, des catÃ©gories, des valeurs boolÃ©ennes (Vrai/Faux), etc. DÃ©pendemment du type de valeur et des observations obtenues, on peut associer des variables Ã  diffÃ©rentes lois de probabilitÃ©. Toujours, lâ€™aire sous la courbe dâ€™une distribution de probabilitÃ© est Ã©gale Ã  1. En statistiques infÃ©rentielles, les distributions sont les modÃ¨les, comprenant certains paramÃ¨tres comme la moyenne et la variance pour les distributions normales, Ã  partir desquelles les donnÃ©es sont gÃ©nÃ©rÃ©es. Il existe deux grandes familles de distribution: discrÃ¨tes et continues. Les distributions discrÃ¨tes sont contraintes Ã  des valeurs prÃ©dÃ©finies (finies ou infinies), alors que les distributions continues prennent nÃ©cessairement un nombre infini de valeur, dont la probabilitÃ© ne peut pas Ãªtre Ã©valuÃ©e ponctuellement, mais sur un intervalle. Lâ€™espÃ©rance mathÃ©matique est une fonction de tendance centrale, souvent dÃ©crite par un paramÃ¨tre. Il sâ€™agit de la moyenne dâ€™une population pour une distribution normale. La variance, quant Ã  elle, dÃ©crit la variabilitÃ© dâ€™une population, i.e.Â son Ã©talement autour de lâ€™espÃ©rance. Pour une distribution normale, la variance dâ€™une population est aussi appelÃ©e variance, souvent prÃ©sentÃ©e par lâ€™Ã©cart-type. 6.4.1 Distribution binomiale En tant que scÃ©nario Ã  deux issues possibles, des tirages Ã  pile ou face suivent une loi binomiale, comme toute variable boolÃ©enne prenant une valeur vraie ou fausse. En biostatistiques, les cas communs sont la prÃ©sence/absence dâ€™une espÃ¨ce, dâ€™une maladie, dâ€™un trait phylogÃ©nÃ©tique, ainsi que les catÃ©gories encodÃ©es. Lorsque lâ€™opÃ©ration ne comprend quâ€™un seul Ã©chantillon (i.e.Â un seul tirage Ã  pile ou face), il sâ€™agit dâ€™un cas particulier dâ€™une loi binomiale que lâ€™on nomme une loi de Bernouilli. Pour 25 tirages Ã  pile ou face indÃ©pendants (i.e.Â dont lâ€™ordre des tirages ne compte pas), on peut dessiner une courbe de distribution dont la somme des probabilitÃ©s est de 1. La fonction dbinom est une fonction de distribution de probabilitÃ©s. Les fonctions de distribution de probabilitÃ©s discrÃ¨tes sont appelÃ©es des fonctions de masse. library(&quot;tidyverse&quot;) x &lt;- 0:25 y &lt;- dbinom(x = x, size = 25, prob = 0.5) print(paste(&#39;La somme des probabilitÃ©s est de&#39;, sum(y))) ## [1] &quot;La somme des probabilitÃ©s est de 1&quot; ggplot(data = tibble(x, y), mapping = aes(x, y)) + geom_segment(aes(x = x, xend = x, y = 0, yend = y), color = &quot;grey50&quot;) + geom_point() 6.4.2 Distribution de Poisson La loi de Poisson (avec un P majuscule, introduite par le mathÃ©maticien franÃ§ais SimÃ©on Denis Poisson et non pas lâ€™animal) dÃ©crit des distributions discrÃ¨tes de probabilitÃ© dâ€™un nombre dâ€™Ã©vÃ¨nements se produisant dans lâ€™espace ou dans le temps. Les distributions de Poisson dÃ©crive ce qui tient du dÃ©compte. Il peut sâ€™agir du nombre de grenouilles traversant une rue quotidiennement, du nombre de plants dâ€™asclÃ©piades se trouvant sur une terre cultivÃ©e, ou du nombre dâ€™Ã©vÃ¨nements de prÃ©cipitation au mois de juin, etc. La distribution de Poisson nâ€™a quâ€™un seul paramÃ¨tre, \\(\\lambda\\), qui dÃ©crit tant la moyenne des dÃ©comptes. Par exemple, en un mois de 30 jours, et une moyenne de 8 Ã©vÃ¨nements de prÃ©cipitation pour ce mois, on obtient la distribution suivante. x &lt;- 1:30 y &lt;- dpois(x, lambda = 8) print(paste(&#39;La somme des probabilitÃ©s est de&#39;, sum(y))) ## [1] &quot;La somme des probabilitÃ©s est de 0.999664536835124&quot; ggplot(data = data.frame(x, y), mapping = aes(x, y)) + geom_segment(aes(x = x, xend = x, y = 0, yend = y), color = &quot;grey50&quot;) + geom_point() 6.4.3 Distribution uniforme La distribution la plus simple est probablement la distribution uniforme. Si la variable est discrÃ¨te, chaque catÃ©gorie est associÃ©e Ã  une probabilitÃ© Ã©gale. Si la variable est continue, la probabilitÃ© est directement proportionnelle Ã  la largeur de lâ€™intervalle. On utilise rarement la distribution uniforme en biostatistiques, sinon pour dÃ©crire des a priori vagues pour lâ€™analyse bayÃ©sienne (ce sujet est traitÃ© dans le chapitre 7). Nous utilisons la fonction dunif. Ã€ la diffÃ©rence des distributions discrÃ¨tes, les fonctions de distribution de probabilitÃ©s continues sont appelÃ©es des fonctions de densitÃ© dâ€™une loi de probabilitÃ© (probability density function). increment &lt;- 0.01 x &lt;- seq(-4, 4, by = increment) y1 &lt;- dunif(x, min = -3, max = 3) y2 &lt;- dunif(x, min = -2, max = 2) y3 &lt;- dunif(x, min = -1, max = 1) print(paste(&#39;La somme des probabilitÃ©s est de&#39;, sum(y3 * increment))) ## [1] &quot;La somme des probabilitÃ©s est de 1.005&quot; gg_unif &lt;- data.frame(x, y1, y2, y3) %&gt;% gather(variable, value, -x) ggplot(data = gg_unif, mapping = aes(x = x, y = value)) + geom_line(aes(colour = variable)) 6.4.4 Distribution normale La plus rÃ©pandue de ces lois est probablement la loi normale, parfois nommÃ©e loi gaussienne et plus rarement loi laplacienne. Il sâ€™agit de la distribution classique en forme de cloche. La loi normale est dÃ©crite par une moyenne, qui dÃ©signe la tendance centrale, et une variance, qui dÃ©signe lâ€™Ã©talement des probabilitÃ©s autour de la moyenne. La racine carrÃ©e de la variance est lâ€™Ã©cart-type. Les distributions de mesures exclusivement positives (comme le poids ou la taille) sont parfois avantageusement approximÃ©es par une loi log-normale, qui est une loi normale sur le logarithme des valeurs: la moyenne dâ€™une loi log-normale est la moyenne gÃ©omÃ©trique. increment &lt;- 0.01 x &lt;- seq(-10, 10, by = increment) y1 &lt;- dnorm(x, mean = 0, sd = 1) y2 &lt;- dnorm(x, mean = 0, sd = 2) y3 &lt;- dnorm(x, mean = 0, sd = 3) print(paste(&#39;La somme des probabilitÃ©s est de&#39;, sum(y3 * increment))) ## [1] &quot;La somme des probabilitÃ©s est de 0.999147010743368&quot; gg_norm &lt;- data.frame(x, y1, y2, y3) %&gt;% gather(variable, value, -x) ggplot(data = gg_norm, mapping = aes(x = x, y = value)) + geom_line(aes(colour = variable)) Quelle est la probabilitÃ© dâ€™obtenir le nombre 0 chez une observation continue distribuÃ©e normalement dont la moyenne est 0 et lâ€™Ã©cart-type est de 1? RÃ©ponse: 0. La loi normale Ã©tant une distribution continue, les probabilitÃ©s non-nulles ne peuvent Ãªtre calculÃ©s que sur des intervalles. Par exemple, la probabilitÃ© de retrouver une valeur dans lâ€™intervalle entre -1 et 2 est calculÃ©e en soustrayant la probabilitÃ© cumulÃ©e Ã  -1 de la probabilitÃ© cumulÃ©e Ã  2. increment &lt;- 0.01 x &lt;- seq(-5, 5, by = increment) y &lt;- dnorm(x, mean = 0, sd = 1) prob_between &lt;- c(-1, 2) gg_norm &lt;- data.frame(x, y) gg_auc &lt;- gg_norm %&gt;% filter(x &gt; prob_between[1], x &lt; prob_between[2]) %&gt;% rbind(c(prob_between[2], 0)) %&gt;% rbind(c(prob_between[1], 0)) ggplot(data.frame(x, y), aes(x, y)) + geom_polygon(data = gg_auc, fill = &#39;#71ad50&#39;) + # #71ad50 est un code de couleur format hexadÃ©cimal geom_line() prob_norm_between &lt;- pnorm(q = prob_between[2], mean = 0, sd = 1) - pnorm(q = prob_between[1], mean = 0, sd = 1) print(paste(&quot;La probabilitÃ© d&#39;obtenir un nombre entre&quot;, prob_between[1], &quot;et&quot;, prob_between[2], &quot;est d&#39;environ&quot;, round(prob_norm_between, 2) * 100, &quot;%&quot;)) ## [1] &quot;La probabilitÃ© d&#39;obtenir un nombre entre -1 et 2 est d&#39;environ 82 %&quot; La courbe normale peut Ãªtre utile pour Ã©valuer la distribution dâ€™une population. Par exemple, on peut calculer les limites de rÃ©gion sur la courbe normale qui contient 95% des valeurs possibles en tranchant 2.5% de part et dâ€™autre de la moyenne. Il sâ€™agit ainsi de lâ€™intervalle de confiance sur la dÃ©viation de la distribution. increment &lt;- 0.01 x &lt;- seq(-5, 5, by = increment) y &lt;- dnorm(x, mean = 0, sd = 1) alpha &lt;- 0.05 prob_between &lt;- c(qnorm(p = alpha/2, mean = 0, sd = 1), qnorm(p = 1 - alpha/2, mean = 0, sd = 1)) gg_norm &lt;- data.frame(x, y) gg_auc &lt;- gg_norm %&gt;% filter(x &gt; prob_between[1], x &lt; prob_between[2]) %&gt;% rbind(c(prob_between[2], 0)) %&gt;% rbind(c(prob_between[1], 0)) ggplot(data = data.frame(x, y), mapping = aes(x, y)) + geom_polygon(data = gg_auc, fill = &#39;#71ad50&#39;) + # #71ad50 est un code de couleur format hexadÃ©cimal geom_line() + geom_text(data = data.frame(x = prob_between, y = c(0, 0), labels = round(prob_between, 2)), mapping = aes(label = labels)) On pourrait aussi Ãªtre intÃ©ressÃ© Ã  lâ€™intervalle de confiance sur la moyenne. En effet, la moyenne suit aussi une distribution normale, dont la tendance centrale est la moyenne de la distribution, et dont lâ€™Ã©cart-type est notÃ© erreur standard. On calcule cette erreur en divisant la variance par le nombre dâ€™observation, ou en divisant lâ€™Ã©cart-type par la racine carrÃ©e du nombre dâ€™observations. Ainsi, pour 10 Ã©chantillons: increment &lt;- 0.01 x &lt;- seq(-5, 5, by = increment) y &lt;- dnorm(x, mean = 0, sd = 1) alpha &lt;- 0.05 prob_between &lt;- c(qnorm(p = alpha/2, mean = 0, sd = 1) / sqrt(10), qnorm(p = 1 - alpha/2, mean = 0, sd = 1) / sqrt(10)) gg_norm &lt;- data.frame(x, y) gg_auc &lt;- gg_norm %&gt;% filter(x &gt; prob_between[1], x &lt; prob_between[2]) %&gt;% rbind(c(prob_between[2], 0)) %&gt;% rbind(c(prob_between[1], 0)) ggplot(data = data.frame(x, y), mapping = aes(x, y)) + geom_polygon(data = gg_auc, fill = &#39;#71ad50&#39;) + # #71ad50 est un code de couleur format hexadÃ©cimal geom_line() + geom_text(data = data.frame(x = prob_between, y = c(0, 0), labels = round(prob_between, 2)), mapping = aes(label = labels)) 6.5 Statistiques descriptives On a vu comment gÃ©nÃ©rer des statistiques sommaires en R avec la fonction summary(). Reprenons les donnÃ©es dâ€™iris. data(&quot;iris&quot;) summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 setosa :50 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 versicolor:50 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 virginica :50 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 Pour prÃ©cisÃ©ment effectuer une moyenne et un Ã©cart-type sur un vecteur, passons par les fonctions mean() et sd(). mean(iris$Sepal.Length) ## [1] 5.843333 sd(iris$Sepal.Length) ## [1] 0.8280661 Pour effectuer un sommaire de tableau pilotÃ© par une fonction, nous passons par la gamme de fonctions summarise(), de dplyr. Dans ce cas, avec group_by(), nous fragmentons le tableau par espÃ¨ce pour effectuer un sommaire sur toutes les variables. iris %&gt;% group_by(Species) %&gt;% summarise_all(mean) ## # A tibble: 3 x 5 ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.01 3.43 1.46 0.246 ## 2 versicolor 5.94 2.77 4.26 1.33 ## 3 virginica 6.59 2.97 5.55 2.03 Vous pourriez Ãªtre intÃ©ressÃ© par les quartiles Ã  25, 50 et 75%. Mais la fonction summarise() nâ€™autorise que les fonctions dont la sortie est dâ€™un seul objet, alors faisons sorte que lâ€™objet soit une liste - lorsque lâ€™on imbrique une fonction funs, le tableau Ã  insÃ©rer dans la fonction est indiquÃ© par un .. iris %&gt;% group_by(Species) %&gt;% summarise_all(funs(list(quantile(.)))) ## # A tibble: 3 x 5 ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;fct&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 setosa &lt;dbl [5]&gt; &lt;dbl [5]&gt; &lt;dbl [5]&gt; &lt;dbl [5]&gt; ## 2 versicolor &lt;dbl [5]&gt; &lt;dbl [5]&gt; &lt;dbl [5]&gt; &lt;dbl [5]&gt; ## 3 virginica &lt;dbl [5]&gt; &lt;dbl [5]&gt; &lt;dbl [5]&gt; &lt;dbl [5]&gt; En mode programmation classique de R, on pourra gÃ©nÃ©rer les quartiles Ã  la piÃ¨ce. quantile(iris$Sepal.Length[iris$Species == &#39;setosa&#39;]) ## 0% 25% 50% 75% 100% ## 4.3 4.8 5.0 5.2 5.8 quantile(iris$Sepal.Length[iris$Species == &#39;versicolor&#39;]) ## 0% 25% 50% 75% 100% ## 4.9 5.6 5.9 6.3 7.0 quantile(iris$Sepal.Length[iris$Species == &#39;virginica&#39;]) ## 0% 25% 50% 75% 100% ## 4.900 6.225 6.500 6.900 7.900 La fonction table() permettra dâ€™obtenir des dÃ©comptes par catÃ©gorie, ici par plages de longueurs de sÃ©pales. Pour obtenir les proportions du nombre total, il sâ€™agit dâ€™encapsuler le tableau croisÃ© dans la fonction prop.table(). tableau_croise &lt;- table(iris$Species, cut(iris$Sepal.Length, breaks = quantile(iris$Sepal.Length))) tableau_croise ## ## (4.3,5.1] (5.1,5.8] (5.8,6.4] (6.4,7.9] ## setosa 35 14 0 0 ## versicolor 4 20 17 9 ## virginica 1 5 18 26 prop.table(tableau_croise) ## ## (4.3,5.1] (5.1,5.8] (5.8,6.4] (6.4,7.9] ## setosa 0.234899329 0.093959732 0.000000000 0.000000000 ## versicolor 0.026845638 0.134228188 0.114093960 0.060402685 ## virginica 0.006711409 0.033557047 0.120805369 0.174496644 6.6 Tests dâ€™hypothÃ¨ses Ã  un et deux Ã©chantillons Un test dâ€™hypothÃ¨se permet de dÃ©cider si une hypothÃ¨se est confirmÃ©e ou rejetÃ©e Ã  un seuil de probabilitÃ© prÃ©dÃ©terminÃ©. Cette section est inspirÃ©e du chapitre 5 de Dalgaard, 2008. Information: lâ€™hypothÃ¨se nulle. Les tests dâ€™hypothÃ¨se Ã©value des effets statistiques (qui ne sont pas nÃ©cessairement des effets de causalitÃ©). Lâ€™effet Ã  Ã©valuer peut Ãªtre celui dâ€™un traitement, dâ€™indicateurs mÃ©tÃ©orologiques (e.g.Â prÃ©cipitations totales, degrÃ©-jour, etc.), de techniques de gestion des paysages, etc. Une recherche est menÃ©e pour Ã©valuer lâ€™hypothÃ¨se que lâ€™on retrouve des diffÃ©rences entre des unitÃ©s expÃ©rimentales. Par convention, lâ€™hypothÃ¨se nulle (Ã©crite \\(H_0\\)) est lâ€™hypothÃ¨se quâ€™il nâ€™y ait pas dâ€™effet (câ€™est lâ€™hypothÃ¨se de lâ€™avocat du diable ğŸ˜ˆ) Ã  lâ€™Ã©chelle de la population (et non pas Ã  lâ€™Ã©chelle de lâ€™Ã©chantillon). Ã€ lâ€™inverse, lâ€™hypothÃ¨se alternative (Ã©crite \\(H_1\\)) est lâ€™hypothÃ¨se quâ€™il y ait un effet Ã  lâ€™Ã©chelle de la population. Ã€ titre dâ€™exercice en stats, on dÃ©bute souvent par en testant si deux vecteurs de valeurs continues proviennent de populations Ã  moyennes diffÃ©rentes ou si un vecteur de valeurs a Ã©tÃ© gÃ©nÃ©rÃ© Ã  partir dâ€™une population ayant une moyenne donner. Dans cette section, nous utiliserons la fonction t.test() pour les tests de t et la fonction wilcox.test() pour les tests de Wilcoxon (aussi appelÃ© de Mann-Whitney). 6.6.1 Test de t Ã  un seul Ã©chantillon Nous devons assumer, pour ce test, que lâ€™Ã©chantillon est recueillit dâ€™une population dont la distribution est normale, \\(\\mathcal{N} \\sim \\left( \\mu, \\sigma^2 \\right)\\), et que chaque Ã©chantillon est indÃ©pendant lâ€™un de lâ€™autre. Lâ€™hypothÃ¨se nulle est souvent celle de lâ€™avocat du diable, que la moyenne soit Ã©gale Ã  une valeur donnÃ©e (donc la diffÃ©rence entre la moyenne de la population et une moyenne donnÃ©e est de zÃ©ro): ici, que \\(\\mu = \\bar{x}\\). Lâ€™erreur standard sur la moyenne (ESM) de lâ€™Ã©chantillon, \\(\\bar{x}\\) est calculÃ©e comme suit. \\[ESM = \\frac{s}{\\sqrt{n}}\\] oÃ¹ \\(s\\) est lâ€™Ã©cart-type de lâ€™Ã©chantillon et \\(n\\) est le nombre dâ€™Ã©chantillons. Pour tester lâ€™intervalle de confiance de lâ€™Ã©chantillon, on multiplie lâ€™ESM par lâ€™aire sous la courbe de densitÃ© couvrant une certaine proportion de part et dâ€™autre de lâ€™Ã©chantillon. Pour un niveau de confiance de 95%, on retranche 2.5% de part et dâ€™autre. set.seed(33746) x &lt;- rnorm(20, 16, 4) level &lt;- 0.95 alpha &lt;- 1-level x_bar &lt;- mean(x) s &lt;- sd(x) n &lt;- length(x) error &lt;- qnorm(1 - alpha/2) * s / sqrt(n) error ## [1] 1.483253 intervalle de confiance est lâ€™erreur de par et dâ€™autre de la moyenne. c(x_bar - error, x_bar + error) ## [1] 14.35630 17.32281 Si la moyenne de la population est de 16, un nombre qui se situe dans lâ€™intervalle de confiance on accepte lâ€™hypothÃ¨se nulle au seuil 0.05. Si le nombre dâ€™Ã©chantillon est rÃ©duit (gÃ©nÃ©ralement &lt; 30), on passera plutÃ´t par une distribution de t, avec \\(n-1\\) degrÃ©s de libertÃ©. error &lt;- qt(1 - alpha/2, n-1) * s / sqrt(n) c(x_bar - error, x_bar + error) ## [1] 14.25561 17.42351 Plus simplement, on pourra utiliser la fonction t.test() en spÃ©cifiant la moyenne de la population. Nous avons gÃ©nÃ©rÃ© 20 donnÃ©es avec une moyenne de 16 et un Ã©cart-type de 4. Nous savons donc que la vraie moyenne de lâ€™Ã©chantillon est de 16. Mais disons que nous testons lâ€™hypothÃ¨se que ces donnÃ©es sont tirÃ©es dâ€™une population dont la moyenne est 18 (et implicitement que sont Ã©cart-type est de 4). t.test(x, mu = 18) ## ## One Sample t-test ## ## data: x ## t = -2.8548, df = 19, p-value = 0.01014 ## alternative hypothesis: true mean is not equal to 18 ## 95 percent confidence interval: ## 14.25561 17.42351 ## sample estimates: ## mean of x ## 15.83956 La fonction retourne la valeur de t (t-value), le nombre de degrÃ©s de libertÃ© (\\(n-1 = 19\\)), une description de lâ€™hypothÃ¨se alternative (alternative hypothesis: true mean is not equal to 18), ainsi que lâ€™intervalle de confiance au niveau de 95%. Le test contient aussi la p-value. Bien que la p-value soit largement utilisÃ©e en science 6.6.1.1 Information: la p-value La p-value, ou valeur-p ou p-valeur, est utilisÃ©e pour trancher si, oui ou non, un rÃ©sultat est significatif (en langage scientifique, le mot significatif ne devrait Ãªtre utilisÃ© que lorsque lâ€™on rÃ©fÃ¨re Ã  un test dâ€™hypothÃ¨se statistique). Vous retrouverez des p-value partout en stats. Les p-values indiquent la confiance que lâ€™hypothÃ¨se nulle soit vraie, selon les donnÃ©es et le modÃ¨le statistique utilisÃ©es. La p-value est la probabilitÃ© que les donnÃ©es aient Ã©tÃ© gÃ©nÃ©rÃ©es pour obtenir un effet Ã©quivalent ou plus prononcÃ© si lâ€™hypothÃ¨se nulle est vraie. Une p-value Ã©levÃ©e indique que le modÃ¨le appliquÃ© Ã  vos donnÃ©es concordent avec la conclusion que lâ€™hypothÃ¨se nulle est vraie, et inversement si la p-value est faible. Le seuil arbitraire utilisÃ©e en Ã©cologie et en agriculture, comme dans plusieurs domaines, est 0.05. Les six principes de lâ€™American Statistical Association guident lâ€™interprÃ©tation des p-values. [ma traduction] Les p-values indique lâ€™ampleur de lâ€™incompatibilitÃ© des donnÃ©es avec le modÃ¨le statistique Les p-values ne mesurent pas la probabilitÃ© que lâ€™hypothÃ¨se Ã©tudiÃ©e soit vraie, ni la probabilitÃ© que les donnÃ©es ont Ã©tÃ© gÃ©nÃ©rÃ©es uniquement par la chance. Les conclusions scientifiques et dÃ©cisions dâ€™affaire ou politiques ne devraient pas Ãªtre basÃ©es sur si une p-value atteint un seuil spÃ©cifique. Une infÃ©rence appropriÃ©e demande un rapport complet et transparent. Une p-value, ou une signification statistique, ne mesure pas lâ€™ampleur dâ€™un effet ou lâ€™importance dâ€™un rÃ©sultat. En tant que tel, une p-value nâ€™offre pas une bonne mesure des Ã©vidences dâ€™un modÃ¨le ou dâ€™une hypothÃ¨se. Cet encadrÃ© est inspirÃ© dâ€™un billet de blogue de Jim Frost et dâ€™un rapport de lâ€™American Statistical Association. Dans le cas prÃ©cÃ©dent, la p-value Ã©tait de 0.01014. Pour aider notre interprÃ©tation, prenons lâ€™hypothÃ¨se alternative: true mean is not equal to 18. Lâ€™hypothÃ¨se nulle Ã©tait bien que la vraie moyenne est Ã©gale Ã  18. InsÃ©rons la p-value dans la dÃ©finition: la probabilitÃ© que les donnÃ©es aient Ã©tÃ© gÃ©nÃ©rÃ©es pour obtenir un effet Ã©quivalent ou plus prononcÃ© si lâ€™hypothÃ¨se nulle est vraie est de 0.01014. Il est donc trÃ¨s peu probable que les donnÃ©es soient tirÃ©es dâ€™un Ã©chantillon dont la moyenne est de 18. Au seuil de signification de 0.05, on rejette lâ€™hypothÃ¨se nulle et lâ€™on conclut quâ€™Ã  ce seuil de confiance, lâ€™Ã©chantillon ne provient pas dâ€™une population ayant une moyenne de 18. 6.6.2 Attention: mauvaises interprÃ©tations des p-values â€œLa p-value nâ€™a jamais Ã©tÃ© conÃ§ue comme substitut au raisonnement scientifiqueâ€ Ron Wasserstein, directeur de lâ€™American Statistical Association [ma traduction]. Un rÃ©sultat montrant une p-value plus Ã©levÃ©e que 0.05 est-il pertinent? Lors dâ€™une confÃ©rence, Dr Evil ne prÃ©sentent que les rÃ©sultats significatifs de ses essais au seuil de 0.05. Certains essais ne sont pas significatifs, mais bon, ceux-ci ne sont pas importantsâ€¦ En Ã©cartant ces rÃ©sultats, Dr Evil commet 3 erreurs: La p-value nâ€™est pas un bon indicateur de lâ€™importance dâ€™un test statistique. Lâ€™importance dâ€™une variable dans un modÃ¨le devrait Ãªtre Ã©valuÃ©e par la valeur de son coefficient. Son incertitude devrait Ãªtre Ã©valuÃ©e par sa variance. Une maniÃ¨re dâ€™Ã©valuer plus intuitive la variance est lâ€™Ã©cart-type ou lâ€™intervalle de confiance. Ã€ un certain seuil dâ€™intervalle de confiance, la p-value traduira la probabilitÃ© quâ€™un coefficient soit rÃ©ellement nul ait pu gÃ©nÃ©rer des donnÃ©es dÃ©montrant un coefficient Ã©gal ou supÃ©rieur. Il est tout aussi important de savoir que le traitement fonctionne que de savoir quâ€™il ne fonctionne pas. Les rÃ©sultats dÃ©montrant des effets sont malheureusement davantage soumis aux journaux et davantage publiÃ©s que ceux ne dÃ©montrant pas dâ€™effets (Decullier et al., 2005). Le seuil de 0.05 est arbitraire. 6.6.2.1 Attention au p-hacking Le p-hacking (ou data dredging) consiste Ã  manipuler les donnÃ©es et les modÃ¨les pour faire en sorte dâ€™obtenir des p-values favorables Ã  lâ€™hypothÃ¨se testÃ©e et, Ã©ventuellement, aux conclusions recherchÃ©es. Ã€ Ã©viter dans tous les cas. Toujours. Toujours. Toujours. VidÃ©o suggÃ©rÃ©e (en anglais). 6.6.3 Test de Wilcoxon Ã  un seul Ã©chantillon Le test de t suppose que la distribution des donnÃ©es est normaleâ€¦ ce qui est rarement le cas, surtout lorsque les Ã©chantillons sont peu nombreux. Le test de Wilcoxon ne demande aucune supposition sur la distribution: câ€™est un test non-paramÃ©trique basÃ© sur le tri des valeurs. wilcox.test(x, mu = 18) ## ## Wilcoxon signed rank test ## ## data: x ## V = 39, p-value = 0.01208 ## alternative hypothesis: true location is not equal to 18 Le V est la somme des rangs positifs. Dans ce cas, la p-value est semblable Ã  celle du test de t, et les mÃªmes conclusions sâ€™appliquent. 6.6.4 Tests de t Ã  deux Ã©chantillons Les tests Ã  un Ã©chantillon servent plutÃ´t Ã  sâ€™exercer: rarement en aura-t-on besoin en recherche, oÃ¹ plus souvent, on voudra comparer les moyennes de deux unitÃ©s expÃ©rimentales. Lâ€™expÃ©rience comprend donc deux sÃ©ries de donnÃ©es continues, \\(x_1\\) et \\(x_2\\), issus de lois de distribution normale \\(\\mathcal{N} \\left( \\mu_1, \\sigma_1^2 \\right)\\) et \\(\\mathcal{N} \\left( \\mu_2, \\sigma_2^2 \\right)\\), et nous testons lâ€™hypothÃ¨se nulle que \\(\\mu_1 = \\mu_2\\). La statistique t est calculÃ©e comme suit. \\[t = \\frac{\\bar{x_1} - \\bar{x_2}}{ESDM}\\] Lâ€™ESDM est lâ€™erreur standard de la diffÃ©rence des moyennes: \\[ESDM = \\sqrt{ESM_1^2 + ESM_2^2}\\] Si vous supposez que les variances sont identiques, lâ€™erreur standard (s) est calculÃ©e pour les Ã©chantillons des deux groupes, puis insÃ©rÃ©e dans le calcul des ESM. La statistique t sera alors Ã©valuÃ©e Ã  \\(n_1 + n_2 - 2\\) degrÃ©s de libertÃ©. Si vous supposez que la variance est diffÃ©rente (procÃ©dure de Welch), vous calculez les ESM avec les erreurs standards respectives, et la statistique t devient une approximation de la distribution de t avec un nombre de degrÃ©s de libertÃ© calculÃ© Ã  partir des erreurs standards et du nombre dâ€™Ã©chantillon dans les groupes: cette procÃ©dure est considÃ©rÃ©e comme plus prudente (Dalgaard, 2008, page 101). Prenons les donnÃ©es dâ€™iris pour lâ€™exemple en excluant lâ€™iris setosa Ã©tant donnÃ©e que les tests de t se restreignent Ã  deux groupes. Nous allons tester la longueur des pÃ©tales. iris_pl &lt;- iris %&gt;% filter(Species != &quot;setosa&quot;) %&gt;% select(Species, Petal.Length) sample_n(iris_pl, 5) ## Species Petal.Length ## 1 virginica 5.1 ## 2 versicolor 4.0 ## 3 virginica 5.0 ## 4 versicolor 4.6 ## 5 versicolor 4.1 Dans la prochaine cellule, nous introduisons lâ€™interface-formule de R, oÃ¹ lâ€™on retrouve typiquement le ~, entre les variables de sortie Ã  gauche et les variables dâ€™entrÃ©e Ã  droite. Dans notre cas, la variable de sortie est la variable testÃ©e, Petal.Length, qui varie en fonction du groupe Species, qui est la variable dâ€™entrÃ©e (variable explicative) - nous verrons les types de variables plus en dÃ©tails dans la section Les modÃ¨les statistiques, plus bas. t.test(formula = Petal.Length ~ Species, data = iris_pl, var.equal = FALSE) ## ## Welch Two Sample t-test ## ## data: Petal.Length by Species ## t = -12.604, df = 95.57, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.49549 -1.08851 ## sample estimates: ## mean in group versicolor mean in group virginica ## 4.260 5.552 Nous obtenons une sortie similaire aux prÃ©cÃ©dentes. Lâ€™intervalle de confiance Ã  95% exclu le zÃ©ro, ce qui est cohÃ©rent avec la p-value trÃ¨s faible, qui nous indique le rejet de lâ€™hypothÃ¨se nulle au seuil 0.05. Les groupes ont donc des moyennes de longueurs de pÃ©tale significativement diffÃ©rentes. 6.6.4.1 Enregistrer les rÃ©sultats dâ€™un test Il est possible dâ€™enregistrer un test dans un objet. tt_pl &lt;- t.test(formula = Petal.Length ~ Species, data = iris_pl, var.equal = FALSE) summary(tt_pl) ## Length Class Mode ## statistic 1 -none- numeric ## parameter 1 -none- numeric ## p.value 1 -none- numeric ## conf.int 2 -none- numeric ## estimate 2 -none- numeric ## null.value 1 -none- numeric ## stderr 1 -none- numeric ## alternative 1 -none- character ## method 1 -none- character ## data.name 1 -none- character str(tt_pl) ## List of 10 ## $ statistic : Named num -12.6 ## ..- attr(*, &quot;names&quot;)= chr &quot;t&quot; ## $ parameter : Named num 95.6 ## ..- attr(*, &quot;names&quot;)= chr &quot;df&quot; ## $ p.value : num 4.9e-22 ## $ conf.int : num [1:2] -1.5 -1.09 ## ..- attr(*, &quot;conf.level&quot;)= num 0.95 ## $ estimate : Named num [1:2] 4.26 5.55 ## ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;mean in group versicolor&quot; &quot;mean in group virginica&quot; ## $ null.value : Named num 0 ## ..- attr(*, &quot;names&quot;)= chr &quot;difference in means&quot; ## $ stderr : num 0.103 ## $ alternative: chr &quot;two.sided&quot; ## $ method : chr &quot;Welch Two Sample t-test&quot; ## $ data.name : chr &quot;Petal.Length by Species&quot; ## - attr(*, &quot;class&quot;)= chr &quot;htest&quot; 6.6.5 Comparaison des variances Pour comparer les variances, on a recours au test de F (F pour Fisher). var.test(formula = Petal.Length ~ Species, data = iris_pl) ## ## F test to compare two variances ## ## data: Petal.Length by Species ## F = 0.72497, num df = 49, denom df = 49, p-value = 0.2637 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.411402 1.277530 ## sample estimates: ## ratio of variances ## 0.7249678 Il semble que lâ€™on pourrait relancer le test de t sans la procÃ©dure Welch, avec var.equal = TRUE. 6.6.6 Tests de Wilcoxon Ã  deux Ã©chantillons Cela ressemble au test de t! wilcox.test(formula = Petal.Length ~ Species, data = iris_pl, var.equal = TRUE) ## ## Wilcoxon rank sum test with continuity correction ## ## data: Petal.Length by Species ## W = 44.5, p-value &lt; 2.2e-16 ## alternative hypothesis: true location shift is not equal to 0 6.6.7 Les tests pairÃ©s Les tests pairÃ©s sont utilisÃ©s lorsque deux Ã©chantillons proviennent dâ€™une mÃªme unitÃ© expÃ©rimentale: il sâ€™agit en fait de tests sur la diffÃ©rence entre deux observations. set.seed(2555) n &lt;- 20 avant &lt;- rnorm(n, 16, 4) apres &lt;- rnorm(n, 18, 3) Il est important de spÃ©cifier que le test est pairÃ©, la valeur par dÃ©faut de paired Ã©tant FALSE. t.test(avant, apres, paired = TRUE) ## ## Paired t-test ## ## data: avant and apres ## t = -1.5168, df = 19, p-value = 0.1458 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -4.5804586 0.7311427 ## sample estimates: ## mean of the differences ## -1.924658 Lâ€™hypothÃ¨se nulle quâ€™il nâ€™y ait pas de diffÃ©rence entre lâ€™avant et lâ€™aprÃ¨s traitement est acceptÃ©e au seuil 0.05. Exercice. Effectuer un test de Wilcoxon pairÃ©. 6.7 Lâ€™analyse de variance Lâ€™analyse de variance consiste Ã  comparer des moyennes de plusieurs groupe distribuÃ©s normalement et de mÃªme variance. Cette section sera Ã©laborÃ©e prochainement plus en profondeur. ConsidÃ©rons-la pour le moment comme une rÃ©gression sur une variable catÃ©gorielle. pl_aov &lt;- aov(Petal.Length ~ Species, iris) summary(pl_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 2 437.1 218.55 1180 &lt;2e-16 *** ## Residuals 147 27.2 0.19 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 La prochaine section, justement, est vouÃ©e aux modÃ¨les statistiques explicatifs, qui incluent la rÃ©gression. 6.8 Les modÃ¨les statistiques La modÃ©lisation statistique consiste Ã  lier de maniÃ¨re explicite des variables de sortie \\(y\\) (ou variables-rÃ©ponse ou variables dÃ©pendantes) Ã  des variables explicatives \\(x\\) (ou variables prÃ©dictives / indÃ©pendantes / covariables). Les variables-rÃ©ponse sont modÃ©lisÃ©es par une fonction des variables explicatives ou prÃ©dictives. Pourquoi garder les termes explicatives et prÃ©dictives? Parce que les modÃ¨les statistiques (basÃ©s sur des donnÃ©es et non pas sur des mÃ©canismes) sont de deux ordres. Dâ€™abord, les modÃ¨les prÃ©dictifs sont conÃ§us pour prÃ©dire de maniÃ¨re fiable une ou plusieurs variables-rÃ©ponse Ã  partir des informations contenues dans les variables qui sont, dans ce cas, prÃ©dictives. Ces modÃ¨les sont couverts dans le chapitre 11 de ce manuel (en dÃ©veloppement). Lorsque lâ€™on dÃ©sire tester des hypothÃ¨ses pour Ã©valuer quelles variables expliquent la rÃ©ponse, on parlera de modÃ©lisation (et de variables) explicatives. En infÃ©rence statistique, on Ã©valuera les corrÃ©lations entre les variables explicatives et les variables-rÃ©ponse. Un lien de corrÃ©lation nâ€™est pas un lien de causalitÃ©. Lâ€™infÃ©rence causale peut en revanche Ãªtre Ã©valuÃ©e par des modÃ¨les dâ€™Ã©quations structurelles, sujet qui fera Ã©ventuellement partie de ce cours. Cette section couvre la modÃ©lisation explicative. Les variables qui contribuent Ã  crÃ©er les modÃ¨les peuvent Ãªtre de diffÃ©rentes natures et distribuÃ©es selon diffÃ©rentes lois de probabilitÃ©. Alors que les modÃ¨les linÃ©aires simples (lm) impliquent une variable-rÃ©ponse distribuÃ©e de maniÃ¨re continue, les modÃ¨les linÃ©aires gÃ©nÃ©ralisÃ©s peuvent aussi expliquer des variables de sorties discrÃ¨tes. Dans les deux cas, on distinguera les variables fixes et les variables alÃ©atoires. Les variables fixes sont les variables testÃ©es lors de lâ€™expÃ©rience: dose du traitement, espÃ¨ce/cultivar, mÃ©tÃ©o, etc. Les variables alÃ©atoires sont les sources de variation qui gÃ©nÃ¨rent du bruit dans le modÃ¨le: les unitÃ©s expÃ©rimentales ou le temps lors de mesures rÃ©pÃ©tÃ©es. Les modÃ¨les incluant des effets fixes seulement sont des modÃ¨les Ã  effets fixes. GÃ©nÃ©ralement, les modÃ¨les incluant des variables alÃ©atoires incluent aussi des variables fixes: on parlera alors de modÃ¨les mixtes. Nous couvrirons ces deux types de modÃ¨le. 6.8.1 ModÃ¨les Ã  effets fixes Les tests de t et de Wilcoxon, explorÃ©s prÃ©cÃ©demment, sont des modÃ¨les statistiques Ã  une seule variable. Nous avons vu dans lâ€™interface-formule quâ€™une variable-rÃ©ponse peut Ãªtre liÃ©e Ã  une variable explicative avec le tilde ~. En particulier, le test de t est rÃ©gression linÃ©aire univariÃ©e (Ã  une seule variable explicative) dont la variable explicative comprend deux catÃ©gories. De mÃªme, lâ€™anova est une rÃ©gression linÃ©aire univariÃ©e dont la variable explicative comprend plusieurs catÃ©gories. Or lâ€™interface-formule peut Ãªtre utilisÃ© dans plusieurs circonstances, notamment pour ajouter plusieurs variables de diffÃ©rents types: on parlera de rÃ©gression multivariÃ©e. La plupart des modÃ¨les statistiques peuvent Ãªtre approximÃ©s comme une combinaison linÃ©aire de variables: ce sont des modÃ¨les linÃ©aires. Les modÃ¨les non-linÃ©aires impliquent des stratÃ©gies computationnelles complexes qui rendent leur utilisation plus difficile Ã  manÅ“uvrer. Un modÃ¨le linÃ©aire univariÃ© prendra la forme \\(y = \\beta_0 + \\beta_1 x + \\epsilon\\), oÃ¹ \\(\\beta_0\\) est lâ€™intercept et \\(\\beta_1\\) est la pente et \\(\\epsilon\\) est lâ€™erreur. Vous verrez parfois la notation \\(\\hat{y} = \\beta_0 + \\beta_1 x\\). La notation avec le chapeau \\(\\hat{y}\\) exprime quâ€™il sâ€™agit des valeurs gÃ©nÃ©rÃ©es par le modÃ¨le. En fait, \\(y = \\hat{y} - \\epsilon\\). 6.8.1.1 ModÃ¨le linÃ©aire univariÃ© avec variable continue Prenons les donnÃ©es lasrosas.corn incluses dans le module agridat, oÃ¹ lâ€™on retrouve le rendement dâ€™une production de maÃ¯s Ã  dose dâ€™azote variable, en Argentine. library(&quot;agridat&quot;) data(&quot;lasrosas.corn&quot;) sample_n(lasrosas.corn, 10) ## year lat long yield nitro topo bv rep nf ## 1 1999 -33.05207 -63.84230 69.57 0.0 LO 185.67 R1 N0 ## 2 1999 -33.05137 -63.84383 67.41 53.0 E 175.12 R2 N2 ## 3 1999 -33.05104 -63.84323 68.33 29.0 LO 168.70 R3 N1 ## 4 1999 -33.05162 -63.84456 68.06 53.0 E 171.71 R1 N2 ## 5 1999 -33.05180 -63.84386 63.99 0.0 LO 172.46 R1 N0 ## 6 2001 -33.05065 -63.84578 35.85 50.6 HT 194.85 R1 N2 ## 7 1999 -33.05170 -63.84553 58.89 131.5 HT 187.98 R1 N5 ## 8 2001 -33.05077 -63.84502 50.95 124.6 HT 184.66 R2 N5 ## 9 1999 -33.05181 -63.84202 78.75 106.0 LO 169.25 R2 N4 ## 10 1999 -33.05154 -63.84468 68.58 29.0 E 169.35 R1 N1 Ces donnÃ©es comprennent plusieurs variables. Prenons le rendement (yield) comme variable de sortie et, pour le moment, ne retenons que la dose dâ€™azote (nitro) comme variable explicative: il sâ€™agit dâ€™une rÃ©gression univariÃ©e. Les deux variables sont continues. Explorons dâ€™abord le nuage de points de lâ€™une et lâ€™autre. ggplot(data = lasrosas.corn, mapping = aes(x = nitro, y = yield)) + geom_point() Lâ€™hypothÃ¨se nulle est que la dose dâ€™azote nâ€™affecte pas le rendement, câ€™est Ã  dire que le coefficient de pente et nul. Une autre hypothÃ¨se est que lâ€™intercept est nul: donc quâ€™Ã  dose de 0, rendement de 0. Un modÃ¨le linÃ©aire Ã  variable de sortie continue est crÃ©Ã© avec la fonction lm(), pour linear model. modlin_1 &lt;- lm(yield ~ nitro, data = lasrosas.corn) summary(modlin_1) ## ## Call: ## lm(formula = yield ~ nitro, data = lasrosas.corn) ## ## Residuals: ## Min 1Q Median 3Q Max ## -53.183 -15.341 -3.079 13.725 45.897 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 65.843213 0.608573 108.193 &lt; 2e-16 *** ## nitro 0.061717 0.007868 7.845 5.75e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 19.66 on 3441 degrees of freedom ## Multiple R-squared: 0.01757, Adjusted R-squared: 0.01728 ## F-statistic: 61.54 on 1 and 3441 DF, p-value: 5.754e-15 Le diagnostic du modÃ¨le comprend plusieurs informations. Dâ€™abord la formule utilisÃ©e, affichÃ©e pour la traÃ§abilitÃ©. Viens ensuite un aperÃ§u de la distribution des rÃ©sidus. La mÃ©diane devrait sâ€™approcher de la moyenne des rÃ©sidus (qui est toujours de 0). Bien que le -3.079 peut sembler important, il faut prendre en considÃ©ration de lâ€™Ã©chelle de y, et ce -3.079 est exprimÃ© en terme de rendement, ici en quintaux (i.e.Â 100 kg) par hectare. La distribution des rÃ©sidus mÃ©rite dâ€™Ãªtre davantage investiguÃ©e. Nous verrons cela un peu plus tard. Les coefficients apparaissent ensuite. Les estimÃ©s sont les valeurs des effets. R fournit aussi lâ€™erreur standard associÃ©e, la valeur de t ainsi que la p-value (la probabilitÃ© dâ€™obtenir cet effet ou un effet plus extrÃªme si en rÃ©alitÃ© il y avait absence dâ€™effet). Lâ€™intercept est bien sÃ»r plus Ã©levÃ© que 0 (Ã  dose nulle, on obtient 65.8 quintaux par hectare en moyenne). La pente de la variable nitro est de ~0.06: pour chaque augmentation dâ€™un kg/ha de dose, on a obtenu ~0.06 quintaux/ha de plus de maÃ¯s. Donc pour 100 kg/ha de N, on a obtenu un rendement moyen de 6 quintaux de plus que lâ€™intercept. Soulignons que lâ€™ampleur du coefficient est trÃ¨s important pour guider la fertilisation: ne rapporter que la p-value, ou ne rapporter que le fait quâ€™elle est infÃ©rieure Ã  0.05 (ce qui arrive souvent dans la littÃ©rature), serait trÃ¨s insuffisant pour lâ€™interprÃ©tation des statistiques. La p-value nous indique nÃ©anmoins quâ€™il serait trÃ¨s improbable quâ€™une telle pente ait Ã©tÃ© gÃ©nÃ©rÃ©e alors que celle-ci est nulle en rÃ©alitÃ©. Les Ã©toiles Ã  cÃ´tÃ© des p-values indiquent lâ€™ampleur selon lâ€™Ã©chelle Signif. codes indiquÃ©e en-dessous du tableau des coefficients. Sous ce tableau, R offre dâ€™autres statistiques. En outre, les RÂ² et RÂ² ajustÃ©s indiquent si la rÃ©gression passe effectivement par les points. Le RÂ² prend un maximum de 1 lorsque la droite passe exactement sur les points. Enfin, le test de F gÃ©nÃ¨re une p-value indiquant la probabilitÃ© que les coefficients de pente ait Ã©tÃ© gÃ©nÃ©rÃ©s si les vrais coefficients Ã©taient nuls. Dans le cas dâ€™une rÃ©gression univariÃ©e, cela rÃ©pÃ¨te lâ€™information sur lâ€™unique coefficient. On pourra Ã©galement obtenir les intervalles de confiance avec la fonction confint(). confint(modlin_1, level = 0.95) ## 2.5 % 97.5 % ## (Intercept) 64.65001137 67.03641474 ## nitro 0.04629164 0.07714271 Ou soutirer lâ€™information de diffÃ©rentes maniÃ¨res, comme avec la fonction coefficients(). coefficients(modlin_1) ## (Intercept) nitro ## 65.84321305 0.06171718 Ã‰galement, on pourra exÃ©cuter le modÃ¨le sur les donnÃ©es qui ont servi Ã  le gÃ©nÃ©rer: predict(modlin_1)[1:5] ## 1 2 3 4 5 ## 73.95902 73.95902 73.95902 73.95902 73.95902 Ou sur des donnÃ©es externes. nouvelles_donnees &lt;- data.frame(nitro = seq(from = 0, to = 100, by = 5)) predict(modlin_1, newdata = nouvelles_donnees)[1:5] ## 1 2 3 4 5 ## 65.84321 66.15180 66.46038 66.76897 67.07756 6.8.1.2 Analyse des rÃ©sidus Les rÃ©sidus sont les erreurs du modÃ¨le. Câ€™est le vecteur \\(\\epsilon\\), qui est un dÃ©calage entre les donnÃ©es et le modÃ¨le. Le RÂ² est un indicateur de lâ€™ampleur du dÃ©calage, mais une rÃ©gression linÃ©aire explicative en bonne et due forme devrait Ãªtre accompagnÃ©e dâ€™une analyse des rÃ©sidus. On peut les calculer par \\(\\epsilon = y - \\hat{y}\\), mais aussi bien utiliser la fonction residuals(). res_df &lt;- data.frame(nitro = lasrosas.corn$nitro, residus_lm = residuals(modlin_1), residus_calcul = lasrosas.corn$yield - predict(modlin_1)) sample_n(res_df, 10) ## nitro residus_lm residus_calcul ## 1 124.6 24.666827 24.666827 ## 2 124.6 11.126827 11.126827 ## 3 99.8 25.417413 25.417413 ## 4 66.0 -11.636547 -11.636547 ## 5 131.5 11.460978 11.460978 ## 6 75.4 -18.686688 -18.686688 ## 7 29.0 -1.763011 -1.763011 ## 8 131.5 -11.289022 -11.289022 ## 9 131.5 -5.639022 -5.639022 ## 10 131.5 -13.129022 -13.129022 Dans une bonne rÃ©gression linÃ©aire, on ne retrouvera pas de structure identifiable dans les rÃ©sidus, câ€™est-Ã -dire que les rÃ©sidus sont bien distribuÃ©s de part et dâ€™autre du modÃ¨le de rÃ©gression. ggplot(res_df, aes(x = nitro, y = residus_lm)) + geom_point() + labs(x = &quot;Dose N&quot;, y = &quot;RÃ©sidus&quot;) + geom_hline(yintercept = 0, col = &quot;red&quot;, size = 1) Bien que le jugement soit subjectif, on peut dire avec confiance quâ€™il nâ€™y a pas structure particuliÃ¨re. En revanche, on pourrait gÃ©nÃ©rer un \\(y\\) qui varie de maniÃ¨re quadratique avec \\(x\\), un modÃ¨le linÃ©aire montrera une structure Ã©vidente. set.seed(36164) x &lt;- 0:100 y &lt;- 10 + x*1 + x^2 * 0.05 + rnorm(length(x), 0, 50) modlin_2 &lt;- lm(y ~ x) ggplot(data.frame(x, residus = residuals(modlin_2)), aes(x = x, y = residus)) + geom_point() + labs(x = &quot;x&quot;, y = &quot;RÃ©sidus&quot;) + geom_hline(yintercept = 0, col = &quot;red&quot;, size = 1) De mÃªme, les rÃ©sidus ne devraient pas croÃ®tre avec \\(x\\). set.seed(3984) x &lt;- 0:100 y &lt;- 10 + x + x * rnorm(length(x), 0, 2) modlin_3 &lt;- lm(y ~ x) ggplot(data.frame(x, residus = residuals(modlin_3)), aes(x = x, y = residus)) + geom_point() + labs(x = &quot;x&quot;, y = &quot;RÃ©sidus&quot;) + geom_hline(yintercept = 0, col = &quot;red&quot;, size = 1) On pourra aussi inspecter les rÃ©sidus avec un graphique de leur distribution. Reprenons notre modÃ¨le de rendement du maÃ¯s. ggplot(res_df, aes(x = residus_lm)) + geom_histogram(binwidth = 2, color = &quot;white&quot;) + labs(x = &quot;Residual&quot;) Lâ€™histogramme devrait prÃ©senter une distribution normale. Les tests de normalitÃ© comme le test de Shapiro-Wilk peuvent aider, mais ils sont gÃ©nÃ©ralement trÃ¨s sÃ©vÃ¨res. shapiro.test(res_df$residus_lm) ## ## Shapiro-Wilk normality test ## ## data: res_df$residus_lm ## W = 0.94868, p-value &lt; 2.2e-16 Lâ€™hypothÃ¨se nulle que la distribution est normale est rejetÃ©e au seuil 0.05. Dans notre cas, il est Ã©vident que la sÃ©vÃ©ritÃ© du test nâ€™est pas en cause, car les rÃ©sidus semble gÃ©nÃ©rer trois ensembles. Ceci indique que les variables explicatives sont insuffisantes pour expliquer la variabilitÃ© de la variable-rÃ©ponse. 6.8.1.3 RÃ©gression multiple Comme câ€™est le cas pour bien des phÃ©nomÃ¨nes en Ã©cologie, le rendement dâ€™une culture nâ€™est certainement pas expliquÃ© seulement par la dose dâ€™azote. Lorsque lâ€™on combine plusieurs variables explicatives, on crÃ©e un modÃ¨le de rÃ©gression multivariÃ©e, ou une rÃ©gression multiple. Bien que les tendances puissent sembler non-linÃ©aires, lâ€™ajout de variables et le calcul des coefficients associÃ©s reste un problÃ¨me dâ€™algÃ¨bre linÃ©aire. On pourra en effet gÃ©nÃ©raliser les modÃ¨les linÃ©aires, univariÃ©s et multivariÃ©s, de la maniÃ¨re suivante. \\[ y = X \\beta + \\epsilon \\] oÃ¹: \\(X\\) est la matrice du modÃ¨le Ã  \\(n\\) observations et \\(p\\) variables. \\[ X = \\left( \\begin{matrix} 1 &amp; x_{11} &amp; \\cdots &amp; x_{1p} \\\\ 1 &amp; x_{21} &amp; \\cdots &amp; x_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; \\cdots &amp; x_{np} \\end{matrix} \\right) \\] \\(\\beta\\) est la matrice des \\(p\\) coefficients, \\(\\beta_0\\) Ã©tant lâ€™intercept qui multiplie la premiÃ¨re colonne de la matrice \\(X\\). \\[ \\beta = \\left( \\begin{matrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_p \\end{matrix} \\right) \\] \\(\\epsilon\\) est lâ€™erreur de chaque observation. \\[ \\epsilon = \\left( \\begin{matrix} \\epsilon_0 \\\\ \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n \\end{matrix} \\right) \\] 6.8.1.4 ModÃ¨les linÃ©aires univariÃ©s avec variable catÃ©gorielle nominale Une variable catÃ©gorielle nominale (non ordonnÃ©e) utilisÃ©e Ã  elle seule dans un modÃ¨le comme variable explicative, est un cas particulier de rÃ©gression multiple. En effet, lâ€™encodage catÃ©goriel (ou dummyfication) transforme une variable catÃ©gorielle nominale en une matrice de modÃ¨le comprenant une colonne dÃ©signant lâ€™intercept (une sÃ©rie de 1) dÃ©signant la catÃ©gorie de rÃ©fÃ©rence, ainsi que des colonnes pour chacune des autres catÃ©gories dÃ©signant lâ€™appartenance (1) ou la non appartenance (0) de la catÃ©gorie dÃ©signÃ©e par la colonne. 6.8.1.4.1 Lâ€™encodage catÃ©goriel Une variable Ã  \\(C\\) catÃ©gories pourra Ãªtre dÃ©clinÃ©e en \\(C\\) variables dont chaque colonne dÃ©signe par un 1 lâ€™appartenance au groupe de la colonne et par un 0 la non-appartenance. Pour lâ€™exemple, crÃ©ons un vecteur dÃ©signant le cultivar de pomme de terre. data &lt;- data.frame(cultivar = c(&#39;Superior&#39;, &#39;Superior&#39;, &#39;Superior&#39;, &#39;Russet&#39;, &#39;Kenebec&#39;, &#39;Russet&#39;)) model.matrix(~cultivar, data) ## (Intercept) cultivarRusset cultivarSuperior ## 1 1 0 1 ## 2 1 0 1 ## 3 1 0 1 ## 4 1 1 0 ## 5 1 0 0 ## 6 1 1 0 ## attr(,&quot;assign&quot;) ## [1] 0 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$cultivar ## [1] &quot;contr.treatment&quot; Nous avons trois catÃ©gories, encodÃ©es en trois colonnes. La premiÃ¨re colonne est un intercept et les deux autres dÃ©crivent lâ€™absence (0) ou la prÃ©sence (1) des cultivars Russet et Superior. Le cultivar Kenebec est absent du tableau. En effet, en partant du principe que lâ€™appartenance Ã  une catÃ©gorie est mutuellement exclusive, câ€™est-Ã -dire quâ€™un Ã©chantillon ne peut Ãªtre assignÃ© quâ€™Ã  une seule catÃ©gorie, on peut dÃ©duire une catÃ©gorie Ã  partir de lâ€™information sur toutes les autres. Par exemple, si cultivar_Russet et cultivar_Superior sont toutes deux Ã©gales Ã  \\(0\\), on conclura que cultivar_Kenebec est nÃ©cessairement Ã©gal Ã  \\(1\\). Et si lâ€™un dâ€™entre cultivar_Russet et cultivar_Superior est Ã©gal Ã  \\(1\\), cultivar_Kenebec est nÃ©cessairement Ã©gal Ã  \\(0\\). Lâ€™information contenue dans un nombre \\(C\\) de catÃ©gorie peut Ãªtre encodÃ©e dans un nombre \\(C-1\\) de colonnes. Câ€™est pourquoi, dans une analyse statistique, on dÃ©signera une catÃ©gorie comme une rÃ©fÃ©rence, que lâ€™on dÃ©tecte lorsque toutes les autres catÃ©gories sont encodÃ©es avec des \\(0\\): cette rÃ©fÃ©rence sera incluse dans lâ€™intercept. La catÃ©gorie de rÃ©fÃ©rence par dÃ©faut en R est celle la premiÃ¨re catÃ©gorie dans lâ€™ordre alphabÃ©tique. On pourra modifier cette rÃ©fÃ©rence avec la fonction relevel(). data$cultivar &lt;- relevel(data$cultivar, ref = &quot;Superior&quot;) model.matrix(~cultivar, data) ## (Intercept) cultivarKenebec cultivarRusset ## 1 1 0 0 ## 2 1 0 0 ## 3 1 0 0 ## 4 1 0 1 ## 5 1 1 0 ## 6 1 0 1 ## attr(,&quot;assign&quot;) ## [1] 0 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$cultivar ## [1] &quot;contr.treatment&quot; Pour certains modÃ¨les, vous devrez vous assurer vous-mÃªme de lâ€™encodage catÃ©goriel. Pour dâ€™autre, en particulier avec lâ€™interface par formule de R, ce sera fait automatiquement. 6.8.1.4.2 Exemple dâ€™application Prenons la topographie du terrain, qui peut prendre plusieurs niveaux. levels(lasrosas.corn$topo) ## [1] &quot;E&quot; &quot;HT&quot; &quot;LO&quot; &quot;W&quot; Explorons le rendement selon la topographie. ggplot(lasrosas.corn, aes(x = topo, y = yield)) + geom_boxplot() Les diffÃ©rences sont Ã©videntes, et la modÃ©lisation devrait montrer des effets significatifs. Lâ€™encodage catÃ©goriel peut Ãªtre visualisÃ© en gÃ©nÃ©rant la matrice de modÃ¨le avec la fonction model.matrix() et lâ€™interface-formule - sans la variable-rÃ©ponse. model.matrix(~ topo, data = lasrosas.corn) %&gt;% tbl_df() %&gt;% # tbl_df pour transformer la matrice en tableau sample_n(10) ## # A tibble: 10 x 4 ## `(Intercept)` topoHT topoLO topoW ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 0 0 ## 2 1 0 1 0 ## 3 1 0 0 1 ## 4 1 0 0 1 ## 5 1 0 1 0 ## 6 1 0 0 1 ## 7 1 1 0 0 ## 8 1 0 1 0 ## 9 1 0 0 1 ## 10 1 0 0 0 Dans le cas dâ€™un modÃ¨le avec une variable catÃ©gorielle nominale seule, lâ€™intercept reprÃ©sente la catÃ©gorie de rÃ©fÃ©rence, ici E. Les autres colonnes spÃ©cifient lâ€™appartenance (1) ou la non-appartenance (0) de la catÃ©gorie pour chaque observation. Cette matrice de modÃ¨le utilisÃ©e pour la rÃ©gression donnera un intercept, qui indiquera lâ€™effet de la catÃ©gorie de rÃ©fÃ©rence, puis les diffÃ©rences entre les catÃ©gories subsÃ©quentes et la catÃ©gorie de rÃ©fÃ©rence. modlin_4 &lt;- lm(yield ~ topo, data = lasrosas.corn) summary(modlin_4) ## ## Call: ## lm(formula = yield ~ topo, data = lasrosas.corn) ## ## Residuals: ## Min 1Q Median 3Q Max ## -47.371 -11.933 -1.593 11.080 44.119 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 78.6653 0.5399 145.707 &lt;2e-16 *** ## topoHT -30.0526 0.7500 -40.069 &lt;2e-16 *** ## topoLO 6.2832 0.7293 8.615 &lt;2e-16 *** ## topoW -11.8841 0.7039 -16.883 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.59 on 3439 degrees of freedom ## Multiple R-squared: 0.4596, Adjusted R-squared: 0.4591 ## F-statistic: 975 on 3 and 3439 DF, p-value: &lt; 2.2e-16 Le modÃ¨le linÃ©aire est Ã©quivalent Ã  lâ€™anova, mais les rÃ©sultats de lm sont plus Ã©laborÃ©s. summary(aov(yield ~ topo, data = lasrosas.corn)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## topo 3 622351 207450 975 &lt;2e-16 *** ## Residuals 3439 731746 213 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Lâ€™analyse de rÃ©sidus peut Ãªtre effectuÃ©e de la mÃªme maniÃ¨re. 6.8.1.5 ModÃ¨les linÃ©aires univariÃ©s avec variable catÃ©gorielle ordinale Bien que jâ€™introduise la rÃ©gression sur variable catÃ©gorielle ordinale Ã  la suite de la section sur les variables nominales, nous revenons dans ce cas Ã  une rÃ©gression simple, univariÃ©e. Voyons un cas Ã  5 niveaux. statut &lt;- c(&quot;Totalement en dÃ©saccord&quot;, &quot;En dÃ©saccord&quot;, &quot;Ni en accord, ni en dÃ©saccord&quot;, &quot;En accord&quot;, &quot;Totalement en accord&quot;) statut_o &lt;- factor(statut, levels = statut, ordered=TRUE) model.matrix(~statut_o) # ou bien, sans passer par model.matrix, contr.poly(5) oÃ¹ 5 est le nombre de niveaux ## (Intercept) statut_o.L statut_o.Q statut_o.C statut_o^4 ## 1 1 -0.6324555 0.5345225 -3.162278e-01 0.1195229 ## 2 1 -0.3162278 -0.2672612 6.324555e-01 -0.4780914 ## 3 1 0.0000000 -0.5345225 -4.095972e-16 0.7171372 ## 4 1 0.3162278 -0.2672612 -6.324555e-01 -0.4780914 ## 5 1 0.6324555 0.5345225 3.162278e-01 0.1195229 ## attr(,&quot;assign&quot;) ## [1] 0 1 1 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$statut_o ## [1] &quot;contr.poly&quot; La matrice de modÃ¨le a 5 colonnes, soit le nombre de niveaux: un intercept, puis 4 autres dÃ©signant diffÃ©rentes valeurs que peuvent prendre les niveaux. Ces niveaux croient-ils linÃ©airement? De maniÃ¨re quadratique, cubique ou plus loin dans des distributions polynomiales? modmat_tidy &lt;- data.frame(statut, model.matrix(~statut_o)[, -1]) %&gt;% gather(variable, valeur, -statut) modmat_tidy$statut &lt;- factor(modmat_tidy$statut, levels = statut, ordered=TRUE) ggplot(data = modmat_tidy, mapping = aes(x = statut, y = valeur)) + facet_wrap(. ~ variable) + geom_point() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) RÃ¨gle gÃ©nÃ©rale, pour les variables ordinales, on prÃ©fÃ©rera une distribution linÃ©aire, et câ€™est lâ€™option par dÃ©faut de la fonction lm(). Lâ€™utilisation dâ€™une autre distribution peut Ãªtre effectuÃ©e Ã  la mitaine en utilisant dans le modÃ¨le la colonne dÃ©sirÃ©e de la sortie de la fonction model.matrix(). 6.8.1.6 RÃ©gression multiple Ã  plusieurs variables Reprenons le tableau de donnÃ©es du rendement de maÃ¯s. head(lasrosas.corn) ## year lat long yield nitro topo bv rep nf ## 1 1999 -33.05113 -63.84886 72.14 131.5 W 162.60 R1 N5 ## 2 1999 -33.05115 -63.84879 73.79 131.5 W 170.49 R1 N5 ## 3 1999 -33.05116 -63.84872 77.25 131.5 W 168.39 R1 N5 ## 4 1999 -33.05117 -63.84865 76.35 131.5 W 176.68 R1 N5 ## 5 1999 -33.05118 -63.84858 75.55 131.5 W 171.46 R1 N5 ## 6 1999 -33.05120 -63.84851 70.24 131.5 W 170.56 R1 N5 Pour ajouter des variables au modÃ¨le dans lâ€™interface-formule, on additionne les noms de colonne. La variable lat dÃ©signe la latitude, la variable long dÃ©signe la latitude et la variable bv (brightness value) dÃ©signe la teneur en matiÃ¨re organique du sol (plus bv est Ã©levÃ©e, plus faible est la teneur en matiÃ¨re organique). modlin_5 &lt;- lm(yield ~ lat + long + nitro + topo + bv, data = lasrosas.corn) summary(modlin_5) ## ## Call: ## lm(formula = yield ~ lat + long + nitro + topo + bv, data = lasrosas.corn) ## ## Residuals: ## Min 1Q Median 3Q Max ## -48.405 -11.071 -1.251 10.592 40.078 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.946e+05 3.309e+04 5.882 4.45e-09 *** ## lat 5.541e+03 4.555e+02 12.163 &lt; 2e-16 *** ## long 1.776e+02 4.491e+02 0.395 0.693 ## nitro 6.867e-02 5.451e-03 12.597 &lt; 2e-16 *** ## topoHT -2.665e+01 1.087e+00 -24.520 &lt; 2e-16 *** ## topoLO 5.565e+00 1.035e+00 5.378 8.03e-08 *** ## topoW -1.465e+01 1.655e+00 -8.849 &lt; 2e-16 *** ## bv -5.089e-01 3.069e-02 -16.578 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.47 on 3435 degrees of freedom ## Multiple R-squared: 0.5397, Adjusted R-squared: 0.5387 ## F-statistic: 575.3 on 7 and 3435 DF, p-value: &lt; 2.2e-16 Lâ€™ampleur des coefficients est relatif Ã  lâ€™Ã©chelle de la variable. En effet, un coefficient de 5541 sur la variable lat nâ€™est pas comparable au coefficient de la variable bv, de -0.5089, Ã©tant donnÃ© que les variables ne sont pas exprimÃ©es avec la mÃªme Ã©chelle. Pour les comparer sur une mÃªme base, on peut centrer (soustraire la moyenne) et rÃ©duire (diviser par lâ€™Ã©cart-type). scale_vec &lt;- function(x) as.vector(scale(x)) # la fonction scale gÃ©nÃ¨re une matrice: nous dÃ©sirons un vecteur lasrosas.corn_sc &lt;- lasrosas.corn %&gt;% mutate_at(c(&quot;lat&quot;, &quot;long&quot;, &quot;nitro&quot;, &quot;bv&quot;), scale_vec) modlin_5_sc &lt;- lm(yield ~ lat + long + nitro + topo + bv, data = lasrosas.corn_sc) summary(modlin_5_sc) ## ## Call: ## lm(formula = yield ~ lat + long + nitro + topo + bv, data = lasrosas.corn_sc) ## ## Residuals: ## Min 1Q Median 3Q Max ## -48.405 -11.071 -1.251 10.592 40.078 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 78.9114 0.6666 118.376 &lt; 2e-16 *** ## lat 3.9201 0.3223 12.163 &lt; 2e-16 *** ## long 0.3479 0.8796 0.395 0.693 ## nitro 2.9252 0.2322 12.597 &lt; 2e-16 *** ## topoHT -26.6487 1.0868 -24.520 &lt; 2e-16 *** ## topoLO 5.5647 1.0347 5.378 8.03e-08 *** ## topoW -14.6487 1.6555 -8.849 &lt; 2e-16 *** ## bv -4.9253 0.2971 -16.578 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.47 on 3435 degrees of freedom ## Multiple R-squared: 0.5397, Adjusted R-squared: 0.5387 ## F-statistic: 575.3 on 7 and 3435 DF, p-value: &lt; 2.2e-16 Typiquement, les variables catÃ©gorielles, qui ne sont pas mises Ã  lâ€™Ã©chelle, donneront des coefficients plus Ã©levÃ©es, et devrons Ãªtre Ã©valuÃ©es entre elles et non comparativement aux variables mises Ã  lâ€™Ã©chelle. Une maniÃ¨re conviviale de reprÃ©senter des coefficients consiste Ã  crÃ©er un tableau (fonction tibble()) incluant les coefficients ainsi que leurs intervalles de confiance, puis Ã  les porter graphiquement. intervals &lt;- tibble(Estimate = coefficients(modlin_5_sc)[-1], # [-1] enlever l&#39;intercept LL = confint(modlin_5_sc)[-1, 1], # [-1, ] enlever la premiÃ¨re ligne, celle de l&#39;intercept UL = confint(modlin_5_sc)[-1, 2], variable = names(coefficients(modlin_5_sc)[-1])) intervals ## # A tibble: 7 x 4 ## Estimate LL UL variable ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 3.92 3.29 4.55 lat ## 2 0.348 -1.38 2.07 long ## 3 2.93 2.47 3.38 nitro ## 4 -26.6 -28.8 -24.5 topoHT ## 5 5.56 3.54 7.59 topoLO ## 6 -14.6 -17.9 -11.4 topoW ## 7 -4.93 -5.51 -4.34 bv ggplot(data = intervals, mapping = aes(x = Estimate, y = variable)) + geom_vline(xintercept = 0, lty = 2) + geom_segment(mapping = aes(x = LL, xend = UL, y = variable, yend = variable)) + geom_point() + labs(x = &quot;Coefficient standardisÃ©&quot;, y = &quot;&quot;) On y voit quâ€™Ã  lâ€™exception de la variable long, tous les coefficients sont diffÃ©rents de 0. Le coefficient bv est nÃ©gatif, indiquant que plus la valeur de bv est Ã©levÃ© (donc plus le sol est pauvre en matiÃ¨re organique), plus le rendement est faible. Plus la latitude est Ã©levÃ©e (plus on se dirige vers le Nord de lâ€™Argentine), plus le rendement est Ã©levÃ©. La dose dâ€™azote a aussi un effet statistique positif sur le rendement. Quant aux catÃ©gories topographiques, elles sont toutes diffÃ©rentes de la catÃ©gorie E, ne croisant pas le zÃ©ro. De plus, les intervalles de confiance ne se chevauchant pas, on peut conclure en une diffÃ©rence significative dâ€™une Ã  lâ€™autre. Bien sÃ»r, tout cela au seuil de confiance de 0.05. On pourra retrouver des cas oÃ¹ lâ€™effet combinÃ© de plusieurs variables diffÃ¨re de lâ€™effet des deux variables prises sÃ©parÃ©ment. Par exemple, on pourrait Ã©valuer lâ€™effet de lâ€™azote et celui de la topographie dans un mÃªme modÃ¨le, puis y ajouter une interaction entre lâ€™azote et la topographie, qui dÃ©finira des effets supplÃ©mentaires de lâ€™azote selon chaque catÃ©gorie topographique. Câ€™est ce que lâ€™on appelle une interaction. Dans lâ€™interface-formule, lâ€™interaction entre lâ€™azote et la topographie est notÃ©e nitro:topo. Pour ajouter cette interaction, la formule deviendra yield ~ nitro + topo + nitro:topo. Une approche Ã©quivalente est dâ€™utiliser le raccourci yield ~ nitro*topo. modlin_5_sc &lt;- lm(yield ~ nitro*topo, data = lasrosas.corn_sc) summary(modlin_5_sc) ## ## Call: ## lm(formula = yield ~ nitro * topo, data = lasrosas.corn_sc) ## ## Residuals: ## Min 1Q Median 3Q Max ## -47.984 -11.985 -1.388 10.339 40.636 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 78.6999 0.5322 147.870 &lt; 2e-16 *** ## nitro 1.8131 0.5351 3.388 0.000711 *** ## topoHT -30.0052 0.7394 -40.578 &lt; 2e-16 *** ## topoLO 6.2026 0.7190 8.627 &lt; 2e-16 *** ## topoW -11.9628 0.6939 -17.240 &lt; 2e-16 *** ## nitro:topoHT 1.2553 0.7461 1.682 0.092565 . ## nitro:topoLO 0.5695 0.7186 0.792 0.428141 ## nitro:topoW 0.7702 0.6944 1.109 0.267460 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.38 on 3435 degrees of freedom ## Multiple R-squared: 0.4756, Adjusted R-squared: 0.4746 ## F-statistic: 445.1 on 7 and 3435 DF, p-value: &lt; 2.2e-16 Les rÃ©sultats montre des effets de lâ€™azote et des catÃ©gories topographiques, mais il y a davantage dâ€™incertitude sur les interactions, indiquant que lâ€™effet statistique de lâ€™azote est sensiblement le mÃªme indÃ©pendamment des niveaux topographiques. 6.8.1.7 Attention Ã  ne pas surcharger le modÃ¨le Il est possible dâ€™ajouter des interactions doubles, triples, quadruples, etc. Mais plus il y a dâ€™interactions, plus votre modÃ¨le comprendra de variables et vos tests dâ€™hypothÃ¨se perdront en puissance statistique. 6.8.1.8 Les modÃ¨les linÃ©aires gÃ©nÃ©ralisÃ©s Dans un modÃ¨le linÃ©aire ordinaire, un changement constant dans les variables explicatives rÃ©sulte en un changement constant de la variable-rÃ©ponse. Cette supposition ne serait pas adÃ©quate si la variable-rÃ©ponse Ã©tait un dÃ©compte, si elle est boolÃ©enne ou si, de maniÃ¨re gÃ©nÃ©rale, la variable-rÃ©ponse ne suivait pas une distribution continue. Ou, de maniÃ¨re plus spÃ©cifique, il nâ€™y a pas moyen de retrouver une distribution normale des rÃ©sidus? On pourra bien sÃ»r transformer les variables (sujet du chapitre 6, en dÃ©veloppement). Mais il pourrait sâ€™avÃ©rer impossible, ou tout simplement non souhaitable de transformer les variables. Le modÃ¨le linÃ©aire gÃ©nÃ©ralisÃ© (MLG, ou generalized linear model - GLM) est une gÃ©nÃ©ralisation du modÃ¨le linÃ©aire ordinaire chez qui la variable-rÃ©ponse peut Ãªtre caractÃ©risÃ© par une distribution de Poisson, de Bernouilli, etc. Prenons dâ€™abord cas dâ€™un dÃ©compte de vers fil-de-fer (worms) retrouvÃ©s dans des parcelles sous diffÃ©rents traitements (trt). Les dÃ©comptes sont typiquement distribuÃ© selon une loi de Poisson. cochran.wireworms %&gt;% ggplot(aes(x = worms)) + geom_histogram(bins = 10) Explorons les dÃ©comptes selon les traitements. cochran.wireworms %&gt;% ggplot(aes(x = trt, y = worms)) + geom_boxplot() Les traitements semble Ã  premiÃ¨re vue avoir un effet comparativement au contrÃ´le. LanÃ§ons un MLG avec la fonction glm(), et spÃ©cifions que la sortie est une distribution de Poisson. Bien que la fonction de lien (link = &quot;log&quot;) soit explictement imposÃ©e, le log est la valeur par dÃ©faut pour les distributions de Poisson. Ainsi, les coefficients du modÃ¨les devront Ãªtre interprÃ©tÃ©s selon un modÃ¨le \\(log \\left(worms \\right) = intercept + pente \\times coefficient\\). modglm_1 &lt;- glm(worms ~ trt, cochran.wireworms, family = stats::poisson(link=&quot;log&quot;)) summary(modglm_1) ## ## Call: ## glm(formula = worms ~ trt, family = stats::poisson(link = &quot;log&quot;), ## data = cochran.wireworms) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.8279 -0.9455 -0.2862 0.6916 3.1888 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.1823 0.4082 0.447 0.655160 ## trtM 1.6422 0.4460 3.682 0.000231 *** ## trtN 1.7636 0.4418 3.991 6.57e-05 *** ## trtO 1.5755 0.4485 3.513 0.000443 *** ## trtP 1.3437 0.4584 2.931 0.003375 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 64.555 on 24 degrees of freedom ## Residual deviance: 38.026 on 20 degrees of freedom ## AIC: 125.64 ## ## Number of Fisher Scoring iterations: 5 Lâ€™interprÃ©tation spÃ©cifique des coefficients dâ€™une rÃ©gression de Poisson doit passer par la fonction de lien \\(log \\left(worms \\right) = intercept + pente \\times coefficient\\). Le traitement de rÃ©fÃ©rence (K), qui correspond Ã  lâ€™intercept, sera accompagnÃ© dâ€™un nombre de vers de \\(exp \\left(0.1823\\right) = 1.20\\) vers, et le traitement M, Ã  \\(exp \\left(1.6422\\right) = 5.17\\) vers. Cela correspond Ã  ce que lâ€™on observe sur les boxplots plus haut. Il est trÃ¨s probable (p-value de ~0.66) quâ€™un intercept (traitement K) de 0.18 ayant une erreur standard de 0.4082 ait Ã©tÃ© gÃ©nÃ©rÃ© depuis une population dont lâ€™intercept est nul. Quant aux autres traitements, leurs effets sont tous significatifs au seuil 0.05, mais peuvent-ils Ãªtre considÃ©rÃ©s comme Ã©quivalents? intervals &lt;- tibble(Estimate = coefficients(modglm_1), # [-1] enlever l&#39;intercept LL = confint(modglm_1)[, 1], # [-1, ] enlever la premiÃ¨re ligne, celle de l&#39;intercept UL = confint(modglm_1)[, 2], variable = names(coefficients(modglm_1))) ## Waiting for profiling to be done... ## Waiting for profiling to be done... intervals ## # A tibble: 5 x 4 ## Estimate LL UL variable ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.182 -0.740 0.888 (Intercept) ## 2 1.64 0.840 2.62 trtM ## 3 1.76 0.972 2.74 trtN ## 4 1.58 0.766 2.56 trtO ## 5 1.34 0.509 2.34 trtP ggplot(data = intervals, mapping = aes(x = Estimate, y = variable)) + geom_vline(xintercept = 0, lty = 2) + geom_segment(mapping = aes(x = LL, xend = UL, y = variable, yend = variable)) + geom_point() + labs(x = &quot;Coefficient&quot;, y = &quot;&quot;) Les intervalles de confiance se superposant, on ne peut pas conclure quâ€™un traitement est liÃ© Ã  une rÃ©duction plus importante de vers quâ€™un autre, au seuil 0.05. Maintenant, Ã  dÃ©faut de trouver un tableau de donnÃ©es plus appropriÃ©, prenons le tableau mtcars, qui rassemble des donnÃ©es sur des modÃ¨les de voitures. La colonne vs, pour v-shaped, inscrit 0 si les pistons sont droit et 1 sâ€™ils sont placÃ©s en V dans le moteur. Peut-on expliquer la forme des pistons selon le poids du vÃ©hicule (wt)? mtcars %&gt;% sample_n(6) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## 2 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 ## 3 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## 4 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## 5 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## 6 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 mtcars %&gt;% ggplot(aes(x = wt, y = vs)) + geom_point() Il semble y avoir une tendance: les vÃ©hicules plus lourds ont plutÃ´t des pistons droits (vs = 0). VÃ©rifions cela. modglm_2 &lt;- glm(vs ~ wt, data = mtcars, family = stats::binomial()) summary(modglm_2) ## ## Call: ## glm(formula = vs ~ wt, family = stats::binomial(), data = mtcars) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.9003 -0.7641 -0.1559 0.7223 1.5736 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 5.7147 2.3014 2.483 0.01302 * ## wt -1.9105 0.7279 -2.625 0.00867 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.860 on 31 degrees of freedom ## Residual deviance: 31.367 on 30 degrees of freedom ## AIC: 35.367 ## ## Number of Fisher Scoring iterations: 5 Exercice. Analyser les rÃ©sultats. 6.8.1.9 Les modÃ¨les non-linÃ©aires La hauteur dâ€™un arbre en fonction du temps nâ€™est typiquement pas linÃ©aire. Elle tend Ã  croÃ®tre de plus en plus lentement jusquâ€™Ã  un plateau. De mÃªme, le rendement dâ€™une culture traitÃ© avec des doses croissantes de fertilisants tend Ã  atteindre un maximum, puis Ã  se stabiliser. Ces phÃ©nomÃ¨nes ne peuvent pas Ãªtre approximÃ©s par des modÃ¨les linÃ©aires. Examinons les donnÃ©es du tableau engelstad.nitro. engelstad.nitro %&gt;% sample_n(10) ## loc year nitro yield ## 1 Knoxville 1966 0 63.0 ## 2 Knoxville 1965 335 61.2 ## 3 Jackson 1965 335 73.0 ## 4 Jackson 1966 201 61.3 ## 5 Jackson 1966 335 59.8 ## 6 Knoxville 1964 0 60.9 ## 7 Knoxville 1964 67 75.9 ## 8 Jackson 1966 67 45.2 ## 9 Jackson 1962 201 73.1 ## 10 Jackson 1964 335 67.8 engelstad.nitro %&gt;% ggplot(aes(x = nitro, y = yield)) + facet_grid(year ~ loc) + geom_line() + geom_point() Le modÃ¨le de Mitscherlich pourrait Ãªtre utilisÃ©. \\[ y = A \\left( 1 - e^{-R \\left( E + x \\right)} \\right) \\] oÃ¹ \\(y\\) est le rendement, \\(x\\) est la dose, \\(A\\) est lâ€™asymptote vers laquelle la courbe converge Ã  dose croissante, \\(E\\) est lâ€™Ã©quivalent de dose fourni par lâ€™environnement et \\(R\\) est le taux de rÃ©ponse. Explorons la fonction. mitscherlich_f &lt;- function(x, A, E, R) { A * (1 - exp(-R*(E + x))) } x &lt;- seq(0, 350, by = 5) y &lt;- mitscherlich_f(x, A = 75, E = 30, R = 0.02) ggplot(tibble(x, y), aes(x, y)) + geom_point(data = engelstad.nitro, aes(x = nitro, y = yield)) + geom_line() + ylim(c(0, 100)) Exercice. Changez les paramÃ¨tres pour visualiser comment la courbe rÃ©agit. Nous pouvons dÃ©crire le modÃ¨le grÃ¢ce Ã  lâ€™interface formule dans la fonction nls(). Notez que les modÃ¨les non-linÃ©aires demandent des stratÃ©gies de calcul diffÃ©rentes de celles des modÃ¨les linÃ©aires. En tout temps, nous devons identifier des valeurs de dÃ©part raisonnables pour les paramÃ¨tres dans lâ€™argument start. Vous rÃ©ussirez rarement Ã  obtenir une convergence du premier coup avec vos paramÃ¨tres de dÃ©part. Le dÃ©fi est dâ€™en trouver qui permettront au modÃ¨le de converger. Parfois, le modÃ¨le ne convergera jamais. Dâ€™autres fois, il convergera vers des solutions diffÃ©rentes selon les variables de dÃ©part choisies. &lt; #modnl_1 &lt;- nls(yield ~ A * (1 - exp(-R*(E + nitro))), # data = engelstad.nitro, # start = list(A = 50, E = 10, R = 0.2)) Le modÃ¨le ne converge pas. Essayons les valeurs prises plus haut, lors de la crÃ©ation du graphique, qui semblent bien sâ€™ajuster. modnl_1 &lt;- nls(yield ~ A * (1 - exp(-R*(E + nitro))), data = engelstad.nitro, start = list(A = 75, E = 30, R = 0.02)) Bingo! Voyons maintenant le sommaire. summary(modnl_1) ## ## Formula: yield ~ A * (1 - exp(-R * (E + nitro))) ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## A 75.023427 3.331860 22.517 &lt;2e-16 *** ## E 66.164110 27.251591 2.428 0.0184 * ## R 0.012565 0.004881 2.574 0.0127 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 12.34 on 57 degrees of freedom ## ## Number of iterations to convergence: 5 ## Achieved convergence tolerance: 8.067e-06 Les paramÃ¨tres sont significativement diffÃ©rents de zÃ©ro au seuil 0.05, et donnent la courbe suivante. x &lt;- seq(0, 350, by = 5) y &lt;- mitscherlich_f(x, A = coefficients(modnl_1)[1], E = coefficients(modnl_1)[2], R = coefficients(modnl_1)[3]) ggplot(tibble(x, y), aes(x, y)) + geom_point(data = engelstad.nitro, aes(x = nitro, y = yield)) + geom_line() + ylim(c(0, 100)) Et les rÃ©sidusâ€¦ tibble(res = residuals(modnl_1)) %&gt;% ggplot(aes(x = res)) + geom_histogram(bins = 20) tibble(nitro = engelstad.nitro$nitro, res = residuals(modnl_1)) %&gt;% ggplot(aes(x = nitro, y = res)) + geom_point() + geom_hline(yintercept = 0, colour = &quot;red&quot;) Les rÃ©sidus ne sont pas distribuÃ©s normalement, mais semble bien partagÃ©s de part et dâ€™autre de la courbe. 6.8.2 ModÃ¨les Ã  effets mixtes Lorsque lâ€™on combine des variables fixes (testÃ©es lors de lâ€™expÃ©rience) et des variables alÃ©atoire (variation des unitÃ©s expÃ©rimentales), on obtient un modÃ¨le mixte. Les modÃ¨les mixtes peuvent Ãªtre univariÃ©s, multivariÃ©s, linÃ©aires ordinaires ou gÃ©nÃ©ralisÃ©s ou non linÃ©aires. Ã€ la diffÃ©rence dâ€™un effet fixe, un effet alÃ©atoire sera toujours distribuÃ© normalement avec une moyenne de 0 et une certaine variance. Dans un modÃ¨le linÃ©aire oÃ¹ lâ€™effet alÃ©atoire est un dÃ©calage dâ€™intercept, cet effet sâ€™additionne aux effets fixes: \\[ y = X \\beta + Z b + \\epsilon \\] oÃ¹: \\(Z\\) est la matrice du modÃ¨le Ã  \\(n\\) observations et \\(p\\) variables alÃ©atoires. Les variables alÃ©atoires sont souvent des variables nominales qui subissent un encodage catÃ©goriel. \\[ Z = \\left( \\begin{matrix} z_{11} &amp; \\cdots &amp; z_{1p} \\\\ z_{21} &amp; \\cdots &amp; z_{2p} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ z_{n1} &amp; \\cdots &amp; z_{np} \\end{matrix} \\right) \\] \\(b\\) est la matrice des \\(p\\) coefficients alÃ©atoires. \\[ b = \\left( \\begin{matrix} b_0 \\\\ b_1 \\\\ \\vdots \\\\ b_p \\end{matrix} \\right) \\] Le tableau lasrosas.corn, utilisÃ© prÃ©cÃ©demment, contenait trois rÃ©pÃ©titions effectuÃ©s au cours de deux annÃ©es, 1999 et 2001. Ã‰tant donnÃ© que la rÃ©pÃ©tition R1 de 1999 nâ€™a rien Ã  voir avec la rÃ©pÃ©tition R1 de 2001, on dit quâ€™elle est emboÃ®tÃ©e dans lâ€™annÃ©e. Le module nlme nous aidera Ã  monter notre modÃ¨le mixte. library(&quot;nlme&quot;) mmodlin_1 &lt;- lme(fixed = yield ~ lat + long + nitro + topo + bv, random = ~ 1|year/rep, data = lasrosas.corn) Ã€ ce stade vous devriez commencer Ã  Ãªtre familier avec lâ€™interface formule et vous deviez saisir lâ€™argument fixed, qui dÃ©signe lâ€™effet fixe. Lâ€™effet alÃ©atoire, random, suit un tilde ~. Ã€ gauche de la barre verticale |, on place les variables dÃ©signant les effets alÃ©atoire sur la pente. Nous nâ€™avons pas couvert cet aspect, alors nous le laissons Ã  1. Ã€ droite, on retrouve un structure dâ€™emboÃ®tement dÃ©signant lâ€™effet alÃ©atoire: le premier niveau est lâ€™annÃ©e, dans laquelle est emboÃ®tÃ©e la rÃ©pÃ©tition. summary(mmodlin_1) ## Linear mixed-effects model fit by REML ## Data: lasrosas.corn ## AIC BIC logLik ## 26535.37 26602.93 -13256.69 ## ## Random effects: ## Formula: ~1 | year ## (Intercept) ## StdDev: 20.35425 ## ## Formula: ~1 | rep %in% year ## (Intercept) Residual ## StdDev: 11.17447 11.35617 ## ## Fixed effects: yield ~ lat + long + nitro + topo + bv ## Value Std.Error DF t-value p-value ## (Intercept) -1379436.9 55894.55 3430 -24.679273 0.000 ## lat -25453.0 1016.53 3430 -25.039084 0.000 ## long -8432.3 466.05 3430 -18.092988 0.000 ## nitro 0.0 0.00 3430 1.739757 0.082 ## topoHT -27.7 0.92 3430 -30.122438 0.000 ## topoLO 6.8 0.88 3430 7.804733 0.000 ## topoW -16.7 1.40 3430 -11.944793 0.000 ## bv -0.5 0.03 3430 -19.242424 0.000 ## Correlation: ## (Intr) lat long nitro topoHT topoLO topoW ## lat 0.897 ## long 0.866 0.555 ## nitro 0.366 0.391 0.247 ## topoHT 0.300 -0.017 0.582 0.024 ## topoLO -0.334 -0.006 -0.621 -0.038 -0.358 ## topoW 0.403 -0.004 0.762 0.027 0.802 -0.545 ## bv -0.121 -0.012 -0.214 -0.023 -0.467 0.346 -0.266 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -4.32360269 -0.66781575 -0.07450856 0.61587533 3.96434001 ## ## Number of Observations: 3443 ## Number of Groups: ## year rep %in% year ## 2 6 La sortie est semblable Ã  celle de la fonction lm(). 6.8.2.1 ModÃ¨les mixtes non-linÃ©aires Le modÃ¨le non linÃ©aire crÃ©Ã© plus haut liait le rendement Ã  la dose dâ€™azote. Toutefois, les unitÃ©s expÃ©rimentales (le site loc et lâ€™annÃ©e year) nâ€™Ã©taient pas pris en considÃ©ration. Nous allons maintenant les considÃ©rer. Nous devons dÃ©cider la structure de lâ€™effet alÃ©atoire, et sur quelles variables il doit Ãªtre appliquÃ© - la dÃ©cision appartient Ã  lâ€™analyste. Il me semble plus convenable de supposer que le site et lâ€™annÃ©e affectera le rendement maximum plutÃ´t que lâ€™environnement et le taux: les effets alÃ©atoires seront donc affectÃ©s Ã  la variable A. Les effets alÃ©atoires nâ€™ont pas de structure dâ€™emboÃ®tement. Lâ€™effet de lâ€™annÃ©e sur A sera celui dâ€™une pente et lâ€™effet de site sera celui de lâ€™intercept. La fonction que nous utiliserons est nlme(). mm &lt;- nlme(yield ~ A * (1 - exp(-R*(E + nitro))), data = engelstad.nitro, start = c(A = 75, E = 30, R = 0.02), fixed = list(A ~ 1, E ~ 1, R ~ 1), random = A ~ year | loc) summary(mm) ## Nonlinear mixed-effects model fit by maximum likelihood ## Model: yield ~ A * (1 - exp(-R * (E + nitro))) ## Data: engelstad.nitro ## AIC BIC logLik ## 477.2286 491.889 -231.6143 ## ## Random effects: ## Formula: A ~ year | loc ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## A.(Intercept) 2.611534836 A.(In) ## A.year 0.003066832 -0.556 ## Residual 11.152757999 ## ## Fixed effects: list(A ~ 1, E ~ 1, R ~ 1) ## Value Std.Error DF t-value p-value ## A.(Intercept) 74.58222 4.722715 56 15.792234 0.0000 ## E 65.56721 25.533993 56 2.567840 0.0129 ## R 0.01308 0.004808 56 2.720215 0.0087 ## Correlation: ## A.(In) E ## E 0.379 ## R -0.483 -0.934 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -1.83373132 -0.89293033 0.07418165 0.68353577 1.82434347 ## ## Number of Observations: 60 ## Number of Groups: 2 Et sur graphique: engelstad.nitro %&gt;% ggplot(aes(x = nitro, y = yield)) + facet_grid(year ~ loc) + geom_line(data = tibble(nitro = engelstad.nitro$nitro, yield = predict(mm, level = 0)), colour = &quot;grey35&quot;) + geom_point() + ylim(c(0, 95)) Les modÃ¨les mixtes non linÃ©aires peuvent devenir trÃ¨s complexes lorsque les paramÃ¨tres, par exemple A, E et R, sont eux-mÃªme affectÃ©s linÃ©airement par des variables (par exemple A ~ topo). Pour aller plus loin, consultez Parent et al. (2017) ainsi que les calculs associÃ©s Ã  lâ€™article. Ou Ã©crivez-moi un courriel pour en discuter! Note. Lâ€™interprÃ©tation de p-values sur les modÃ¨les mixtes est controversÃ©e. Ã€ ce sujet, Douglas Bates a Ã©crit une longue lettre Ã  la communautÃ© de dÃ©veloppement du module lme4, une alternative Ã  nlme, qui remet en cause lâ€™utilisation des p-values, ici. De plus en plus, pour les modÃ¨les mixtes, on se tourne vers les statistiques bayÃ©siennes, couvertes dans le chapitre 7 avec le module greta. Mais en ce qui a trait aux modÃ¨les mixtes, le module brms automatise bien des aspects de lâ€™approche bayÃ©sienne. 6.8.3 Aller plus loin 6.8.3.1 Statistiques gÃ©nÃ©rales: The analysis of biological data 6.8.3.2 Statistiques avec R Disponibles en version Ã©lectronique Ã  la bibliothÃ¨que de lâ€™UniversitÃ© Laval: Introduction aux statistiques avec R: Introductory statistics with R Approfondir les statistiques avec R: The R Book, Second edition Approfondir les modÃ¨les Ã  effets mixtes avec R: Mixed Effects Models and Extensions in Ecology with R ModernDive, un livre en ligne offrant une approche moderne avec le package moderndive. "],
["chapitre-biostats-bayes.html", "7 Introduction Ã  lâ€™analyse bayÃ©sienne en Ã©cologie 7.1 Quâ€™est-ce que câ€™est? 7.2 Pourquoi lâ€™utiliser? 7.3 Comment lâ€™utiliser? 7.4 Faucons pÃ©lerins 7.5 Statistiques dâ€™une population 7.6 Test de t: DiffÃ©rence entre des groupes 7.7 ModÃ©lisation multiniveau 7.8 Pour aller plus loin", " 7 Introduction Ã  lâ€™analyse bayÃ©sienne en Ã©cologie ï¸Â Objectifs spÃ©cifiques: Ce chapitre est un extra. Il ne fait pas partie des objectifs du cours. Il ne sera pas Ã©valuÃ©. Ã€ la fin de ce chapitre, vous serez en mesure de dÃ©finir ce que sont les statistiques bayÃ©siennes serez en mesure de calculer des statistiques descriptives de base en mode bayÃ©sien avec le module greta. Les statistiques bayÃ©siennes forment une trousse dâ€™outils Ã  garder dans votre pack sack. 7.1 Quâ€™est-ce que câ€™est? En deux mots: modÃ©lisation probabiliste. Un approche de modÃ©lisation probabiliste se servant au mieux de lâ€™information disponible. Pour calculer les probabilitÃ©s dâ€™une variable inconnu en mode bayÃ©sien, nous avons besoin: De donnÃ©es Dâ€™un modÃ¨le Dâ€™une idÃ©e plus ou moins prÃ©cise du rÃ©sultat avant dâ€™avoir analysÃ© les donnÃ©es De maniÃ¨re plus formelle, le thÃ©orÃ¨me de Bayes (qui forme la base de lâ€™analyse bayÃ©seienne), dit que la distribution de probabilitÃ© des paramÃ¨tres dâ€™un modÃ¨le (par exemple, la moyenne ou une pente) est proportionnelle Ã  la mutliplication de la distribution de probabilitÃ© estimÃ©e des paramÃ¨tres et la distribution de probabilitÃ© Ã©mergeant des donnÃ©es. Plus formellement, \\[P\\left(\\theta | y \\right) = \\frac{P\\left(y | \\theta \\right) \\times P\\left(\\theta\\right)}{P\\left(y \\right)}\\], oÃ¹ \\(P\\left(\\theta | y \\right)\\) \\(-\\) la probabilitÃ© dâ€™obtenir des paramÃ¨tres \\(\\theta\\) Ã  partir des donnÃ©es \\(y\\) \\(-\\) est la distribution de probabilitÃ© a posteriori, calculÃ©e Ã  partir de votre a prioti \\(P\\left(\\theta\\right)\\) \\(-\\) la probabilitÃ© dâ€™obtenir des paramÃ¨tres \\(\\theta\\) sans Ã©gard aux donnÃ©es, selon votre connaissance du phÃ©nomÃ¨ne \\(-\\) et vos donnÃ©es observÃ©es \\(P\\left(y | \\theta \\right)\\) \\(-\\) la probabilitÃ© dâ€™obtenir les donnÃ©es \\(y\\) Ã©tant donnÃ©s les paramÃ¨tres \\(\\theta\\) qui rÃ©gissent le phÃ©nomÃ¨ne. \\(P\\left(y\\right)\\), la probabilitÃ© dâ€™observer les donnÃ©es, est appellÃ©e la vraissemblance marginale, et assure que la somme des probabilitÃ©s est nulle. 7.2 Pourquoi lâ€™utiliser? Avec la notion frÃ©quentielle de probabilitÃ©, on teste la probabilitÃ© dâ€™observer les donnÃ©es recueillies Ã©tant donnÃ©e lâ€™absence dâ€™effet rÃ©el (qui est lâ€™hypothÃ¨se nulle gÃ©nÃ©ralement adoptÃ©e). La notion bayÃ©sienne de probabilitÃ© combine la connaissance que lâ€™on a dâ€™un phÃ©nomÃ¨ne et les donnÃ©es observÃ©es pour estimer la probabilitÃ© quâ€™il existe un effet rÃ©el. En dâ€™autre mots, les stats frÃ©quentielles testent si les donnÃ©es concordent avec un modÃ¨le du rÃ©el, tandis que les stats bayÃ©siennes Ã©valuent, selon les donnÃ©es, la probabilitÃ© que le modÃ¨le soit rÃ©el. Le hic, câ€™est que lorsquâ€™on utilise les statistiques frÃ©quentielles pour rÃ©pondre Ã  une question bayÃ©sienne, on sâ€™expose Ã  de mauvaises interprÃ©tations. Par exemple, lors dâ€™un projet considÃ©rant la vie sur Mars, les stats frÃ©quentielles Ã©valueront si les donnÃ©es recueillies sont conformes ou non avec lâ€™hypothÃ¨se de la vie sur Mars. Par contre, pour Ã©valuer la probabilitÃ© de lâ€™existance de vie sur Mars, on devra passer par les stats bayÃ©siennes (exemple tirÃ©e du billet Dynamic Ecology â€“ Frequentist vs.Â Bayesian statistics: resources to help you choose). 7.3 Comment lâ€™utiliser? Bien que la formule du thÃ©orÃ¨me de Bayes soit plutÃ´t simple, calculer une fonction a posteriori demandera de passer par des algorithmes de simulation, ce qui pourrait demander une bonne puissance de calcul, et des outils appropriÃ©s. R comporte une panoplie dâ€™outils pour le calcul bayÃ©sien gÃ©nÃ©rique (rstan, rjags, MCMCpack, etc.), et dâ€™autres outils pour des besoins particuliers (brms: R package for Bayesian generalized multivariate non-linear multilevel models using Stan). Nous utiliserons ici le module gÃ©nÃ©rique greta, qui permet de gÃ©nÃ©rer de maniÃ¨re conviviale plusieurs types de modÃ¨les bayÃ©siens. Pour installer greta, vous devez prÃ©alablement installer Python, grÃ©Ã© des modules tensorflow et tensorflow-probability en suivant le guide. En somme, vous devez dâ€™abord installer greta (install.packages(&quot;greta&quot;)). Puis vous devez installer une distribution de Python â€“ je vous suggÃ¨re Anaconda (~500 Mo) ou Miniconda pour une installation minimale (~60 Mo). Enfin, lancez les commandes suivantes (une connection internet est nÃ©cessaire pour tÃ©lÃ©charger les modules). Si vous avez installÃ© la version complÃ¨te dâ€™Anaconda, vous avez accÃ¨s Ã  Anaconda-navigator, une interface pour la gestion de vos environnements de calcul: assurez-vous quâ€™il soit fermer pour Ã©viter que la commande se butte Ã  des fichiers verouillÃ©s. greta::install_tensorflow( method = &quot;conda&quot;, envname = &quot;r-greta&quot;, version = &quot;1.14.0&quot;, extra_packages = &quot;tensorflow-probability==0.7.0&quot; ) Puis, vous devez installer une distribution de Python â€“ je vous suggÃ¨re Anaconda (~500 Mo) ou Miniconda pour une installation minimale (~60 Mo). Enfin, lancez les commandes suivantes pour installer Python, tensorflow et tensorflow-probability dans un nouvel environnement de calcul (nommÃ© r-greta). reticulate::conda_create(envname = &quot;r-greta&quot;, packages = c(&quot;python&quot;, &quot;tensorflow=1.14&quot;, &quot;tensorflow-probability=0.7&quot;)) 7.4 Faucons pÃ©lerins Empruntons un exemple du livre Introduction to WinBUGS for Ecologists: A Bayesian Approach to Regression, ANOVA and Related Analyses, de Marc KÃ©ry et examinons la masse de faucons pÃ©lerins. Mais alors que Marc KÃ©ry utilise WinBUGS, un logiciel de rÃ©solution de problÃ¨me en mode bayÃ©sien, nous utiliserons greta. Source: Wikimedia Commons Pour une premiÃ¨re approche, nous allons estimer la masse moyenne dâ€™une population de faucons pÃ©lerins. Ã€ titre de donnÃ©es, gÃ©nÃ©rons des nombres alÃ©atoires. Cette stratÃ©gie permet de valider les statistiques en les comparant aux paramÃ¨tre que lâ€™on impose. Ici, nous imposons une moyenne de 600 grammes et un Ã©cart-type de 30 grammes. GÃ©nÃ©rons une sÃ©ries de donnÃ©es avec 20 Ã©chantillons. library(&quot;tidyverse&quot;) set.seed(5682) y20 &lt;- rnorm(n = 20, mean=600, sd = 30) y200 &lt;- rnorm(n = 200, mean=600, sd = 30) par(mfrow = c(1, 2)) hist(y20, breaks=5) hist(y200, breaks=20) Je crÃ©e une fonction qui retourne la moyenne et lâ€™erreur sur la moyenne ou sur la distribution. Calculons les statistiques classiques. confidence_interval &lt;- function(x, on=&quot;deviation&quot;, distribution=&quot;t&quot;, level=0.95) { m &lt;- mean(x) se &lt;- sd(x) n &lt;- length(x) if (distribution == &quot;t&quot;) { error &lt;- se * qt((1+level)/2, n-1) } else if (distribution == &quot;normal&quot;) { error &lt;- se * qnorm((1+level)/2) } if (on == &quot;error&quot;) { error &lt;- error/sqrt(n) } return(c(ll = m-error, mean = m, ul = m+error)) } print(&quot;DÃ©viation, 95%&quot;) ## [1] &quot;DÃ©viation, 95%&quot; print(round(confidence_interval(y20, on=&#39;deviation&#39;, level=0.95), 2)) ## ll mean ul ## 532.23 598.85 665.47 print(&quot;Erreur, 95%&quot;) ## [1] &quot;Erreur, 95%&quot; print(round(confidence_interval(y20, on=&#39;error&#39;, level=0.95), 2)) ## ll mean ul ## 583.96 598.85 613.75 print(&quot;Ã‰cart-type&quot;) ## [1] &quot;Ã‰cart-type&quot; print(round(sd(y20), 2)) ## [1] 31.83 En faisant cela, nous prenons pour acquis que les donnÃ©es sont distribuÃ©es normalement. En fait, nous savons quâ€™elles devraient lâ€™Ãªtre pour de grands Ã©chantillons, puisque nous avons nous-mÃªme gÃ©nÃ©rÃ© les donnÃ©es. Par contre, comme observateur par exemple de la sÃ©rie de 20 donnÃ©es gÃ©nÃ©rÃ©es, la distribution est dÃ©finitivement asymÃ©trique. Sous cet angle, la moyenne, ainsi que lâ€™Ã©cart-type, pourraient Ãªtre des paramÃ¨tres biaisÃ©s. Nous pouvons justifier le choix dâ€™une loi normale par des connaissances a priori des distributions de masse parmi des espÃ¨ces dâ€™oiseau. Ou bien transformer les donnÃ©es pour rendre leur distribution normale (chapitre 8). 7.5 Statistiques dâ€™une population 7.5.1 Calcul analytique Supposer une distribution normale dâ€™une population implique dâ€™estimer deux paramÃ¨tres: sa moyenne et son Ã©cart-type. Toutefois, pour cet exemple, nous supposons que lâ€™Ã©cart-type est connu, ce qui nâ€™est Ã  toute fin pratique jamais le cas, mais vous dÃ©couvrirez bientÃ´t pourquoi nous laisse tomber lâ€™Ã©cart-type Ã  cette Ã©tape. Nous allons donc estimer la moyenne dâ€™une population de faucons dont lâ€™Ã©cart-type est de 30: \\(X \\sim \\mathcal{N}(\\mu, 30)\\). On sait quâ€™une distribution normale est dÃ©finir par la fonction suivante. \\[f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\] Ou, en R: normdist &lt;- function(x, mean, sd) { f &lt;- 1 / (sd * sqrt(2*pi)) * exp(-0.5 * ((x-mean)/sd)^2) return(f) } Ce qui nâ€™est utile que pour une petite dÃ©monstration, Ã©tant donnÃ©e cette opÃ©ration peut Ãªtre effectuÃ©e avec la fonction dnorm(), qui vient avec le module stats chargÃ© en R par dÃ©faut. x_ &lt;- seq(0, 2000, 100) plot(x_, dnorm(x = x_, mean = 750, sd = 300), &quot;l&quot;, lwd = 4, col = &quot;pink&quot;) points(x_, normdist(x = x_, mean = 750, sd = 300)) Reprenons notre Ã©quation de Bayes. \\[P\\left(\\theta | y \\right) = \\frac{P\\left(y | \\theta \\right) \\times P\\left(\\theta\\right)}{P\\left(y \\right)}\\], En mode bayÃ©sien, nous devons dÃ©finir la connaissance a priori, P(), sous forme de variables alÃ©atoires non-observÃ©es selon une distribution. Prenons lâ€™exemple des faucons pÃ©lerins. Disons que nous ne savons pas Ã  quoi ressemble la moyenne du groupe a priori. Nous pouvons utiliser un a priori peu informatif, oÃ¹ la masse moyenne peut prendre nâ€™importe quelle valeur entre 0 et 2000 grammes, sans prÃ©fÃ©rence: nous lui imposons donc un a priori selon une distribution uniforme. Idem pour lâ€™Ã©cart-type. Câ€™est ce quâ€™on appelle des a priori plats. Mais il est plutÃ´t conseiller (Gelman et al., 2013) dâ€™utiliser des a priori vagues plutÃ´t que plats ou non-informatifs. En effet, des masses de 0 g ou 2000 g ne sont pas aussi probables quâ€™une masse de 750 g. Si vous Ã©tudiez les faucons pÃ©lerins (ce qui nâ€™est pas mon cas), vous aurez une idÃ©e de sa masse, ne serait-ce quâ€™en arcourrant la littÃ©rature Ã  son sujet. Mais disons que jâ€™estime trÃ¨s vaguement quâ€™une masse moyenne Ã©chantillonale devrait Ãªtre autour de 750 g, avec un large Ã©cart-type de 200 g sur la moyenne. Il sâ€™agit de lâ€™Ã©cart-type de la moyenne, pas de lâ€™Ã©cart-type de lâ€™Ã©chantillon que nous supposons Ãªtre connu. Notez que lâ€™a priori peu avoir la forme que lâ€™on dÃ©sire: il sâ€™agit seulement de crÃ©er un vecteur. Toutefois, gÃ©nÃ©rer ce vecteur avec des distributions connues est aussi pratique quâ€™Ã©lÃ©gant. x_mean &lt;- seq(400, 1200, 5) prob &lt;- tibble(mass = x_mean, Prior = dnorm(x = x_mean, mean = 750, sd = 200)) prob$Prior &lt;- prob$Prior / sum(prob$Prior) prob %&gt;% ggplot(aes(mass, Prior))+ geom_line() + expand_limits(y=0) Note sur le jargon: Ã©tant donnÃ©e que cet a priori aura la mÃªme distribution que lâ€™a posteriori, on dit que cet a priori est conjuguÃ©. Nous allons Ã©galement utiliser nos donnÃ©es pour crÃ©er une fonction de vraissemblance (likelihood), \\(P\\left(y | \\theta \\right)\\), qui est la distribution de probabilitÃ© issue des donnÃ©es: une distribution normale avec une moyenne calculÃ©e et une variance connue. prob$Likelihood &lt;- dnorm(x = x_mean, mean = mean(y20), sd = 30) prob$Likelihood &lt;- prob$Likelihood / sum(prob$Likelihood) prob %&gt;% pivot_longer(-mass, names_to = &quot;type&quot;, values_to = &quot;probability&quot;) %&gt;% ggplot(aes(mass, probability, colour = type)) + geom_line() + expand_limits(y=0) Noter distribution a posteriori est proportionnelle Ã  la multiplication de lâ€™a priori et de la vraissemblance. Puis nous allons normaliser lâ€™a posteriori pour faire en sorte que la somme des probabilitÃ©s soit de 1. prob$Posterior &lt;- prob$Likelihood * prob$Prior prob$Posterior &lt;- prob$Posterior / sum(prob$Posterior) prob %&gt;% pivot_longer(-mass, names_to = &quot;type&quot;, values_to = &quot;probability&quot;) %&gt;% ggplot(aes(mass, probability, colour = type)) + geom_line() + expand_limits(y=0) La distribution a posteriori est presque callÃ©e sur les donnÃ©es. Pas Ã©tonnant, Ã©tant donnÃ©e que lâ€™a priori est trÃ¨s vague. En revanche, un a priori plus affirmÃ©, avec un Ã©cart-type plus faible, aurait davantage de poids sur lâ€™a posteriori. Exercice. Changez lâ€™a priori et visualisez lâ€™effet sur lâ€™a posteriori. Maintenant, imaginez ajouter lâ€™Ã©cart-type. Cela reste faisable en calcul analytique, mais Ã§a complique le calcul pour normaliser les proabilitÃ©. Ajoutez encore une variable et le calcul bayÃ©sien devient un vÃ©ritable casse-tÃªte. En fait, en bayÃ©sien, la difficultÃ© de mettre Ã  lâ€™Ã©chelle de plus dâ€™un paramÃ¨tre rend rare la multiplication distributions de probabilitÃ©. Câ€™est pourquoi lâ€™on prÃ©fÃ¨re les simuler et Ã©chantillonnant, Ã  lâ€™aide de diffÃ©rents algorithmes, la distribution a posteriori. En R, le module greta est conÃ§u pour cela. 7.5.2 greta Chargeons dâ€™abord les modules nÃ©cessaires. Avant de charger greta, il faut sÃ©lectionner lâ€™environnement coda (Python) auquel se connecter. Lors de lâ€™installation, nous avions spÃ©cifiÃ© que lâ€™installation se fasse dans lâ€™environnement nommÃ© r-greta. reticulate::use_condaenv(&quot;r-greta&quot;, required = TRUE) library(&quot;greta&quot;) ## ## Attaching package: &#39;greta&#39; ## The following object is masked from &#39;package:plotly&#39;: ## ## slice ## The following object is masked from &#39;package:dplyr&#39;: ## ## slice ## The following objects are masked from &#39;package:stats&#39;: ## ## binomial, cov2cor, poisson ## The following objects are masked from &#39;package:base&#39;: ## ## %*%, apply, backsolve, beta, chol2inv, colMeans, colSums, diag, eigen, forwardsolve, gamma, identity, ## rowMeans, rowSums, sweep, tapply library(&quot;DiagrammeR&quot;) library (&quot;bayesplot&quot;) ## This is bayesplot version 1.7.1 ## - Online documentation and vignettes at mc-stan.org/bayesplot ## - bayesplot theme set to bayesplot::theme_default() ## * Does _not_ affect other ggplot2 plots ## * See ?bayesplot_theme_set for details on theme setting library(&quot;tidybayes&quot;) Reprenons lâ€™a piori utilisÃ© prÃ©cÃ©demment. Dans greta, nous dÃ©finissons notre a priori ainsi. param_mean &lt;- normal(mean = 750, 200) Lâ€™Ã©cart-type dâ€™un Ã©chantillon ne peut pas Ãªtre nÃ©gatif. Il est commun pour les Ã©carts-types dâ€™utiliser une distribution en tronquÃ©e Ã  0. On pourrait utiliser une normale tronquÃ©e, mais la cauchy tronquÃ©e est souvent recommandÃ©e (e.g. Gelman, 2006) puisque la queue, plus Ã©paisse que la distribution normale, permet davantage de flexibilitÃ©. Disons que nous supposons un Ã©cart-type dâ€™une moyenne de 50, et dâ€™un Ã©cart-type de 100, tronquÃ© Ã  0. x_ &lt;- seq(0, 500, 10) plot(x_, dcauchy(x = x_, location = 50, scale = 100), &quot;l&quot;) param_sd &lt;- cauchy(location = 50, scale = 100, dim = NULL, truncation = c(0, Inf)) La fonction a porteriori inclue la fonction de vraissemblance ainsi que la connaissancew a priori. distribution(y20) &lt;- normal(param_mean, param_sd) Le tout forme un modÃ¨le pour apprÃ©cier y, la masse des faucons pÃ©lerins. m &lt;- model(param_mean, param_sd) plot(m) LÃ©gende: Le graphique du modÃ¨le montre que deux paramÃ¨tres sont attachÃ©s Ã  des distributions pour gÃ©nÃ©rer une distribution de sortie. Nous pouvons enfin lancer le modÃ¨le. draws &lt;- mcmc(m, n_samples = 1000) ## ## running 4 chains simultaneously on up to 4 cores ## warmup 0/1000 | eta: ?s warmup == 50/1000 | eta: 19s | 24% bad warmup ==== 100/1000 | eta: 13s | 12% bad warmup ====== 150/1000 | eta: 11s | 8% bad warmup ======== 200/1000 | eta: 9s | 6% bad warmup ========== 250/1000 | eta: 8s | 5% bad warmup =========== 300/1000 | eta: 7s | 4% bad warmup ============= 350/1000 | eta: 7s | 4% bad warmup =============== 400/1000 | eta: 6s | 3% bad warmup ================= 450/1000 | eta: 5s | 3% bad warmup =================== 500/1000 | eta: 5s | 2% bad warmup ===================== 550/1000 | eta: 4s | 2% bad warmup ======================= 600/1000 | eta: 4s | 2% bad warmup ========================= 650/1000 | eta: 3s | 2% bad warmup =========================== 700/1000 | eta: 3s | 2% bad warmup ============================ 750/1000 | eta: 2s | 2% bad warmup ============================== 800/1000 | eta: 2s | 2% bad warmup ================================ 850/1000 | eta: 1s | 1% bad warmup ================================== 900/1000 | eta: 1s | 1% bad warmup ==================================== 950/1000 | eta: 0s | 1% bad warmup ====================================== 1000/1000 | eta: 0s | 1% bad ## sampling 0/1000 | eta: ?s sampling == 50/1000 | eta: 4s sampling ==== 100/1000 | eta: 5s sampling ====== 150/1000 | eta: 4s sampling ======== 200/1000 | eta: 4s sampling ========== 250/1000 | eta: 3s sampling =========== 300/1000 | eta: 3s sampling ============= 350/1000 | eta: 3s sampling =============== 400/1000 | eta: 3s sampling ================= 450/1000 | eta: 3s sampling =================== 500/1000 | eta: 3s sampling ===================== 550/1000 | eta: 2s sampling ======================= 600/1000 | eta: 2s sampling ========================= 650/1000 | eta: 2s sampling =========================== 700/1000 | eta: 2s sampling ============================ 750/1000 | eta: 1s sampling ============================== 800/1000 | eta: 1s sampling ================================ 850/1000 | eta: 1s sampling ================================== 900/1000 | eta: 1s sampling ==================================== 950/1000 | eta: 0s sampling ====================================== 1000/1000 | eta: 0s Lâ€™inspection de lâ€™Ã©chantillonnage peut Ãªtre effectuÃ©e grÃ¢ce au module bayesplot. mcmc_combo(draws, combo = c(&quot;hist&quot;, &quot;trace&quot;)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Ã€ gauche, nous obtenons la distribution des paramÃ¨tres. Ã€ droite, nous pouvons observer que lâ€™Ã©chantillonnage semble stable. Dans le cas oÃ¹ il ne le serait pas, il faudrait revoir le modÃ¨le. Nous pouvons calculer des intervales de crÃ©dibiltÃ©. draws_tidy &lt;- draws %&gt;% spread_draws(param_mean, param_sd) draws_mean &lt;- confidence_interval(x = draws_tidy$param_mean, on = &quot;deviation&quot;, distribution = &quot;normal&quot;, level = 0.95) print(&quot;Moyenne:&quot;) ## [1] &quot;Moyenne:&quot; draws_mean ## ll mean ul ## 583.5554 598.7328 613.9103 draws_sd &lt;- confidence_interval(x = draws_tidy$param_sd, on = &quot;deviation&quot;, distribution = &quot;normal&quot;, level = 0.95) print(&quot;Ã‰cart-type:&quot;) ## [1] &quot;Ã‰cart-type:&quot; draws_sd ## ll mean ul ## 22.18403 34.16479 46.14555 Lâ€™a priori Ã©tant vague, les rÃ©sultats de lâ€™analyse bayÃ©sienne sont comparables aux statistiques frÃ©quentielles. print(&quot;Erreur, 95%&quot;) ## [1] &quot;Erreur, 95%&quot; print(round(confidence_interval(y20, on=&#39;error&#39;, level=0.95), 2)) ## ll mean ul ## 583.96 598.85 613.75 Les rÃ©sultats des deux approches doivent nÃ©anmoins Ãªtre interprÃ©tÃ©s de maniÃ¨re diffÃ©rente. En ce qui a trait Ã  la moyenne: FrÃ©quentiel. Il y a une probabilitÃ© de 95% que mes donnÃ©es aient Ã©tÃ© gÃ©nÃ©rÃ©es Ã  partir dâ€™une moyenne se situant entre 584 et 614 grammes. BayÃ©sien. Ã‰tant donnÃ©e mes connaissances (vagues) de la moyenne et de lâ€™Ã©cart-type avant de procÃ©der Ã  lâ€™analyse (a priori), il y a une probabilitÃ© de 95% que la moyenne de la masse de la population se situe entre 583.6 et 613.9 grammes. Nous avons une idÃ©e de la distribution des paramÃ¨tresâ€¦ mais pas de la masse dans la population. Pas de problÃ¨me: nous avons des Ã©chantillons de moyennes et dâ€™Ã©cart-type. Nous pouvons les Ã©chantilonnÃ©s avec remplacement pour gÃ©nÃ©rer des possibilitÃ©s de distrbution, puis Ã©chantillonnÃ© une masse selon ces distributions Ã©chantillonnÃ©es. Disonsâ€¦ 10000? Figure 7.1: Source: Star Wars, a new hope Yep, 10 000. n_mass &lt;- 10000 sim_mass &lt;- rep(0, n_mass) for (i in 1:n_mass) { sim_mean &lt;- sample(draws_tidy$param_mean) sim_sd &lt;- sample(draws_tidy$param_sd) sim_mass[i] &lt;- rnorm(1, sim_mean, sim_sd) } La distribution avec laquelle jâ€™ai crÃ©Ã© les donnÃ©es y20 plus haut avait une moyenne de 600 et un Ã©cart-type de 30. Je la superpose ici avec La distribution modÃ©lisÃ©e avec notre petit modÃ¨le bayÃ©sien. x_ &lt;- seq(450, 750, 5) plot(x_, dnorm(x_, 600, 30), lty = 3, col = &quot;red&quot;, type = &quot;l&quot;, xlab = &quot;Mass (g)&quot;, ylab = &quot;Density&quot;) lines(density(sim_mass), col = &quot;blue&quot;) sim_mass_limits &lt;- confidence_interval(x = sim_mass, on = &quot;deviation&quot;, distribution = &quot;normal&quot;, level = 0.95) abline(v = sim_mass_limits[1], lty = 2, col = &quot;blue&quot;) abline(v = sim_mass_limits[3], lty = 2, col = &quot;blue&quot;) text(x = sim_mass_limits[1], y = 0.01, labels = round(sim_mass_limits[1]), pos = 2, col = &quot;blue&quot;) text(x = sim_mass_limits[3], y = 0.01, labels = round(sim_mass_limits[3]), pos = 4, col = &quot;blue&quot;) Raisonnement bayÃ©sien: Ã‰tant donnÃ©e mes connaissances vagues de la moyenne et de lâ€™Ã©cart-type avant de procÃ©der Ã  lâ€™analyse, il y a une probabilitÃ© de 95% que la masse de la population se situe entre 529.1 et 668.9 grammes. Nous avons maintenant une idÃ©e de la distribution de moyenne de la population. Mais, rarement, une analyse sâ€™arrÃªtera Ã  ce stade. Il arrive souvent que lâ€™on doive comparer les paramÃ¨tres de deux, voire plusieurs groupes. Par exemple, comparer des populations vivants dans des Ã©cosystÃ¨mes diffÃ©rents, ou comparer un traitement Ã  un placÃ©bo. Ou bien, comparer, dans une mÃªme population de faucons pÃ©lerins, lâ€™envergure des ailes des mÃ¢les et celle des femelles. 7.6 Test de t: DiffÃ©rence entre des groupes Pour comparer des groupes, on exprime gÃ©nÃ©ralement une hypothÃ¨se nulle, qui typiquement pose quâ€™il nâ€™y a pas de diffÃ©rence entre les groupes. Puis, on choisit un test statistique pour dÃ©terminer si les distributions des donnÃ©es observÃ©es sont plausibles dans si lâ€™hypothÃ¨se nulle est vraie. En dâ€™autres mots, le test statistique exprime la probabilitÃ© que lâ€™on obtienne les donnÃ©es obtenues sâ€™il nâ€™y avait pas de diffÃ©rence entre les groupes. Par exemple, si vous obtenez une p-value de moins de 0.05 aprÃ¨s un test de comparaison et lâ€™hypothÃ¨se nulle pose quâ€™il nâ€™y a pas de diffÃ©rence entre les groupes, cela signifie quâ€™il y a une probabilitÃ© de 5% que vous ayiez obtenu ces donnÃ©es sâ€™il nâ€™y avait en fait pas de diffÃ©rence entre les groupe. Il serait donc peu probable que vos donnÃ©es euent Ã©tÃ© gÃ©nÃ©rÃ©es comme telles sâ€™il nâ€™y avait en fait pas de diffÃ©rence. n_f &lt;- 30 moy_f &lt;- 105 n_m &lt;- 20 moy_m &lt;- 77.5 sd_fm &lt;- 2.75 set.seed(21526) envergure_f &lt;- rnorm(mean=moy_f, sd=sd_fm, n=n_f) envergure_m &lt;- rnorm(mean=moy_m, sd=sd_fm, n=n_m) envergure_f_df &lt;- data.frame(Sex = &quot;Female&quot;, Wingspan = envergure_f) envergure_m_df &lt;- data.frame(Sex = &quot;Male&quot;, Wingspan = envergure_m) envergure_df &lt;- rbind(envergure_f_df, envergure_m_df) envergure_df %&gt;% ggplot(aes(x=Wingspan)) + geom_histogram(aes(y=..density.., fill=Sex)) + geom_density(aes(value=Sex, y=..density..)) ## Warning: Ignoring unknown aesthetics: value ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Et les statistiques des deux groupesL envergure_df %&gt;% group_by(Sex) %&gt;% summarise(mean = mean(Wingspan), sd = sd(Wingspan), n = n()) ## # A tibble: 2 x 4 ## Sex mean sd n ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Female 105. 2.46 30 ## 2 Male 77.0 3.19 20 Ã‰valuer sâ€™il y a une diffÃ©rence significative peut se faire avec un test de t (ou de Student). t.test(envergure_f, envergure_m) ## ## Welch Two Sample t-test ## ## data: envergure_f and envergure_m ## t = 33.235, df = 33.665, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 26.29232 29.71848 ## sample estimates: ## mean of x mean of y ## 105.04267 77.03727 La probabilitÃ© que les donnÃ©es ait Ã©tÃ© gÃ©nÃ©rÃ©es de la sorte si les deux groupes nâ€™Ã©tait semblables est trÃ¨s faible (p-value &lt; 2.2e-16). On obtiendrait sensiblement les mÃªmes rÃ©sultats avec une rÃ©gression linÃ©aire. linmod &lt;- lm(Wingspan ~ Sex, envergure_df) summary(linmod) ## ## Call: ## lm(formula = Wingspan ~ Sex, data = envergure_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.221 -1.938 0.219 2.046 4.686 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 105.0427 0.5062 207.51 &lt;2e-16 *** ## SexMale -28.0054 0.8004 -34.99 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.773 on 48 degrees of freedom ## Multiple R-squared: 0.9623, Adjusted R-squared: 0.9615 ## F-statistic: 1224 on 1 and 48 DF, p-value: &lt; 2.2e-16 Le modÃ¨le linÃ©aire est plus informatif. Il nous apprend que lâ€™envergure des ailes des mÃ¢les est en moyenne plus faible de 28.0 cm que celle des femellesâ€¦ confint(linmod, level = 0.95) ## 2.5 % 97.5 % ## (Intercept) 104.02487 106.06047 ## SexMale -29.61468 -26.39612 â€¦ avec un intervalle de confiance entre -29.6 cm Ã  -26.4 cm. Utilisons lâ€™information dÃ©rivÃ©e de statistiques classiques dans nos a priori. Oui-oui, on peut faire Ã§a. Mais attention, un a priori trop prÃ©cis ou trop collÃ© sur nos donnÃ©es orientera le modÃ¨le vers une solution prÃ©alablement Ã©tablie: ce qui constituerait aucune avancÃ©e par rapport Ã  lâ€™a priori. Nous allons utiliser a priori pour les deux groupes la moyenne des deux groupes, et comme dispersion la moyenne le double de lâ€™Ã©cart-type. Rappelons que cet Ã©cart-type est lâ€™a priori de Ã©cart-type sur la moyenne, non pas de la population. ProcÃ©dons Ã  la crÃ©ation dâ€™un modÃ¨le greta. Nous utiliserons la rÃ©gression linÃ©aire prÃ©fÃ©rablement au test de t. is_female &lt;- model.matrix(~envergure_df$Sex)[, 2] int &lt;- normal(600, 30) coef &lt;- normal(30, 10) sd &lt;- cauchy(0, 10, truncation = c(0, Inf)) mu &lt;- int + coef * is_female distribution(envergure_df$Wingspan) &lt;- normal(mu, sd) m &lt;- model(int, coef, sd, mu) plot(m) Go! draws &lt;- mcmc(m, n_samples = 1000) Et les rÃ©sultats. mcmc_combo(draws, combo = c(&quot;dens&quot;, &quot;trace&quot;), pars = c(&quot;int&quot;, &quot;coef&quot;, &quot;sd&quot;)) draws_tidy &lt;- draws %&gt;% spread_draws(int, coef, sd) draws_tidy print(&quot;Intercept:&quot;) confidence_interval(x = draws_tidy$int, on = &quot;deviation&quot;, distribution = &quot;normal&quot;, level = 0.95) print(&quot;Pente:&quot;) confidence_interval(x = draws_tidy$coef, on = &quot;deviation&quot;, distribution = &quot;normal&quot;, level = 0.95) 7.7 ModÃ©lisation multiniveau Vous souvenez-vous en quoi consiste un effet alÃ©atoire? Pour rappel, il sâ€™agit dâ€™un effet global nul mais variable dâ€™un groupe Ã  lâ€™autre, alors quâ€™un effet fixe ne subit pas la contrainte dâ€™effet nul. En modÃ©lisation linÃ©aire, lâ€™effet alÃ©atoire peut se trouver sur lâ€™intercept ou sur une pente (ou plusieurs pentes). Ce concept peut Ãªtre portÃ© naturellement en modÃ©lisation bayÃ©sienne en ajoutant Ã  lâ€™intercept ou Ã  une pente un effet dont lâ€™a priori est une distribution Ã©talÃ©e autour de zÃ©ro (effet global nul, mais variable). Reprenons le modÃ¨le considÃ©rÃ© Ã  la section 6. mmodlin_1 &lt;- lme(fixed = yield ~ lat + long + nitro + topo + bv, random = ~ 1|year/rep, data = lasrosas.corn) data(lasrosas.corn, package = &quot;agridat&quot;) lasrosas.corn$year_rep &lt;- paste0(lasrosas.corn$year, &quot;_&quot;, lasrosas.corn$rep) lasrosas.corn_sc &lt;- lasrosas.corn %&gt;% select(lat, long, nitro, bv) %&gt;% mutate_all(scale) %&gt;% bind_cols(lasrosas.corn %&gt;% select(-lat, -long, -nitro, -bv)) %&gt;% mutate(year = as.factor(year)) corn_modmat &lt;- model.matrix(~lat + long + nitro + topo + bv + year + year_rep, data = lasrosas.corn_sc) #head(corn_modmat) DÃ©finissons dâ€™abord nos a priori sur les pentes 7.8 Pour aller plus loin Le module greta est conÃ§u et maintenu par Nick Golding, du Quantitative &amp; Applied Ecology Group de lâ€™University of Melbourne, Australie. La documentation de greta offre des recettes pour toutes sortes dâ€™analyses en Ã©cologie. Les livres de Mark KÃ©ry, bien que rÃ©digÃ©s pour les calculs en langage R et WinBUGS, offre une approche bien structurÃ©e et traduisible en greta, qui est plus moderne que WinBUGS. Introduction to WinBUGS for Ecologists (2010) Bayesian Population Analysis using WinBUGS: A Hierarchical Perspective (2011) Applied Hierarchical Modeling in Ecology: Analysis of distribution, abundance and species richness in R and BUGS (2015) "],
["chapitre-explorer.html", "8 Explorer R 8.1 R sur le web 8.2 R en chaire et en os 8.3 Quelques outils en Ã©cologie mathÃ©matique avec R", " 8 Explorer R Lâ€™apprentissage de R peut Ãªtre Ã©tourdissant. Cette section est une petite pause fourre-tout qui vous introduira aux nombreuses possibilitÃ©s de R. ï¸Â Objectifs spÃ©cifiques: Ã€ la fin de ce chapitre, vous serez en mesure dâ€™identifier les sources dâ€™information principales sur le dÃ©veloppement de R et de ses modules comprendrez lâ€™importance du prÃ©traitement des donnÃ©es, en particulier dans le cadre de lâ€™analyse de donnÃ©es compositionnelles, et saurez effectuer un prÃ©traitement adÃ©quat saurez comment acquÃ©rir des donnÃ©es mÃ©tÃ©o dâ€™Environnement Canada avec le module weathercan saurez identifier les modules dâ€™analyse de sols (soiltexture et aqp) saurez comment dÃ©buter un projet de mÃ©ta-analyse et de dÃ©ploiement dâ€™un logiciel sur R Pour certains, le langage R est un labyrinthe. Pour dâ€™autres, câ€™est une myriade de portes ouvertes. Si vous lisez ce manuel, vous vous Ãªtes peut-Ãªtre engagÃ© dans un labyrinthe dans lâ€™objectif dâ€™y trouver la clÃ© qui dÃ©vÃ©rouillera une porte bien prÃ©cise qui mÃ¨ne Ã  un trÃ©sor, un objet magiqueâ€¦ ou un diplÃ´me. Peut-Ãªtre aussi prendrez-vous le goÃ»t dâ€™errer dans ce labyrinthe, explorant ses dÃ©bouchÃ©s, pour y dÃ©nicher au hasard des petits outils et des dÃ©bouchÃ©s. SÃ©quence du jeu vidÃ©o The legend of Zelda. Cette section est un amalgame de plusieurs outils de R pertinents en analyse Ã©cologique. 8.1 R sur le web Dans un environnement de travail en Ã©volution rapide et constante, il est difficile de considÃ©rer que ses compÃ©tences sont abouties. Rester informÃ© sur le dÃ©veloppement de R vous permettra de dÃ©nicher de rÃ©soudre des problÃ¨mes persistants de maniÃ¨re plus efficace ou par de nouvelles avenues, et vous offrira mÃªme lâ€™occasion de dÃ©nicher des problÃ¨mes dont vous ne soupÃ§onniez pas lâ€™existance. Plusieurs sources dâ€™information vous permettront de vous tenir Ã  jour sur le dÃ©veloppement de R, de ses environnement de travail (RStudio, Jupyter, Atom, etc.) et des nouveaux modules qui sâ€™y greffent. Plus largement, vous gagnerez Ã  vous informer sur les derniÃ¨res tendances en calcul scientifique sur dâ€™autres plate-forme que R (Python, Javascript, Julia, etc.). Ã‰videmment, nos tÃ¢ches quotidiennes ne nous permettent pas de tout suivre. MÃªme si vous pouviez nâ€™attrapper quâ€™1% du dÃ©filement, ce sera dÃ©jÃ  1% de plus que rien du tout. Ã‰videmment, rester au courant aide parce que vous en apprenez davantage sur les outils et leurs applications. Mais Ã§a aide aussi parce que Ã§a vous permet de connaÃ®tre des gens et des organisations! Il est trÃ¨s utile de savoir qui travaille sur quoi et oÃ¹ se dÃ©roulent les dÃ©veloppements sur un sujet donnÃ©, car si vous cherchez consciemment quelque chose plus tard, Ã§a vous aidera Ã  trouver votre chemin plus facilement. - MaÃ«lle Salmon, Keeping up to date with R news (ma traduction) Je vous propose une liste de ressources. Ne vous y tenez surtout pas: discartez ce qui ne vous convient pas, et partez Ã  lâ€™aventure! The Hobbit: An Unexpected Journey, Peter Jackson (2012) 8.1.1 GitHub Nous verrons au chapitre 5 lâ€™importance dâ€™utilser des outils dâ€™archivage et de suivi de version, comme git, dans le dÃ©ploiement de la science ouverte. Pour lâ€™instant, retenons que GitHub est une plate-forme git en ligne acquise par Microsoft qui est devenue un rÃ©seau social de dÃ©veloppement informatique. De nombreux modules de R y sont dÃ©veloppÃ©s. Au chapitre 5, vous serez invitÃ©s Ã  y ouvrir un compte et Ã  y archiver du contenu. Vous pourrez alors suivre (dans le mÃªme sens que sur Facebook ou Twitter) le dÃ©veloppement de projets et suivre les travaux des personnes qui vous semblent dâ€™intÃ©rÃªt. 8.1.2 Twitter Le hashtag #rstats rassemble sur Twitter ce qui se tweete sur le sujet. On y retrouve les comptes de R-bloggers, RStudio et rOpenSci. Certaines communautÃ© y sont aussi actives, comme R4DS online learning community, qui partage des nouvelles sur R, et R-Ladies Global, qui vise Ã  amener davantage de diversitÃ© Ã  la communautÃ© de R. Des comptes thÃ©matiques comme Daily R Cheatsheets et One R Package a Day permettent de dÃ©couvrir quotidiennement de nouvelles possibilitÃ©s. Enfin, plusieurs personnes contribuent positivement Ã  la communautÃ© R. Hadley Wickham brille parmi les Ã©toiles de R. Les comptes de Mara Averick, Claus Wilke et David Robinson sont aussi intÃ©ressants. 8.1.3 Nouvelles Le site dâ€™aggrÃ©gation R-bloggers, mis Ã  jour quotidiennement, republie des articles en anglais tirÃ©s dâ€™un peu partout sur la toile. On y trouve principalement des tutoriels et des annonces de nouveaux dÃ©veloppement. Deux fois par mois, lâ€™organisation rOpenSci offre un portrait de lâ€™univ-R (ğŸ’©), ce que R Weekely offre de maniÃ¨re hebdomadaire (lâ€™information sera probablement redondante). Le tidyverse a quant Ã  lui son propre blogue. 8.1.4 Des questions? Bien que davantage vouÃ©s Ã  la rÃ©solution de problÃ¨me quâ€™ Ã  lâ€™exploration de nouvelles opportunitÃ©s, Stackoverflow et Cross Validated sont des plate-forme prisÃ©es. De plus, la liste de courriels r-sig-ecology permet des Ã©changes entre professionnels et novices en analyse de donnÃ©es Ã©cologiques avec R. 8.1.5 Participer R est un logiciel basÃ© sur une communautÃ© de dÃ©veloppement, dâ€™utilisation et de vulgarisation. Des personnes offrent gÃ©nÃ©reusement du temps de support. Si vous vous sentez Ã  lâ€™aise, offrez aussi le vÃ´tre! 8.1.6 Mise en garde Les modules de R sont dÃ©veloppÃ©s par quiconque le veut bien: leur qualitÃ© nâ€™est pas nÃ©cessairement auditÃ©e. Souvent, ils ne sont vÃ©rifiÃ©s que par une vigilance communautaire: dans ce cas, vous Ãªtes les cobailles. Ce qui nâ€™est pas nÃ©cessairement une mauvaise chose, mais cela nÃ©cessite de prendre ses prÃ©cautions. Dans sa confÃ©rence How to be a resilient R user, MaÃ«lle Salmon propose quelques guides pour juger de la qualitÃ© dâ€™un module. 1. Le module est-il activement dÃ©veloppÃ©? Bien! Attention! 2. Le module est-il bien testÃ©? VÃ©rifiez si le module a fait lâ€™objet dâ€™une publication scientifique, sâ€™il a Ã©tÃ© utilisÃ© avec succÃ¨s dans la litÃ©rature ou dans des documents crÃ©dibles. 3. Le module est-il bien documentÃ©? Un site internet dÃ©diÃ© est-il utilisÃ© pour documenter lâ€™utilisation du module? Les fichiers dâ€™aide sont-ils complets, et sont-ils de bonne qualitÃ©? 4. Le module est-il largement utilisÃ©? Un module peu populaire nâ€™est pas nÃ©cessessairement de mauvaise qualitÃ©: peut-Ãªtre est-il seulement destinÃ© Ã  des applications de niche. Sâ€™il nâ€™est pas un indicateur Ã  lui seul de la soliditÃ© ou la validitÃ© dâ€™un module, une masse critique indique que le module a passÃ© sous la surveillance de plusieurs utilisateurs. Dans GitHub, ceci peut Ãªtre Ã©valuÃ© par le nombre dâ€™Ã©toiles attribuÃ© au module (Ã©quivalent Ã  un Jâ€™aime). 5. Le module est-il dÃ©veloppÃ© par une personne ou une organisation crÃ©dible? On peut affirmer sans trop se compromettre que lâ€™Ã©quipe de RStudio dÃ©veloppe des modules de confiance. Tout comme il faudrait se mÃ©fier dâ€™un module dÃ©veloppÃ© par une personne anonyme. Le module packagemetrics permet dâ€™Ã©valuer ces critÃ¨res. # devtools::install_github(&quot;ropenscilabs/packagemetrics&quot;) library(&quot;packagemetrics&quot;) pm &lt;- package_list_metrics(c(&quot;dplyr&quot;, &quot;ggplot2&quot;, &quot;vegan&quot;, &quot;greta&quot;)) metrics_table(pm) 8.1.7 Prendre tout Ã§a en note Un logiciel de prise de notes (comme Evernote, OneNote, Notion, Simplenote, Turtl, etc.) pourrait vous Ãªtre utile pour retrouver lâ€™information soutirÃ©e de vos flux dâ€™information. Mais certaines personnes consignent simplement leurs informations dans un carnet ou un document de traitement de texte. 8.2 R en chaire et en os Lâ€™UniversitÃ© Laval (institution auprÃ¨s de laquelle ce manuel est dÃ©veloppÃ©) sera haute en mai 2019 de la confÃ©rence R Ã  QuÃ©bec 2019. Des ateliers seront offerts pour les utilisateurs novices et avancÃ©s. 8.3 Quelques outils en Ã©cologie mathÃ©matique avec R 8.3.1 PrÃ©traitement des donnÃ©es Il arrive souvent ques les donnÃ©es brutes ne soient pas exprimÃ©es de maniÃ¨re appropriÃ©e ou optimale pour lâ€™analyse statistique ou la modÃ©lisation. Vous devrez alors effectuer un prÃ©traitement sur ces donnÃ©es. Lors du chapitre 6, nous avons abordÃ© la mise Ã  lâ€™Ã©chelle, oÃ¹ des variables numÃ©riques Ã©taient transformÃ©es pour avoir une moyenne de zÃ©ro et un Ã©cart-type de 1. Cette opÃ©ration permettait dâ€™apprÃ©cier les coefficients et leur incertitude sur une mÃªme Ã©chelle. Lâ€™encodage catÃ©gorielle a quant Ã  lui permi dâ€™utiliser des mÃ©thodes quantitatives sur des donnÃ©es qualitatives. Dans les deux cas, nous nâ€™avons pas utilisÃ© le terme, mais il sâ€™agissait dâ€™un prÃ©traitement, câ€™est-Ã -dire une transformation des donnÃ©es prÃ©alable Ã  lâ€™analyse ou la modÃ©lisation. Un prÃ©traitement peut consister simplement en une transformation logarithmique ou exponentielle. En particulier, si vos donnÃ©es forment une partie dâ€™un tout (exprimÃ©es en pourcentages ou fractions), vous devriez probablement utiliser un prÃ©traitement grÃ¢ce aux outils de lâ€™analyse compositionnelle. Avant de les aborder, nous allons traiter des transformations de base. 8.3.1.1 Standardisation La standardisation consiste Ã  centrer vos donnÃ©es Ã  une moyenne de 0 et Ã  les Ã©chelonner Ã  une variance de 1, câ€™est-Ã -dire \\[x_{standard} = \\frac{x - \\bar{x}}{\\sigma}\\] oÃ¹ \\(\\bar{x}\\) est la moyenne du vecteur \\(x\\) et oÃ¹ \\(\\sigma\\) est son Ã©cart-type. Ce prÃ©traitement des donnÃ©es peut sâ€™avÃ©rÃ©r utile lorsque la modÃ©lisation tient compte de lâ€™Ã©chelle de vos mesures (par exemple, les paramÃ¨tres de rÃ©gression vus au chapitre 6 ou les distances que nous verrons au chapitre 9). En effet, les pentes dâ€™une rÃ©gression linÃ©aire multiple ne pourront Ãªtre comparÃ©es entre elles que si elles sont une mÃªme Ã©chelle. Par exemple, on veut modÃ©liser la consommation en miles au gallon (mpg) de voitures en fonction de leur puissance (hp), le temps en secondes pour parcourir un quart de mile (qsec) et le nombre de cylindre. data(&quot;mtcars&quot;) modl &lt;- lm(mpg ~ hp + qsec + cyl, mtcars) summary(modl) ## ## Call: ## lm(formula = mpg ~ hp + qsec + cyl, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.3223 -1.9483 -0.5656 1.5452 7.7773 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 55.30540 9.03697 6.120 1.33e-06 *** ## hp -0.03552 0.01622 -2.190 0.03700 * ## qsec -0.89424 0.42755 -2.092 0.04567 * ## cyl -2.26960 0.54505 -4.164 0.00027 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.003 on 28 degrees of freedom ## Multiple R-squared: 0.7757, Adjusted R-squared: 0.7517 ## F-statistic: 32.29 on 3 and 28 DF, p-value: 3.135e-09 Les pentes signifient que la distance parcourue par gallon dâ€™essence diminue de 0.03552 miles au gallon pour chaque HP, de 0.89242 par seconde au quart de mile et de 2.2696 par cyclindre additionnel. Lâ€™interprÃ©tation est conviviale Ã  cette Ã©chelle. Mais lequel de ces effets est le plus important? L t value indique que ce seraient les cylindres. Mais pour juger lâ€™importance en terme de pente, il vaudrait mieux standardiser. library(&quot;tidyverse&quot;) standardise &lt;- function(x) (x-mean(x))/sd(x) mtcars_sc &lt;- mtcars %&gt;% mutate_if(is.numeric, standardise) # ou bien scale(mtcars, center = TRUE, scale = TRUE) modl_sc &lt;- lm(mpg ~ hp + qsec + cyl, mtcars_sc) summary(modl_sc) ## ## Call: ## lm(formula = mpg ~ hp + qsec + cyl, data = mtcars_sc) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.71716 -0.32326 -0.09384 0.25639 1.29042 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.061e-16 8.808e-02 0.000 1.00000 ## hp -4.041e-01 1.845e-01 -2.190 0.03700 * ## qsec -2.651e-01 1.268e-01 -2.092 0.04567 * ## cyl -6.725e-01 1.615e-01 -4.164 0.00027 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4983 on 28 degrees of freedom ## Multiple R-squared: 0.7757, Adjusted R-squared: 0.7517 ## F-statistic: 32.29 on 3 and 28 DF, p-value: 3.135e-09 Les valeurs des pentes ne peuvent plus Ãªtre interprÃ©tÃ©es directement, mais peuvent maintenant Ãªtre comparÃ©es entre elles. Dans ce cas, le nombre de cilyndres a en effet une importance plus grande que la puissance et le temps pour parcourir un 1/4 de mile. Les algorithmes basÃ©s sur des distances auront, de mÃªme, avantage Ã  Ãªtre standardisÃ©s. 8.3.1.2 Ã€ lâ€™Ã©chelle de la plage Si vous dÃ©sirez prÃ©server le zÃ©ro dans le cas de donnÃ©es positives ou plus gÃ©nÃ©ralement vous voulez que vos donnÃ©es prÃ©traitÃ©es soient positives, vous pouvez les transformer Ã  lâ€™Ã©chelle de la plage, câ€™est-Ã -dire les forcer Ã  sâ€™Ã©taler de 0 Ã  1: \\[ x_{range01} = \\frac{x - x_{min}}{x_{max} - x_{min}} \\] Cette transformation est sensible aux valeurs aberrantes, et une fois le vecteur transformÃ© les valeurs aberrantes seront toutefois plus difficiles Ã  dÃ©tecter. range_01 &lt;- function(x) (x-min(x))/(max(x) - min(x)) mtcars %&gt;% mutate_if(is.numeric, range_01) %&gt;% # en fait, toutes les colonnes sont numÃ©riques, alors mutate_all aurait pu Ãªtre utilisÃ© au lieu de mutate_if sample_n(4) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 0.3531915 1 0.7206286 0.4346290 0.17972350 0.49271286 0.3000000 0 0 0.0 0.1428571 ## 2 1.0000000 0 0.0000000 0.0459364 0.67281106 0.08233188 0.6428571 1 1 0.5 0.0000000 ## 3 0.5276596 0 0.1738588 0.1519435 0.53456221 0.41856303 1.0000000 1 0 0.5 0.1428571 ## 4 0.0000000 1 1.0000000 0.5406360 0.07834101 0.95551010 0.4142857 0 0 0.0 0.4285714 8.3.1.3 Normaliser Le terme normaliser est associer Ã  des opÃ©rations diffÃ©rentes dans la littÃ©rature. Nous prendrons la nomenclature de scikit-learn, pour qui la normalisation consiste Ã  faire en sorte que la longueur du vecteur (sa norme, dâ€™oÃ¹ normaliser) soit unitaire. Cette opÃ©ration est le plus souvent utilisÃ©e par observation (ligne), non pas par variable (colonne). Il existe plusieurs maniÃ¨res de mesures la distance dâ€™un vecteur, mais la plus commune est la distance euclidienne. La seule fois que jâ€™ai eu Ã  utiliser ce prÃ©traitement Ã©tait en analyse spectrale (Chemometrics with R, Ron Wehrens, 2011, chapitre 3.5). En R, library(&quot;pls&quot;) ## ## Attaching package: &#39;pls&#39; ## The following object is masked from &#39;package:vegan&#39;: ## ## scores ## The following object is masked from &#39;package:stats&#39;: ## ## loadings data(&quot;gasoline&quot;) spectro &lt;- gasoline$NIR %&gt;% unclass() %&gt;% as_tibble() normalise &lt;- function(x) x/sqrt(sum(x^2)) spectro_norm &lt;- spectro %&gt;% rowwise() %&gt;% # diffÃ©rentes approches possibles pour les opÃ©rations sur les lignes normalise() spectro_norm[1:4, 1:4] ## 900 nm 902 nm 904 nm 906 nm ## 1 -0.0011224834 -0.0010265446 -0.0009434425 -0.0008314021 ## 2 -0.0009890637 -0.0008856332 -0.0007977676 -0.0006912734 ## 3 -0.0010481029 -0.0009227116 -0.0008269742 -0.0007035061 ## 4 -0.0010444801 -0.0009446277 -0.0008623530 -0.0007718261 8.3.1.4 Analyse compositionnelle en R En 1898, le statisticien Karl Pearson nota que des corrÃ©lations Ã©taient induites lorsque lâ€™on effectuait des ratios par rapport Ã  une variable commune. Source Karl Pearson, 1897. Mathematical contributions to the theory of evolution.â€”on a form of spurious correlation which may arise when indices are used in the measurement of organs. Proceedings of the royal society of London Faisons lâ€™exercice! Nous gÃ©nÃ©rons au hasard 1000 donnÃ©es (comme le proposait Pearson) pour trois dimensions: le fÃ©mur, le tibia et lâ€™humÃ©rus. Ces dimensions ne sont pas gÃ©nÃ©rÃ©es par des distributions corrÃ©lÃ©es. set.seed(3570536) n &lt;- 1000 bones &lt;- tibble(femur = rnorm(n, 10, 3), tibia = rnorm(n, 8, 2), humerus = rnorm(n, 6, 2)) plot(bones) cor(bones) ## femur tibia humerus ## femur 1.000000000 -0.069006171 0.002652292 ## tibia -0.069006171 1.000000000 -0.008994704 ## humerus 0.002652292 -0.008994704 1.000000000 Pourtant, si jâ€™utilise des ratios allomÃ©triques avec lâ€™humÃ©rus comme base, bones_r &lt;- bones %&gt;% transmute(fh = femur/humerus, th = tibia/humerus) plot(bones_r) text(30, 20, paste(&quot;corrÃ©lation =&quot;, round(cor(bones_r$fh, bones_r$th), 2)), col = &quot;blue&quot;) Nous avons induit ce que Pearson appelait une fausse corrÃ©lation (spurious correlation). En 1960, Chayes proposa que de telles fausses corrÃ©lations sont induites non seulement sur des ratios de valeurs absolues, mais aussi sur des ratios dâ€™une somme totale. Par exemple, dans une composition simple de deux types dâ€™utilisation du territoire, si une proportion augmente, lâ€™autre doit nÃ©cessairement diminuer. n &lt;- 100 tibble(A = runif(n, 0, 1)) %&gt;% mutate(B = 1 - A) %&gt;% ggplot(aes(x=A, y=B)) + geom_point() Les variables exprimÃ©es relativement Ã  une somme totale sont dites compositionnelles. Elles possÃ¨dent les caractÃ©ristiques suivantes. Redondance dâ€™information. Un systÃ¨me de deux proportions ne contient quâ€™une seule variable du fait que lâ€™on puisse dÃ©duire lâ€™une en soutrayant lâ€™autre de la somme totale. Un vecteur compositionnel contient de lâ€™information redondante. Pourtant, effectuer des statistiques sur lâ€™une plutÃ´t que sur lâ€™autre donnera des rÃ©sultats diffÃ©rents. DÃ©pendance dâ€™Ã©chelle. Les statistiques devraient Ãªtre indÃ©pendantes de la somme totale utilisÃ©e. Pourtant, elles diffÃ©reront sur lâ€™on utilise par exemple, une proportion des mÃ¢les dâ€™une part et des femelles dâ€™autre part, ou la proportion de la somme des deux, de mÃªme que les rÃ©sultats dâ€™un test sanguin diffÃ©rera si lâ€™on utilise une base sÃ¨che ou une base humide. Distribution thÃ©orique des donnÃ©es. Ã‰tant donnÃ©e que les proportions sont confinÃ©es entre 0 et 1 (ou 100%, ou une somme totale quelconque), la distribution normale (qui sâ€™Ã©tend de -âˆ Ã  +âˆ) nâ€™est souvent pas appropriÃ©e. On pourra utiliser la distribution de Dirichlet ou la distribution logitique-normale, mais dâ€™autres approches sont souvent plus pratiques. Pour illustrer lâ€™effet de la distribution, voyons un diagramme ternaire incluant le sable, le limon et lâ€™argile. En utilisant des Ã©cart-types univariÃ©s, nous obtenons lâ€™ellipse en rouge, qui non seulement reprÃ©sente peu lâ€™Ã©talement des donnÃ©es, mais elle dÃ©passe les bornes du triangle, admettant ainsi des proportions nÃ©gatives. En bleu, la distribution logistique normale (issue des mÃ©thodes prÃ©sentÃ©es plus loin dans cette section) convient davantage. Les consÃ©quences dâ€™effectuer des statistiques linÃ©aires sur des donnÃ©es compositionnelles brutes peuvent Ãªtre majeures. En outre, Pawlowksy-Glahn et Egozcue (2006), sâ€™appuyant en outre sur Rock (1988), note les problÃ¨mes suivants (exprimÃ©s en mes mots). les rÃ©gressions, les regroupements et les analyses en composantes principales peuvent avoir peu ou pas de signification les propriÃ©tÃ©s des distributions peuvent Ãªtre gÃ©nÃ©rÃ©es par lâ€™opÃ©ration de fermeture de la composition (sâ€™assurer que le total des proportions donne 100%) les rÃ©sultats dâ€™analyses discriminantes linÃ©aires sont propices Ã  Ãªtre illusoires tous les coefficients de corrÃ©lation seront affectÃ©s Ã  des degrÃ©s inconnus les rÃ©sultats des tests dâ€™hypothÃ¨ses seront intrinsÃ¨quement faussÃ©s Pour contourner ces problÃ¨mes, il faut dâ€™abord aborder les donnÃ©es compositionnelles pour ce quâ€™elles sont: des donnÃ©es intrinsÃ¨quement multivariÃ©es. Elles sont un nuage de point, et non pas une collection de variables individuelles. Ceci qui nâ€™empÃªche pas dâ€™effectuer des analyses consciencieusement sous des angles particuliers. En R, on pourra aisÃ©ment rapporter une composition en somme unitaire grÃ¢ce Ã  la fonction apply. Mais auparavant, chargeons le module compositions (nâ€™oubliez pas de lâ€™installer au prÃ©alable) pour accÃ©der Ã  des donnÃ©es fictives de proportions de sable, limon et argile dans des sÃ©diments. library(&quot;compositions&quot;) ## Loading required package: tensorA ## ## Attaching package: &#39;tensorA&#39; ## The following object is masked from &#39;package:base&#39;: ## ## norm ## Loading required package: robustbase ## Loading required package: bayesm ## Welcome to compositions, a package for compositional data analysis. ## Find an intro with &quot;? compositions&quot; ## ## Attaching package: &#39;compositions&#39; ## The following object is masked from &#39;package:pls&#39;: ## ## R2 ## The following object is masked from &#39;package:greta&#39;: ## ## %*% ## The following objects are masked from &#39;package:stats&#39;: ## ## cor, cov, dist, var ## The following objects are masked from &#39;package:base&#39;: ## ## %*%, scale, scale.default data(&quot;ArcticLake&quot;) ArcticLake &lt;- ArcticLake %&gt;% as_tibble() head(ArcticLake) ## # A tibble: 6 x 4 ## sand silt clay depth ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 77.5 19.5 3 10.4 ## 2 71.9 24.9 3.2 11.7 ## 3 50.7 36.1 13.2 12.8 ## 4 52.2 40.9 6.6 13 ## 5 70 26.5 3.5 15.7 ## 6 66.5 32.2 1.3 16.3 comp &lt;- ArcticLake %&gt;% select(-depth) %&gt;% apply(., 1, function(x) x/sum(x)) %&gt;% t() comp[1:5, ] ## sand silt clay ## [1,] 0.7750000 0.1950000 0.0300000 ## [2,] 0.7190000 0.2490000 0.0320000 ## [3,] 0.5070000 0.3610000 0.1320000 ## [4,] 0.5235707 0.4102307 0.0661986 ## [5,] 0.7000000 0.2650000 0.0350000 On pourra aussi utiliser la fonction acomp (pour Aitchison-composition) pour fermer la composition Ã  une somme de 1. comp &lt;- ArcticLake %&gt;% select(-depth) %&gt;% acomp(.) comp[1:5, ] ## sand silt clay ## [1,] 0.7750000 0.1950000 0.0300000 ## [2,] 0.7190000 0.2490000 0.0320000 ## [3,] 0.5070000 0.3610000 0.1320000 ## [4,] 0.5235707 0.4102307 0.0661986 ## [5,] 0.7000000 0.2650000 0.0350000 Cette stratÃ©gie a pour avantage dâ€™attribuer Ã  la variable comp la classe acomp, qui automatise les opÃ©rations dans lâ€™espace compositionnel (que lâ€™on nomme aussi le simplex). La reprÃ©sentation ternaire est souvent utilisÃ©e pour prÃ©senter des compositions. Toutefois, il est difficile dâ€™interprÃ©ter les compositions de plus de trois parties. La classe acomp automatise aussi la reprÃ©sentation teranaire. plot(comp) Afin de transposer cet espace clÃ´t en un espace ouvert, on pourra diviser chaque proportion par une proportion de rÃ©fÃ©rence choisie parmi nâ€™importe quelle proportion. Du coup, on retire une dimension redondante! Dans ce ratio, on choisit dâ€™utiliser la proportion de rÃ©fÃ©rence au dÃ©nominateur, ce qui est arbitraire. En utilisant le log du ratio, lâ€™inverse du ratio ne sera quâ€™un changement de signe, ce qui est pratique en statistiques linÃ©aries. Cette solution, proposÃ©e par Aitchison (1986), sâ€™applique non seulement sur les compositions Ã  deux composantes, mais sur toute composition. Il sâ€™agit alors dâ€™utiliser une composition de rÃ©fÃ©rence pour effecteur les ratios. Pour une composition de \\(A\\), \\(B\\), \\(C\\), \\(D\\) et \\(E\\): \\[alr_A = log \\left( \\frac{A}{E} \\right), alr_B = log \\left( \\frac{B}{E} \\right), alr_C = log \\left( \\frac{C}{E} \\right), alr_D = log \\left( \\frac{D}{E} \\right)\\] Dans R, la colonne de rÃ©fÃ©rence est par dÃ©faut la derniÃ¨re colonne de la matrice des compositions. add_lr &lt;- alr(comp) Cette derniÃ¨re stratÃ©gie se nomme les log-ratios aditifs (\\(alr\\) pour additive log-ratio). Bien que valide pour effectuer des tests statistiques, cette stratÃ©gie a le dÃ©savantage de dÃ©pendre de la dÃ©cision arbitraire de la composante Ã  utiliser au numÃ©rateur. DeuxiÃ¨me restriction des alr: les axes de lâ€™espace des alr nâ€™Ã©tant pas orthogonaux, ils ne peuvent pas Ãªtre utilisÃ©s pour effectuer des statistiques basÃ©es sur les distances (que nous couvrirons au chapitre 9). Lâ€™autre stratÃ©gie proposÃ©e par Aitchison Ã©tait dâ€™effectuer un log-ratio entre chaque composante et la moyenne gÃ©omÃ©trique de toutes les composantes. Cette transformation se nomme le log-ratio centrÃ© (\\(clr\\), pour centered log-ratio) \\[clr_i = log \\left( \\frac{x_i}{g \\left( x \\right)} \\right)\\] En R, cen_lr &lt;- clr(comp) Avec des CLRs, les distances sont valides. Maisâ€¦ nous restons avec le problÃ¨me de la redondance dâ€™information. En fait, la somme de chacunes des lignes dâ€™une matrice de clr est de 0. Pas trÃ¨s pratique lorsque lâ€™on effectue des statistiques incluant une inversion de la matrice de covariance (distance de Mahalanobis, gÃ©ostatistiques, etc.) cen_lr %&gt;% cov() %&gt;% solve() Error in solve.default(.) : le systÃ¨me est numÃ©riquement singulier : conditionnement de la rÃ©ciproque = 4.44407e-17 Enfin, une autre mÃ©thode de transformation dÃ©veloppÃ©e par Egoscue et al. (2003), les log-ratios isomÃ©triques (ou isometric log-ratios, ilr) projette les compositions comprenant D composantes dans un espace restreint de D-1 dimensions orthonormÃ©es. Ces dimensions doivent doivent Ãªtre prÃ©alablement Ã©tablie dans un dendrogramme de bifurcation, oÃ¹ chaque composante ou groupe de composante est successivement divisÃ© en deux embranchement. La maniÃ¨re dâ€™arranger ces balances importe peu, mais on aura avantage Ã  crÃ©er des balances interprÃ©tables. Le diagramme de balances peut Ãªtre encodÃ© dans une partition binaire sÃ©quentielle (ou sequential bianry partition, sbp). Une sbp est une matrice de contraste ou chaque ligne reprÃ©sente une partition entre deux variables ou groupes de variables. Une composante Ã©tiquettÃ©e +1 correspondra au groupe du numÃ©rateur, une composante Ã©tiquettÃ©e -1 au dÃ©nominateur et une composante Ã©tiquettÃ©e 0 sera exclue de la partition (Parent et al., 2013). Jâ€™ai reformulÃ© la fonction CoDaDendrogram pour que lâ€™on puisse ajouter des informations intÃ©ressantes sur les balants horizontaux. Cette fonction est disponible sur github. source(&quot;https://raw.githubusercontent.com/essicolo/AgFun/master/codadend2.R&quot;) sbp &lt;- matrix(c(1, 1,-1, 1,-1, 0), byrow = TRUE, ncol = 3) CoDaDendrogram2(comp, V = gsi.buildilrBase(t(sbp)), ylim = c(0, 1), equal.height = TRUE) Si la SBP est plus imposante, il pourrait Ãªtre plus aisÃ© de monter dans un chiffrier, puis de lâ€™importer dans R via un fichier csv. Le calcul des ILRs est effectuÃ© comme suit. \\[ilr_j = \\sqrt{\\frac{n_j^+ n_j^-}{n_j^+ + n_j^-}} log \\left( \\frac{g \\left( c_j^+ \\right)}{g \\left( c_j^+ \\right)} \\right)\\] ou, Ã  la ligne \\(j\\) de la SBP, \\(n_j^+\\) et \\(n_j^-\\) sont respectivement le nombre de composantes au numÃ©rateur et au dÃ©nominateur, \\(g \\left( c_j^+ \\right)\\) est la moyenne gÃ©omÃ©trique des composantes au numÃ©rateur et \\(g \\left( c_j^- \\right)\\) est la moyenne gÃ©omÃ©trique des composantes au dÃ©nominateur. Les balances sont conventionnellement notÃ©es [A,B | C,D], ou les composantes A et B au dÃ©nominateur sont balancÃ©es avec les composantes C and D au numÃ©rateur. Une balance positive signifie que la moyenne gÃ©omÃ©trique des concentrations au numÃ©rateur est supÃ©rieur Ã  celle au dÃ©nominateur, et inversement, alors quâ€™une balance nulle signifie que les moyennes gÃ©omÃ©triques sont Ã©gales (Ã©quilibre). Ainsi, en modÃ©lisation linÃ©aire, un coefficient positif sur [A,B | C,D] signifie que lâ€™augmentation de lâ€™importance de C et D comparativement Ã  A et B est associÃ© Ã  une augmentation de la variable rÃ©ponse du modÃ¨le. En R, iso_lr &lt;- ilr(comp, V = gsi.buildilrBase(t(sbp))) Notez la forme gsi.buildilrBase(t(sbp)) est une opÃ©ration pour obtenir la matrice dâ€™orthonormalitÃ© Ã  partir de la SBP. Les ILRs sont des balances multivariÃ©es sur lesquelles on pourra effectuer des statistiques linÃ©aries. Bien que lâ€™interprÃ©tation des rÃ©sultats comme collection dâ€™interprÃ©tations sur des balances univariÃ©es pourra Ãªtre affectÃ©e par la structure de la SBP, ni les statistiques linÃ©aires multivariÃ©es, ni la distance entre les points ne seront affectÃ©s. En effet, chaque variante de la SBP est une rotation (dâ€™un facteur de 60Â°) par rapport Ã  lâ€™origine: source(&quot;lib/ilr-rotation-sbp.R&quot;) Pour les transformations inverses, vous pourrez utiliser les fonctions alrInv, clrInv et ilrInv. Dans tous les cas, si vous tenez Ã  garder la trace de vos donnÃ©es dans leur format original, vous aurez avantage Ã  ajouter Ã  votre vecteur compositionnel la valeur de remplissage, constituÃ© dâ€™un amalgame des composantes non mesurÃ©es. Par exemple, pourc &lt;- c(N = 0.03, P = 0.001, K = 0.01) acomp(pourc) # vous perdez la trace des proportions originales ## N P K ## 0.73170732 0.02439024 0.24390244 ## attr(,&quot;class&quot;) ## [1] acomp pourc &lt;- c(N = 0.03, P = 0.001, K = 0.01) Fv &lt;- 1 - sum(pourc) comp &lt;- acomp(c(pourc, Fv = Fv)) comp ## N P K Fv ## 0.030 0.001 0.010 0.959 ## attr(,&quot;class&quot;) ## [1] acomp iso_lr &lt;- ilr(comp) # avec une sbp par dÃ©faut ilrInv(iso_lr) ## 1 2 3 4 ## [1,] 0.03 0.001 0.01 0.959 ## attr(,&quot;class&quot;) ## [1] acomp Si vos donnÃ©es font partie dâ€™un tout, je vous recommande chaudement dâ€™utiliser des mÃ©thodes compositionnelles autant pour lâ€™analyse que la modÃ©lisation. Pour en savoir davantage, le livre Compositional data analysis with R, de van den Boogart et Tolosana-Delgado, est disponible en format Ã©lectronique Ã  la bibliothÃ¨que de lâ€™UniversitÃ© Laval. Pour aller plus loin, jâ€™ai Ã©cri un billet Ã  ce sujet (auquel Ã  ce jour il manque toujours un cas dâ€™Ã©tude): We should use balances and machine learning to diagnose ionomes. 8.3.2 AcquÃ©rir des donnÃ©es mÃ©tÃ©o Une tÃ¢che commune en Ã©cologie est de lier des observations Ã  la mÃ©tÃ©oâ€¦ qui sont rarement collectÃ©s lors dâ€™expÃ©riences. Environnement Canada possÃ¨de sont rÃ©seau de stations. Les donnÃ©es sont disponibles sur internet en libre accÃ¨s. Vous pouvez chercher des stations, effectuer des requÃªtes et tÃ©lÃ©charger des fichiers csv. Pour un petit tableau, la tÃ¢che est plutÃ´t triviale. Mais Ã§a devient rapidement laborieux Ã  mesure que lâ€™on doit rechercher de nombreuses donnÃ©es. Le module weathercan, dÃ©veloppÃ© par Steffi LaZerte, permet dâ€™effectuer des requÃªtes rapidement Ã  partir des coordonnÃ©es de votre site expÃ©rimental. Par exemple, si je cherche une station mÃ©tÃ©o sfournissant des donnÃ©es horaires situÃ© Ã  moins de 20 km du sommet du Mont-Bellevue, Ã  Sherbrooke, aux coordonnÃ©es [latitude 45.35, longitude -71.90], library(&quot;weathercan&quot;) station_site &lt;- stations_search(coords = c(45.35, -71.90), dist = 20, interval = &quot;hour&quot;) station_site ## # A tibble: 4 x 15 ## prov station_name station_id climate_id WMO_id TC_id lat lon elev tz interval start end normals distance ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 QC LENNOXVILLE 5397 7024280 71611 WQH 45.4 -71.8 181 Etc/GMT+5 hour 1994 2019 TRUE 6.65 ## 2 QC SHERBROOKE 48371 7028123 71610 YSC 45.4 -71.7 241. Etc/GMT+5 hour 2009 2019 FALSE 19.2 ## 3 QC SHERBROOKE A 5530 7028124 71610 YSC 45.4 -71.7 241. Etc/GMT+5 hour 1962 2005 FALSE 19.4 ## 4 QC SHERBROOKE A 30171 7028126 NA GSC 45.4 -71.7 241. Etc/GMT+5 hour 2004 2009 FALSE 19.4 Je prends en note lâ€™identifiant de la station dÃ©sirÃ©e (ou des stations, disons 5397 et 48371), puis je lance une requÃªte pour obtenir la mÃ©tÃ©o horaire entre les dates dÃ©sirÃ©es. mont_bellevue &lt;- weather_dl(station_ids = c(5397, 48371), start = &quot;2019-02-01&quot;, end = &quot;2019-02-07&quot;, interval = &quot;hour&quot;, verbose = TRUE, tz_disp = &quot;Etc/GMT+5&quot;) ## Warning in weather_dl(station_ids = c(5397, 48371), start = &quot;2019-02-01&quot;, : &#39;tz_disp&#39; is deprecated, see Details under ? ## weather_dlFALSE ## Getting station: 5397 ## Formatting station data: 5397 ## Adding header data: 5397 ## Getting station: 48371 ## Formatting station data: 48371 ## Adding header data: 48371 ## Trimming missing values before and after ## As of weathercan v0.3.0 time display is either local time or UTC ## See Details under ?weather_dl for more information. ## This message is shown once per session mont_bellevue %&gt;% head(5) ## # A tibble: 5 x 35 ## station_name station_id station_operator prov lat lon elev climate_id WMO_id TC_id date time ## &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;dttm&gt; ## 1 LENNOXVILLE 5397 NA QC 45.4 -71.8 181 7024280 71611 WQH 2019-02-01 2019-02-01 00:00:00 ## 2 LENNOXVILLE 5397 NA QC 45.4 -71.8 181 7024280 71611 WQH 2019-02-01 2019-02-01 01:00:00 ## 3 LENNOXVILLE 5397 NA QC 45.4 -71.8 181 7024280 71611 WQH 2019-02-01 2019-02-01 02:00:00 ## 4 LENNOXVILLE 5397 NA QC 45.4 -71.8 181 7024280 71611 WQH 2019-02-01 2019-02-01 03:00:00 ## 5 LENNOXVILLE 5397 NA QC 45.4 -71.8 181 7024280 71611 WQH 2019-02-01 2019-02-01 04:00:00 ## # â€¦ with 23 more variables: year &lt;chr&gt;, month &lt;chr&gt;, day &lt;chr&gt;, hour &lt;chr&gt;, weather &lt;chr&gt;, hmdx &lt;dbl&gt;, hmdx_flag &lt;chr&gt;, ## # pressure &lt;dbl&gt;, pressure_flag &lt;chr&gt;, rel_hum &lt;dbl&gt;, rel_hum_flag &lt;chr&gt;, temp &lt;dbl&gt;, temp_dew &lt;dbl&gt;, temp_dew_flag &lt;chr&gt;, ## # temp_flag &lt;chr&gt;, visib &lt;dbl&gt;, visib_flag &lt;chr&gt;, wind_chill &lt;dbl&gt;, wind_chill_flag &lt;chr&gt;, wind_dir &lt;dbl&gt;, ## # wind_dir_flag &lt;chr&gt;, wind_spd &lt;dbl&gt;, wind_spd_flag &lt;chr&gt; Et voilÃ . mont_bellevue %&gt;% ggplot(aes(x = time, y = temp)) + geom_line(aes(colour = station_name)) 8.3.3 PÃ©domÃ©trie avec R Cette section a Ã©tÃ© Ã©crite par Michael Leblanc. Plusieurs fonctionnalitÃ©s ont Ã©tÃ© dÃ©veloppÃ©es sur R afin dâ€™aider les pÃ©domÃ©triciens Ã  visualiser, explorer et traiter les donnÃ©es numÃ©riques en science des sols. Voici quelques exemples. 8.3.3.1 Texture du sol La texture du sol est dÃ©finie par sa composition granulomÃ©trique, habituellement reprÃ©sentÃ©e par trois fractions (sable, limon, argile), laquelle peut Ãªtre gÃ©nÃ©ralisÃ©e en classe texturale. La dÃ©finition des classes texturales diffÃ¨re dâ€™un systÃ¨me ou dâ€™un pays Ã  lâ€™autre comme en tÃ©moigne lâ€™article Perdus dans le triangle des textures (Richer de Forges et al.Â 2008). La dÃ©finition des fractions granulomÃ©triques peut Ã©galement diffÃ©rer selon le domaine dâ€™Ã©tude (ingÃ©nierie, pÃ©dologie) ou le pays. Par exemple, le diamÃ¨tre du limon est de 0,002 mm Ã  0,05 mm dans le systÃ¨me canadien, amÃ©ricain et franÃ§ais alors quâ€™il est de 0,002 mm Ã  0,02 mm dans le systÃ¨me australien et de 0,002 mm Ã  0,063 mm dans le systÃ¨me allemand. Il est donc important de vÃ©rifier la mÃ©thodologie et le systÃ¨me de classification utilisÃ©s pour interprÃ©ter les donnÃ©es de texture du sol. Le module soilTexture propose des fonctions permettant dâ€™aborder ces multiples dÃ©finitions. library(&quot;soiltexture&quot;) ## soiltexture 1.5.1 (git revision: 4b25ba2). For help type: help(pack=&#39;soiltexture&#39;) 8.3.3.1.1 Les triangles texturaux Avec la fonction TT.plot, vous pouvez prÃ©senter vos donnÃ©es granulomÃ©triques dans un triangle textural tel que dÃ©fini par les diffÃ©rents systÃ¨mes nationaux. Auparavant, crÃ©ons un objet comprenant des textures alÃ©atoires. set.seed(848341) # random.org rand_text &lt;- TT.dataset(n=100, seed.val=29) head(rand_text) ## CLAY SILT SAND Z ## 1 54.650857 40.37101 4.978129 13.2477582 ## 2 44.745954 40.81782 14.436221 20.8433109 ## 3 18.192509 48.26752 33.539970 7.1814626 ## 4 17.750492 40.14405 42.105458 -0.2077358 ## 5 65.518360 23.36110 11.120538 10.8656027 ## 6 6.610293 22.45353 70.936173 3.7108567 Avec le module soiltexture, les tableaux de texture doivent inclure les intitullÃ©s exactes CLAY, SILT et SAND (notez les majuscules). Les points des textures gÃ©nÃ©rÃ©es peuvent Ãªtre portÃ©s dans des diagrammes ternaires texturaux de diffÃ©rents systÃ¨mes de classification, par exemple le systÃ¨me canadioen et le systÃ¨me USDA. par(mfrow=c(1, 2)) TT.plot(class.sys = &quot;CA.FR.TT&quot;, tri.data = rand_text, col = &quot;blue&quot;) TT.plot(class.sys = &quot;USDA.TT&quot;, tri.data = rand_text, col = &quot;blue&quot;) Les paramÃ¨tres de la figure (titres, polices, style de la grille, etc.) peuvent Ãªtre personnalisÃ©s avec les arguments TT.plot. 8.3.3.1.2 Les classes texturales La fonction TT.points.in.classes est utile pour dÃ©signer la classe texturale Ã  partir des donnÃ©es granulomÃ©triques, en spÃ©cifiant bien le systÃ¨me de classification dÃ©sirÃ©. TT.points.in.classes( tri.data = rand_text[1:10, ], # class.sys = &quot;CA.FR.TT&quot;, PiC.type = &quot;t&quot; ) ## [1] &quot;ALi&quot; &quot;ALi&quot; &quot;L&quot; &quot;L&quot; &quot;ALo&quot; &quot;LS&quot; &quot;ALo&quot; &quot;A&quot; &quot;LLi&quot; &quot;LSA&quot; Plusieurs autres fonctions sont proposÃ©es par soiltexture afin de visualiser, classifier et transformer les donnÃ©es de texture du sol : Functions in soiltexture. Julien Moeys (2018) propose Ã©galement le tutoriel The soil texture wizard: a tutorial. 8.3.3.2 Profils de sols Le profil de sols est une entitÃ© dÃ©crite par une sÃ©quence de couches ou dâ€™horizons avec diffÃ©rentes caractÃ©ristiques morphologiques. Le module AQP, pour Algorithms for Quantitative Pedology, propose des fonctions de visualisation, dâ€™agrÃ©gation et de classification permettant dâ€™aborder la complexitÃ© inhÃ©rente aux informations pÃ©dologiques. 8.3.3.2.1 La visualisation de profils Vous devez dâ€™abord structurer vos donnÃ©es dans un tableau (data.frame) incluant minimalement ces trois colonnes : Identifiant unique du profil (groupes dâ€™horizons) (id) Limites supÃ©rieures de lâ€™horizon (top) Limites infÃ©rieures de lâ€™horizon (down) Vos donnÃ©es morphologiques, physico-chimiques, etc., sont incluses dans les autres colonnes. Chargeons un fichier pÃ©dologique Ã  titre dâ€™exemple. profils &lt;- read_csv(&quot;data/06_pedometric-profile.csv&quot;) ## Parsed with column specification: ## cols( ## id = col_double(), ## horizon = col_character(), ## top = col_double(), ## bottom = col_double(), ## hue = col_character(), ## value = col_double(), ## chroma = col_double(), ## pH.CaCl2 = col_double(), ## C.CNS.pc = col_double() ## ) head(profils) ## # A tibble: 6 x 9 ## id horizon top bottom hue value chroma pH.CaCl2 C.CNS.pc ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Ap1 0 23 10YR 2 3 4.78 2.71 ## 2 1 Ap2 23 34 10YR 2 2 4.74 2.2 ## 3 1 Bfcj 34 46 7.5YR 4 5 4.79 2.4 ## 4 1 BC 46 83 2.5Y 4 5 4.93 0.22 ## 5 1 C 83 100 2.5Y 5 4 4.82 0.18 ## 6 2 Ap 0 29 10YR 2 2 4.6 4.22 La fonction munsell2rgb permet de convertir le code de couleur Munsell en format RGB. library(&quot;aqp&quot;) ## This is aqp 1.18.1 ## ## Attaching package: &#39;aqp&#39; ## The following object is masked from &#39;package:greta&#39;: ## ## slice ## The following object is masked from &#39;package:plotly&#39;: ## ## slice ## The following objects are masked from &#39;package:dplyr&#39;: ## ## slice, union ## The following object is masked from &#39;package:base&#39;: ## ## union profils$soil_color &lt;- with(profils, munsell2rgb(hue, value, chroma)) PrÃ©alablement Ã  la visualisation, le tableau est transformÃ© en objet SoilProfileCollection par la fonction depths. Pour ce faire, le tableau doit Ãªtre un pur data.frame, non pas un tibble. profils &lt;- profils %&gt;% as.data.frame() depths(profils) &lt;- id ~ top + bottom La fonction plot dÃ©tectera le type dâ€™objet et appellera la fonction de visualisation en consÃ©quence. par(mfrow = c(1, 3)) plot(profils, name=&quot;horizon&quot;) title(&#39;Couleur des horizons&#39;, cex.main=1) plot(profils, name=&quot;horizon&quot;, color=&#39;C.CNS.pc&#39;, col.label=&#39;C total (%)&#39;) plot(profils, name=&quot;horizon&quot;, color=&#39;pH.CaCl2&#39;, col.label=&#39;pH CaCl2&#39;) De multiples figures thÃ©matiques peuvent Ãªtre gÃ©nÃ©rÃ©es afin de reprÃ©senter les particuliaritÃ©s des profils. Pour aller plus loin, consultez les guides Introduction to SoilProfileCollection Objects et Generating Sketches from SPC Objects. 8.3.3.2.2 Les plans verticaux (depth functions) Les plans verticaux sont des diagrammes qui permettent dâ€™interprÃ©ter les donnÃ©es en fonction de la profondeur. La fonction slab permet le calcul de statistiques descriptives par intervalles de profondeur rÃ©guliers, lesquelles permettent de visualiser la variabilitÃ© verticale des propriÃ©tÃ©s des sols. agg &lt;- slab(profils, fm = ~ C.CNS.pc + pH.CaCl2) La visualisation est gÃ©nÃ©rÃ©e par le module graphique ggplot2 agg %&gt;% ggplot(mapping = aes(x = -top, y = p.q50)) + facet_grid(. ~ variable, scale = &quot;free&quot;) + geom_ribbon(aes(ymin = p.q25, ymax = p.q75), fill = &quot;grey75&quot;, alpha = 0.5) + geom_path() + labs(x = &quot;Profondeur (cm)&quot;, y = &quot;MÃ©diane bordÃ©e des 25e and 75e percentiles&quot;) + coord_flip() 8.3.3.2.3 Le regroupement de profils Le calcul des distances de dissimilaritÃ© entre les profils avec profile_compare permet la construction de dendrogramme et le regroupement des profils. Notez que nous survolerons au chapitre 9 les concepts de dissimilaritÃ© et de partitionnement. library(&quot;cluster&quot;) library(&quot;mvtnorm&quot;) library(&quot;sharpshootR&quot;) # remotes::install_github(&quot;ncss-tech/sharpshootR&quot;) d &lt;- profile_compare(profils, vars=c(&#39;C.CNS.pc&#39;, &#39;pH.CaCl2&#39;), k=0, max_d=40) ## Computing dissimilarity matrices from 10 profiles [0.08 Mb] d_diana &lt;- diana(d) plotProfileDendrogram(profils, name=&quot;horizon&quot;, d_diana, scaling.factor = 0.3, y.offset = 5, color=&#39;pH.CaCl2&#39;, col.label=&#39;pH CaCl2&#39;) 8.3.3.2.4 Diagramme de relations entre les horizons Il est possible de visualiser les transitions dâ€™horizon les plus probables dans un groupe de profils de sols. tp &lt;- hzTransitionProbabilities(profils, name=&quot;horizon&quot;) ## Warning: ties in transition probability matrix par(mar = c(0, 0, 0, 0), mfcol = c(1, 2)) plot(profils, name=&quot;horizon&quot;) plotSoilRelationGraph(tp, graph.mode = &quot;directed&quot;, edge.arrow.size = 0.5, edge.scaling.factor = 2, vertex.label.cex = 0.75, vertex.label.family = &quot;sans&quot;) Consultez AQP project pour des prÃ©sentations, des tutoriels et des exemples de figures qui montrent les nombreuses possibilitÃ©s du package AQP. 8.3.4 MÃ©ta-analyses en R Je conseille les livres Introduction to Meta-Analysis, Meta-analysis with R et Handbook of Meta-analysis in Ecology and Evolution pour les mÃ©ta-analyses sur des Ã©cosystÃ¨mes. Le module metafor est un ioncournable pour effectuer des mÃ©taanalyses en R. On ne passe pas tout Ã  fait Ã  cÃ´tÃ© si lâ€™on utilise le module meta, lui-mÃªme basÃ© en partie sur metafor. Le module meta a touttefois lâ€™avantage dâ€™Ãªtre simple dâ€™utilisation. Par exemple, pour une mÃ©ta-analyse dâ€™une rÃ©ponse continue, library(&quot;meta&quot;) ## Loading &#39;meta&#39; package (version 4.9-9). ## Type &#39;help(meta)&#39; for a brief overview. ## ## Attaching package: &#39;meta&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## ci meta_data &lt;- read_csv(&quot;https://portal.uni-freiburg.de/imbi/_SUPPRESS_ACCESSRULE/lehre/lehrbuecher/meta-analysis-with-r/dataset02.csv&quot;) ## Parsed with column specification: ## cols( ## author = col_character(), ## Ne = col_double(), ## Me = col_double(), ## Se = col_double(), ## Nc = col_double(), ## Mc = col_double(), ## Sc = col_double() ## ) meta_analyse &lt;- metacont(n.e = Ne, mean.e = Me, sd.e = Se, n.c = Nc, mean.c = Mc, sd.c = Sc, data = meta_data, sm = &quot;SMD&quot;) meta_analyse ## SMD 95%-CI %W(fixed) %W(random) ## 1 -0.5990 [-1.3300; 0.1320] 3.5 5.7 ## 2 -0.9518 [-1.6770; -0.2266] 3.6 5.7 ## 3 -0.5909 [-1.6301; 0.4483] 1.7 4.1 ## 4 -0.7064 [-1.7986; 0.3858] 1.6 3.9 ## 5 -0.2815 [-0.6076; 0.0445] 17.6 8.1 ## 6 -0.5375 [-1.0816; 0.0065] 6.3 6.8 ## 7 -1.3204 [-2.1896; -0.4513] 2.5 4.9 ## 8 -0.4800 [-1.3514; 0.3914] 2.5 4.9 ## 9 0.0918 [-0.2549; 0.4385] 15.6 8.0 ## 10 -3.2433 [-4.2035; -2.2831] 2.0 4.5 ## 11 0.0000 [-0.7427; 0.7427] 3.4 5.6 ## 12 -0.7061 [-1.2020; -0.2102] 7.6 7.1 ## 13 -0.4724 [-1.2537; 0.3089] 3.1 5.4 ## 14 -0.1849 [-0.5071; 0.1373] 18.0 8.2 ## 15 -0.0265 [-0.6045; 0.5515] 5.6 6.6 ## 16 -1.1648 [-2.0828; -0.2468] 2.2 4.7 ## 17 -0.2127 [-0.9651; 0.5397] 3.3 5.6 ## ## Number of studies combined: k = 17 ## ## SMD 95%-CI z p-value ## Fixed effect model -0.3915 [-0.5283; -0.2548] -5.61 &lt; 0.0001 ## Random effects model -0.5858 [-0.8703; -0.3013] -4.04 &lt; 0.0001 ## ## Quantifying heterogeneity: ## tau^2 = 0.2309 [0.1376; 0.9813]; tau = 0.4806 [0.3710; 0.9906]; ## I^2 = 72.5% [55.4%; 83.1%]; H = 1.91 [1.50; 2.43] ## ## Test of heterogeneity: ## Q d.f. p-value ## 58.27 16 &lt; 0.0001 ## ## Details on meta-analytical method: ## - Inverse variance method ## - DerSimonian-Laird estimator for tau^2 ## - Jackson method for confidence interval of tau^2 and tau ## - Hedges&#39; g (bias corrected standardised mean difference) Et pour effectuer un forest plot, forest(meta_analyse) 8.3.5 CrÃ©er des applications avec R RStudio vous permet de dÃ©ployer vos rÃ©sultats sous forme dâ€™applications web grÃ¢ce Ã  son module shiny. Pour ce faire, le seul prÃ©alable est de savoir programmer en R. En agenÃ§ant une interface avec des inputs (listes de sÃ©lection, des boÃ®tes de dialogue, des sÃ©lecteurs, des boutons, etc.) avec des modÃ¨les que vous dÃ©veloppez, vous pourrez crÃ©er des interfaces intÃ©ractives. Pour crÃ©er une application shiny, vous devez crÃ©er une partie pour lâ€™interface (ui) et une autre pour le calcul (server). Je nâ€™irai pas dans les dÃ©tails, Ã©tant donnÃ©e quâ€™il sâ€™agit dâ€™un sujet Ã  part entiÃ¨re. Pour aller plus loin, visitez le site du projet shiny. library(&quot;shiny&quot;) ui &lt;- basicPage( sliderInput(&quot;A&quot;, &quot;Asymptote:&quot;, min = 0, max = 100, value = 50), sliderInput(&quot;E&quot;, &quot;Environnement:&quot;, min = -10, max = 100, value = 20), sliderInput(&quot;R&quot;, &quot;Taux:&quot;, min = 0, max = 0.1, value = 0.035), sliderInput(&quot;prix_dose&quot;, &quot;Prix dose:&quot;, min = 0, max = 5, value = 1), sliderInput(&quot;prix_vente&quot;, &quot;Prix vente:&quot;, min = 0, max = 200, value = 100), sliderInput(&quot;dose&quot;, &quot;Dose:&quot;, min = 0, max = 300, value = c(0, 200)), plotOutput(&quot;distPlot&quot;) ) server &lt;- function(input, output) { mitsch_f &lt;- reactive({ input$A * (1 - exp(-input$R * (seq(input$dose[1], input$dose[2], length = 100) + input$E))) }) mitsch_opt &lt;- reactive({ (log((input$A * input$R * input$prix_vente) / input$prix_dose - input$E * input$R) / input$R ) }) output$distPlot &lt;- renderPlot({ plot(seq(input$dose[1], input$dose[2], length = 100), mitsch_f(), type = &quot;l&quot;, ylim = c(0, 100)) abline(v = mitsch_opt() ) text(mitsch_opt(), 2, paste(&quot;Dose optimale:&quot;, round(mitsch_opt(), 0))) }) } shinyApp(ui, server) Une fois lâ€™application crÃ©Ã©e, il est possible de la dÃ©ployer sur le site shninyapps.io. Dâ€™abord crÃ©er une application shiny dans RStudio: File &gt; New File &gt; Shiny Web App. Ã‰crivez votre code dans le fichier app.R (dans ce cas, ce peut Ãªtre un copier-coller), puis cliquez sur Run App en haut Ã  droite de la fenÃªtre dâ€™Ã©dition du code. Lorsque lâ€™application fonctionne, vous pourrez la publier via RStudio en cliquant sur le bouton Publish dans la fenÃªtre Viewer (vous devez au prÃ©alable avoir un comte sur shinyapp.io). Une application sera publique et sera ouverte. https://essicolo.shinyapps.io/Mitscherlich/ Pour dÃ©ployer en mode privÃ©, vous devrez dÃ©bourser pour un forfait ou installer votre propre serveur. "],
["chapitre-ordination.html", "9 Association, partitionnement et ordination 9.1 Espaces dâ€™analyse 9.2 Analyse dâ€™association 9.3 Partitionnement 9.4 Ordination", " 9 Association, partitionnement et ordination ï¸Â Objectifs spÃ©cifiques: Ã€ la fin de ce chapitre, vous serez en mesure dâ€™effectuer des calculs permettant de mesurer des diffÃ©rence entre des observations, des groupes dâ€™observation ou des variables observÃ©es serez en mesure dâ€™effection des analyses de partitionnement hiÃ©rarchiques et non-hiÃ©rarchiques serez en mesure dâ€™effectuer des calculs dâ€™ordination Ã  lâ€™aide des techniques de rÃ©duction dâ€™axe communes: analyse en composante principale, lâ€™analyse de correspondance, lâ€™analyse en coordonnÃ©es principales, analyse discriminante linÃ©aire, lâ€™analyse de redondance et lâ€™analyse canonique des correspondances. Les donnÃ©es Ã©cologiques incluent gÃ©nÃ©ralement plusieurs variables qui doivent Ãªtre analysÃ©es conjointement. Les techniques pour lâ€™analyse multivariÃ©e de donnÃ©es Ã©cologiques ont grandi en nombre et en complexitÃ©, laissant Ã©merger lâ€™Ã©cologie numÃ©rique, un nouveau domaine dâ€™Ã©tude scientifique initiÃ© par Pierre Legendre et Louis Legendre dont lâ€™ouvrage Numerical Ecology, aujourdâ€™hui Ã  sa troisiÃ¨me Ã©dition, reste un incontournable pour qui sâ€™intÃ©resse aux mathÃ©matiques sous-jacentes au domaine. Pour la rÃ©daction de ces notes, câ€™est toutefois le livre Numerical ecology with R, Ã©crit par Borcard et al. (2011) pour offrir un guide Ã  qui voudrait une approche plus appliquÃ©e. Lâ€™Ã©cologie numÃ©rique sera effleurÃ©e dans ce chapitre, qui introduit Ã  trois concepts. Les associations permettent de quantifier la ressemblance ou la diffÃ©rence entre deux observation (Ã©chantillons) ou variables (descripteurs). Lorsque lâ€™on a plus de deux variables ou plus de deux site, nous obtenons des matrices dâ€™association. Le partitionnement permet de regrouper des observations ou des variables selon des mÃ©triques dâ€™association. Lâ€™ordination vise par lâ€™intermÃ©diaire de techniques de rÃ©duction dâ€™axe Ã  mettre de lâ€™ordre dans des donnÃ©es dont le nombre Ã©levÃ© de variables peut amener Ã  des difficultÃ©s dâ€™apprÃ©ciation et dâ€™interprÃ©taion. library(&quot;tidyverse&quot;) 9.1 Espaces dâ€™analyse 9.1.1 Abondance et occurence Lâ€™abondance est le dÃ©compte dâ€™espÃ¨ces observÃ©es, tandis que lâ€™occurence est la prÃ©sence ou lâ€™absence dâ€™une espÃ¨ce. Le tableau suivant contient des donnÃ©es dâ€™abondance. abundance &lt;- tibble(&#39;Bruant familier&#39; = c(1, 0, 0, 3), &#39;Citelle Ã  poitrine rousse&#39; = c(1, 0, 0, 0), &#39;Colibri Ã  gorge rubis&#39; = c(0, 1, 0, 0), &#39;Geai bleu&#39; = c(3, 2, 0, 0), &#39;Bruant chanteur&#39; = c(1, 0, 5, 2), &#39;Chardonneret&#39; = c(0, 9, 6, 0), &#39;Bruant Ã  gorge blanche&#39; = c(1, 0, 0, 0), &#39;MÃ©sange Ã  tÃªte noire&#39; = c(20, 1, 1, 0), &#39;Jaseur borÃ©al&#39; = c(66, 0, 0, 0)) Ce tableau peut Ãªtre rapidement transformÃ© en donnÃ©es dâ€™occurence, qui ne comprennent que lâ€™information boolÃ©enne de prÃ©sence (notÃ© 1) et dâ€™absence (notÃ© 0). occurence &lt;- abundance %&gt;% transmute_all(~if_else(. &gt; 0, 1, 0)) Lâ€™espace des espÃ¨ces (ou des variables ou descripteurs) est celui oÃ¹ les espÃ¨ces forment les axes et oÃ¹ les sites sont positionnÃ©s dans cet espace. Il sâ€™agit dâ€™une perspective en mode R, qui permet principalement dâ€™identifier quels espÃ¨ces se retrouvent plus courrament ensemble. abundance %&gt;% select(&quot;Bruant chanteur&quot;, &quot;Chardonneret&quot;, &quot;MÃ©sange Ã  tÃªte noire&quot;) ## # A tibble: 4 x 3 ## `Bruant chanteur` Chardonneret `MÃ©sange Ã  tÃªte noire` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 20 ## 2 0 9 1 ## 3 5 6 1 ## 4 2 0 0 Dans lâ€™espace des sites (ou les Ã©chantillons ou objets), on transpose la matrice dâ€™abondance. On passe ici en mode Q, oÃ¹ chaque point est une espÃ¨ce, et oÃ¹ lâ€™on peut observer quels Ã©chantillons sont similaires. abundance %&gt;% t() ## [,1] [,2] [,3] [,4] ## Bruant familier 1 0 0 3 ## Citelle Ã  poitrine rousse 1 0 0 0 ## Colibri Ã  gorge rubis 0 1 0 0 ## Geai bleu 3 2 0 0 ## Bruant chanteur 1 0 5 2 ## Chardonneret 0 9 6 0 ## Bruant Ã  gorge blanche 1 0 0 0 ## MÃ©sange Ã  tÃªte noire 20 1 1 0 ## Jaseur borÃ©al 66 0 0 0 9.1.2 Environnement Lâ€™espace de lâ€™environnement comprend souvent un autre tableau contenant lâ€™information sur lâ€™environnement oÃ¹ se trouve les espÃ¨ces: les coordonnÃ©es et lâ€™Ã©lÃ©vation, la pente, le pH du sol, la pluviomÃ©trie, etc. 9.2 Analyse dâ€™association Nous utiliserons le terme association comme une mesure pour quantifier la ressemblance ou la diffÃ©rence entre deux objets (Ã©chantillons) ou variables (descripteurs). Alors que la corrÃ©lation et la covariance sont des mesures dâ€™association entre des variables (analyse en mode R), la similaritÃ© et la distance sont deux types de une mesure dâ€™association entre des objets (analyse en mode Q). Une distance de 0 est mesurÃ©e chez deux objets identiques. La distance augmente au fur et Ã  mesure que les objets sont dissociÃ©s. Une similaritÃ© ayant une valeur de 0 indique aucune association, tandis quâ€™une valeur de 1 indique une association parfaite. Ã€ lâ€™opposÃ©, la dissimilaritÃ© est Ã©gale Ã  1-similaritÃ©. La distance peut Ãªtre liÃ©e Ã  la similaritÃ© par la relation: \\[distance=\\sqrt{1-similaritÃ©}\\] ou \\[distance=\\sqrt{dissimilaritÃ©}\\] La racine carrÃ©e permet, pour certains indices de similaritÃ©, dâ€™obtenir des propriÃ©tÃ©s euclÃ©diennes. Pour plus de dÃ©tails, voyez le tableau 7.2 de Legendre et Legendre (2012). Les matrices dâ€™association sont gÃ©nÃ©ralement prÃ©sentÃ©es comme des matrices carrÃ©es, dont les dimensions sont Ã©gales au nombre dâ€™objets (mode Q) ou de vrariables (mode R) dans le tableau. Chaque Ã©lÃ©ment (â€œcelluleâ€) de la matrice est un indice dâ€™association entre un objet (ou une variable) et un autre. Ainsi, la diagonale de la matrice est un vecteur nul (distance ou dissimilaritÃ©) ou unitaire (similaritÃ©), car elle correspond Ã  lâ€™association entre un objet et lui-mÃªme. Puisque lâ€™association entre A et B est la mÃªme quâ€™entre B et A, et puisque la diagonale retourne une valeur convenue, il est possible dâ€™exprimer une matrice dâ€™association en mode â€œcompactâ€, sous forme de vecteur. Le vecteur dâ€™association entre des objets A, B et C contiendra toute lâ€™information nÃ©cessaire en un vecteur de trois chiffres, [AB, AC, BC], plutÃ´t quâ€™une matrice de dimension \\(3 \\times 3\\). Lâ€™impact sur la mÃ©moire vive peut Ãªtre considÃ©rable pour les calculs comprenant de nombreuses dimensions. En R, les calculs de similaritÃ© et de distances peuvent Ãªtre effectuÃ©s avec le module vegan. La fonction vegdist permet de calculer les indices dâ€™association en forme carrÃ©e. Nous verons plus tard les mÃ©thodes de mesure de similaritÃ© et de distance plus loin. Pour lâ€™instant, utilisons la mÃ©thode de Jaccard pour une dÃ©monstration sur des donnÃ©es dâ€™occurence. library(&quot;vegan&quot;) vegdist(occurence, method = &quot;jaccard&quot;, diag = TRUE, upper = TRUE) ## 1 2 3 4 ## 1 0.0000000 0.7777778 0.7500000 0.7142857 ## 2 0.7777778 0.0000000 0.6000000 1.0000000 ## 3 0.7500000 0.6000000 0.0000000 0.7500000 ## 4 0.7142857 1.0000000 0.7500000 0.0000000 Remarquez que vegdist retourne une matrice dont la diagonale est de 0 (on lâ€™affiche en spÃ©cifiant diag = TRUE). La diagonale est lâ€™association dâ€™un objet avec lui-mÃªme. Or la similaritÃ© dâ€™un objet avec lui-mÃªme devrait Ãªtre de 1! En fait, par convention vegdist retourne des dissimilaritÃ©s, non pas des similaritÃ©s. La matrice de distance serait donc calculÃ©e en extrayant la racine carrÃ©e des Ã©lÃ©ments de la matrice de dissimilaritÃ©: dissimilarity &lt;- vegdist(occurence, method = &quot;jaccard&quot;, diag = TRUE, upper = TRUE) distance &lt;- sqrt(dissimilarity) distance ## 1 2 3 4 ## 1 0.0000000 0.8819171 0.8660254 0.8451543 ## 2 0.8819171 0.0000000 0.7745967 1.0000000 ## 3 0.8660254 0.7745967 0.0000000 0.8660254 ## 4 0.8451543 1.0000000 0.8660254 0.0000000 Dans le chapitre sur lâ€™analyse compositionnelle, nous avons abordÃ© les significations diffÃ©rentes que peuvent prendre le zÃ©ro. Lâ€™information fournie par un zÃ©ro peut Ãªtre diffÃ©rente selon les circonstances. Dans le cas dâ€™une variable continue, un zÃ©ro signifie gÃ©nÃ©ralement une mesure sous le seuil de dÃ©tection. Deux tissus dont la concentration en cuivre est nulle ont une afinitÃ© sous la perspective de la concentration en cuivre. Dans le cas de mesures dâ€™abondance (dÃ©compte) ou dâ€™occurence (prÃ©sence-absence), on pourra dÃ©crire comme similaires deux niches Ã©cologiques oÃ¹ lâ€™on retrouve une espÃ¨ce en particulier. Mais deux sites oÃ¹ lâ€™on de retouve pas dâ€™ours polaires ne correspondent pas nÃ©cessairement Ã  des niches similaires! En effet, il peut exister de nombreuses raisons Ã©cologiques et mÃ©thodologiques pour lesquelles lâ€™espÃ¨ces ou les espÃ¨ces nâ€™ont pas Ã©tÃ© observÃ©es. Câ€™est le problÃ¨me des double-zÃ©ros (espÃ¨ces non observÃ©es Ã  deux sites), problÃ¨me qui est amplifiÃ© avec les grilles comprenant des espÃ¨ces rares. La ressemblance entre des objets comprenant des donnÃ©es continues devrait Ãªtre calculÃ©e grÃ¢ce Ã  des indicateurs symÃ©triques. Inversement, les affinitÃ©s entre les objets dÃ©crits par des donnÃ©es dâ€™abondance ou dâ€™occurence susceptibles de gÃ©nÃ©rer des problÃ¨mes de double-zÃ©ros devraient Ãªtre Ã©valuÃ©es grÃ¢ce Ã  des indicateurs asymÃ©triques. Un dÃ©fi supplÃ©mentaire arrive lorsque les donnÃ©es sont de type mixte. Nous utiliserons la convention de vegan et nous calculerons la dissimilaritÃ©, non pas la similaritÃ©. Les mesures de dissimilaritÃ© sont calculÃ©es sur des donnÃ©es dâ€™abondance ou des donnÃ©es dâ€™occurence. Notons quâ€™il existe beaucoup de confusion dans la littÃ©rature sur la maniÃ¨re de nommer les dissimilaritÃ©s (ce qui nâ€™est pas le cas des distances, dont les noms sont reconnus). Dans les sections suivantes, nous noterons la dissimilaritÃ© avec un \\(d\\) minuscule et la distance avec un \\(D\\) majuscule. 9.2.1 Association entre objets (mode Q) 9.2.1.1 Objets: Abondance La dissimilaritÃ© de Bray-Curtis est asymÃ©trique. Elle est aussi appelÃ©e lâ€™indice de Steinhaus, de Czekanowski ou de SÃ¸rensen. Il est important de sâ€™assurer de bien sâ€™entendre la mÃ©thode Ã  laquelle on fait rÃ©fÃ©rence. Lâ€™Ã©quation enlÃ¨ve toute ambiguitÃ©. La dissimilaritÃ© de Bray-Curtis entre les points A et B est calculÃ©e comme suit. \\[d_{AB} = \\frac {\\sum \\left| A_{i} - B_{i} \\right| }{\\sum \\left(A_{i}+B_{i}\\right)}\\] Utilisons vegdist pour gÃ©nÃ©rer les matrices dâ€™association. Le format â€œlisteâ€ de R est pratique pour enregistrer la collection dâ€™objets, dont les matrice dâ€™association que nous allons crÃ©er dans cette section. associations_abund &lt;- list() associations_abund[[&#39;BrayCurtis&#39;]] &lt;- vegdist(abundance, method = &quot;bray&quot;) associations_abund[[&#39;BrayCurtis&#39;]] ## 1 2 3 ## 2 0.9433962 ## 3 0.9619048 0.4400000 ## 4 0.9591837 1.0000000 0.7647059 La dissimilaritÃ© de Bray-Curtis est souvent utilisÃ©e dans la littÃ©rature. Toutefois, la version originale de Bray-Curtis nâ€™est pas tout Ã  fait mÃ©trique (semimÃ©trique). ConsÃ©quemment, la dissimilaritÃ© de Ruzicka (une variante de la dissimilaritÃ© de Jaccard pour les donnÃ©es dâ€™abondance) est mÃ©trique, et devrait probablement Ãªtre prÃ©fÃ©rÃ© Ã  Bary-Curtis (Oksanen, 2006). \\[d_{AB, Ruzicka} = \\frac { 2 \\times d_{AB, Bray-Curtis} }{1 + d_{AB, Bray-Curtis}}\\] associations_abund[[&#39;Ruzicka&#39;]] &lt;- associations_abund[[&#39;BrayCurtis&#39;]] * 2 / (1 + associations_abund[[&#39;BrayCurtis&#39;]]) La dissimilaritÃ© de Kulczynski (aussi Ã©crit Kulsinski) est asymÃ©trique et semimÃ©trique, tout comme celle de Bray-Curtis. Elle est calculÃ©e comme suit. \\[d_{AB} = 1-\\frac{1}{2} \\times \\left[ \\frac{\\sum min(A_i, B_i)}{\\sum A_i} + \\frac{\\sum min(A_i, B_i)}{\\sum B_i} \\right]\\] associations_abund[[&#39;Kulczynski&#39;]] &lt;- vegdist(abundance, method = &quot;kulczynski&quot;) Une approche commune pour mesurer lâ€™association entre sites dÃ©crits par des donnÃ©es dâ€™abondance est la distance de Hellinger. Notez quâ€™il sâ€™agit ici dâ€™une distance, non pas dâ€™une dissimilaritÃ©. Pour lâ€™obtenir, on doit dâ€™abord diviser chaque donnÃ©e dâ€™abondance par lâ€™abondance totale pour chaque site pour obtenir les espÃ¨ces en tant que proportions, puis on extrait la racine carrÃ©e de chaque Ã©lÃ©ment. Enfin, on calcule la distance euclidienne entre les proportions de chaque site. Pour rappel, une distance euclidienne est la gÃ©nÃ©ralisation en plusieurs dimensions du thÃ©orÃ¨me de Pythagore, \\(c = \\sqrt{a^2 + b^2}\\). \\[D_{AB} = \\sqrt {\\sum \\left( \\frac{A_i}{\\sum A_i} - \\frac{B_i}{\\sum B_i} \\right)^2}\\] ğŸ˜±Â Attention La distance dâ€™Hellinger hÃ©rite des biais liÃ©es aux donnÃ©es compositionnelles. Elle peut Ãªtre substitiÃ©e par une matrice de distances dâ€™Aitchison. associations_abund[[&#39;Hellinger&#39;]] &lt;- dist(decostand(abundance, method=&quot;hellinger&quot;)) Toute comme la distance dâ€™Hellinger, la distance de chord est calculÃ©e par une distance euclidienne sur des donnÃ©es dâ€™abondance transformÃ©es de sorte que chaque ligne ait une longueur (norme) de 1. associations_abund[[&#39;Chord&#39;]] &lt;- dist(decostand(abundance, method=&quot;normalize&quot;)) La mÃ©trique du chi-carrÃ©, ou \\(\\chi\\)-carrÃ©, ou chi-square, donne davantage de poids aux espÃ¨ces rares quâ€™aux espÃ¨ces communes. Son utilisation est recommandÃ©e lorsque les espÃ¨ces rares sont de bons indicateurs de conditions Ã©cologiques particuliÃ¨res (Legendre et Legendre, 2012, p.Â 308). \\[ d_{AB} = \\sqrt{\\sum _j \\frac{1}{\\sum y_j} \\left( \\frac{A_j}{\\sum A} - \\frac{B_j}{\\sum B} \\right)^2 } \\] La mÃ©trique peut Ãªtre transformÃ©e en distance en la multipliant par la racine carrÃ©e de la somme totale des espÃ¨ces dans la matric dâ€™abondance (\\(X\\)). \\[ D_{AB} = \\sqrt{\\sum X} \\times d_{AB} \\] associations_abund[[&#39;ChiSquare&#39;]] &lt;- dist(decostand(abundance, method=&quot;chi.square&quot;)) Une manniÃ¨re visuellement plus intÃ©ressante de prÃ©senter une matrice dâ€™association est un graphique de type heatmap. associations_abund_df &lt;- list() for (i in 1:length(associations_abund)) { associations_abund_df[[i]] &lt;- data.frame(as.matrix(associations_abund[[i]])) colnames(associations_abund_df[[i]]) &lt;- rownames(associations_abund_df[[i]]) associations_abund_df[[i]]$row &lt;- rownames(associations_abund_df[[i]]) associations_abund_df[[i]] &lt;- associations_abund_df[[i]] %&gt;% gather(key=row) associations_abund_df[[i]]$column = rep(1:4, 4) associations_abund_df[[i]]$dist &lt;- names(associations_abund)[i] } associations_abund_df &lt;- do.call(rbind, associations_abund_df) ggplot(associations_abund_df, aes(x=row, y=column)) + facet_wrap(. ~ dist, nrow = 2) + geom_tile(aes(fill = value)) + geom_text(aes(label = round(value, 2))) + scale_fill_gradient2(low = &quot;#00ccff&quot;, mid = &quot;#aad400&quot;, high = &quot;#ff0066&quot;, midpoint = 2) + labs(x=&quot;Site&quot;, y=&quot;Site&quot;) Peu importe le type dâ€™association utilisÃ©e, les heatmaps montrent les mÃªmes tendances. Les assocaitions de dissimilaritÃ© (Bray-Curtis, Kulczynski et Ruzicka) sâ€™Ã©talent de 0 Ã  1, tandis que les distances (Chi-Square, Chord et Hellinger) partent de zÃ©ro, mais nâ€™ont pas de limite supÃ©rieure. On note les plus grandes diffÃ©rences entre les sites 2 et 4, tandis que les sites 2 et 3 sont les plus semblables pour toutes les mesures dâ€™association Ã  lâ€™exception de la dissimilaritÃ© de Kulczynski. 9.2.1.2 Objets: Occurence (prÃ©sence-absence) Des indices dâ€™association diffÃ©rents devraient Ãªtre utilisÃ©s lorsque des donnÃ©es sont compilÃ©es sous forme boolÃ©enne. En gÃ©nÃ©ral, les tableaux de donnÃ©es dâ€™occurence seront compilÃ©s avec des 1 (prÃ©sence) et des 0 (absence). La similaritÃ© de Jaccard entre le site A et le site B est la proportion de double 1 (prÃ©sences de 1 dans A et B) parmi les espÃ¨ces. La dissimilariÃ© est la proportion complÃ©mentaire (comprenant [1, 0], [0, 1] et [0, 0]). La distance de Jaccard est la racine carrÃ©e de la dissimilaritÃ©. associations_occ &lt;- list() associations_occ[[&#39;Jaccard&#39;]] &lt;- vegdist(occurence, method = &quot;jaccard&quot;) Les distances dâ€™Hellinger, de chord et de chi-carrÃ© sont aussi appropriÃ©es pour les calculs de distances sur des tableaux dâ€™occurence. associations_occ[[&#39;Hellinger&#39;]] &lt;- dist(decostand(occurence, method=&quot;hellinger&quot;)) associations_occ[[&#39;Chord&#39;]] &lt;- dist(decostand(occurence, method=&quot;normalize&quot;)) associations_occ[[&#39;ChiSquare&#39;]] &lt;- dist(decostand(occurence, method=&quot;chi.square&quot;)) Graphiquement, associations_occ_df &lt;- list() for (i in 1:length(associations_occ)) { associations_occ_df[[i]] &lt;- data.frame(as.matrix(associations_occ[[i]])) colnames(associations_occ_df[[i]]) &lt;- rownames(associations_occ_df[[i]]) associations_occ_df[[i]]$row &lt;- rownames(associations_occ_df[[i]]) associations_occ_df[[i]] &lt;- associations_occ_df[[i]] %&gt;% gather(key=row) associations_occ_df[[i]]$column = rep(1:4, 4) associations_occ_df[[i]]$dist &lt;- names(associations_occ)[i] } associations_occ_df &lt;- do.call(rbind, associations_occ_df) ggplot(associations_occ_df, aes(x=row, y=column)) + facet_wrap(. ~ dist) + geom_tile(aes(fill = value)) + geom_text(aes(label = round(value, 2))) + scale_fill_gradient2(low = &quot;#00ccff&quot;, mid = &quot;#aad400&quot;, high = &quot;#ff0066&quot;, midpoint = 1) + labs(x=&quot;Site&quot;, y=&quot;Site&quot;) Il est attendu que les matrices dâ€™association sur lâ€™occurence sont semblables Ã  celles sur lâ€™abondance. Dans ce cas-ci, la distance dâ€™Hellinger donne des rÃ©sultats semblables Ã  la dissimilaritÃ© de Jaccard. 9.2.1.3 Objets: DonnÃ©es quantitatives Les donnÃ©es quantitative en Ã©cologie peuvent dÃ©crire lâ€™Ã©tat de lâ€™environnement: le climat, lâ€™hydrologie, lâ€™hydrogÃ©ochimie, la pÃ©dologie, etc. En rÃ¨gle gÃ©nÃ©rale, les coordonnÃ©es des sites ne sot pas des variables environnementales, Ã  que lâ€™on soupÃ§onne la coordonnÃ©e elle-mÃªme dâ€™Ãªtre responsable dâ€™effets sur notre systÃ¨me: mais il sâ€™agira la plupart du temps dâ€™effets confondants (par exemple, on peut mesurer un effet de lattitude sur le rendement des agrumes, mais il sâ€™agira probablement avant tout dâ€™effets dus aux conditions climatiques, qui elles changent en fonction de la lattitude). Dâ€™autre types de donnÃ©es quantitative pouvant Ãªtre apprÃ©hendÃ©es par des distances sont les traits phÃ©nologiques, les ionomes, les gÃ©nomes, etc. La distance euclidienne est la racine carrÃ©e de la somme des carrÃ©s des distances sur tous les axes. Il sâ€™agit dâ€™une application multidimensionnelle du thÃ©orÃ¨me de Pythagore. La distance dâ€™Aitchison, couverte dans le chapitre 8, est une distance euclidienne calculÃ©e sur des donnÃ©es compositionnelles prÃ©alablement transformÃ©es. La distance euclidienne est sensible aux unitÃ©s utilisÃ©s: utiliser des milimÃ¨tres plutÃ´t que des mÃ¨tres enflera la distance euclidienne. Il est recommandÃ© de porter une attention particuliÃ¨re aux unitÃ©s, et de standardiser les donnÃ©es au besoin (par exemple, en centrant la moyenne Ã  zÃ©ro et en fixant lâ€™Ã©cart-type Ã  1). On pourrait, par exemple, mesurer la distance entre des observations des dimensions de diffÃ©rentes espÃ¨ces dâ€™iris. Ce tableau est inclu dans R par dÃ©faut. data(iris) iris %&gt;% sample_n(5) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 6.6 3.0 4.4 1.4 versicolor ## 2 7.7 3.8 6.7 2.2 virginica ## 3 4.4 3.2 1.3 0.2 setosa ## 4 5.6 3.0 4.1 1.3 versicolor ## 5 6.9 3.1 5.1 2.3 virginica Les mesures du tableau sont en centimÃ¨tres. Pour Ã©viter de donner davantage de poids aux longueur des sÃ©pales et en mÃªme temps de nÃ©gliger la largeur des pÃ©tales, nous allons standardiser le tableau. iris_sc &lt;- iris %&gt;% select(-Species) %&gt;% scale(.)%&gt;% as_tibble(.) %&gt;% mutate(Species = iris$Species) iris_sc ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 -0.898 1.02 -1.34 -1.31 setosa ## 2 -1.14 -0.132 -1.34 -1.31 setosa ## 3 -1.38 0.327 -1.39 -1.31 setosa ## 4 -1.50 0.0979 -1.28 -1.31 setosa ## 5 -1.02 1.25 -1.34 -1.31 setosa ## 6 -0.535 1.93 -1.17 -1.05 setosa ## 7 -1.50 0.786 -1.34 -1.18 setosa ## 8 -1.02 0.786 -1.28 -1.31 setosa ## 9 -1.74 -0.361 -1.34 -1.31 setosa ## 10 -1.14 0.0979 -1.28 -1.44 setosa ## # â€¦ with 140 more rows Pour les comparaisons des dimensions, prenons la moyenne des dimensions (mises Ã  lâ€™Ã©chelle) par espÃ¨ce. iris_means &lt;- iris_sc %&gt;% group_by(Species) %&gt;% summarise_all(mean) %&gt;% select(-Species) iris_means ## # A tibble: 3 x 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -1.01 0.850 -1.30 -1.25 ## 2 0.112 -0.659 0.284 0.166 ## 3 0.899 -0.191 1.02 1.08 Nous pouvons utiliser la distance euclidienne, commune en gÃ©omÃ©trie, pour comparer les espÃ¨ces. La distance euclidienne est calculÃ©e comme suit. \\[ \\mathcal{E} = \\sqrt{\\Sigma_i \\left( A_i - B_i \\right) ^2 } \\] associations_cont = list() associations_cont[[&#39;Euclidean&#39;]] &lt;- dist(iris_sc %&gt;% select(-Species), method=&quot;euclidean&quot;) La distance de Mahalanobis est semblable Ã  la distance euclidienne, mais qui tient compte de la covariance de la matrice des objets. Cette covariance peut Ãªtre utilisÃ©e pour dÃ©crire la structure dâ€™un nuage de points. La diastance de Mahalanobis se calcule comme suit. \\[\\mathcal{M} = \\sqrt{(A - B)^T S^{-1} (A-B)}\\] Notez quâ€™il sâ€™agit dâ€™une gÃ©nÃ©ralisation de la distance euclidienne, qui Ã©quivaut Ã  une distance de Mahalanobis dont la matrice de covariance est une matrice identitÃ©. La distance de Mahalanobis permet de reprÃ©senter des distances dans un espace fortement corrÃ©lÃ©. Elle est courramment utilisÃ©e pour dÃ©tecter les valeurs aberrantes selon des critÃ¨res de distance Ã  partir du centre dâ€™un jeu de donnÃ©es multivariÃ©es. associations_cont[[&#39;Mahalanobis&#39;]] &lt;- vegdist(iris_sc %&gt;% select(-Species), &#39;mahalanobis&#39;) La distance de Manhattan porte aussi le nom de distance de cityblock ou de taxi. Câ€™est la distance que vous devrez parcourir pour vous rendre du point A au point B Ã  Manhattan, câ€™est-Ã -dire selon une sÃ©quence de tronÃ§ons perpendiculaires. \\[ D_{AB} = \\sum _i \\left| A_i - B_i \\right| \\] La distance de Manhattan est appropriÃ©e lorsque les gradients (changements dâ€™un Ã©tat Ã  lâ€™autre ou dâ€™une rÃ©gion Ã  lâ€™autre) ne permettent pas des changements simultanÃ©s. Mieux vaut standardiser les variables pour Ã©viter quâ€™une dimension soit prÃ©pondÃ©rante. associations_cont[[&#39;Manhattan&#39;]] &lt;- vegdist(iris_sc %&gt;% select(-Species), &#39;manhattan&#39;) Avant de prÃ©senter les rÃ©sultats des espÃ¨ces dâ€™iris, voici une reprÃ©sentation des distances euclidiennes (rouge), de Mahalanobis (bleu) et de Manhattan (vert), chacune de 1 et 2 unitÃ©s Ã  partir du centre et, pour ce qui est de la distance de Mahalanobis, selon la covariance. library(&quot;car&quot;) ## Loading required package: carData ## ## Attaching package: &#39;car&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## recode ## The following object is masked from &#39;package:purrr&#39;: ## ## some library(&quot;MASS&quot;) ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:plotly&#39;: ## ## select ## The following object is masked from &#39;package:dplyr&#39;: ## ## select select &lt;- dplyr::select # Ã©viter les conflits de fonctions entre MASS et dplyr filter &lt;- dplyr::filter sigma &lt;- matrix(c(1, 0.6, 0.6, 1), ncol = 2) # matrice de covariance mu &lt;- c(0, 0) # centre data &lt;- mvrnorm(n = 100, mu, sigma) # gÃ©nÃ©rer des donnÃ©es plot(data, ylim = c(-2, 2), xlim = c(-2, 2), asp = 1) ## cercles t &lt;- seq(0,2*pi,length=100) c1 &lt;- t(rbind(mu[2] + sin(t)*1, mu[1] + cos(t)*1)) c2 &lt;- t(rbind(mu[2] + sin(t)*2, mu[1] + cos(t)*2)) lines(c1, lwd = 2, col = &quot;red&quot;) lines(c2, lwd = 2, col = &quot;red&quot;) ## ellipses e1 &lt;- ellipse(mu, sigma, radius=1, add=TRUE) e2 &lt;- ellipse(mu, sigma, radius=2, add=TRUE) ## carrÃ©s lines(c(1, 0, -1, 0, 1), c(0, 1, 0, -1, 0), lwd = 2, col = &quot;green&quot;) lines(c(2, 0, -2, 0, 2), c(0, 2, 0, -2, 0), lwd = 2, col = &quot;green&quot;) Et, graphiquement, les rÃ©sultats des distances des iris. associations_cont_df &lt;- list() for (i in 1:length(associations_cont)) { associations_cont_df[[i]] &lt;- data.frame(as.matrix(associations_cont[[i]])) colnames(associations_cont_df[[i]]) &lt;- rownames(associations_cont_df[[i]]) associations_cont_df[[i]]$row &lt;- rownames(associations_cont_df[[i]]) associations_cont_df[[i]] &lt;- associations_cont_df[[i]] %&gt;% gather(key=row) associations_cont_df[[i]]$column = rep(1:nrow(iris), nrow(iris)) associations_cont_df[[i]]$dist &lt;- names(associations_cont)[i] } associations_cont_df &lt;- do.call(rbind, associations_cont_df) ggplot(associations_cont_df, aes(x=row, y=column)) + facet_wrap(. ~ dist) + geom_tile(aes(fill = value), colour = NA) + #geom_text(aes(label = round(value, 2))) + scale_fill_gradient2(low = &quot;#00ccff&quot;, mid = &quot;#aad400&quot;, high = &quot;#ff0066&quot;, midpoint = 5) + labs(x=&quot;Site&quot;, y=&quot;Site&quot;) Le tableau iris est ordonnÃ© par espÃ¨ce. Les distances euclidienne et de Manhattan permettent aisÃ©ment de distinguer les espÃ¨ces selon les dimensions des pÃ©tales et des sÃ©pales. Toutefois, lâ€™utilsation de la covariance avec la distance de Mahalanobis crÃ©e des distinction moins tranchÃ©es. 9.2.1.4 Objets: DonnÃ©es mixtes Les donnÃ©es catÃ©gorielles ordinales peuvent Ãªtre transformÃ©es en donnÃ©es continues par gradations linÃ©aires ou quadratiques. Les donnÃ©es catÃ©gorielles nominales, quant Ã  elles, peuvent Ãªtre encodÃ©es (encodage catÃ©goriel) en donnÃ©es similaires Ã  des occurences. Attention toutefois: contrairement Ã  la rÃ©gression linÃ©aire qui demande dâ€™exclure une catÃ©gorie, lâ€™encodage catÃ©goriel doit inclure toutes les catÃ©gories. Le comportement par dÃ©faut de la fonction model.matrix est dâ€™exclure la catÃ©gorie de rÃ©fÃ©rence: on doit spÃ©cifier que lâ€™intercept est de zÃ©ro, câ€™est-Ã -dire model.matrix(~ + categorie). La similaritÃ© de Gower a Ã©tÃ© dÃ©veloppÃ©e pour mesurer des associations entre des objets dont les donnÃ©es sont mixtes: boolÃ©ennes, catÃ©gorielles et continues. La similaritÃ© de Gower est calculÃ©e en additionnant les distances calculÃ©es par colonne, individuellement. Si la colonne est boolÃ©enne, on utilise les distances de Jaccard (qui exclue les double-zÃ©ro) de maniÃ¨re univariÃ©e: une variable Ã  la fois. Pour les variables continues, on utilise la distance de Manhattan divisÃ©e par la plage de valeurs de la variable (pour fin de standardisation). Puisquâ€™elle hÃ©rite de la particularitÃ© de la distance de Manhattan et de la similaritÃ© de Jaccard univariÃ©e, la similaritÃ© de Gower reste une combinaison linÃ©aire de distances univariÃ©es. X &lt;- tibble(ID = 1:8, age = c(21, 21, 19, 30, 21, 21, 19, 30), gender = c(&#39;M&#39;,&#39;M&#39;,&#39;N&#39;,&#39;M&#39;,&#39;F&#39;,&#39;F&#39;,&#39;F&#39;,&#39;F&#39;), civil_status = c(&#39;MARRIED&#39;,&#39;SINGLE&#39;,&#39;SINGLE&#39;,&#39;SINGLE&#39;,&#39;MARRIED&#39;,&#39;SINGLE&#39;,&#39;WIDOW&#39;,&#39;DIVORCED&#39;), salary = c(3000.0,1200.0 ,32000.0,1800.0 ,2900.0 ,1100.0 ,10000.0,1500.0), children = c(TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE), available_credit = c(2200,100,22000,1100,2000,100,6000,2200)) X ## # A tibble: 8 x 7 ## ID age gender civil_status salary children available_credit ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 1 21 M MARRIED 3000 TRUE 2200 ## 2 2 21 M SINGLE 1200 FALSE 100 ## 3 3 19 N SINGLE 32000 TRUE 22000 ## 4 4 30 M SINGLE 1800 TRUE 1100 ## 5 5 21 F MARRIED 2900 TRUE 2000 ## 6 6 21 F SINGLE 1100 TRUE 100 ## 7 7 19 F WIDOW 10000 FALSE 6000 ## 8 8 30 F DIVORCED 1500 TRUE 2200 Il faut prÃ©alablement procÃ©der Ã  lâ€™encodage catÃ©goriel pour les variables catÃ©gorielles nominales. X_dum &lt;- model.matrix(~ 0 + ., X[, -1]) X_dum ## age genderF genderM genderN civil_statusMARRIED civil_statusSINGLE civil_statusWIDOW salary childrenTRUE available_credit ## 1 21 0 1 0 1 0 0 3000 1 2200 ## 2 21 0 1 0 0 1 0 1200 0 100 ## 3 19 0 0 1 0 1 0 32000 1 22000 ## 4 30 0 1 0 0 1 0 1800 1 1100 ## 5 21 1 0 0 1 0 0 2900 1 2000 ## 6 21 1 0 0 0 1 0 1100 1 100 ## 7 19 1 0 0 0 0 1 10000 0 6000 ## 8 30 1 0 0 0 0 0 1500 1 2200 ## attr(,&quot;assign&quot;) ## [1] 1 2 2 2 3 3 3 4 5 6 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$gender ## [1] &quot;contr.treatment&quot; ## ## attr(,&quot;contrasts&quot;)$civil_status ## [1] &quot;contr.treatment&quot; ## ## attr(,&quot;contrasts&quot;)$children ## [1] &quot;contr.treatment&quot; Calculons la dissimilaritÃ© de Gower (cette fois le graphique est fait avec pheatmap). library(&quot;pheatmap&quot;) d_gow &lt;- as.matrix(vegdist(X_dum, &#39;gower&#39;)) colnames(d_gow) &lt;- rownames(d_gow) &lt;- X$ID pheatmap(d_gow) Les dendrogrammes apparaissants sur les axes du graphique sont issus dâ€™un processus de partitionnement basÃ© sur la distance, que nous verrons plus loin dans ce chapiter. Les profils des clients 4 et 7, ainsi que ceux des clients 3 et 7 diffÃ¨rent le plus. Les profils 3 et 4 sont nÃ©anmoins plutÃ´t diffÃ©rents. 9.2.2 Associations entre variables (mode R) Il existe de nombreuses approches pour mesurer les associations entre variables. La plus connue est la corrÃ©lation. Mais les donnÃ©es dâ€™abondance et dâ€™occurence demandent des approches diffÃ©rentes. 9.2.2.1 Variables: Abondance La distance du chi-carrÃ© est suggÃ©rÃ©e par Borcard et al. (2011). abundance_r &lt;- t(abundance) D_chisq_R &lt;- as.matrix(dist(decostand(abundance_r, method=&quot;chi.square&quot;))) pheatmap(D_chisq_R, display_numbers = round(D_chisq_R, 2)) Des coabondances sont notables pour la mÃ©sange Ã  tÃªte noire, le jaseur borÃ©al, la citelle Ã  poitrine rousse et le bruant Ã  gorge blanche (tache bleu au centre). 9.2.2.2 Variables: Occurence La dissimilaritÃ© de Jaccard peut Ãªtre utilisÃ©e. occurence_r &lt;- t(occurence) D_jacc_R &lt;- as.matrix(vegdist(occurence_r, method = &quot;jaccard&quot;)) pheatmap(D_jacc_R, display_numbers = round(D_jacc_R, 2)) Des cooccurences sont notables pour le jaseur borÃ©al, la citelle Ã  poitrine rousse et le bruant Ã  gorge blanche (tache bleu au centre). 9.2.2.3 Variables: QuantitÃ©s La matrice des corrÃ©lations de Pearson peut Ãªtre utilisÃ©e pour les donnÃ©es continues. Quant aux variables ordinales, elles devraient idÃ©alement Ãªtre liÃ©es linÃ©airement ou quadratiquement. Si ce nâ€™est pas le cas, câ€™est-Ã -dire que les catÃ©gories sont ordonnÃ©es par rang seulement, vous pourrez avoir recours aux coefficients de corrÃ©lation de Spearman ou de Kendall. iris_cor &lt;- iris %&gt;% select(-Species) %&gt;% cor(.) pheatmap(cor(iris[, -5]), cluster_rows = FALSE, cluster_cols = FALSE, display_numbers = round(iris_cor, 2)) 9.2.3 Conclusion sur les associations Il nâ€™existe pas de rÃ¨gle claire pour dÃ©terminer quelle technique dâ€™association utiliser. Cela dÃ©pend en premier lieu de vos donnÃ©es. Vous sÃ©lectionnerez votre mÃ©thode dâ€™association selon le type de donnÃ©es que vous abordez, la question Ã  laquelle vous dÃ©sirez rÃ©pondre ainsi lâ€™expÃ©rience dans la littÃ©rature comme celle de vos collÃ¨gues scientifiques. Sâ€™il nâ€™existe pas de rÃ¨gle clair, câ€™est quâ€™il existe des dizaines de mÃ©thodes diffÃ©rentes, et la plupart dâ€™entre elles vous donneront une perspective juste et valide. Il faut nÃ©anmoins faire attention pour Ã©viter de sÃ©lectionner les mÃ©thodes qui ne sont pas appropriÃ©es. 9.3 Partitionnement Les donnÃ©es suivantes ont Ã©tÃ© gÃ©nÃ©rÃ©es par Leland McInnes (Tutte institute of mathematics, Ottawa). ÃŠtes-vous en mesure dâ€™identifier des groupes? Combien en trouvez-vous? df_mcinnes &lt;- read_csv(&quot;data/clusterable_data.csv&quot;, col_names = c(&quot;x&quot;, &quot;y&quot;), skip = 1) ## Parsed with column specification: ## cols( ## x = col_double(), ## y = col_double() ## ) ggplot(df_mcinnes, aes(x=x, y=y)) + geom_point() + coord_fixed() En 2D, lâ€™oeil humain peut facilement dÃ©tecter les groupes. En 3D, câ€™est toujours possible, mais au-delÃ  de 3D, le partitionnement cognitive devient rapidement maladroite. Les algorithmes sont alors dâ€™une aide prÃ©cieuse. Mais ils transportent en pratique tout un baggage de limitations. Quel est le critÃ¨re dâ€™association entre les groupes? Combien de groupe devrions-nous crÃ©er? Comment distinguer une donnÃ©e trop bruitÃ©e pour Ãªtre classifiÃ©e? Le partitionnement de donnÃ©es (clustering en anglais), et inversement leur regroupement, permet de crÃ©er des ensembles selon des critÃ¨res dâ€™association. On suppose donc que Le partitionnement permet de crÃ©er des groupes selon lâ€™information que lâ€™on fait Ã©merger des donnÃ©es. Il est consÃ©quemment entendu que les donnÃ©es ne sont pas catÃ©gorisÃ©es Ã  priori: il ne sâ€™agit pas de prÃ©dire la catÃ©gorie dâ€™un objet, mais bien de crÃ©er des catÃ©gories Ã  partir des objets par exemple selon leurs dimensions, leurs couleurs, leurs signature chimique, leurs comportements, leurs gÃ¨nes, etc. Plusieurs mÃ©thodes sont aujourdâ€™hui offertes aux analystes pour partitionner leurs donnÃ©es. Dans le cadre de ce manuel, nous couvrirons ici deux grandes tendances dans les algorithmes. MÃ©thodes hiÃ©rarchique et non hiÃ©rarchiques. Dans un partitionnement hiÃ©rarchique, lâ€™ensemble des objets forme un groupe, comprenant des sous-regroupements, des sous-sous-regroupements, etc., dont les objets forment lâ€™ultime partitionnement. On pourra alors identifier comment se dÃ©cline un partitionnement. Ã€ lâ€™inverse, un partitionnement non-hiÃ©rarchique des algorhitmes permettent de crÃ©er les groupes non hiÃ©rarchisÃ©s les plus diffÃ©rents que possible. Membership exclusif ou flou. Certaines techniques attribuent Ã  chaque objet une classe unique: lâ€™appartenance sera indiquÃ©e par un 1 et la non appartenance par un 0. Dâ€™autres techniques vont attribuer un membership flou oÃ¹ le degrÃ© dâ€™appartenance est une variable continue de 0 Ã  1. Parmi les mÃ©thodes floues, on retrouve les mÃ©thodes probabilistes. 9.3.1 Ã‰valuation dâ€™un partitionnement Le choix dâ€™une technique de partitionnement parmi de nombreuses disponibles, ainsi que le choix des paramÃ¨tres gouvernant chacune dâ€™entre elles, est avant tout basÃ© sur ce que lâ€™on dÃ©sire dÃ©finir comme Ã©tant un groupe, ainsi que la maniÃ¨re dâ€™interprÃ©ter les groupes. En outre, le nombre de groupe Ã  dÃ©partager est toujours une dÃ©cision de lâ€™analyste. NÃ©anmoins, on peut se fier des indicateurs de performance de partitionnement. Parmis ceux-ci, retenons le score silouhette ainsi que lâ€™indice de Calinski-Harabaz. 9.3.1.1 Score silouhette En anglais, le h dans silouhette se trouve aprÃ¨s le l: on parle donc de silhouette coefficient pour dÃ©signer le score de chacun des objets dans le partitionnement. Pour chaque objet, on calcule la distance moyenne qui le sÃ©pare des autres points de son groupe (\\(a\\)) ainsi que la distance moyenne qui le sÃ©pare des points du groupe le plus rapprochÃ©. \\[s = \\frac{b-a}{max \\left(a, b \\right)}\\] Un coefficient de -1 indique le pire classement, tandis quâ€™un coefficient de 1 indique le meilleur classement. La moyenne des coefficients silouhette est le score silouhette. 9.3.1.2 Indice de Calinski-Harabaz Lâ€™indice de Calinski-Harabaz est proportionnel au ratio des dispersions intra-groupe et la moyenne des dispersions inter-groupes. Plus lâ€™indice est Ã©levÃ©, mieux les groupes sont dÃ©finis. La mathÃ©matique est dÃ©crite dans la documentation de scikit-learn, un module dâ€™analyse et autoapprentissage sur Python. Note. Les coefficients silouhette et lâ€™indice de Calinski-Harabaz sont plus appropriÃ©s pour les formes de groupes convexes (cercles, sphÃ¨res, hypersphÃ¨res) que pour les formes irrÃ©guliÃ¨res (notamment celles obtenues par la DBSCAN, discutÃ©e ci-desssous). 9.3.2 Partitionnement non hiÃ©rarchique Il peut arriver que vous nâ€™ayez pas besoin de comprendre la structure dâ€™agglomÃ©ration des objets (ou variables). Plusieurs techniques de partitionnement non hiÃ©rarchique sont disponibles sur R. On sâ€™intÃ©ressera en particulier aux k-means et au dbscan. 9.3.2.1 Kmeans Lâ€™objectif des kmeans est de minimiser la distance euclÃ©dienne entre un nombre prÃ©dÃ©fini de k groupes exclusifs. Lâ€™algorhitme commence par placer une nombre k de centroides au hasard dans lâ€™espace dâ€™un nombre p de variables (vous devez fixer k, et p est le nombre de colonnes de vos donnÃ©es). Ensuite, chaque objet est Ã©tiquettÃ© comme appartenant au groupe du centroid le plus prÃ¨s. La position du centroide est dÃ©placÃ©e Ã  la moyenne de chaque groupe. Recommencer Ã  partir de lâ€™Ã©tape 2 jusquâ€™Ã  ce que lâ€™assignation des objets aux groupes ne change plus. Source: David Sheehan La technique des kmeans suppose que les groupes ont des distributions multinormales - reprÃ©sentÃ©es par des cercles en 2D, des sphÃ¨res en 3D, des hypersphÃ¨res en plus de 3D. Cette limitation est problÃ©matique lorsque les groupes se prÃ©sentent sous des formes irrÃ©guliÃ¨res, comme celles du nuage de points de Leland McInnes, prÃ©sentÃ© plus haut. De plus, la technique classique des kmeans est basÃ©e sur des distances euclidiennes: lâ€™utilisation des kmeans nâ€™est appropriÃ©e pour les donnÃ©es comprenant beaucoup de zÃ©ros, comme les donnÃ©es dâ€™abondance, qui devraient prÃ©alablement Ãªtre transformÃ©es en variables centrÃ©es et rÃ©duites (Legendre et Legendre, 2012). La technique des mixtures gaussiennes (gaussian mixtures) est une gÃ©nÃ©ralisation des kmeans permettant dâ€™intÃ©grer la covariance des groupes. Les groupes ne sont plus des hyper-sphÃ¨res, mais des hyper-ellipsoÃ¯des. 9.3.2.1.1 Application Nous pouvons utilisÃ© la fonction kmeans de R. Toutefois, puisque lâ€™on dÃ©sire ici effectuer des tests de partitionnement pour plusieurs nombres de groupes, nous utiliserons cascadeKM, du module vegan. Notez que de nombreux paramÃ¨tres par dÃ©faut sont utilisÃ©s dans les exÃ©cutions ci-dessous. Ces notes de cours ne forment pas un travail de recherche scientifique. Lors de travaux de recherche, lâ€™utilsation dâ€™un argument ou dâ€™un autre dans une fonction doit Ãªtre justifiÃ©: quâ€™un paramÃ¨tre soit utilisÃ© par dÃ©faut dans une fonction nâ€™est a priori pas une justification convainquante. Pour les kmeans, on doit fixer le nombre de groupes. Le graphique des donnÃ©es de Leland McInnes montrent 6 groupes. Toutefois, il est rare que lâ€™on puisse visualiser des dÃ©marquations aussi tranchÃ©es que celles de lâ€™exemple, qui plus est dans des cas oÃ¹ lâ€™on doit traiter de plus de deux dimensions. Je vais donc lancer le partitionnement en boucle pour plusieurs nombres de groupes, de 3 Ã  10 et pour chaque groupe, Ã©valuer le score silouhette et de Calinski-Habaraz. Jâ€™utilise un argument random_state pour mâ€™assurer que les groupes seront les mÃªmes Ã  chaque fois que la cellule sera lancÃ©e. library(&quot;vegan&quot;) mcinnes_kmeans &lt;- cascadeKM(df_mcinnes, inf.gr = 3, sup.gr = 10, criterion = &quot;calinski&quot;) str(mcinnes_kmeans) ## List of 4 ## $ partition: int [1:2309, 1:8] 1 1 1 1 1 1 1 1 1 1 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:2309] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. ..$ : chr [1:8] &quot;3 groups&quot; &quot;4 groups&quot; &quot;5 groups&quot; &quot;6 groups&quot; ... ## $ results : num [1:2, 1:8] 85.1 2164.5 61.4 2294.6 51.4 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:2] &quot;SSE&quot; &quot;calinski&quot; ## .. ..$ : chr [1:8] &quot;3 groups&quot; &quot;4 groups&quot; &quot;5 groups&quot; &quot;6 groups&quot; ... ## $ criterion: chr &quot;calinski&quot; ## $ size : int [1:10, 1:8] 1243 505 561 NA NA NA NA NA NA NA ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:10] &quot;Group 1&quot; &quot;Group 2&quot; &quot;Group 3&quot; &quot;Group 4&quot; ... ## .. ..$ : chr [1:8] &quot;3 groups&quot; &quot;4 groups&quot; &quot;5 groups&quot; &quot;6 groups&quot; ... ## - attr(*, &quot;class&quot;)= chr &quot;cascadeKM&quot; Lâ€™objet mcinnes_kmeans, de type cascadeKM, peut Ãªtre visualisÃ© directement avec la fonction plot. plot(mcinnes_kmeans) On obtient un maximum de Calinski Ã  4 groupes, qui correspons Ã  la deuxiÃ¨me simulation effectuÃ©e de 3 Ã  10. Examinons les scores silouhette (module: cluster). library(&quot;cluster&quot;) asw &lt;- c() for (i in 1:ncol(mcinnes_kmeans$partition)) { mcinnes_kmeans_silhouette &lt;- silhouette(mcinnes_kmeans$partition[, i], dist = vegdist(df_mcinnes, method = &quot;euclidean&quot;)) asw[i] &lt;- summary(mcinnes_kmeans_silhouette)$avg.width } plot(3:10, asw, type = &#39;b&#39;) Le score silouhette maximum est Ã  3 groupes. La forme des groupes nâ€™Ã©tant pas convexe, il fallait sâ€™attendre Ã  ce que indicateurs maximaux pour les deux indicateurs soient diffÃ©rents. Câ€™est dâ€™ailleurs souvent le cas. Cet exemple supporte que le choix du nombre de groupe Ã  dÃ©partager repose sur lâ€™analyste, non pas uniquement sur les indicateurs de performance. Choisissons 6 groupes, puisque que câ€™est visuellement ce que lâ€™on devrait chercher pour ce cas dâ€™Ã©tude. kmeans_group &lt;- mcinnes_kmeans$partition[, 4] mcinnes_kmeans$partition %&gt;% head(3) ## 3 groups 4 groups 5 groups 6 groups 7 groups 8 groups 9 groups 10 groups ## 1 1 4 5 3 1 6 7 10 ## 2 1 4 4 1 1 3 7 10 ## 3 1 1 5 3 6 6 3 3 df_mcinnes %&gt;% mutate(kmeans_group = kmeans_group) %&gt;% # ajouter une colonne de regoupement ggplot(aes(x=x, y=y)) + geom_point(aes(colour = factor(kmeans_group))) + coord_fixed() Lâ€™algorithme kmeans est loin dâ€™Ãªtre statisfaisant. Cela est attendu, puisque les kmeans recherchent des distribution gaussiennes sur des groupes vraisemblablement non-gaussiens. Nous pouvons crÃ©er un graphique silouhette pour nos 6 groupes. Notez quâ€™Ã  cause dâ€™un bogue, il nâ€™est pas possible de prÃ©senter les donnÃ©es clairement lorsquâ€™elles sont nombreuses. sil &lt;- silhouette(mcinnes_kmeans$partition[, 6], dist = vegdist(df_mcinnes[, ], method = &quot;euclidean&quot;)) sil &lt;- sortSilhouette(sil) plot(sil, col = &#39;black&#39;) 9.3.2.2 DBSCAN La technique DBSCAN (* Density-Based Spatial Clustering of Applications with Noise) sousentend que les groupes sont composÃ©s de zones oÃ¹ lâ€™on retrouve plus de points (zones denses) sÃ©parÃ©es par des zones de faible densitÃ©. Pour lancer lâ€™algorithme, nous devons spÃ©cifier une mesure dâ€™association critique (distance ou dissimilaritÃ©) d* ainsi quâ€™un nombre de point critique k dans le voisinage de cette distance. Lâ€™algorithme commence par Ã©tiqueter chaque point selon lâ€™une de ces catÃ©gories: Noyau: le point a au moins k points dans son voisinage, câ€™est-Ã -dire Ã  une distance infÃ©rieure ou Ã©gale Ã  d. Bordure: le point a moins de k points dans son voisinage, mais lâ€™un de des points voisins est un noyau. Bruit: le cas Ã©chÃ©ant. Ces points sont considÃ©rÃ©s comme des outliers. Les noyaux distancÃ©s de d ou moins sont connectÃ©s entre eux en englobant les bordures. Le nombre de groupes est prescrit par lâ€™algorithme DBSCAN, qui permet du coup de dÃ©tecter des donnÃ©es trop bruitÃ©es pour Ãªtre classÃ©es. Damiani et al. (2014) a dÃ©veloppÃ© une approche utilisant la technique DBSCAN pour partitionner des zones dâ€™escale pour les flux de populations migratoires. 9.3.2.2.1 Application La technique DBSCAN nâ€™est pas basÃ©e sur le nombre de groupe, mais sur la densitÃ© des points. Lâ€™argument x ne constitue pas les donnÃ©es, mais une matrice dâ€™association. Lâ€™argument minPts spÃ©cifie le nombre minimal de points qui lâ€™on doit retrouver Ã  une distance critique d* pour la formation des *noyaux et la propagation des groupes, spÃ©cifiÃ©e dans lâ€™argument eps. La distance d peut Ãªtre estimÃ©e en prenant une fraction de la moyenne, mais on aura volontiers recours Ã  sont bon jugement. library(&quot;dbscan&quot;) mcinnes_dbscan &lt;- dbscan(x = vegdist(df_mcinnes[, ], method = &quot;euclidean&quot;), eps = 0.03, minPts = 10) dbscan_group &lt;- mcinnes_dbscan$cluster unique(dbscan_group) ## [1] 1 0 2 6 3 4 5 Les paramÃ¨tres spÃ©cifiÃ©s donnent 5 groupes (1, 2, ..., 5) et des points trop bruitÃ©s pour Ãªtre classifiÃ©s (Ã©tiquetÃ©s 0). Voyons comment les groupes ont Ã©tÃ© formÃ©s. df_mcinnes %&gt;% mutate(dbscan_group = dbscan_group) %&gt;% # ajouter une colonne de regoupement ggplot(aes(x=x, y=y)) + geom_point(aes(colour = factor(dbscan_group))) + coord_fixed() Le partitionnement semble plus conforme Ã  ce que lâ€™on recherche. NÃ©anmoins, DBSCAN crÃ© quelques petits groupes indÃ©sirables (groupe 6, en rose) ainsi quâ€™un grand groupe (violet) qui auraient lieu dâ€™Ãªtre partitionnÃ©. Ces dÃ©faut pourraient Ãªtre rÃ©glÃ©s en jouant sur les paramÃ¨tres eps et minPts. 9.3.3 Partitionnement hiÃ©rarchique Les techniques de partitionnement hiÃ©rarchique sont basÃ©es sur les matrices dâ€™association. La technique pour mesurer lâ€™association (entre objets ou variables) dÃ©terminera en grande partie le paritionnement des donnÃ©es. Les partitionnements hiÃ©rarchiques ont lâ€™avantage de pouvoir Ãªtre reprÃ©sentÃ©s sous forme de dendrogramme (ou arbre) de partition. Un tel dendrogramme prÃ©sente des sous-groupes qui se joignent en groupes jusquâ€™Ã  former un seul ensemble. Le partitionnement hiÃ©rarchique est abondamment utilisÃ© en phylogÃ©nie, pour Ã©tudier les relations de parentÃ© entre organismes vivants, populations dâ€™organismes et espÃ¨ces. La phÃ©nÃ©tique, branche empirique de la phylogÃ©nÃ¨se interspÃ©cifique, fait usage du partitionnement hiÃ©rarchique Ã  partir dâ€™associations gÃ©nÃ©tiques entre unitÃ©s taxonomiques. On retrouve de nombreuses ressources acadÃ©miques en phylogÃ©nÃ©tique ainsi que des outils pour R et Python. Toutefois, la phylogÃ©nÃ©tique en particulier ne fait pas partie de la prÃ©sente ittÃ©ration de ce manuel. 9.3.3.1 Techniques de partitionnement hiÃ©rarchique Le partitionnement hiÃ©rarchique est typiquement effectuÃ© avec une des quatres mÃ©thodes suivantes, dont chacune possÃ¨de ses particularitÃ©s, mais sont toutes agglomÃ©ratives: Ã  chaque Ã©tape dâ€™agglomÃ©ration, on fusionne les deux groupes ayant le plus dâ€™affinitÃ© sur la base des deux sous-groupes les plus rapprochÃ©s. Single link (single). Les groupes sont agglomÃ©rÃ©s sur la base des deux points parmi les groupes, qui sont les plus proches. Complete link (complete). Ã€ la diffÃ©rence de la mÃ©thode single, on considÃ¨re comme critÃ¨re dâ€™agglomÃ©ration les Ã©lÃ©ments les plus Ã©loignÃ©s de chaque groupe. AgglomÃ©ration centrale. Il sâ€™agit dâ€™une fammlle de mÃ©thode basÃ©es sur les diffÃ©rences entre les tendances centrales des objets ou des groupes. Average (average). AppelÃ©e UPGMA (Unweighted Pair-Group Method unsing Average), les groupes sont agglomÃ©rÃ©s selon un centre calculÃ©s par la moyenne et le nombre dâ€™objet pondÃ¨re lâ€™agglomÃ©ration (le poids des groupes est retirÃ©). Cette technique est historiquement utilisÃ©e en bioinformatique pour partitionner des groupes phylogÃ©nÃ©tiques (Sneath et Sokal, 1973). Weighted (weighted). La version de average, mais non pondÃ©rÃ©e (WPGMA). Centroid (centroid). Tout comme average, mais le centroÃ¯de (centre gÃ©omÃ©trique) est utilisÃ© au lieu de la moyenne. Accronyme: UPGMC. Median (median). AppelÃ©e WPGMC. Devinez! ;) Ward (ward). Lâ€™optimisation vise Ã  minimiser les sommes des carrÃ©s par regroupement. 9.3.3.2 Quel outil de partitionnement hiÃ©rarchique utiliser? Alors que le choix de la matrice dâ€™association dÃ©pend des donnÃ©es et de leur contexte, la technique de partitionnement hiÃ©rarchique peut, quant Ã  elle, Ãªtre basÃ©e sur un critÃ¨re numÃ©rique. Il en existe plusieurs, mais le critÃ¨re recommandÃ© pour le choix dâ€™une technique de partitionnement hiÃ©rarchique est la corrÃ©lation cophÃ©nÃ©tique. La distance cophÃ©nÃ©tique est la distance Ã  laquelle deux objets ou deux sous-groupes deviennent membres dâ€™un mÃªme groupe. La corrÃ©lation cophÃ©nÃ©tique est la corrÃ©lation de Pearson entre le vecteur dâ€™association des objets et le vecteur de distances cophÃ©nÃ©tiques. 9.3.3.3 Application Les techniques de partitionnement hiÃ©rarchique prÃ©sentÃ©es ci-dessus sont disponibles dans le module stats de R, qui est chargÃ© automatiquement lors de lâ€™ouversture de R. Nous allons classifier les dimensions des iris grÃ¢ce Ã  la distance de Manhattan. mcinnes_hclust_distmat &lt;- vegdist(df_mcinnes, method = &quot;manhattan&quot;) clustering_methods &lt;- c(&#39;single&#39;, &#39;complete&#39;, &#39;average&#39;, &#39;centroid&#39;, &#39;ward&#39;) clust_l &lt;- list() coph_corr_l &lt;- c() for (i in seq_along(clustering_methods)) { clust_l[[i]] &lt;- hclust(mcinnes_hclust_distmat, method = clustering_methods[i]) coph_corr_l[i] &lt;- cor(mcinnes_hclust_distmat, cophenetic(clust_l[[i]])) } ## The &quot;ward&quot; method has been renamed to &quot;ward.D&quot;; note new &quot;ward.D2&quot; tibble(clustering_methods, coph_corr = coph_corr_l) %&gt;% ggplot(aes(x = fct_reorder(clustering_methods, -coph_corr), y = coph_corr)) + geom_col() + labs(x = &quot;MÃ©thode de partitionnement&quot;, y = &quot;CorrÃ©lation cophÃ©nÃ©tique&quot;) La mÃ©thode average retourne la corrÃ©lation la plus Ã©levÃ©e. Pour plus de flexibilitÃ©, enchÃ¢ssons le nom de la mÃ©thode dans une variable. Ainsi, en chageant le nom de cette variable, le reste du code sera consÃ©quent. names(clust_l) &lt;- clustering_methods best_method &lt;- &quot;average&quot; Le partitionnement hiÃ©rarchique peut Ãªtre visualisÃ© par un dendrogramme. plot(clust_l[[best_method]]) 9.3.3.4 Combien de groupes utiliser? La longueur des lignes verticales est la distance sÃ©parant les groupes enfants. Bien que la sÃ©lection du nombre de groupe soit avant tout basÃ©e sur les besoins du problÃ¨me, nous pouvons nous appuyer sur certains outils. La hauteur totale peut servir de critÃ¨re pour dÃ©finir un nombre de groupes adÃ©quat. On pourra sÃ©lectionner le nombre de groupe oÃ¹ la hauteur se stabilise en fonction du nombre de groupe. On pourra aussi utiliser le graphique silhouette, comprenant une collection de largeurs de silouhette, reprÃ©sentant le degrÃ© dâ€™appartenance Ã  son groupe. La fonction sklearn.metrics.silhouette_score, du module scikit-learn, sâ€™en occupe. asw &lt;- c() num_groups &lt;- 3:10 for(i in seq_along(num_groups)) { sil &lt;- silhouette(cutree(clust_l[[best_method]], k = num_groups[i]), mcinnes_hclust_distmat) asw[i] &lt;- summary(sil)$avg.width } plot(num_groups, asw, type = &quot;b&quot;) Le nombre optimal de groupes serait de 5. Coupons le dendrorgamme Ã  la hauteur correspondant Ã  5 groupes avec la fonction cutree. k_opt &lt;- num_groups[which.max(asw)] hclust_group &lt;- cutree(clust_l[[best_method]], k = k_opt) plot(clust_l[[best_method]]) rect.hclust(clust_l[[best_method]], k = k_opt) La classification hiÃ©rarchique, uniquement basÃ©e sur la distance, peut Ãªtre inappropriÃ©e pour dÃ©finir des formes complexes. df_mcinnes %&gt;% mutate(hclust_group = hclust_group) %&gt;% # ajouter une colonne de regoupement ggplot(aes(x=x, y=y)) + geom_point(aes(colour = factor(hclust_group))) + coord_fixed() 9.3.4 Partitionnement hiÃ©rarchique basÃ©e sur la densitÃ© des points La tecchinque HDBSCAN, dont lâ€™algorithme est relativement rÃ©cent (Campello et al., 2013), permet une partitionnement hiÃ©rarchique sur le mÃªme principe des zones de densitÃ© de la technique DBSCAN. Le HDBSCAN a Ã©tÃ© utilisÃ©e pour partitionner les lieux dâ€™escale dâ€™oiseaux migrateurs en Chine (Xu et al., 2013). Avec DBSCAN, un rayon est fixÃ© dans une mÃ©trique appropriÃ©e. Pour chaque point, on compte le nombre de point voisins, câ€™est Ã  dire le nombre de point se situant Ã  une distance (ou une dissimilaritÃ©) Ã©gale ou infÃ©rieure au rayon fixÃ©. Avec HDBSCAN, on spÃ©cifie le nombre de points devant Ãªtre recouverts et on calcule le rayon nÃ©cessaire pour les recouvrir. Ainsi, chaque point est associÃ© Ã  un rayon critique que lâ€™on nommera \\(d_{noyau}\\). La mÃ©trique initiale est ensuite altÃ©rÃ©e: on remplace les associations entre deux objets A et B par la valeur maximale entre cette association, le rayon critique de A et le rayon critique de B. Cette nouvelle distance est appelÃ©e la distance dâ€™atteinte mutuelle: elle accentue les distances pour les points se trouvant dans des zones peu denses. On applique par la suite un algorithme semblable Ã  la partition hiÃ©rarchique single link: En sâ€™Ã©largissant, les rayons se superposent, chaque superposition de rayon forment graduellement des groupes qui sâ€™agglomÃ¨rent ainsi de maniÃ¨re hiÃ©rarchique. Au lieu dâ€™effectuer une tranche Ã  une hauteur donnÃ©e dans un dendrogramme de partitionnement, la technique HDBSCAN se base sur un dendrogramme condensÃ© qui discarte les sous-groupes comprenant moins de n objets (\\(n_{gr min}\\)). Dans nouveau dendrogramme, on recherche des groupes qui occupent bien lâ€™espace dâ€™analyse. Pour ce faitre, on utilise lâ€™inverse de la distance pour crÃ©er un indicateur de persistance (semblable Ã  la similaritÃ©), \\(\\lambda\\). Pour chaque groupe hiÃ©rarchique dans le dendrogramme condensÃ©, on peut calculer la persistance oÃ¹ le groupe prend naissance. De plus, pour chaque objet dâ€™un groupe, on peut aussi calculer une distance Ã  laquelle il quitte le groupe. La stabilitÃ© dâ€™un groupe est la domme des diffÃ©rences de persistance entre la persistance Ã  la naissance et les persistances des objets. On descend dans le dendrogramme. Si la somme des stabilitÃ© des groupes enfants est plus grande que la stabilitÃ© du groupe parent, on accepte la division. Sinon, le parent forme le groupe. La documentation du module hdbscan pour Python offre une description intuitive et plus exhaustive des principes et algorithme de HDBSCAN. 9.3.4.1 ParamÃ¨tres Outre la mÃ©trique dâ€™association dont nous avons discutÃ©, HDBSCAN demande dâ€™Ãªtre nourri avec quelques paramÃ¨tres importants. En particulier, le nombre minimum dâ€™objets par groupe, \\(n_{gr min}\\) dÃ©pend de la quantitÃ© de donnÃ©es que vous avez Ã  votre disposition, ainsi que de la quantitÃ© dâ€™objets que vous jugez suffisante pour crÃ©er des groupes. Nous utiliserons lâ€™implÃ©mentation de HDBSCAN du module dbscan. Si vous dÃ©sirez davantage dâ€™options, vous prÃ©fÃ©rerez probablement lâ€™implÃ©mentation du module largeVis. mcinnes_hdbscan &lt;- hdbscan(x = vegdist(df_mcinnes, method = &quot;euclidean&quot;), minPts = 20, gen_hdbscan_tree = TRUE, gen_simplified_tree = FALSE) hdbscan_group &lt;- mcinnes_hdbscan$cluster unique(hdbscan_group) ## [1] 6 0 4 3 5 1 2 Nous avons 6 groupes, numÃ©rotÃ©s de 1 Ã  6, ainsi que des Ã©tiquettes identifiant des objets dÃ©signÃ©s comme Ã©tant du bruit de fond, numÃ©rotÃ© 0. Le dendrogramme non condensÃ© peu Ãªtre produit. plot(mcinnes_hdbscan$hdbscan_tree) Difficile dâ€™y voir clair avec autant dâ€™objets. Lâ€™objet mcinnes_hdbscan a un nombre minimum dâ€™objets par groupe de 20. Ce qui permet de prÃ©senter le dendrogramme de maniÃ¨re condensÃ©e. plot(mcinnes_hdbscan) Enfin, un aperÃ§u des stratÃ©gies de partitionnement utilisÃ©s jusquâ€™ici. clustering_group &lt;- df_mcinnes %&gt;% mutate(kmeans_group, hclust_group, dbscan_group, hdbscan_group) %&gt;% gather(-x, -y, key = &quot;method&quot;, value = &quot;cluster&quot;) ## Warning: attributes are not identical across measure variables; ## they will be dropped clustering_group$cluster &lt;- factor(clustering_group$cluster) clustering_group %&gt;% ggplot(aes(x = x, y = y)) + geom_point(aes(colour = cluster)) + facet_wrap(~method, ncol = 2) + coord_equal() + theme_bw() Clairement, le partitionnement avec HDBSCAN donne les meilleurs rÃ©sultats. 9.3.5 Conclusion sur le partitionnement Au chapitre 4, nous avons vu avec le jeu de donnÃ©es â€œdatasaurusâ€ que la visualisation peut permettre de dÃ©tecter des structures en segmentant les donnÃ©es selon des groupes. Or, si les donnÃ©es nâ€™Ã©taient pas Ã©tiquetÃ©es, leur structure serait indÃ©tectable avec les algorithmes disponibles actuellement. Le partitionnement permet dâ€™explorer des donnÃ©es, de dÃ©tecter des tendances et de dÃ©gager des groupes permettant la prise de dÃ©cision. Plusieurs techniques de partitionnement ont Ã©tÃ© prÃ©sentÃ©es. Le choix de la technique sera dÃ©terminante sur la maniÃ¨re dont les groupes seront partitionnÃ©s. La dÃ©finition dâ€™un groupe variant dâ€™un cas Ã  lâ€™autre, il nâ€™existe pas de rÃ¨gle pour prescrire une mÃ©thode ou une autre. La partitionnement hiÃ©rarchique a lâ€™avantage de permetre de visualiser comment les groupes sâ€™agglomÃ¨rent. Parmi les mÃ©thodes de partitionnement hiÃ©rarchique disponibles, les mÃ©thodes basÃ©es sur la densitÃ© permettent une grande flexibilitÃ©, ainsi quâ€™une dÃ©tection dâ€™observations ne faisant partie dâ€™aucun goupe. 9.4 Ordination En Ã©cologie, biologie, agronommie comme en foresterie, la plupart des tableaux de donnÃ©es comprennent de nombreuses variables: pH, nutriments, climat, espÃ¨ces ou cultivars, etc. Lâ€™ordination vise Ã  mettre de lâ€™ordre dans des donnÃ©es dont le nombre Ã©levÃ© de variables peut amener Ã  des difficultÃ©s dâ€™apprÃ©ciation et dâ€™interprÃ©taion (Legendre et Legendre, 2012). Plus prÃ©cisÃ©ment, le terme ordination est utilisÃ© en Ã©cologie pour dÃ©signer les techniques de rÃ©duction dâ€™axe. Lâ€™analyse en composante principale est probablement la plus connue de ces techniques. Mais de nombreuses techniques dâ€™ordination ont Ã©tÃ© dÃ©veloppÃ©es au cours des derniÃ¨res annÃ©es, chacune ayant ses domaines dâ€™application. Les techniques de rÃ©duction dâ€™axe permettent de dÃ©gager lâ€™information la plus importante en projetant une synthÃ¨se des relations entre les observations et entre les variables. Les techniques ne supposant aucune structure a priori sont dites non-contraignantes: elles ne comprennent pas de tests statistiques. Ã€ lâ€™inverse, les ordinations contraignantes lient des variables descriptives avec une ou plusieurs variables prÃ©dictives. La rÃ©fÃ©rence en la matiÃ¨re est indiscutablement (Legendre et Legendre, 2012). Cette section en couvrira quelques unes et vous guidera vers la technique la plus appropriÃ©e pour vos donnÃ©es. 9.4.1 Ordination non contraignante Cette section couvrira lâ€™analyse en composantes principales (ACP), lâ€™analyse de correspondance (AC), lâ€™analyse factorielle (AF) ainsi que lâ€™analyse en coordonnÃ©es principales (ACoP). MÃ©thode Distance prÃ©servÃ©e Variables Analyse en composantes principales (ACP) Distance euclidienne DonnÃ©es quantitatives, relations linÃ©aires (attention aux double-zÃ©ros) Analyse de correspondance (AC) Distance de \\(\\chi^2\\) DonnÃ©es non-nÃ©gatives, dimentionnellement homogÃ¨nes ou binaires, abondance ou occurence Positionnement multidimensionnel (PoMd) Toute mesure de dissimilaritÃ© DonnÃ©es quantitatives, qualitatives nominales/ordinales ou mixtes Source: AdaptÃ© de (Legendre et Legendre, 2012, chapitre 9) 9.4.1.1 Analyse en composantes principales Lâ€™objectif dâ€™une ACP est de reprÃ©senter les donnÃ©es dans un nombre rÃ©duit de dimensions reprÃ©sentant le plus possible la variation dâ€™un tableau de donnÃ©es: elle permet de projetter les donnÃ©es dans un espace oÃ¹ les variables sont combinÃ©es en axes orthogonaux dont le premier axe capte le maximum de variance. Lâ€™ACP peut par exemple Ãªtre utilisÃ©e pour analyser des corrÃ©lations entre variables ou dÃ©gager lâ€™information la plus pertinente dâ€™un tableau de donnÃ©es mÃ©tÃ©o ou de signal en un nombre plus retreint de variables. Lâ€™ACP effectue une rotation des axes Ã  partir du centre (moyenne) du nuage de points effectuÃ©e de maniÃ¨re Ã  ce que le premier axe dÃ©finisse la direction oÃ¹ lâ€™on retrouve la variance maximale. Ce premier axe est une combinaison linÃ©aire des variables et forme la premiÃ¨re composante principale. Une fois cet axe dÃ©finit, on trouve de deuxiÃ¨me axe, orthogonal au premier, oÃ¹ lâ€™on retouve la variance maximale - cet axe forme la deuxiÃ¨me composante principale, et ainsi de suite jusquâ€™Ã  ce que le nombre dâ€™axe corresponde au nombre de variables. Les projections des observations sur ces axes principaux sont appelÃ©s les scores. Les projections des variables sur les axes principaux sont les vecteurs propres (eigenvectors, ou loadings). La variance des composantes principales diminue de la premiÃ¨re Ã  la derniÃ¨re, et peut Ãªtre calculÃ©e comme une proportion de la variance totale: câ€™est le pourcentage dâ€™inertie. Par convention, on utilise les valeurs propres (eigenvalues) pour mesurer lâ€™importance des axes. Si la premiÃ¨re composante principale a une inertie de 50% et la deuxiÃ¨me a une intertie de 30%, la reprÃ©sentation en 2D des projection reprÃ©sentera 80% de la variance du nuage de points. Lâ€™hÃ©tÃ©rogÃ©nÃ©itÃ© des Ã©chelles de mesure peut avoir une grande importance sur les rÃ©sultats dâ€™une ACP (les donnÃ©es doivent Ãªtre dimensionnellement homogÃ¨nes). En effet, la hauteur dâ€™un ceriser aura une variance plus grande que le diamÃ¨tre dâ€™une cerise exprimÃ© dans les mÃªmes unitÃ©s, et cette derniÃ¨re aura plus de variance que la teneur en cuivre dâ€™une feuille. Il est consÃ©quemment avisÃ© de mettre les donnÃ©es Ã  lâ€™Ã©chelle en centrant la moyenne Ã  zÃ©ro et lâ€™Ã©cart-type Ã  1 avant de procÃ©der Ã  une ACP. Lâ€™ACP a Ã©tÃ© conÃ§ue pour projetter en un nombre moindre de dimensions des observations dont les distributions sont multinormales. Bien que lâ€™ACP soit une technique robuste, il est prÃ©fÃ©rable de transformer prÃ©alablement les variables dont la distribution est particuliÃ¨rement asymÃ©triques (Legendre et Legendre, 2012, p.Â 450). Le cas Ã©chÃ©ant, les valeurs extrÃªmes pourraient faire dÃ©vier les vecteurs propres et biaiser lâ€™analyse. En particulier, les donnÃ©es ACP menÃ©es sur des donnÃ©es compositionnelles sont rÃ©putÃ©es pour gÃ©nÃ©rer des analyses biaisÃ©es (Pawlowsky-Glahn and Egozcue, 2006). Le test de Mardia (Korkmaz, 2014) peut Ãªtre utilisÃ© pour tester la multinormalitÃ©. Une distribution multinormale devrait gÃ©nÃ©rer des scores en forme dâ€™hypersphÃ¨re (en forme de cercle sur un biplot: voir plus loin). 9.4.1.1.1 Vecteurs propres et valeurs propres Une matrice carrÃ©e (comme une matrice de covariance \\(\\Sigma\\)) multipliÃ©e par un vecteur propre \\(e\\) est Ã©gale aux valeurs propres \\(\\lambda\\) multipliÃ©es par les vecteurs propres \\(e\\). \\[ \\Sigma e = \\lambda e \\] De maniÃ¨re intuitive, les vecteurs propres indiquent lâ€™orientation de la covariance, et les valeurs propres indique la longueur associÃ©e Ã  cette direction. Lâ€™ACP est basÃ©e sur le calcul des vecteurs propres et des valeurs propres de la matrice de covariance des variables. Pour dâ€™abord obtenir les valeurs propres \\(\\lambda\\), il faut rÃ©soudre lâ€™Ã©quation \\[ det(cov(X) - \\lambda I) = 0 \\], oÃ¹ \\(det\\) est lâ€™opÃ©ration permettant de calculer le dÃ©terminant, \\(cov\\) est lâ€™opÃ©ration pour calculer la covariance, \\(X\\) est la matrice de donnÃ©es, \\(\\lambda\\) sont les valeurs propres et \\(I\\) est une matrice dâ€™identitÃ©. Pour \\(p\\) variables dans votre tableau \\(X\\), vous obtiendrex \\(p\\) valeurs propres. Ensuite, on trouve les vecteurs propres en rÃ©solvant lâ€™Ã©quation $ e = e $. Bien quâ€™il soit possible dâ€™effectuer cette opÃ©ration Ã  la main pour des cas trÃ¨s simples, vous aurez avantage Ã  utiliser un langage de programmation. Chargeons les donnÃ©es dâ€™iris, puis isolons seulement les deux dimensions des sÃ©pales lâ€™espÃ¨ce setosa. data(&quot;iris&quot;) setosa_sepal &lt;- iris %&gt;% filter(Species == &quot;setosa&quot;) %&gt;% select(starts_with(&quot;Sepal&quot;)) setosa_sepal ## Sepal.Length Sepal.Width ## 1 5.1 3.5 ## 2 4.9 3.0 ## 3 4.7 3.2 ## 4 4.6 3.1 ## 5 5.0 3.6 ## 6 5.4 3.9 ## 7 4.6 3.4 ## 8 5.0 3.4 ## 9 4.4 2.9 ## 10 4.9 3.1 ## 11 5.4 3.7 ## 12 4.8 3.4 ## 13 4.8 3.0 ## 14 4.3 3.0 ## 15 5.8 4.0 ## 16 5.7 4.4 ## 17 5.4 3.9 ## 18 5.1 3.5 ## 19 5.7 3.8 ## 20 5.1 3.8 ## 21 5.4 3.4 ## 22 5.1 3.7 ## 23 4.6 3.6 ## 24 5.1 3.3 ## 25 4.8 3.4 ## 26 5.0 3.0 ## 27 5.0 3.4 ## 28 5.2 3.5 ## 29 5.2 3.4 ## 30 4.7 3.2 ## 31 4.8 3.1 ## 32 5.4 3.4 ## 33 5.2 4.1 ## 34 5.5 4.2 ## 35 4.9 3.1 ## 36 5.0 3.2 ## 37 5.5 3.5 ## 38 4.9 3.6 ## 39 4.4 3.0 ## 40 5.1 3.4 ## 41 5.0 3.5 ## 42 4.5 2.3 ## 43 4.4 3.2 ## 44 5.0 3.5 ## 45 5.1 3.8 ## 46 4.8 3.0 ## 47 5.1 3.8 ## 48 4.6 3.2 ## 49 5.3 3.7 ## 50 5.0 3.3 library(&quot;MVN&quot;) ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 ## sROC 0.1-2 loaded setosa_sepal_mvn &lt;- mvn(setosa_sepal, mvnTest = &quot;mardia&quot;) setosa_sepal_mvn$multivariateNormality ## Test Statistic p value Result ## 1 Mardia Skewness 0.759503524380438 0.943793240544741 YES ## 2 Mardia Kurtosis 0.0934600553610254 0.925538081956867 YES ## 3 MVN &lt;NA&gt; &lt;NA&gt; YES Pour considÃ©rer la distribution comme multinormale, la p-value de la distortion (Mardia Skewness) et la statistique de Kurtosis (Mardia Kurtosis) doit Ãªtre Ã©gale ou plus Ã©levÃ©e que 0.05 (Kormaz, 2019, fiche dâ€™aide de la fonction mvn de R). Câ€™est bien le cas pour les donnÃ©es du tableau setosa_sepal. Retirons de la matrice de covariance les valeurs et vecteurs propres avec la fonction eigen. setosa_eigen &lt;- eigen(cov(setosa_sepal)) setosa_eigenval &lt;- setosa_eigen$values setosa_eigenvec &lt;- setosa_eigen$vectors Le premier vecteur propre correspond Ã  la premiÃ¨re colonne, et le second Ã  la deuxiÃ¨me. Les coordonnÃ©es x et y sont les premiÃ¨res et deuxiÃ¨mes lignes. Les vecteurs propres ont une longueur unitaire (norme de 1). Ils peuvent Ãªtre mis Ã  lâ€™Ã©chelles Ã  la racine carrÃ©e des valeurs propres. setosa_eigenvec_sc &lt;- setosa_eigenvec %*% diag(sqrt(setosa_eigen$values)) Pour effectuer une translation des vecteurs propres au centre du nuage de point, nous avons besoin du centroÃ¯de. centroid &lt;- setosa_sepal %&gt;% apply(., 2, mean) plot(setosa_sepal, asp = 1) # vecteurs propres brutes lines(x=c(centroid[1], centroid[1] + setosa_eigenvec[1, 1]), y=c(centroid[2], centroid[2] + setosa_eigenvec[2, 1]), col = &quot;green&quot;, lwd = 3) # vecteur propre 1 lines(x=c(centroid[1], centroid[1] + setosa_eigenvec[1, 2]), y=c(centroid[2], centroid[2] + setosa_eigenvec[2, 2]), col = &quot;green&quot;, lwd = 3) # vecteur propre 1 # vecteurs propres Ã  l&#39;Ã©chelle lines(x=c(centroid[1], centroid[1] + setosa_eigenvec_sc[1, 1]), y=c(centroid[2], centroid[2] + setosa_eigenvec_sc[2, 1]), col = &quot;red&quot;, lwd = 4) # vecteur propre 1 lines(x=c(centroid[1], centroid[1] + setosa_eigenvec_sc[1, 2]), y=c(centroid[2], centroid[2] + setosa_eigenvec_sc[2, 2]), col = &quot;red&quot;, lwd = 4) # vecteur propre 1 points(x=centroid[1], y=centroid[2], pch = 16, cex = 2, col =&quot;blue&quot;) # centroid On peut observer que, comme je lâ€™ai mentionnÃ© plus haut, les vecteurs propres indiquent lâ€™orientation de la covariance, et les valeurs propres indique la longueur associÃ©e Ã  cette direction. 9.4.1.1.2 Biplot Imaginez un nuage de points en 3D, axes y compris. Vous tournez votre nuage de points pour trouver la perspective en 2D qui fera en sorte que vos donnÃ©es soient les plus dispersÃ©es possibles. Avec une lampe de poche, vous illuminez votre nuage de points dans lâ€™axe de cette perspective: vous venez dâ€™effectuer une analyse en composantes principales, et lâ€™ombre des points et des axes sur le mur formera votre biplot. Pour crÃ©er un biplot, on juxtapose les descripteurs (variables) en tant que vecteurs propres, reprÃ©sentÃ©s par des flÃ¨ches, et les objets (observations) en tant que scores, reprÃ©sentÃ©s par des points. Les rÃ©sultats dâ€™une ordination peuvent Ãªtre prÃ©sentÃ©s selon deux types de biplots (Legendre et Legendre, 2012). Biplot de corrÃ©lation permettant de visualiser les corrÃ©lations entre des variables mÃ©tÃ©orologiques. Source: Parent, 2017 Deux types de projection sont courramment utilisÃ©s. Biplot de distance. Ce type de projection permet de visualiser la position des objets entre eux et par rapport aux descripteurs et dâ€™apprÃ©cier la contribution des descripteurs pour crÃ©er les composantes principales. Pour crÃ©er un biplot de distance, on projette directement les vecteurs propres (\\(U\\)) en guise de descripteurs. Pour ce qui est des objets, on utilise les scores de lâ€™ACP (\\(F\\)). De cette maniÃ¨re, les distances euclidiennes entre les scores sont des approximations des distances euclidiennes dans lâ€™espace multidimentionnel, la projection dâ€™un objet sur un descripteur perpendiculairement Ã  ce dernier est une approximation de la position de lâ€™objet sur le descripteur et la projection dâ€™un descripteur sur un axe principal est proportionnelle Ã  sa contribution pour gÃ©nÃ©rer lâ€™axe. Biplot de corrÃ©lation. Cette projection permet dâ€™apprÃ©cier les corrÃ©lations entre les descripteurs. Pour ce faire, les objets et les valeurs propres doivent Ãªtre transformÃ©s. Pour gÃ©nÃ©rer les descripteurs, les vecteurs propres (\\(U\\)) doivent Ãªtre multipliÃ©s par la matrice diagonalisÃ©e de la racine carrÃ©e des valeurs propres (\\(\\Lambda\\)), câ€™est-Ã -dire \\(U \\Lambda ^{\\frac{1}{2}}\\). En ce qui a trait aux objets, on multiplie les scores par (\\(F\\)) par la racine carrÃ©e nÃ©gative des valeurs propres diagonalisÃ©es, câ€™est-Ã -dire \\(F \\Lambda ^{- \\frac{1}{2}}\\). De cette maniÃ¨re, tout comme câ€™est le cas pour le biplot de distance, la projection dâ€™un objet sur un descripteur perpendiculairement Ã  ce dernier est une approximation de la position de lâ€™objet sur le descripteur, la projection dâ€™un descripteur sur un axe principal est proportionnelle Ã  son Ã©cart-type et les angles entre les descripteurs sont proportionnelles Ã  leur corrÃ©lation (et non pas leur proximitÃ©). En dâ€™autres mots, le bilot de distances devrait Ãªtre utilisÃ© pour apprÃ©cier la distance entre les objets et le biplot de corrÃ©lation devrait Ãªtre utilisÃ© pour apprÃ©cier les corrÃ©lations entre les descripteurs. Mais dans tous les cas, le type de biplot utilisÃ© doit Ãªtre indiquÃ©. Le triplot est une forme apparentÃ©e au biplot, auquel on ajoute des variables prÃ©dictives. Le triplot est utile pour reprÃ©senter les rÃ©sultats des ordinations contraignantes comme les analyses de redondance et les analyse de correspondance canoniques. 9.4.1.1.3 Application Bien que lâ€™ACP puisse Ãªtre effectuÃ©e grÃ¢ce Ã  des modules de base de R, nous utiliserons le module vegan. Le tableau varechem comprend des donnÃ©es issues dâ€™analyse de sols identifiÃ©s par leur composition chimique, leur pH, leur profondeur totale et la profondeur de lâ€™humus publiÃ©es dans VÃ¤re et al. (1995) et exportÃ©es du module vegan. library(&quot;vegan&quot;) data(&quot;varechem&quot;) varechem %&gt;% sample_n(5) ## N P K Ca Mg S Al Fe Mn Zn Mo Baresoil Humdepth pH ## 1 26.6 36.7 171.4 738.6 94.9 33.8 20.7 2.5 77.6 7.4 0.3 23.0 2.8 2.8 ## 2 26.2 61.9 202.2 741.2 86.3 48.6 124.3 23.6 94.5 10.2 0.6 56.9 2.5 2.9 ## 3 22.8 50.6 151.7 648.0 64.8 30.2 12.1 2.3 122.9 8.1 0.2 23.7 2.6 2.9 ## 4 18.0 64.9 224.5 517.6 59.7 52.9 435.1 101.2 38.0 9.5 1.1 21.3 1.8 2.9 ## 5 19.1 26.4 61.1 259.1 37.0 21.4 155.1 81.4 20.6 4.0 0.6 5.8 1.9 3.0 Comme nous lâ€™avons vu prÃ©cdemment, les donnÃ©es de concentration sont de type compositionnelles. Les donnÃ©es compositionnelles du tableau varechem mÃ©riteraient dâ€™Ãªtre transformÃ©es (Aitchison et Greenacre, 2002). Utilisons les log-ratios centrÃ©s (clr). library(&quot;compositions&quot;) varecomp &lt;- varechem %&gt;% select(-Baresoil, -Humdepth, -pH) %&gt;% mutate(Fv = apply(., 1, function(x) 1e6 - sum(x))) vareclr &lt;- varecomp %&gt;% acomp(.) %&gt;% clr(.) %&gt;% as_tibble() %&gt;% bind_cols(varechem %&gt;% select(Baresoil, Humdepth, pH)) ## Warning: Calling `as_tibble()` on a vector is discouraged, because the behavior is likely to change in the future. Use `tibble::enframe(name = NULL)` instead. ## This warning is displayed once per session. vareclr %&gt;% sample_n(5) ## # A tibble: 5 x 15 ## N P K Ca Mg S Al Fe Mn Zn Mo Fv Baresoil Humdepth pH ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -1.34 -0.760 0.673 2.07 0.549 -0.762 0.181 -1.60 -0.696 -2.26 -5.39 9.34 40.5 3.8 2.7 ## 2 -1.37 -0.655 0.697 2.01 -0.0269 -0.705 0.547 -1.89 -0.869 -2.35 -5.06 9.67 11.2 2.2 2.9 ## 3 -0.773 -0.988 0.175 1.05 -0.174 -0.960 1.50 0.628 -1.88 -3.09 -5.11 9.62 18.6 1.7 3.1 ## 4 -0.871 -0.623 0.872 1.92 -0.244 -0.757 0.250 -1.77 -0.862 -2.41 -5.26 9.76 29.8 2 2.8 ## 5 -1.68 -0.469 0.833 2.57 0.347 -0.910 1.04 0.0773 -1.27 -2.39 -7.49 9.32 7.6 1.1 3.6 Effectuons lâ€™ACP. Pour cet exemple, nous standardiserons les donnÃ©es Ã©tant donnÃ©es que les colonnes Baresoil, Humedepth et pH ne sont pas Ã  la mÃªme Ã©chelle que les colonnes des clr. vareclr_sc &lt;- scale(vareclr) vare_pca &lt;- rda(vareclr_sc) # ou bien rda(vareclr, scale = TRUE, mais la mise Ã  l&#39;Ã©chelle prÃ©alable est plus explicite) Lâ€™objet vareclr_pca contient lâ€™information nÃ©cessaire pour mener notre ACP. summary(vare_pca, scaling = 2) # scaling = 2 pour obtenir les infos pour les biplots de corrÃ©lation ## ## Call: ## rda(X = vareclr_sc) ## ## Partitioning of variance: ## Inertia Proportion ## Total 15 1 ## Unconstrained 15 1 ## ## Eigenvalues, and their contribution to the variance ## ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 PC10 PC11 PC12 PC13 ## Eigenvalue 7.1523 2.4763 2.1122 0.93015 0.57977 0.48786 0.36646 0.29432 0.19686 0.15434 0.107357 0.095635 0.042245 ## Proportion Explained 0.4768 0.1651 0.1408 0.06201 0.03865 0.03252 0.02443 0.01962 0.01312 0.01029 0.007157 0.006376 0.002816 ## Cumulative Proportion 0.4768 0.6419 0.7827 0.84473 0.88338 0.91590 0.94034 0.95996 0.97308 0.98337 0.990527 0.996902 0.999719 ## PC14 ## Eigenvalue 0.0042200 ## Proportion Explained 0.0002813 ## Cumulative Proportion 1.0000000 ## ## Scaling 2 for species and site scores ## * Species are scaled proportional to eigenvalues ## * Sites are unscaled: weighted dispersion equal on all dimensions ## * General scaling constant of scores: 4.309777 ## ## ## Species scores ## ## PC1 PC2 PC3 PC4 PC5 PC6 ## N 0.1437 0.7606 -0.6792 0.19837 0.1128526 -0.050149 ## P 0.8670 -0.3214 -0.2950 -0.22940 0.1437960 -0.042884 ## K 0.9122 -0.3857 0.2357 0.03469 0.2737020 0.075717 ## Ca 0.9649 -0.3362 -0.2147 0.17757 -0.2188717 0.008051 ## Mg 0.8263 -0.2723 0.1035 0.52135 -0.1495399 -0.342214 ## S 0.8825 -0.3169 0.3539 -0.21216 0.1176279 -0.191386 ## Al -1.0105 -0.2442 0.2146 0.02674 -0.1005560 -0.043569 ## Fe -1.0338 -0.2464 0.1492 0.13162 0.1512218 0.081571 ## Mn 0.9556 0.1041 -0.1256 -0.21300 0.2565831 0.275275 ## Zn 0.7763 -0.1031 -0.3123 -0.36493 -0.5665691 0.153089 ## Mo -0.2152 0.8717 0.4065 -0.33643 -0.2134335 -0.167725 ## Fv 0.2360 0.5776 -0.8112 0.12736 0.1280097 -0.109737 ## Baresoil 0.5147 0.4210 0.4472 0.54980 -0.1438570 0.463148 ## Humdepth 0.7455 0.4379 0.5194 0.16493 0.0004757 -0.273056 ## pH -0.5754 -0.5864 -0.5957 0.23408 -0.1517661 -0.056641 ## ## ## Site scores (weighted sums of species scores) ## ## PC1 PC2 PC3 PC4 PC5 PC6 ## sit1 0.16862 0.423777 0.46731 0.91175 1.10380 1.06421 ## sit2 -0.09705 -0.097482 0.61143 -0.29049 1.14916 0.40622 ## sit3 0.02831 -0.795737 0.74176 -0.19097 -2.43337 -0.81762 ## sit4 1.39081 -0.354376 -0.19377 -0.45160 0.46020 -0.31446 ## sit5 1.30346 0.357866 0.29887 0.76856 0.20913 -0.64145 ## sit6 0.43636 0.495037 1.21722 1.18128 -0.98242 -0.74474 ## sit7 1.07306 0.467575 -0.32245 0.03717 0.13956 -0.64972 ## sit8 0.02545 0.659714 -0.28861 -0.01424 0.47105 0.45173 ## sit9 1.42005 0.007356 -0.29000 -0.78474 0.97592 -0.80263 ## sit10 -0.50638 -0.220909 1.52981 0.26289 0.42135 0.94054 ## sit11 0.45392 0.649297 0.44573 -0.26620 -0.74522 -0.53228 ## sit12 0.18623 0.259640 0.89112 0.21096 -0.51393 2.24361 ## sit13 1.26264 0.225744 -0.96668 -0.69334 0.61990 0.43312 ## sit14 -1.48685 0.739545 -0.20926 1.09256 0.61856 -0.87999 ## sit15 -0.50622 1.108685 -2.61287 -1.00433 -1.35383 1.21964 ## sit16 -1.28653 0.898663 -0.38778 -0.47556 -0.02449 -0.29419 ## sit17 -1.72773 0.476962 -0.48878 0.71156 1.06398 -1.33473 ## sit18 -0.82844 -0.296515 1.20315 -1.49821 -0.18330 1.05231 ## sit19 -1.00247 -0.609253 0.25185 -0.85420 0.71031 0.14854 ## sit20 -0.43405 -0.338912 0.55348 -1.35776 -0.81986 -1.02468 ## sit21 -0.05083 0.122645 -0.04611 -0.56047 -0.26151 -0.98053 ## sit22 0.17891 -2.315489 -0.69084 -0.19547 0.80628 0.04291 ## sit23 -0.46443 -2.592018 -1.21615 1.56359 -0.62334 0.28748 ## sit24 0.46316 0.728185 -0.49843 1.89726 -0.80791 0.72671 La deuxiÃ¨me ligne de Importance of components, Proportion Explained, indique la proportion de la variance totale captÃ©e successivement par les axes principaux. Le premier axe principal comporte 47.68% de la variance. Le deuxiÃ¨me axe principal ajoutant une proportion de 16,51%, une reprÃ©sentation en deux axes principaux prÃ©sentent 64.19 % de la variance. prop_expl &lt;- vare_pca$CA$eig / sum(vare_pca$CA$eig) prop_expl ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 ## 0.4768180610 0.1650859388 0.1408156459 0.0620101490 0.0386511040 0.0325238535 0.0244303815 0.0196215021 0.0131238464 ## PC10 PC11 PC12 PC13 PC14 ## 0.0102890284 0.0071571089 0.0063756951 0.0028163495 0.0002813359 La dÃ©cision du nombre dâ€™axes principaux Ã  retenir est arbitraire. Elle peut dÃ©pendre dâ€™un nombre maximal de paramÃ¨tre Ã  retenir pour Ã©viter de surdimensionner un modÃ¨le (curse of dimensionality, section 11) ou dâ€™un seuil de pourcentage de variance minimal Ã  retenir, par exemple 75%. Ou bien, vous retiendrez deux composantes principales si vous dÃ©sirez prÃ©senter un seul biplot. Lâ€™approche de Kaiser-Guttmann (Borcard et al., 2011) consiste Ã  sÃ©lectionner les composantes principales dont la valeur propre est supÃ©rieure Ã  leur moyenne. plot(x = 1:length(vare_pca$CA$eig), y = vare_pca$CA$eig, type = &quot;b&quot;, xlab = &quot;Rang de la valeur propre&quot;, ylab = &quot;Valeur propre&quot;) abline(h = mean(vare_pca$CA$eig), col = &quot;red&quot;, lty = 2) Lâ€™approche du broken stick consiste Ã  couper un bÃ¢ton dâ€™une longueur de 1 en n tranches. La premiÃ¨re tranche est de longueur \\(\\frac{1}{n}\\). La tranche suivante est dâ€™une longueur de la tranche prÃ©cÃ©dente Ã  laquelle on aditionne \\(\\frac{1}{longueur~restante}\\). Puis on place les longueurs en ordre dÃ©croissant. On retient les composantes principales dont les valeurs propres cumulÃ©es sont plus grandes que le broken stick. broken_stick &lt;- function(x) { bsm &lt;- vector(&quot;numeric&quot;, length = x) bsm[1] &lt;- 1/x for (i in 2:x) { bsm[i] &lt;- bsm[i-1] + 1/(x+1-i) } bsm &lt;- rev(bsm/x) return(bsm) } Le graphique du broken stick: plot(x = 1:length(vare_pca$CA$eig), y = prop_expl, type = &quot;b&quot;, xlab = &quot;Rang de la valeur propre&quot;, ylab = &quot;Valeur propre&quot;) lines(x = 1:length(vare_pca$CA$eig), y = broken_stick(length(vare_pca$CA$eig)), col = &quot;red&quot;, lty = 2) Les approches Kaiser-Guttmann et broken stick suggÃ¨rent que les trois premiÃ¨res composantes sont suffisantes pour dÃ©crire la dispersion des donnÃ©es. Examinons les loadings (vecteurs propres) plus en particulier. Dans le langage du module vegan, les vecteurs propres sont les espÃ¨ces (species) et les scores sont les sites. vare_eigenvec &lt;- vegan::scores(vare_pca, scaling = 2, display = &quot;species&quot;, choices = 1:(ncol(vareclr)-1)) vare_eigenvec ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 ## N 0.1437343 0.7606006 -0.6792046 0.1983670 0.1128526122 -0.050148980 -0.09111164 -0.06122008 0.315645453 ## P 0.8669892 -0.3213683 -0.2949864 -0.2294036 0.1437959857 -0.042883754 0.26894062 0.34111276 0.021124287 ## K 0.9122089 -0.3857245 0.2356904 0.0346904 0.2737019601 0.075717162 -0.21662612 -0.01641260 0.143099440 ## Ca 0.9648855 -0.3361651 -0.2147486 0.1775746 -0.2188716732 0.008050762 0.03630015 0.04775616 -0.073609828 ## Mg 0.8263327 -0.2723055 0.1035276 0.5213484 -0.1495399242 -0.342213793 0.04617838 -0.12098602 -0.051599273 ## S 0.8824519 -0.3169039 0.3538854 -0.2121562 0.1176278503 -0.191386377 -0.26825994 0.15822845 0.038378858 ## Al -1.0105173 -0.2441785 0.2145614 0.0267422 -0.1005559874 -0.043569364 -0.22737412 0.10598673 0.040586196 ## Fe -1.0337676 -0.2463987 0.1491865 0.1316173 0.1512218115 0.081571443 0.10553041 -0.09254655 -0.079426433 ## Mn 0.9555632 0.1041030 -0.1256178 -0.2130047 0.2565830557 0.275275174 0.20224538 -0.19347804 -0.038859808 ## Zn 0.7763480 -0.1030878 -0.3122919 -0.3649341 -0.5665691228 0.153089144 -0.12332232 -0.14862229 0.024026151 ## Mo -0.2152399 0.8717229 0.4064967 -0.3364279 -0.2134335302 -0.167725160 0.13788948 0.17165900 0.032981311 ## Fv 0.2360040 0.5775863 -0.8111953 0.1273582 0.1280096553 -0.109737235 -0.20911147 0.11289753 -0.281443886 ## Baresoil 0.5147445 0.4209983 0.4472351 0.5497950 -0.1438569673 0.463148072 -0.02103009 0.23028292 0.004554036 ## Humdepth 0.7455213 0.4379436 0.5193895 0.1649306 0.0004756685 -0.273056212 0.17061078 -0.11310394 0.027515405 ## pH -0.5753858 -0.5863743 -0.5957495 0.2340826 -0.1517660977 -0.056640816 0.19890884 0.12152266 0.150118818 ## PC10 PC11 PC12 PC13 PC14 ## N 0.08090232 -0.019251478 0.045420621 0.05020956 0.002340519 ## P 0.08756299 -0.045741546 0.145128883 -0.03337551 -0.010109130 ## K -0.08737113 0.183005607 0.002260341 -0.10566808 0.001169065 ## Ca -0.10601799 0.161460554 0.041210064 0.14341793 0.007419161 ## Mg 0.18373857 -0.009862571 -0.063493608 -0.03782662 -0.023575986 ## S 0.05100717 -0.138785063 -0.117144869 0.06075094 0.025874035 ## Al -0.14473132 -0.089462074 0.058212507 0.01983102 -0.037901576 ## Fe 0.09908706 -0.006376211 0.049837173 -0.01169516 0.036048221 ## Mn -0.07637994 -0.083300112 -0.133353213 0.02679781 -0.021373612 ## Zn 0.02643462 -0.064973307 0.051057277 -0.06538348 0.010896560 ## Mo 0.01419924 0.128814989 -0.114803631 -0.01989539 -0.001335923 ## Fv -0.08391004 -0.012456867 -0.020157331 -0.05448619 0.005707928 ## Baresoil 0.02604286 -0.061147847 -0.019696758 -0.01640490 0.003823725 ## Humdepth -0.23068827 -0.102189307 0.109293684 -0.02485030 0.016559206 ## pH -0.15240317 -0.037691048 -0.153813168 -0.04523353 0.014193061 ## attr(,&quot;const&quot;) ## [1] 4.309777 Lâ€™ordre dâ€™importance des vecteurs propres est Ã©tabli en ordre croissant des Ã©lÃ©ment des vecteurs propres associÃ©es. Un vecteur propre est une combinaison linÃ©aire des variables. Par exemple, le premier vecteur propre pointe surtout dans la direction du Fe (-1.497) et de lâ€™Al (-1.463). Le deuxiÃ¨me pointe surtout vers le Mo (2.145). Les vecteurs (loadings) dâ€™un biplot de distance prÃ©sentant les des deux premiÃ¨res composantes principales prendront les coordonnÃ©es des deux premiÃ¨res colonnes. Le vecteur Al aura la coordonnÃ©e [-1.463 ; -0.601], le vecteur de Fe sera placÃ© Ã  [-1.497 ; -0.606] et le vecteur Mo Ã  [-0.312 ; 2.145]. Il existe diffÃ©rentes fonctions dâ€™affichage des biplots. Notez que leur longueur peut Ãªtre magnifiÃ©e pour amÃ©liorer la visualisation. LanÃ§ons la fonction biplot pour crÃ©er un biplot de distance et un autre de corrÃ©lation. par(mfrow = c(1, 2)) biplot(vare_pca, scaling = 1, main = &quot;Biplot de distance&quot;) biplot(vare_pca, scaling = 2, main = &quot;Biplot de corrÃ©lation&quot;) Le biplot de distance permet de dÃ©gager les variables qui expliquent davantage la variabilitÃ© dans notre tableau: les clr du Fe et de lâ€™Al forment en grande partie le premier axe principal, alors que le clr du Mo forme en grande partie le second axe. Le biplot de corrÃ©lation montre que les clr du Fe et du Al sont corrÃ©lÃ©s dans le mÃªme sens, mais das le sens contraire du clr du Mn. Lâ€™information sur la teneur en Fe et celle de lâ€™Al est en grande partie redondante. Toutefois, le clr du Mo est presque indÃ©pendant du clr du Fe, ceux-ci Ã©tant Ã  angle presque droit (~90Â°). Ces relations peuvent Ãªtre explorÃ©es directement. par(mfrow = c(1, 2)) plot(vareclr$Al, vareclr$Fe) plot(vareclr$Mo, vareclr$Fe) Nous avons mentionnÃ© que lâ€™ACP est une rotation. Prenons un second exemple pour bien en saisir les tenants et aboutissants. Le tableau de donnÃ©es que nous chargerons provient dâ€™un infographie dâ€™un dauphin, intitullÃ©e Bottlenose Dolphin, conÃ§u par lâ€™artiste Tarnyloo. Les points correspondent Ã  la surface dâ€™un dauphin. Jâ€™ai ajoutÃ© une colonne anatomy, qui indique Ã  quelle partie anatomique le point appartient. dolphin &lt;- read_csv(&quot;data/07_dolphin.csv&quot;) ## Parsed with column specification: ## cols( ## x = col_double(), ## y = col_double(), ## z = col_double(), ## anatomy = col_character() ## ) dolphin %&gt;% sample_n(5) ## # A tibble: 5 x 4 ## x y z anatomy ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 -0.440 1.06 1.24 Caudal fin ## 2 0.0360 -0.494 -0.198 Head ## 3 -0.177 -0.324 0.00479 Head ## 4 0.0921 -0.436 -0.0795 Head ## 5 -0.333 1.07 1.23 Caudal fin Voici en vue isomÃ©trique ce en quoi consiste ce nuage de points. library(&quot;scatterplot3d&quot;) scatterplot3d(x = dolphin$x, y = dolphin$y, z = dolphin$z, pch = 16, cex.symbols = 0.2) Effectuons lâ€™ACP sur le dauphin. dolph_pca &lt;- rda(dolphin %&gt;% select(x, y, z), scale = FALSE) biplot(dolph_pca, scaling = 2) On nâ€™y voit pas grand chose, mais si lâ€™on extrait les scores et que lâ€™on raccourcit les vecteurs: dolph_scores &lt;- vegan::scores(dolph_pca, display = &quot;sites&quot;) dolph_loads &lt;- vegan::scores(dolph_pca, display = &quot;species&quot;) dolph_loads ## PC1 PC2 ## x -0.02990131 0.01608095 ## y -7.13731672 -1.43221776 ## z -4.56612084 2.23859843 ## attr(,&quot;const&quot;) ## [1] 9.089026 plot(dolph_scores, pch = 16, cex = 0.24, asp = 1, col = factor(dolphin$anatomy)) segments(x0 = rep(0, 3), y0 = rep(0, 3), x = dolph_loads[, 1]/50, y = dolph_loads[, 2]/50, col = &quot;chocolate&quot;, lwd = 4) La meilleure reprÃ©sentation du dauphin en 2D, selon la variance, est son profil - en effet, il est plus long et haut que large. Note. Une ACP effectue seulement une rotation des points. Les distances euclidiennes entre les points sont maintenues. Note. Lâ€™ACP a Ã©tÃ© conÃ§ue pour projetter en un nombre moindre de dimensions des observations dont les distributions sont multinormales (ce nâ€™est Ã©videmment pas le cas du dauphin). Note. Les axes principaux dâ€™une ACP sont des variables alÃ©atoires. Elles peuvent Ãªtre assujetties Ã  des tests ststistiques, des modÃ¨les, du partitionnement de donnÃ©es, etc. Excercice. Effectuez maintenant une ACP avec les donnÃ©es dâ€™iris. 9.4.1.2 Analyse de correspondance (AC) Lâ€™analyse de correspondance (AC) est particuliÃ¨rement appropriÃ©e pour traiter des donnÃ©es dâ€™abondance et dâ€™occurence. Tout comme lâ€™analyse en composantes principales, les donnÃ©es apportÃ©s vers une AC doivent Ãªtre dimensionnellement homogÃ¨nes, câ€™est-Ã -dire que chaque variable doit Ãªtre de mÃªme mÃ©trique: pour des donnÃ©es dâ€™abondance, cela signifie que les dÃ©comptes rÃ©fÃ¨rent tous au mÃªme concept: individus, colonies, surfaces occupÃ©es, etc. Alors que la distance euclidienne est prÃ©servÃ©e avec lâ€™ACP, lâ€™AC prÃ©serve la distance du \\(\\chi^2\\), qui est insensible aux double-zÃ©ros. Lâ€™AC produit \\(min(n,p)-1\\) axes principaux orthogonaux qui captent non pas le maximum de variance, mais la proportion de mesures aux carrÃ© par rapport Ã  la somme des carrÃ©s de la matrice. Le biplot obtenu peut Ãªtre prÃ©sentÃ© sous forme de biplot de site (scaling 1), oÃ¹ la distance du \\(\\chi^2\\) est prÃ©servÃ©e entre les sites ou biplot dâ€™espÃ¨ces (scaling 2), ou la distance du \\(\\chi^2\\) est prÃ©servÃ©e entre les espÃ¨ces. Lâ€™AC hÃ©rite du coup une propriÃ©tÃ© importate de la distance du \\(\\chi^2\\), qui accorde davantage de distance entre un compte de 0 et de 1 quâ€™entre 1 et 2, et davantage entre 1 et 2 quâ€™entre 2 et 3. Par exemple, sur ces trois sites, on a comptÃ© un individu A de moins que dâ€™individu B. abundance_0123 = tibble(Site = c(&quot;Site 1&quot;, &quot;Site 2&quot;, &quot;Site 3&quot;), A = c(0, 1, 9), B = c(1, 2, 10)) abundance_0123 ## # A tibble: 3 x 3 ## Site A B ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Site 1 0 1 ## 2 Site 2 1 2 ## 3 Site 3 9 10 Pourtant, la distance du \\(\\chi^2\\) est plus Ã©levÃ©e entre le site 1 et le site 2 quâ€™entre le site 2 et le site 3. dist(decostand(abundance_0123 %&gt;% select(-Site), method=&quot;chi.square&quot;)) ## 1 2 ## 2 0.6724111 ## 3 0.9555316 0.2831205 La distance du \\(\\chi^2\\) donne davantage dâ€™importance aux espÃ¨ces rares, ce dont une analyse doit tenir compte. Il pourrait Ãªtre envisageable de retirer dâ€™un tableau des espÃ¨ces rare, ou bien prÃ©transformer des donnÃ©es dâ€™abondance par une transformation de chord ou de Hellinger (tel que discutÃ© au chapitre 6), puis procÃ©der Ã  une ACP sur ces donnÃ©es (Legendre et Gallagher, 2001). 9.4.1.2.1 Application Le tableau varespec comprend des donnÃ©es de surface de couverture de 44 espÃ¨ces de plantes en lien avec les donnÃ©es environnementales du tableau varechem. Ces donnÃ©es ont Ã©tÃ© publiÃ©es dans VÃ¤re et al. (1995) et exportÃ©es du module vegan. data(&quot;varespec&quot;) varespec %&gt;%sample_n(5) ## Callvulg Empenigr Rhodtome Vaccmyrt Vaccviti Pinusylv Descflex Betupube Vacculig Diphcomp Dicrsp Dicrfusc Dicrpoly Hylosple ## 1 0.03 3.65 0.00 0.00 4.43 0.00 0.00 0 1.65 0.50 0.0 0.55 0.00 0.00 ## 2 0.00 1.63 0.35 18.27 7.13 0.05 0.40 0 0.20 0.00 0.3 0.52 0.20 9.97 ## 3 3.75 5.65 0.00 0.08 5.30 0.10 0.00 0 0.00 0.00 0.0 11.32 0.00 0.00 ## 4 3.40 0.63 0.00 0.00 1.98 0.05 0.05 0 0.03 0.00 0.0 0.20 0.00 0.00 ## 5 0.00 5.30 0.00 0.00 8.20 0.00 0.05 0 8.10 0.28 0.0 0.45 0.03 0.00 ## Pleuschr Polypili Polyjuni Polycomm Pohlnuta Ptilcili Barbhatc Cladarbu Cladrang Cladstel Cladunci Cladcocc Cladcorn ## 1 0.05 0 0.00 0.00 0.03 0.03 0 8.80 29.50 55.60 0.25 0.08 0.25 ## 2 70.03 0 0.08 0.00 0.07 0.03 0 0.17 0.87 0.00 0.05 0.02 0.03 ## 3 7.75 0 0.30 0.02 0.07 0.00 0 17.45 1.32 0.12 23.68 0.22 0.50 ## 4 1.53 0 0.10 0.00 0.05 0.00 0 15.73 20.03 28.20 0.73 0.10 0.15 ## 5 0.10 0 0.25 0.00 0.03 0.00 0 35.00 42.50 0.28 0.35 0.08 0.20 ## Cladgrac Cladfimb Cladcris Cladchlo Cladbotr Cladamau Cladsp Cetreric Cetrisla Flavniva Nepharct Stersp Peltapht Icmaeric ## 1 0.25 0.15 0.10 0.03 0.00 0.03 0.00 0.05 0.00 0.15 0.15 0.28 0 0.00 ## 2 0.07 0.10 0.02 0.00 0.02 0.00 0.00 0.00 0.02 0.00 0.00 0.02 0 0.00 ## 3 0.15 0.23 0.97 0.00 0.00 0.00 0.00 0.68 0.02 0.00 0.00 0.33 0 0.02 ## 4 0.13 0.10 0.15 0.00 0.00 0.00 0.05 0.28 0.05 10.03 0.00 0.95 0 0.00 ## 5 0.25 0.18 0.13 0.08 0.00 0.00 0.00 0.05 0.00 0.23 0.20 0.93 0 0.03 ## Cladcerv Claddefo Cladphyl ## 1 0.00 0.08 0.00 ## 2 0.00 0.08 0.00 ## 3 0.00 1.57 0.05 ## 4 0.05 0.08 0.00 ## 5 0.00 0.10 0.00 Pour effectuer lâ€™AC, nous utiliserons, comme pour lâ€™ACP, le module vegan mais cette fois-ci avec la fonction cca. Lâ€™AC en scaling 1 est effectuÃ©e sur le tableau des abondances avec les espÃ¨ces comme colonnes et les sites comme lignes. Les matrices dâ€™abondance transposÃ©es indique les sites oÃ¹ chque espÃ¨ce ont Ã©tÃ© dÃ©nombrÃ©es: pour une analyse en scaling 2, on effectue une analyse de correspondance sur la matrice dâ€™abondance (ou dâ€™occurence) transposÃ©e. Pour chacune des AC, je filtre pour mâ€™assurer que toutes les lignes contiennent au moins une observation. Ce nâ€™est pas nÃ©cessaire dans notre cas, mais je le laisse pour lâ€™exemple. vare_cca &lt;- cca(varespec %&gt;% filter(rowSums(.) &gt; 0)) summary(vare_cca, scaling = 1) ## ## Call: ## cca(X = varespec %&gt;% filter(rowSums(.) &gt; 0)) ## ## Partitioning of scaled Chi-square: ## Inertia Proportion ## Total 2.083 1 ## Unconstrained 2.083 1 ## ## Eigenvalues, and their contribution to the scaled Chi-square ## ## Importance of components: ## CA1 CA2 CA3 CA4 CA5 CA6 CA7 CA8 CA9 CA10 CA11 CA12 CA13 ## Eigenvalue 0.5249 0.3568 0.2344 0.19546 0.17762 0.12156 0.11549 0.08894 0.07318 0.05752 0.04434 0.02546 0.01710 ## Proportion Explained 0.2520 0.1713 0.1125 0.09383 0.08526 0.05835 0.05544 0.04269 0.03513 0.02761 0.02129 0.01222 0.00821 ## Cumulative Proportion 0.2520 0.4233 0.5358 0.62962 0.71489 0.77324 0.82868 0.87137 0.90650 0.93411 0.95539 0.96762 0.97583 ## CA14 CA15 CA16 CA17 CA18 CA19 CA20 CA21 CA22 CA23 ## Eigenvalue 0.014896 0.010160 0.007830 0.006032 0.004008 0.002865 0.0019275 0.0018074 0.0005864 0.0002434 ## Proportion Explained 0.007151 0.004877 0.003759 0.002896 0.001924 0.001375 0.0009253 0.0008676 0.0002815 0.0001168 ## Cumulative Proportion 0.982978 0.987855 0.991614 0.994510 0.996434 0.997809 0.9987341 0.9996017 0.9998832 1.0000000 ## ## Scaling 1 for species and site scores ## * Sites are scaled proportional to eigenvalues ## * Species are unscaled: weighted dispersion equal on all dimensions ## ## ## Species scores ## ## CA1 CA2 CA3 CA4 CA5 CA6 ## Callvulg 0.0303167 -1.597460 0.11455 -2.894569 0.1376073 2.291129 ## Empenigr 0.0751030 0.379305 0.39303 0.023675 0.8568729 -0.400964 ## Rhodtome 1.1052309 1.499299 3.04284 0.120106 3.2324306 -0.283510 ## Vaccmyrt 1.4614812 1.622935 2.72375 0.231688 0.4604556 0.712538 ## Vaccviti 0.1468014 0.313436 0.14696 0.243505 0.6868371 -0.147815 ## Pinusylv -0.4820096 0.588517 -0.36020 -0.127094 0.4064754 0.386604 ## Descflex 1.5348239 1.218806 1.87562 -0.001340 -1.3136979 -0.070731 ## Betupube 0.6694503 1.951826 3.84017 1.389423 7.5959115 -0.244478 ## Vacculig -0.0830789 -1.629259 1.05063 0.802648 -0.3058811 -1.625341 ## Diphcomp -0.5446464 -1.037570 0.52282 0.940275 0.3682126 -1.082929 ## Dicrsp 1.8120408 0.360290 -4.92082 3.088562 1.3867372 0.157815 ## Dicrfusc 1.2704743 -0.562978 -0.39718 -2.929542 0.3848272 -2.408710 ## Dicrpoly 0.7248118 1.409347 0.80341 1.915549 4.5674148 1.295447 ## Hylosple 2.0062408 1.743883 2.27549 0.928884 -3.7648428 2.254851 ## Pleuschr 1.3102086 0.583036 -0.01004 0.137298 -1.1216144 0.200422 ## Polypili -0.3805097 -1.243904 0.54593 1.477188 -0.7276341 -0.387641 ## Polyjuni 1.0133795 0.099043 -2.24697 1.510641 0.7729714 -3.062378 ## Polycomm 0.8468241 1.321773 1.13585 1.140723 2.6836594 -0.605038 ## Pohlnuta -0.0136453 0.589290 -0.35542 0.135481 0.9369707 0.397246 ## Ptilcili 0.4223631 1.598584 3.43474 1.400065 6.3209491 0.198935 ## Barbhatc 0.5018348 2.119334 4.57303 1.693188 8.1101807 0.645995 ## Cladarbu -0.1531729 -1.483884 0.20024 0.193680 0.0734141 0.358926 ## Cladrang -0.5502561 -1.084008 0.40552 0.724060 -0.3357992 -0.335924 ## Cladstel -1.4373146 1.077753 -0.44397 -0.375926 -0.2421525 0.004212 ## Cladunci 0.8151727 -1.006186 -1.82587 -1.389523 1.6046713 3.675908 ## Cladcocc -0.2133215 -0.584429 -0.21434 -0.567886 -0.0003788 -0.145303 ## Cladcorn 0.2631227 -0.177858 -0.44464 0.272422 0.3992282 -0.306738 ## Cladgrac 0.1956947 -0.311167 -0.23894 0.379013 0.4933026 0.037581 ## Cladfimb 0.0009213 -0.161418 0.18463 -0.435908 0.4831233 -0.143751 ## Cladcris 0.3373031 -0.470369 -0.05093 -0.823855 0.7182250 0.636140 ## Cladchlo -0.6200021 1.207278 0.21889 0.426447 1.9506082 0.120722 ## Cladbotr 0.5647242 1.047333 2.65330 0.907734 4.4946805 1.201655 ## Cladamau -0.6598144 -1.512880 0.83251 1.577699 -0.0407227 -1.419139 ## Cladsp -0.8209003 0.476164 -0.49752 -0.998241 -0.2393208 0.390785 ## Cetreric 0.2458192 -0.689228 -1.68427 -0.131681 0.7439412 2.374535 ## Cetrisla -0.3465221 1.362693 0.85897 0.396752 2.7526968 0.396591 ## Flavniva -1.4391907 -0.833589 -0.12919 0.007071 -1.4841375 2.956977 ## Nepharct 1.6813309 0.199484 -4.33509 2.229917 0.9561223 -5.472858 ## Stersp -0.5172793 -2.280900 0.99775 2.377013 -0.8892757 -1.441228 ## Peltapht 0.4035858 -0.043265 0.04538 0.711040 0.1824679 -0.841227 ## Icmaeric 0.0378754 -2.419595 0.72135 0.361302 -0.3736424 -2.092136 ## Cladcerv -0.9232858 -0.005233 -1.22058 0.305290 -0.8142627 0.414135 ## Claddefo 0.5190399 -0.496632 -0.15271 -0.695927 0.9042143 0.909191 ## Cladphyl -1.2836161 1.155872 -0.79912 -0.741170 -0.1608002 0.490526 ## ## ## Site scores (weighted averages of species scores) ## ## CA1 CA2 CA3 CA4 CA5 CA6 ## sit1 -0.108122 -0.53705 0.229574 0.24412 0.1405624 -0.14253 ## sit2 0.697118 -0.14441 -0.031788 -0.21743 -0.2738522 -0.08146 ## sit3 0.987603 0.15042 -1.348447 0.80472 0.3095168 0.46773 ## sit4 0.851765 0.49901 0.443559 0.12277 -0.4814871 0.07589 ## sit5 0.359881 -0.05608 0.145813 0.15087 0.2405263 -0.17770 ## sit6 0.003545 0.37017 0.027760 0.06168 -0.1158930 -0.03413 ## sit7 0.860732 -0.11504 0.110869 -1.02169 0.0772348 -0.60530 ## sit8 0.636936 -0.33250 0.001120 -0.79797 0.0130769 -0.54049 ## sit9 1.279352 0.81557 0.670053 0.23137 -0.8929976 0.41783 ## sit10 -0.195009 -0.80564 0.117686 -0.58286 -0.0007212 0.53071 ## sit11 0.528532 -0.70420 -0.517771 -0.86836 0.5713441 0.91671 ## sit12 0.382866 -0.18686 -0.004789 0.10156 0.0458125 0.21087 ## sit13 0.990715 0.11967 -1.110040 0.44929 0.1885902 -0.70694 ## sit14 -0.264704 -1.06013 0.334900 0.45973 -0.0326631 -0.19945 ## sit15 -0.428410 -1.20765 0.374344 0.74970 -0.2596294 -0.30467 ## sit16 -0.330534 -0.77498 0.130760 0.22391 0.0632686 0.09060 ## sit17 -0.899601 0.12075 -0.075742 0.03842 -0.1489585 -0.12031 ## sit18 -0.770294 -0.35351 -0.033779 -0.01795 -0.3007839 0.44303 ## sit19 -0.992193 0.50319 -0.157505 -0.07070 -0.1065172 -0.09928 ## sit20 -0.937173 0.78688 -0.258119 -0.19377 -0.0343535 -0.01259 ## sit21 -0.726413 0.49163 -0.157235 -0.08698 -0.0105774 -0.02801 ## sit22 -1.002083 0.71239 -0.236526 -0.18643 -0.0231666 -0.04928 ## sit23 -0.322647 -0.03871 -0.001297 0.09029 -0.1481448 0.06934 ## sit24 0.259527 0.80746 1.124258 0.36083 1.5437866 0.07051 varespec_eigenval &lt;- eigenvals(vare_cca, scaling = 1) prop_expl &lt;- varespec_eigenval / sum(varespec_eigenval) par(mfrow = c(1, 2)) plot(x = 1:length(varespec_eigenval), y = vare_cca$CA$eig, type = &quot;b&quot;, xlab = &quot;Rang de la valeur propre&quot;, ylab = &quot;Valeur propre&quot;) abline(h = mean(varespec_eigenval), col = &quot;red&quot;, lty = 2) plot(x = 1:length(varespec_eigenval), y = prop_expl, type = &quot;b&quot;, xlab = &quot;Rang de la valeur propre&quot;, ylab = &quot;Valeur propre&quot;) lines(x = 1:length(varespec_eigenval), y = broken_stick(length(varespec_eigenval)), col = &quot;red&quot;, lty = 2) CrÃ©ons les biplots. par(mfrow = c(1, 2)) plot(vare_cca, scaling = 1, main = &quot;Biplot des espÃ¨ces&quot;) plot(vare_cca, scaling = 2, main = &quot;Biplot des sites&quot;) Le biplot des espÃ¨ces, Ã  gauche (scaling = 1), montre la distribution des sites selon les espÃ¨ces. Les emplacements des scores (en noir) montrent les contrastes entre sites selon les espÃ¨ces qui les recouvrent. Les sites 14 et 15, par exemple, contrastent les sites 19, 20, 21 et 22 selon le 2iÃ¨me axe principal. Par ailleurs, les axes principaux sont formÃ© de plusieurs espÃ¨ces dont aucune ne domine clairement. Le biplot des sites, Ã  droite (scaling = 2), montre la distribution des recouvrements dâ€™espÃ¨ces selon les sites. Par exemple, les espÃ¨ces Betupube (Betula pubescens) et Barbhatc (Barbilophozia hatcheri ) se recouvrent en particulier le site 24. Le site 1 est difficile Ã  identifier, car il est couvert par plusieurs noms dâ€™espÃ¨ces, au bas au centre. Les sites 3 et 13 se confondent avec Dicrsp (une espÃ¨ce de Dicranum) qui le recouvre amplement. Pour les deux types de biplot, les sites oÃ¹ les espÃ¨ces situÃ©s prÃ¨s de lâ€™origine, car ils peuvent Ãªtre soit prÃ¨s de la moyenne, soit distribuÃ©s uniformÃ©ment. Le nombre de composantes Ã  retenir peut Ãªtre Ã©valuÃ© par les approches Kaiser-Guttmann et broken-stick. scaling &lt;- 1 varespec_eigenval &lt;- eigenvals(vare_cca, scaling = scaling) # peut Ãªtre effectuÃ© sur les deux types de scaling prop_expl &lt;- varespec_eigenval / sum(varespec_eigenval) par(mfrow = c(1, 2)) plot(x = 1:length(varespec_eigenval), y = vare_cca$CA$eig, type = &quot;b&quot;, xlab = &quot;Rang de la valeur propre&quot;, ylab = &quot;Valeur propre&quot;, main = paste(&quot;Eigenvalue - Kaiser-Guttmann, scaling =&quot;, scaling)) abline(h = mean(varespec_eigenval), col = &quot;red&quot;, lty = 2) plot(x = 1:length(varespec_eigenval), y = prop_expl, type = &quot;b&quot;, xlab = &quot;Rang de la valeur propre&quot;, ylab = &quot;Valeur propre&quot;, main = paste(&quot;Proportion - broken stick, scaling =&quot;, scaling)) lines(x = 1:length(varespec_eigenval), y = broken_stick(length(varespec_eigenval)), col = &quot;red&quot;, lty = 2) Pour les deux scalings, lâ€™approche Kaiser-Guttmann propose 7 axes, tandis que lâ€™approche broken-stick en propose 5. Les reprÃ©sentations biplot dâ€™analyse de correspondance peuvent prendre la forme dâ€™un boomerang, en particulier celles qui sont basÃ©es sur des donnÃ©es dâ€™occurence. Le tableau suivant initialement de Chessel et al. (1987) et est distribuÃ© dans le module ade4. library(&quot;ade4&quot;) data(&quot;doubs&quot;) fish &lt;- doubs$fish doubs_cca &lt;- cca(fish %&gt;% filter(rowSums(.) &gt; 0)) plot(doubs_cca, scaling = 2) Les numÃ©ros de sites correspondent Ã  la position dans une riviÃ¨re, 1 Ã©tant en amont et 30 en aval. Le premier axe discrimine lâ€™amont et lâ€™aval, tandis que le deuxiÃ¨me montre deux niches en amont. Bien que lâ€™on observe une discontinuitÃ© dans le cours dâ€™eau, il y a une continuitÃ© dans les abondances. Cet effet peut Ãªtre corrigÃ© en retirant la tendance de lâ€™analyse de correspondance par une detrended correspondance analysis. Pour cela, il faudra utiliser la fonction decorana, ce qui ne sera pas couvert ici. Lâ€™analyse des correspondances multiples (ACM) est utile pour lâ€™ordination des donnÃ©es catÃ©gorielles. Le module ade4 est en mesure dâ€™effectuer des AMC, mais nâ€™est pas couvert dans ce manuel. Excercice. Effectuez et analysez une AC avec les donnÃ©es de recouvrement varespec. 9.4.1.3 Positionnement multidimensionnel (PoMd) Le positionnement multidimensionnel (PoMd), ou manifold analysis, se base sur les assiciations entre les objets (mode Q) ou les variables (mode R) pour en rÃ©duire les dimensions. Alors que lâ€™analyse en composantes principales conserve la distance euclidienne et que lâ€™analyse de correspondance conserve la distance du \\(\\chi^2\\), le PoMd conserve lâ€™association que vous sÃ©lectionnerez Ã  votre convenance. Le PoMd vise Ã  reprÃ©senter en un nombre limitÃ© de dimensions (souvent 2) la distance (ou dissimilaritÃ©) quâ€™ont les objets (ou des variables) les uns par rapport aux autres dans lâ€™espace multidimensionnel. Il existe deux types dâ€™AEM. Le PoMd-mÃ©trique (metric multidimentional scaling MMDS, parfois le metric est retirÃ©, MDS, et parfois lâ€™on parle de classic MDS) vise Ã  reprÃ©senter fidÃ¨lement la distance entre les objets ou les variables. Le PoMd-mÃ©trique ne devrait Ãªtre utilisÃ©e que lorsque la mÃ©trique nâ€™est ni euclidienne, ni de \\(\\chi^2\\) et que lâ€™on dÃ©sire prÃ©server les distances entre les objets. Lâ€™PoMd-mÃ©trique aussi appelÃ©e analyse en coordonnÃ©es principales (ACoP ou de lâ€™anglais PCoA) . Le PoMd-non-mÃ©trique (nonmetric multidimentional scaling, NMDS) vise quant Ã  lui Ã  reprÃ©senter lâ€™ordre des distances entre les objets ou les variables. Câ€™est une approche par rang: le PoMd-non-mÃ©trique vise reprÃ©senter les objets sont plus proches ou plus Ã©loignÃ©es les uns des autres plutÃ´t que de reprÃ©senter leur similaritÃ© dans lâ€™espace multidimentionnelle. Lâ€™IsoMap, pour isometric feature mapping, est une extension du PoMd qui recontruit les distances selon les points retrouvÃ©s dans le voisinage. Les isomaps sont en mesure dâ€™applatir des donnÃ©es ayant des formes complexes. Nous ne traitons pour lâ€™instant que de lâ€™PoMd-mÃ©trique (fonction vegan::cmdscale) et des PoMd-non-mÃ©trique (fonction vegan::metaMDS). 9.4.1.3.1 Application Utilisons les donnÃ©es dâ€™abondance que nous avions au tout dÃ©but de ce chapitre. La matrice dâ€™association de Bray-Curtis sera utilisÃ©e. assoc_mat &lt;- vegdist(abundance, method = &quot;bray&quot;) pheatmap(assoc_mat %&gt;% as.matrix(), cluster_rows = FALSE, cluster_cols = FALSE, display_numbers = round(assoc_mat %&gt;% as.matrix(), 2)) Les sites 2 et 3 devraient Ãªtre plus prÃ¨s lâ€™un et lâ€™autre, puis les sites 3 et 4. Les autres associations sont Ã©loignÃ©s dâ€™environ la mÃªme distance. LanÃ§ons le calcul de la PoMd-mÃ©trique. pcoa &lt;- cmdscale(assoc_mat, k = nrow(abundance)-1, eig = TRUE) spec_scores &lt;- wascores(pcoa$points, abundance) ordiplot(vegan::scores(pcoa), type = &#39;t&#39;, cex = 1.5) ## species scores not available text(spec_scores, row.names(spec_scores), col = &quot;red&quot;, cex = 0.75) On observe en effet que les sites 2 et 3 sont les plus prÃ¨s. Les sites 3 et 4sont plus Ã©loignÃ©s. Les sites 1, 2 et 4 font Ã  peu prÃ¨s un triangle Ã©quilatÃ©ral, ce qui correspond Ã  ce Ã  quoi on devrait sâ€™attendre. Les wa-scores permettent de juxtaposer les espÃ¨ces sur les sites, pour rÃ©fÃ©rence. Le colibri nâ€™est prÃ©sent que sur le site 2. Le site 1 est populÃ© par des jaseurs et des mÃ©sanges, et câ€™est le seul site oÃ¹ lâ€™on a observÃ© une citelle. On a observÃ© des chardonnerets sur les sites 2 et 3. Sur le site 4, on nâ€™a observÃ© que des bruants, que lâ€™on a aussi observÃ© ailleurs, sauf au site 2. Le PoMd-non-mÃ©trique (non metric dimensional scaling, NMDS) fonctionne de la mÃªme maniÃ¨re que la PoMd-mÃ©trique, Ã  la diffÃ©rence que la distance est basÃ©e sur les rangs. Ã€ cet Ã©gard, le site 4 Ã  une distance de 0.76 du site 3, mais plutÃ´t le deuxiÃ¨me plus loin, aprÃ¨s le site 2 et avant le site 1. Utilisons la fonction metaMDS. nmds &lt;- metaMDS(assoc_mat, k = nrow(abundance)-1, eig = TRUE) ## Run 0 stress 0 ## Run 1 stress 0 ## ... Procrustes: rmse 0.1712075 max resid 0.2311738 ## Run 2 stress 0 ## ... Procrustes: rmse 0.1038622 max resid 0.1447403 ## Run 3 stress 0 ## ... Procrustes: rmse 0.1355982 max resid 0.1807723 ## Run 4 stress 0 ## ... Procrustes: rmse 0.1149604 max resid 0.1456473 ## Run 5 stress 0 ## ... Procrustes: rmse 0.08167542 max resid 0.1146596 ## Run 6 stress 0 ## ... Procrustes: rmse 0.1284115 max resid 0.1569041 ## Run 7 stress 0 ## ... Procrustes: rmse 0.1834823 max resid 0.2366359 ## Run 8 stress 0 ## ... Procrustes: rmse 0.1367236 max resid 0.1848335 ## Run 9 stress 0 ## ... Procrustes: rmse 0.09492646 max resid 0.1233517 ## Run 10 stress 0 ## ... Procrustes: rmse 0.12845 max resid 0.1738103 ## Run 11 stress 8.215235e-05 ## ... Procrustes: rmse 0.07684147 max resid 0.1148002 ## Run 12 stress 0 ## ... Procrustes: rmse 0.06054277 max resid 0.07388255 ## Run 13 stress 0 ## ... Procrustes: rmse 0.1239983 max resid 0.1544609 ## Run 14 stress 0 ## ... Procrustes: rmse 0.09964013 max resid 0.1300011 ## Run 15 stress 0 ## ... Procrustes: rmse 0.06571301 max resid 0.08654521 ## Run 16 stress 0 ## ... Procrustes: rmse 0.1092868 max resid 0.1707372 ## Run 17 stress 0 ## ... Procrustes: rmse 0.1520543 max resid 0.202858 ## Run 18 stress 0 ## ... Procrustes: rmse 0.158808 max resid 0.2101473 ## Run 19 stress 0 ## ... Procrustes: rmse 0.1124983 max resid 0.1450684 ## Run 20 stress 0 ## ... Procrustes: rmse 0.09344561 max resid 0.1301929 ## *** No convergence -- monoMDS stopping criteria: ## 20: stress &lt; smin ## Warning in metaMDS(assoc_mat, k = nrow(abundance) - 1, eig = TRUE): stress is (nearly) zero: you may have insufficient data spec_scores &lt;- wascores(nmds$points, abundance) ordiplot(vegan::scores(nmds), type = &#39;t&#39;, cex = 1.5) ## species scores not available text(spec_scores, row.names(spec_scores), col = &quot;red&quot;, cex = 0.75) Dans ce cas, entre PoMd-mÃ©trique et non-mÃ©trique, les rÃ©sultats peuvent Ãªtre interprÃ©tÃ©s de maniÃ¨re similaire. En ce qui a trait au dauphin, Pour plus de dÃ©tails, je vous invite Ã  vous rÃ©fÃ©rer Ã  Borcard et al. (2011)) ou de consulter lâ€™excellent site GUSTA ME. 9.4.1.4 Conclusion sur lâ€™ordination non contraignante Lorsque les donnÃ©es sont euclidiennes, lâ€™analyse en composantes principales (ACP) dervait Ãªtre utilisÃ©e. Lorsque la mÃ©trique est celle du \\(\\chi^2\\), on prÃ©fÃ©rera lâ€™analyse de correspondance (AC). Si la mÃ©trique est autre, le positionnement multidimensionel (PoMd) est prÃ©fÃ©rable. Dans ce dernier cas, si lâ€™on recherche une reprÃ©sentation simplifiÃ©e de la distance entre les objets ou variables, on utilisera un PoMd-mÃ©trique. Ã€ lâ€™inverse, si lâ€™on dÃ©sire une reprÃ©sentation plus fidÃ¨le au rang des distances, on prÃ©fÃ©rera lâ€™PoMd-non-mÃ©trique. 9.4.2 Ordination contraignante Alors que lâ€™ordination non contraignante vous permet de dresser un protrait de vos variables, lâ€™ordination contraignante (ou canonique) permet de tester statistiquement ainsi que de reprÃ©senter la relation entre plusieurs variables explicatives (par exemple, des conditions environnementales) et une ou plusieurs variables rÃ©ponses (par exemple, les espÃ¨ces observÃ©es). Lâ€™analyse discriminante nâ€™a fondamentalement quâ€™une seulement variable rÃ©ponse, et celle-ci doit dÃ©crire lâ€™appartenance Ã  une catÃ©gorie. Lâ€™analyse de redondance sera prÃ©fÃ©rÃ©e lorsque le nombre de variable est plus restreint (variables ionomiques et indicateurs de performance des cultures). Les dÃ©tails, ainsi que les tenants et aboutissants de ces mÃ©thodes, sont prÃ©sentÃ©s dans Numerical Ecology (Legendre et Legendre, 2012). Lâ€™analyse canonique des corrÃ©lations sera prÃ©fÃ©rÃ©e lorsque les variables sont parsemÃ©es (beaucoup de colonnes avec beaucoup de zÃ©ros, comme les variables dâ€™abondance). 9.4.2.1 Analyse discriminante Alors que lâ€™analyse en composante principale vise Ã  prÃ©senter la perspective (les axes) selon laquelle les points sont les plus Ã©clatÃ©es, lâ€™analyse discriminante, le plus souvent utilisÃ© dans sa forme linÃ©aire (ADL) et quadratique (ADQ), vise Ã  prÃ©senter la perspective selon laquelle les groupes sont les plus Ã©clatÃ©s, les groupes formant la variable contraignante. Ces groupes peuvent Ãªtre connus (e.g.Â cultivar, rÃ©gion gÃ©ographique) ou attribuÃ©s (exemple: par partitionnement). Lâ€™ADL est parfois nommÃ©e analyse canonique de la variance. Lâ€™AD vise Ã  reprÃ©senter des diffÃ©rences entre des groupes aux moyens de combinaisons linÃ©aires (ADL) ou quadratique (ADQ) de variables mesurÃ©es. Sa reprÃ©sentation sous forme de biplot permet dâ€™apprÃ©cier les diffÃ©rences entre les groupes dâ€™identifier les variables qui sont responsables de la discrimination. Biplot de distance de lâ€™analyse discriminante des ionomes dâ€™espÃ¨ces de plantes Ã  fruits cultivÃ©es sauvages et domestiquÃ©es, Source: Parent et al. (2013) Lâ€™ADL a Ã©tÃ© dÃ©veloppÃ©e par Fisher (1936), qui Ã  titre dâ€™exemple dâ€™application a utilisÃ© un jeu de donnÃ©es de dimensions dâ€™iris collectÃ©es par Edgar Anderson, du Jardin botanique du Missouri, sur 150 spÃ©cimens dâ€™iris collectÃ©s en GaspÃ©sie (Est du QuÃ©bec), ma rÃ©gion natale (suis-je assez chauvin?). Ce jeu de donnÃ©es est amplement utilisÃ© Ã  titre dâ€™exemple en analyse multivariÃ©e. Williams (1983) a prÃ©sentÃ© les tenants et aboutissants de lâ€™ADL en Ã©cologie. Tout comme les donnÃ©es passant pas une ACP doivent suivre une distribution multinormale pour Ãªtre statistiquement valide, les distributions des groupes dans une ADL doivent Ãªtre multinormales et les variances des points par groupe doivent Ãªtre homogÃ¨nesâ€¦ ce qui est rarement le cas en science. NÃ©anmoins: Heureusement, il y a des Ã©vidences dans la littÃ©rature que certaines dâ€™entre [ces rÃ¨gles] peuvent Ãªtre transgressÃ©es modÃ©rÃ©ment sans de grands changement dans les taux de classification. Cette conclusion dÃ©pends, toutefois, de la sÃ©vÃ©ritÃ© des transgressions, et de facteurs structueaux comme la position relative des moyennes des populations et de la nature des dispersions. - Williams (1983) Lâ€™ADL peut servir autant dâ€™outil dâ€™interprÃ©tation que dâ€™outil de classification, câ€™est Ã  dire de prÃ©dire une catÃ©gorie selon les variables (chapitre 12). Dans les deux cas, lorsque le nombre de variables approchent le nombre dâ€™observation, les rÃ©sultats dâ€™une ADL risque dâ€™Ãªtre difficilement interprÃ©tables. Le test appropriÃ© pour Ã©valuer lâ€™homodÃ©nÃ©itÃ© de la covariance est le M-test de Box. Ce test est peu documentÃ© dans la littÃ©rature, est rarement utilisÃ© mais a la rÃ©putation dâ€™Ãªtre particuliÃ¨rement sÃ©vÃ¨re. Il est rare que des donnÃ©es Ã©cologiques aient des dispersions (covariances) homogÃ¨nes. Contrairement Ã  lâ€™ADL, lâ€™ADQ ne demande pas Ã  ce que les dispersions (covariances) soient homogÃ¨nes. NÃ©anmoins, lâ€™ADQ ne gÃ©nÃ¨re ni de scores, ni de loadings: il sâ€™agit dâ€™un outil pour prÃ©dire des catÃ©gories (classification), non pas dâ€™un outil dâ€™ordination. 9.4.2.1.1 Application Utilisons les donnÃ©es dâ€™iris. data(&quot;iris&quot;) Testons la multinormalitÃ© par groupe. Rappelons-nous que pour considÃ©rer la distribution comme multinormale, la p-value de la distortion ainsi que la statistique de Kurtosis doivent Ãªtre Ã©gale ou plus Ã©levÃ©e que 0.05. La fonction split sÃ©pare le tableau en listes et la fonction map applique la fonction spÃ©cifiÃ©e Ã  chaque Ã©lÃ©ment de la liste. Cela permet dâ€™effectuer des tests de multinormalitÃ© sur chacune des espÃ¨ces dâ€™iris. iris %&gt;% split(.$Species) %&gt;% map(~ mvn(.x %&gt;% select(-Species), mvnTest = &quot;mardia&quot;)$multivariateNormality) ## $setosa ## Test Statistic p value Result ## 1 Mardia Skewness 25.6643445196298 0.177185884467652 YES ## 2 Mardia Kurtosis 1.29499223711605 0.195322907441935 YES ## 3 MVN &lt;NA&gt; &lt;NA&gt; YES ## ## $versicolor ## Test Statistic p value Result ## 1 Mardia Skewness 25.1850115362466 0.194444483140265 YES ## 2 Mardia Kurtosis -0.57186635893429 0.567412516528727 YES ## 3 MVN &lt;NA&gt; &lt;NA&gt; YES ## ## $virginica ## Test Statistic p value Result ## 1 Mardia Skewness 26.2705981752915 0.157059707690356 YES ## 2 Mardia Kurtosis 0.152614173978342 0.878702546726567 YES ## 3 MVN &lt;NA&gt; &lt;NA&gt; YES Le test est passÃ© pour toutes les espÃ¨ces. Voyons maintenant lâ€™homogÃ©nÃ©itÃ© de la covariance. Pour ce faire, nous aurons besoin de la fonction boxM, disponible avec le module biotools. Pour que les covariances soient considÃ©rÃ©es comme Ã©gales, la p-vaule doit Ãªtre supÃ©rieure Ã  0.05. library(&quot;heplots&quot;) ## ## Attaching package: &#39;heplots&#39; ## The following object is masked from &#39;package:pls&#39;: ## ## coefplot boxM(iris %&gt;% select(-Species), group = iris$Species) ## ## Box&#39;s M-test for Homogeneity of Covariance Matrices ## ## data: iris %&gt;% select(-Species) ## Chi-Sq (approx.) = 140.94, df = 20, p-value &lt; 2.2e-16 On est loin dâ€™un cas oÃ¹ les distributions sont homogÃ¨nes. Nous allons nÃ©anmoins procÃ©der Ã  lâ€™analyse discriminante avec le module ade4. Nous aurons dâ€™abord besoin dâ€™effectuer une ACP avec la fonction dudi.pca de ade4 (en spÃ©cifiant une mise Ã  lâ€™Ã©chelle), que nous projeterons en ADL avec discrimin. library(&quot;ade4&quot;) iris_pca &lt;- dudi.pca(df = iris %&gt;% select(-Species), scannf = FALSE, # ne pas gÃ©nÃ©rer de graphique scale = TRUE) iris_lda &lt;- discrimin(dudi = iris_pca, fac = iris$Species, scannf = FALSE) La visualisation peut Ãªtre effectuÃ©e directement sur lâ€™objet issu de la fonction discrimin. plot(iris_lda) Il sâ€™agit toutefois dâ€™une visualisation pour le diagnostic davantage que pour la publication. Si lâ€™objectif est la pubilcation, vous pourriez utiliser la fonction plotDA que jâ€™ai conÃ§ue Ã  cet effet. Jâ€™ai aussi conÃ§u une fonction similaire qui utilise le module graphique de base de R. source(&quot;https://raw.githubusercontent.com/essicolo/AgFun/master/plotDA_gg.R&quot;) plotDA(scores = iris_lda$li, loadings = iris_lda$fa, fac = iris$Species, level=0.95, facname = &quot;Species&quot;, propLoadings = 1) ## Loading required package: ellipse ## ## Attaching package: &#39;ellipse&#39; ## The following object is masked from &#39;package:car&#39;: ## ## ellipse ## The following object is masked from &#39;package:graphics&#39;: ## ## pairs ## Loading required package: grid ## Loading required package: plyr ## ---------------------------------------------------------------------------------------------------------------------------- ## You have loaded plyr after dplyr - this is likely to cause problems. ## If you need functions from both plyr and dplyr, please load plyr first, then dplyr: ## library(plyr); library(dplyr) ## ---------------------------------------------------------------------------------------------------------------------------- ## ## Attaching package: &#39;plyr&#39; ## The following objects are masked from &#39;package:plotly&#39;: ## ## arrange, mutate, rename, summarise ## The following objects are masked from &#39;package:dplyr&#39;: ## ## arrange, count, desc, failwith, id, mutate, rename, summarise, summarize ## The following object is masked from &#39;package:purrr&#39;: ## ## compact Ã€ la diffÃ©rence de lâ€™ACP, lâ€™ADL maximise la sÃ©patation des groupes. Nous avions notÃ© avec lâ€™ACP que les dimensions des pÃ©tales distingaient les groupes. Puisque nous avions justement des informations sur les groupes, nous aurions pu procÃ©der directement Ã  un ADL pour obtenir des conclusions plus directes. Si la longueur des pÃ©tales permet de distinguer lâ€™espÃ¨ce setosa des deux autres, la largeur des pÃ©tales permet de distinguer virginica et versicolor, bien que les nuages de points se superposent. De maniÃ¨re bivariÃ©e, les rÃ©gions de confiance des moyennes des scores discriminants (petites ellipses) montrent des diffÃ©rence significatives au seuil 0.05. Excercice. Si lâ€™on effectuait lâ€™ADL sur notre dauphin, avec la colonne anatomy comme variable de regroupement, quâ€™obtiendrions-nous? Si lâ€™on consiÃ¨re la nageoire codale (queue) comme faisant partie du corps? Quelles sont les limitations? 9.4.2.2 Analyse de redondance (RDA) En anglais, on la nomme redundancy analysis, souvent abrÃ©gÃ©e RDA. Elle est utilisÃ©e pour rÃ©sumer les relations linÃ©aires entre des variables rÃ©ponse et des variables explicatives. La â€œredondanceâ€ se situe dans lâ€™utilisation de deux tableaux de donnÃ©es contenant de lâ€™information concordante. Lâ€™analyse de redondance est une maniÃ¨re Ã©lÃ©gante dâ€™effectuer une rÃ©gresssion linÃ©aire multiple, oÃ¹ la matrice de valeurs prÃ©dites par la rÃ©gression est assujettie Ã  une analyse en composantes principales. Il est ainsi possible de superposer les scores des variables explicatives Ã  ceux des variables rÃ©ponse. Plus prÃ©cisÃ©ment, une RDA effectue les Ã©tapes suivantes (Borcard et al. (2011)) entre une matrice de variables indÃ©pendantes (explicatives) \\(X\\) et une matrice de variables dÃ©pendantes (rÃ©ponse) \\(Y\\). 9.4.2.2.1 1. RÃ©gression entre \\(Y\\) et \\(X\\) Pour chacune des variables rÃ©ponse de \\(Y\\) (\\(y_1\\), \\(y_2\\), , \\(y_j\\)), effectuer une rÃ©gression linÃ©aire sur les variables explicatives \\(X\\). \\[\\hat{y}_j = b_j + m_{1, j} \\times x_1 + m_{2, j} \\times x_2 + ... + m_{i, j} \\times x_i\\] \\[\\hat{y}_j = y_j + y_{res, j}\\] Pour chaque observation (\\(n\\)), nous obtenons une sÃ©rie de valeurs de \\(\\hat{y}_j\\) et de \\(y_{res, j}\\). Donc chaque cellule de la matrice \\(Y\\) a ses pendant \\(\\hat{y}\\) et \\(y_{res}\\). Nous obtenons ainsi une matrice de prÃ©diction \\(\\hat{Y}\\) et une matrice des rÃ©sidus \\(Y_{res} = Y - \\hat{Y}\\). 9.4.2.2.2 2. Analyse en composantes principales Ensuite, on effectue une analyse en composantes principales (ACP) sur la matrice des prÃ©dictions \\(\\hat{Y}\\). On obtient ainsi ses valeurs et vecteurs propres. Nommons \\(U\\) ses vecteurs propres. Les fonctions de RDA mettent souvent ces veceturs Ã  lâ€™Ã©chelle avant de les retourner Ã  lâ€™utilisateur. En ordination Ã©cologique, ces vecteurs mis Ã  lâ€™Ã©chelle sont souvent appelÃ©s les scores des espÃ¨ces, bien quâ€™il ne sâ€™agisse pas nÃ©cessairement dâ€™espÃ¨ces, mais plus gÃ©nÃ©ralement des variables de la matrice dÃ©pendante \\(Y\\). Il est aussi possible dâ€™effectuer une ACP sur \\(Y_{res}\\). 9.4.2.2.3 3. Calculer les scores Les vecteurs propres \\(U\\) sont utilisÃ©s pour calculer les scores des sites, \\(Y \\times U\\), ainsi que les contraintes de site \\(\\hat{Y} \\times U\\). 9.4.2.2.4 Application Nous allons utiliser la fonction rda du module vegan. En ce qui a trait aux donnÃ©es, utilisons les donnÃ©es varespec (matrice Y) et varechem (matrice X). La fonction rda peut fonctionner avec lâ€™interface-formule de R, oÃ¹ Ã  gauche du ~ on retrouve le Y (la matrice de la communautÃ© Ã©cologique, i.e.Â les abondances dâ€™espÃ¨ces) contre le X (l), Ã  gauche, ce qui peut Ãªtre pratique pour lâ€™analyse dâ€™intÃ©ractions. Mais pour comparer deux matrices, nous pouvons dÃ©finir X et Y. Ce qui est mÃ©langeant, câ€™est que vegan, contrairement aux conventions, dÃ©fini X comme Ã©tant la matrice rÃ©ponse et Y comme Ã©tant la matrice explicative. vare_rda &lt;- rda(X = varespec, Y = vareclr, scale = FALSE) par(mfrow = c(1, 2)) ordiplot(vare_rda, scaling = 1, type = &quot;text&quot;, main = &quot;Scaling 1: triplot de distance&quot;) ordiplot(vare_rda, scaling = 2, type = &quot;text&quot;, main = &quot;Scaling 2: triplot de corrÃ©lation&quot;) La fonction ordiplot permet de crÃ©er un triplot de base. La reprÃ©sentation des wascores est rÃ©putÃ©e plus robuste (moins susceptible dâ€™Ãªtre bruitÃ©e), mais leur interprÃ©tation porte Ã  confusion (Borcard et al. (2011)). Triplot de distance (scaling 1). Les angles entre les variables explicatives reprÃ©sentent leur corrÃ©lation (non pas les variables rÃ©ponse). Triplot de corrÃ©lation (scaling 2). Les angles entre les variables reprÃ©sentent leurs corrÃ©lation, que les variables soient rÃ©ponse ou explicative, ou entre variables rÃ©ponses et variables explicatives. Les distances entre les objets sur le triplot ne sont pas des approximation de leur distance euclidienne. Les triplots montrent que les variables ont toutes un rÃ´le important sur la dispersion des sites autours des axeds principaux. Le premier axe principal est composÃ© de maniÃ¨re plus marquÃ©e par le clr de lâ€™Al et celui du Fe. Le deuxiÃ¨me axe principal est composÃ© de maniÃ¨re plus marquÃ©e par le clr du S, du P et du K. Le triplot de corrÃ©lation ne prÃ©sente pas de tendance apprÃ©ciable pour la plupart des espÃ¨ces, qui ne possÃ¨dent pas de niche particuliÃ¨re. Toutefois, lâ€™espÃ¨ce Cladstel, prÃ©sente surtout dans les sites 9 et 10, est liÃ©e Ã  de basses teneurs en N et Ã  de faibles valeurs de Baresoil (sol nu). Lâ€™espÃ¨ce Pleuschr est liÃ©e Ã  des sols oÃ¹ lâ€™on retrouve une grande Ã©paisseur dâ€™humus, ainsi que des teneurs Ã©levÃ©es en nutriment K, P, S, Ca, Mg et Zn. Elle semble apprÃ©cier les sols Ã  bas pH, mais Ã  faible teneur en Fe et Al. La teneur en N lui semble plus indiffÃ©rente (son vecteur Ã©tant presque perpendiculaire). On pourra personnaliser les graphiques en extrayant les scores. scaling &lt;- 2 sites &lt;- vegan::scores(vare_rda, display = &quot;wa&quot;, scaling = scaling) species &lt;- vegan::scores(vare_rda, display = &quot;species&quot;, scaling = scaling) env &lt;- vegan::scores(vare_rda, display = &quot;reg&quot;, scaling = scaling) plot(0, 0, type = &quot;n&quot;, xlim = c(-3, 5), ylim = c(-3, 4), asp = 1) abline(h=0, v = 0, col = &quot;grey80&quot;) text(sites/2, labels = rownames(sites), cex = 0.7, col = &quot;grey50&quot;) text(species/2, labels = rownames(species), col = &quot;green&quot;, cex = 0.7) segments(x0 = 0, y0 = 0, x = env[, 1], y = env[, 2], col = &quot;blue&quot;) text(env, labels = rownames(env), col = &quot;blue&quot;, cex = 1) On pourra effectuer une analyse de Kaiser-Guttmann ou de broken-stick de la mÃªme maniÃ¨re que prÃ©cÃ©demment. Ã‰tant une collection de rÃ©gressions, une RDA est en mesure dâ€™effectuer des tests statistiques sur les coefficients de la rÃ©gression en utilisant des permutations pour tester la signification des coefficients et des axes dâ€™une RDA. On doit nÃ©anmoins obligatoirement effectuer la RDA avec lâ€™interface formule. Lâ€™a variable de gaucheâ€™objet Ã  gauche du ~ peut Ãªtre une matrice ou un tableau, et celui de droite est dÃ©fini dans data. Le . dans lâ€™interface formule signifie â€œune combinaison linÃ©aire de toutes les variables, sans intÃ©ractionâ€. vare_rda &lt;- rda(varespec ~ ., data = vareclr, scale = FALSE) perm_test_term &lt;- anova(vare_rda, by = &quot;term&quot;) #perm_test_axis &lt;- anova(vare_rda, by = &quot;axis&quot;) La signification des axes est difficile Ã  interprÃ©ter. Toutefois, celui des variables prÃ©sente un intÃ©rÃªt. perm_test_term ## Permutation test for rda under reduced model ## Terms added sequentially (first to last) ## Permutation: free ## Number of permutations: 999 ## ## Model: rda(formula = varespec ~ N + P + K + Ca + Mg + S + Al + Fe + Mn + Zn + Mo + Fv + Baresoil + Humdepth + pH, data = vareclr, scale = FALSE) ## Df Variance F Pr(&gt;F) ## N 1 216.13 4.8470 0.009 ** ## P 1 272.71 6.1159 0.003 ** ## K 1 194.97 4.3724 0.016 * ## Ca 1 24.92 0.5589 0.686 ## Mg 1 52.61 1.1799 0.303 ## S 1 100.07 2.2441 0.095 . ## Al 1 177.91 3.9900 0.024 * ## Fe 1 118.59 2.6595 0.053 . ## Mn 1 25.96 0.5822 0.621 ## Zn 1 35.81 0.8030 0.483 ## Mo 1 23.51 0.5273 0.668 ## Baresoil 1 98.64 2.2122 0.100 . ## Humdepth 1 43.59 0.9777 0.402 ## pH 1 38.93 0.8730 0.460 ## Residual 9 401.31 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 La p-value est la probabilitÃ© que les pentes calculÃ©es pour les variables Ã©mergent de distributions dont la moyenne est nulle. Au seuil 0.05, les variables significatives sont (les clr de) lâ€™azote, le phosphore, le potassium et lâ€™aluminium. Dans le cas des matrices dâ€™abondance (ce nâ€™est pas le cas de varespec, constituÃ©e de donnÃ©es de recouvrement), il est prÃ©fÃ©rable avec les RDA de les transformer prÃ©alablement avec la transformation compositionnelle, de chord ou de Hellinger (chapitre 8). Une autre option est dâ€™effectuer une RDA sur des matrices dâ€™association en passant par une analyse en coordonnÃ©es principales (Legendre et Anderson, 1999). Enfin, les donnÃ©es dâ€™abondance Ã  lâ€™Ã©tat brutes devraient plutÃ´t passer utiliser une analyse canonique des corrÃ©lations. 9.4.2.3 Analyse canonique des correspondances (ACC) Lâ€™analyse canonique des correspondances (Canonical correspondance analysis), ACC, a Ã©tÃ© Ã  lâ€™origine conÃ§ue pour Ã©tudier les liens entre des variables environnementales et lâ€™abondance (dÃ©compte) ou lâ€™occurence (prÃ©sence-absence) dâ€™espÃ¨ces (ter Braak, 1986). Lâ€™ACC est Ã  la RDA ce que la CA est Ã  lâ€™ACP. Alors que la RDA prÃ©serve les distance euclidiennes entre variables dÃ©pendantes et indpendantes, lâ€™ACC prÃ©serve les distances du \\(\\chi^2\\). Tout comme lâ€™AC, elle hÃ©rite du coup une propriÃ©tÃ© importate de la distance du \\(\\chi^2\\): il y a davantage davantage dâ€™importance aux espÃ¨ces rares. Lâ€™analyse des correspondances canoniques est souvent utilisÃ©e dans la littÃ©rature, mais dans bien des cas une RDA sur des donnÃ©es dâ€™abondance transformÃ©es donnera des rÃ©sultats davantage intÃ©rprÃ©tables (Legendre et Gallagher, 2001). 9.4.2.3.1 Application Cet exemple dâ€™application concerne des donnÃ©es dâ€™abondance. Nous allons consÃ©quemment utiliser une CCA avec la fonction cca, toujours avec le module vegan. Les tableaux doubs_fish et doubs_env comprennent respectivement des donnÃ©es dâ€™abondance dâ€™espÃ¨ces de poissons et dans diffÃ©rents environnements de la riviÃ¨re Doubs (Europe) publiÃ©es dans Verneaux. (1973) et exportÃ©es du module ade4. data(&quot;doubs&quot;) doubs_fish &lt;- doubs$fish doubs_env &lt;- doubs$env Sur le site no 8, aucun poisson nâ€™a pas Ã©tÃ© observÃ©. Les observations ne comprenant que des zÃ©ro doivent Ãªtre prÃ©alablement retirÃ©es. tot_spec &lt;- doubs_fish %&gt;% transmute(tot_spec = apply(., 1, sum)) doubs_fish &lt;- doubs_fish %&gt;% filter(tot_spec != 0) doubs_env &lt;- doubs_env %&gt;% filter(tot_spec != 0) De la mÃªme maniÃ¨re quâ€™avec la fonction rda de vegan, nous utilisons cca pour lâ€™ACC. doubs_cca &lt;- cca(doubs_fish ~ ., data = doubs_env, scale = FALSE) Comparons les rÃ©sultats par(mfrow = c(1, 2)) ordiplot(doubs_cca, scaling = 1, type = &quot;text&quot;, main = &quot;CCA - Scaling 1 - Triplot de distance&quot;) ordiplot(doubs_cca, scaling = 2, type = &quot;text&quot;, main = &quot;CCA - Scaling 2 - Triplot de corrÃ©lation&quot;) Triplot de distance (scaling 1). La projection des variables rÃ©ponse Ã  angle droit sur les variables explicatives est une approximation de la rÃ©ponse sur lâ€™explication. (2) Un objet (site ou rÃ©ponse) situÃ© prÃ¨s dâ€™une variable explicative est plus susceptible dâ€™avoir le dÃ©compte 1. (3) Les distances entre les variables (rÃ©ponse et explicatives) approximent la distance du \\(\\chi^2\\) (traduction adaptÃ©e de Borcard et al. (2011)). Triplot de corrÃ©lation (scaling 2). La valeur optmiale de lâ€™espÃ¨ce sur une variable environnementale quantitative peut Ãªtre obtenue en projetant lâ€™espÃ¨ce Ã  angle droit sur la variable. (2) Une espÃ¨ce se trouvant prÃ¨s dâ€™une variable environnementale est susceptible de se trouver en plus grande abondance aux sites de statut 1 pour cette variable. (3) Les distances nâ€™approximent pas la distance du \\(\\chi^2\\) (traduction adaptÃ©e de Borcard et al. (2011)). "],
["chapitre-outliers.html", "10 DÃ©tection de valeurs aberrantes et imputation de donnÃ©es manquantes 10.1 DonnÃ©es manquantes: dÃ©finition, origine, typologie et traitement 10.2 Valeurs et Ã©chantillons aberrants: dÃ©finition, origines, mÃ©thodes de dÃ©tection et traitement", " 10 DÃ©tection de valeurs aberrantes et imputation de donnÃ©es manquantes ï¸Â Objectifs spÃ©cifiques: Ã€ la fin de ce chapitre, vous saurez comment procÃ©der Ã  lâ€™imputation de valeurs manquantes en mode univariÃ© et multivariÃ© saurez comment dÃ©tecter des valeurs aberrantes en mode univariÃ© et multivariÃ© Note. Ce chapitre a Ã©tÃ© initialement rÃ©digÃ© par Zonlehoua Coulibali, qui a gracieusement acceptÃ© de contribuer Ã  ces notes de cours. Le texte a Ã©tÃ© adaptÃ© au format du manuel par Serge-Ã‰tienne Parent. Les donnÃ©es Ã©cologiques sont gÃ©nÃ©ralement recueillies Ã  diffÃ©rentes Ã©chelles, concernent plusieurs sites et plusieurs variables (corrÃ©lÃ©es ou non), impliquent diffÃ©rents individus de diffÃ©rentes agences et peuvent sâ€™Ã©tendre sur plusieurs annÃ©es (Alameddine et al., 2010; Lokupitiya et al., 2006). De ce fait, la plupart de ces bases de donnÃ©es contiennent des valeurs manquantes et/ou aberrantes liÃ©es Ã  diffÃ©rentes sources dâ€™erreurs, pouvant parfois limiter lâ€™utilitÃ© des infÃ©rences statistiques (Collins et al., 2001; Glasson-Cicognani et Berchtold, 2010). Il convient alors de les traiter correctement avant dâ€™effectuer les analyses statistiques car les ignorer peut entraÃ®ner, outre une perte de prÃ©cision, de forts biais dans les modÃ¨les dâ€™analyse (Alameddine et al., 2010; Filzmoser et al., 2008; Glasson-Cicognani et Berchtold, 2010). 10.1 DonnÃ©es manquantes: dÃ©finition, origine, typologie et traitement 10.1.1 DÃ©finition Les tableaux de donnÃ©es sont organisÃ©s en lignes et colonnes. Les lignes reprÃ©sentent les observations, les unitÃ©s, les sujets ou les cas Ã©tudiÃ©s selon le contexte, et les colonnes reprÃ©sentent les variables mesurÃ©es pour chaque observation. Les entrÃ©es qui sont les valeurs (ou contenus) des cellules ou encore les valeurs observÃ©es, peuvent Ãªtre des valeurs continues, ou des valeurs catÃ©goriales (Little et Rubin, 2002). ConsidÃ©rant une variable alÃ©atoire \\(X\\) quelconque, une donnÃ©e manquante \\(x_m\\), est une donnÃ©e pour laquelle la valeur de la variable \\(X\\) est inconnue (ou absente). En dâ€™autres termes, on ne dispose pas de la valeur de \\(X\\) pour le sujet \\(i\\) donnÃ©. Câ€™est une donnÃ©e non disponible qui serait utile pour lâ€™analyse si elle Ã©tait observÃ©e (Ware et al., 2012). La littÃ©rature sur les donnÃ©es manquantes est plus abondante dans les domaines des sciences sociales sur les donnÃ©es dâ€™enquÃªtes, et des sciences mÃ©dicales (Davey et al., 2001; Graham, 2012). Pour reprÃ©senter leur rÃ©partition dans la table de donnÃ©es, une matrice indicatrice des valeurs manquantes \\(M = (m_{ij})\\) est gÃ©nÃ©ralement utilisÃ©e oÃ¹ \\(m_{ij}\\) est une variable binaire qui prend la valeur 1 si la valeur de la variable (\\(X\\)) est observÃ©e et 0 si \\(x\\) est absent (Collins et al., 2001; Graham, 2012; Little et Rubin, 2002). 10.1.2 Origines des donnÃ©es manquantes Les donnÃ©es manquantes ont des origines matÃ©rielles diverses. Des valeurs peuvent Ãªtre absentes soit parce quâ€™elles nâ€™ont pas Ã©tÃ© observÃ©es, ou quâ€™elles ont Ã©tÃ© perdues ou Ã©taient incohÃ©rentes (Glasson-Cicognani et Berchtold, 2010. La donnÃ©e peut avoir Ã©tÃ© perdue lors de la collecte ou du processus dâ€™enregistrement des donnÃ©es, non mesurÃ©e en raison du dysfonctionnement dâ€™un Ã©quipement, non mesurable en raison de la disparition du sujet dâ€™Ã©tude (mort, fugue, champ non rÃ©coltÃ©, etc.), Ã©cartÃ©e en raison dâ€™une contamination, oubliÃ©e, non Ã©tudiÃ©e, etc. 10.1.3 Profils des donnÃ©es manquantes Les auteurs traitant des donnÃ©es manquantes distinguent des formes de rÃ©partition des donnÃ©es manquantes et des mÃ©canismes conduisant Ã  ces derniÃ¨res. La rÃ©partition des donnÃ©es manquantes dÃ©crit les dispositions des valeurs prÃ©sentes et celles qui sont manquantes dans la matrice indicatrice. Les mÃ©canismes Ã  lâ€™origine des donnÃ©es manquantes dÃ©crivent la relation probabiliste entre les valeurs observÃ©es et les valeurs manquantes de la table de donnÃ©es. 10.1.3.1 RÃ©partition des donnÃ©es manquantes Les donnÃ©es manquantes se rÃ©partissent selon diffÃ©rents cas de figures (Graham, 2012; Little et Rubin, 2002) dont les trois principaux sont les valeurs manquantes univariÃ©es, les valeurs manquantes monotones et celles non monotones ou arbitraires. Cette distinction est fonction de la matrice indicatrice des valeurs manquantes. Cette matrice est dite Ã  valeurs manquantes univariÃ©es ou de non-rÃ©ponse univariÃ©e, lorsque pour une variable donnÃ©e, si une observation est absente, alors toutes les observations suivantes pour cette variable sont absentes (figure 10.1a). En expÃ©rimentation agricole, ce cas de figure est qualifiÃ© de problÃ¨me de la parcelle manquante oÃ¹, pour une raison quelconque (par exemple : une absence de germination, une destruction accidentelle dâ€™une parcelle ou des enregistrements incorrects), un facteur Ã  lâ€™Ã©tude est non disponible. Les valeurs manquantes monotones surviennent lorsque la valeur dâ€™une variable \\(Y_j\\) manquante pour un individu \\(i\\) implique que toutes les variables suivantes \\(Y_k\\) (\\(k &gt; j\\)) sont manquantes pour cet individu (figure 10.1b). Les valeurs manquantes arbitraires ou non monotones ou encore gÃ©nÃ©rales, surviennent lorsque la matrice ne dessine spÃ©cifiquement aucune des formes prÃ©cÃ©dentes (figure 10.1c). Figure 10.1: Exemple de profils de donnÃ©es manquantes Le module VIM permet de visualiser la structure des donnÃ©es manquantes. ## Loading required package: colorspace ## Loading required package: data.table ## data.table 1.12.8 using 2 threads (see ?getDTthreads). Latest news: r-datatable.com ## ## Attaching package: &#39;data.table&#39; ## The following objects are masked from &#39;package:dplyr&#39;: ## ## between, first, last ## The following object is masked from &#39;package:purrr&#39;: ## ## transpose ## VIM is ready to use. ## Since version 4.0.0 the GUI is in its own package VIMGUI. ## ## Please use the package to use the new (and old) GUI. ## Suggestions and bug-reports can be submitted at: https://github.com/alexkowa/VIM/issues ## ## Attaching package: &#39;VIM&#39; ## The following object is masked from &#39;package:dbscan&#39;: ## ## kNN ## The following object is masked from &#39;package:datasets&#39;: ## ## sleep Pour lâ€™exemple, prenons le tableau iris puis remplaÃ§ons au hasard des donnÃ©es par des valeurs manquantes (NA), puis vÃ©rifions les proportions de donnÃ©es manquantes et les proportions de combinaisons de donnÃ©es manquantes. set.seed(2868374) data(&quot;iris&quot;) iris_NA &lt;- iris n_NA &lt;- 20 row_NA &lt;- sample(1:nrow(iris), n_NA, replace = TRUE) col_NA &lt;- sample(1:ncol(iris), n_NA, replace = TRUE) for (i in 1:n_NA) iris_NA[row_NA[i], col_NA[i]] &lt;- NA summary(aggr(iris_NA, sortVar = TRUE)) ## ## Variables sorted by number of missings: ## Variable Count ## Sepal.Width 0.046666667 ## Species 0.040000000 ## Petal.Length 0.020000000 ## Petal.Width 0.020000000 ## Sepal.Length 0.006666667 ## ## Missings per variable: ## Variable Count ## Sepal.Length 1 ## Sepal.Width 7 ## Petal.Length 3 ## Petal.Width 3 ## Species 6 ## ## Missings in combinations of variables: ## Combinations Count Percent ## 0:0:0:0:0 132 88.0000000 ## 0:0:0:0:1 4 2.6666667 ## 0:0:0:1:0 2 1.3333333 ## 0:0:0:1:1 1 0.6666667 ## 0:0:1:0:0 3 2.0000000 ## 0:1:0:0:0 6 4.0000000 ## 0:1:0:0:1 1 0.6666667 ## 1:0:0:0:0 1 0.6666667 Avec la fonction matrixplot, il est possible de visualiser les donnÃ©es manquantes en rouge, tandis que les donnÃ©es prÃ©sentes prennent un niveau de gris selon leur valeur. matrixplot(iris_NA) 10.1.3.2 MÃ©canismes conduisant aux donnÃ©es manquantes Les mÃ©canismes conduisant aux donnÃ©es manquantes dÃ©crivent la relation entre les valeurs manquantes et celles observÃ©es des variables de la table (Collins et al., 2001; Graham, 2012; Little et Rubin, 2002). En considÃ©rant la table de donnÃ©e \\(Y = \\{O,M\\}\\) oÃ¹ \\(O = \\left[ o_{i, j} \\right]\\) reprÃ©sente les donnÃ©es observÃ©es et \\(M = \\left[ m_{i, j} \\right]\\) la matrice indicatrice des donnÃ©es manquantes, le mÃ©canisme Ã  lâ€™origine des donnÃ©es manquantes est dÃ©fini par la distribution conditionnelle de \\(M\\) sachant \\(Y\\). Lorsque la probabilitÃ© quâ€™une valeur soit manquante ne dÃ©pend ni des valeurs observÃ©es, ni de celles manquantes, les donnÃ©es sont dites manquantes complÃ¨tement au hasard (* MCAR, missing completely at random*). La probabilitÃ© dâ€™absence est donc la mÃªme pour toutes les observations et elle ne dÃ©pend que de paramÃ¨tres extÃ©rieurs indÃ©pendants de cette variable (Collins et al., 2001; Graham, 2012; Heitjan, 1997; Little et Rubin, 2002; Rubin, 1976). Avec de telles donnÃ©es (MCAR), les rÃ©gressions qui nâ€™utilisent que les enregistrements complets, les moyennes des cas disponibles, les tests non-paramÃ©triques et les mÃ©thodes basÃ©es sur les â€œmomentsâ€, sont toutes valides (Heitjan, 1997). Toutefois, une perte de prÃ©cision est Ã  prÃ©voir dans les rÃ©sultats (Collins et al., 2001). Selon les mÃªmes auteurs, lorsque la probabilitÃ© quâ€™une valeur soit manquante dÃ©pend uniquement de la composante observÃ©e â€œOâ€ (une ou plusieurs variables observÃ©es) mais pas des valeurs manquantes elles-mÃªmes, les donnÃ©es sont dites manquantes au hasard (* MAR: missing at random*). Dans ce cas, les mÃ©thodes du maximum de vraisemblance sont valides pour estimer les paramÃ¨tres du modÃ¨le. Les procÃ©dures dâ€™imputation multiples utilisent implicitement le mÃ©canisme MAR (Collins et al., 2001; Heitjan, 1997). Lorsque la probabilitÃ© quâ€™une valeur manque dÃ©pend de la valeur non observÃ©e de la variable elle-mÃªme (\\(M\\)), les donnÃ©es ne manquent pas au hasard (* MNAR: missing not at random*). Ce type de donnÃ©es ne doit pas Ãªtre ignorÃ© dans lâ€™ajustement de modÃ¨les car elles induisent une perte de prÃ©cision (inhÃ©rente Ã  tout cas de donnÃ©es manquantes) mais aussi un biais dans lâ€™estimation des paramÃ¨tres (Collins et al., 2001; Heitjan, 1997). 10.1.4 Traitement des donnÃ©es manquantes La prÃ©sence de donnÃ©es manquantes dans une analyse peut conduire Ã  des estimÃ©s de paramÃ¨tres biaisÃ©s, gonfler les erreurs de type I et II, baisser les performances des intervalles de confiance (Collins et al., 2001) et entacher la gÃ©nÃ©ralisation des rÃ©sultats (Taylor et al., 2002). Plusieurs mÃ©thodes existent pour calculer des estimÃ©s de paramÃ¨tres de modÃ¨les approximativement sans biais, en prÃ©sence de donnÃ©es manquantes. 10.1.4.1 Lâ€™analyse des cas complets Cette mÃ©thode consiste Ã  exclure du fichier de donnÃ©es tous les individus ayant au moins une donnÃ©e manquante (Glasson-Cicognani et Berchtold, 2010. Elle serait la plus utilisÃ©e pour traiter les valeurs manquantes mais nâ€™est efficace que pour les cas de donnÃ©es manquant complÃ¨tement au hasard (MCAR) lorsque le nombre de dâ€™observations Ã  Ã©liminer nâ€™est pas trop important (Davey et al., 2001). En R, de maniÃ¨re gÃ©nÃ©rique, il est possible dâ€™identifier une donnÃ©e manquante dans un tableau, une matrice ou un vecteur avec is.na, qui retourne un objet boolÃ©en (TRUE / FALSE). La fonction any permet dâ€™identifier si au moins une valeur est vraie ou fausse dans un objet, alors que la fonction all permet dâ€™identifier si toutes les valeurs sont vraies. On pourra vÃ©rifier si une ligne contient une valeur manquante avec la fonction apply, dans lâ€™axe des lignes. Il faudra toutefois inverser le rÃ©sultat boolÃ©en avec un ! pour faire en sorte que lâ€™on Ã©carte les valeurs manquantes. row_missing &lt;- iris_NA %&gt;% filter(apply(., 1, function(x) any(is.na(x)))) row_complete &lt;- iris_NA %&gt;% filter(!apply(., 1, function(x) any(is.na(x)))) row_missing ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 4.6 3.4 1.4 0.3 &lt;NA&gt; ## 2 5.7 NA 1.5 0.4 setosa ## 3 4.4 NA 1.3 0.2 setosa ## 4 5.1 NA 1.9 0.4 setosa ## 5 6.7 3.1 4.4 1.4 &lt;NA&gt; ## 6 6.2 2.2 4.5 NA versicolor ## 7 6.7 NA 5.0 1.7 versicolor ## 8 7.1 NA 5.9 2.1 virginica ## 9 6.7 NA 5.8 1.8 virginica ## 10 6.4 2.7 NA 1.9 virginica ## 11 6.5 NA 5.5 1.8 &lt;NA&gt; ## 12 5.6 2.8 4.9 NA virginica ## 13 7.7 2.8 6.7 2.0 &lt;NA&gt; ## 14 6.4 2.8 5.6 NA &lt;NA&gt; ## 15 6.3 2.8 NA 1.5 virginica ## 16 6.1 2.6 5.6 1.4 &lt;NA&gt; ## 17 NA 3.1 5.5 1.8 virginica ## 18 6.7 3.0 NA 2.3 virginica Au lieu de apply, R fournit la fontion raccourci complete.cases. row_missing &lt;- iris_NA %&gt;% filter(complete.cases(.)) Le module tidyr (inclus dans tidyverse) nous facilite la vie avec la fonction tidyr::drop_na, qui retire toutes les lignes contenant au moins une valeur manquante. row_complete &lt;- iris_NA %&gt;% drop_na() De mÃªme, on pourra Ã©valuer la proportion de donnÃ©es manquantes. nrow(row_complete) / nrow(iris) ## [1] 0.88 Ou bien, Ã©valuer la proportion de donnÃ©e manquante par groupe. iris_NA %&gt;% group_by(Species) %&gt;% summarise_each(funs(sum(is.na(.))/length(.))) ## Warning: Factor `Species` contains implicit NA, consider using `forcats::fct_explicit_na` ## # A tibble: 4 x 5 ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 0 0.0612 0 0 ## 2 versicolor 0 0.0204 0 0.0204 ## 3 virginica 0.0217 0.0435 0.0652 0.0217 ## 4 &lt;NA&gt; 0 0.167 0 0.167 Pour terminer cette section, il est possible que certaines variables soient peu mesurÃ©es dans une Ã©tude. Au jugement, on pourra sacrifier une colonne contenant plusieurs donnÃ©es manquantes en vue de conserver des lignes. 10.1.4.2 Lâ€™imputation Lâ€™imputation permet de crÃ©er des bases de donnÃ©es complÃ¨tes (DonzÃ©, 2001). Elle corrige la non-rÃ©ponse partielle en substituant une â€œvaleur artificielleâ€ Ã  la valeur manquante. Les auteurs distinguent lâ€™imputation unique et lâ€™imputation multiple. 10.1.4.2.1 Lâ€™imputation unique Lâ€™imputation unique consiste Ã  remplacer chaque donnÃ©e manquante par une seule valeur plausible telle que la moyenne calculÃ©e sur les donnÃ©es rÃ©ellement observÃ©es, lâ€™imputation par le ou les plus proche(s) voisin(s) (la technique des plus proches voisins est couverte au chapitre 12). Cette derniÃ¨re remplace les donnÃ©es manquantes par des valeurs provenant dâ€™individus similaires pour lesquels toute lâ€™information a Ã©tÃ© observÃ©e. Lâ€™imputation peut aussi se faire par rÃ©gression en remplaÃ§ant les valeurs manquantes par des valeurs prÃ©dites selon un modÃ¨le de rÃ©gression ou des mÃ©thodes bayÃ©siennes plus sophistiquÃ©es. Lâ€™imputation unique est valide en prÃ©sence de donnÃ©es manquantes de type MAR (Davey et al., 2001; DonzÃ©, 2001; Glasson-Cicognani et Berchtold, 2010. Selon Heitjan (1997), il nâ€™existe pas de rÃ¨gles strictes pour dÃ©cider quand il faut entreprendre une imputation multiple. NÃ©anmoins, si la fraction des observations avec des donnÃ©es manquantes est infÃ©rieure Ã  par exemple 5%, et le mÃ©canisme est ignorable (MCAR ou MAR), les analyses les plus simples sont satisfaisantes. Bien que conÃ§u principalement pour lâ€™imputation multiple (on y arrive bientÃ´t), le module mice permet lâ€™imputation univariÃ©e. Nous allons tester lâ€™imputation par la moyenne. Voyons par exemple la moyenne des longueurs des sÃ©pales. mean(iris_NA$Sepal.Length[!complete.cases(iris_NA)], na.rm = TRUE) ## [1] 6.170588 LanÃ§ons lâ€™imputation par la fonction mice, puis la prÃ©diction du tableau imputÃ© par la fonction complete. library(&quot;mice&quot;) iris_mice &lt;- mice(iris_NA, method = &quot;mean&quot;) iris_imp &lt;- complete(iris_mice) Le tableau original peut Ãªtre comparÃ© au tableau imputÃ©. iris_NA[!complete.cases(iris_NA), ] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 7 4.6 3.4 1.4 0.3 &lt;NA&gt; ## 16 5.7 NA 1.5 0.4 setosa ## 39 4.4 NA 1.3 0.2 setosa ## 45 5.1 NA 1.9 0.4 setosa ## 66 6.7 3.1 4.4 1.4 &lt;NA&gt; ## 69 6.2 2.2 4.5 NA versicolor ## 78 6.7 NA 5.0 1.7 versicolor ## 103 7.1 NA 5.9 2.1 virginica ## 109 6.7 NA 5.8 1.8 virginica ## 112 6.4 2.7 NA 1.9 virginica ## 117 6.5 NA 5.5 1.8 &lt;NA&gt; ## 122 5.6 2.8 4.9 NA virginica ## 123 7.7 2.8 6.7 2.0 &lt;NA&gt; ## 133 6.4 2.8 5.6 NA &lt;NA&gt; ## 134 6.3 2.8 NA 1.5 virginica ## 135 6.1 2.6 5.6 1.4 &lt;NA&gt; ## 138 NA 3.1 5.5 1.8 virginica ## 146 6.7 3.0 NA 2.3 virginica iris[!complete.cases(iris_NA), ] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 7 4.6 3.4 1.4 0.3 setosa ## 16 5.7 4.4 1.5 0.4 setosa ## 39 4.4 3.0 1.3 0.2 setosa ## 45 5.1 3.8 1.9 0.4 setosa ## 66 6.7 3.1 4.4 1.4 versicolor ## 69 6.2 2.2 4.5 1.5 versicolor ## 78 6.7 3.0 5.0 1.7 versicolor ## 103 7.1 3.0 5.9 2.1 virginica ## 109 6.7 2.5 5.8 1.8 virginica ## 112 6.4 2.7 5.3 1.9 virginica ## 117 6.5 3.0 5.5 1.8 virginica ## 122 5.6 2.8 4.9 2.0 virginica ## 123 7.7 2.8 6.7 2.0 virginica ## 133 6.4 2.8 5.6 2.2 virginica ## 134 6.3 2.8 5.1 1.5 virginica ## 135 6.1 2.6 5.6 1.4 virginica ## 138 6.4 3.1 5.5 1.8 virginica ## 146 6.7 3.0 5.2 2.3 virginica iris_imp[!complete.cases(iris_NA), ] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 7 4.600000 3.400000 1.400000 0.300000 &lt;NA&gt; ## 16 5.700000 3.052174 1.500000 0.400000 setosa ## 39 4.400000 3.052174 1.300000 0.200000 setosa ## 45 5.100000 3.052174 1.900000 0.400000 setosa ## 66 6.700000 3.100000 4.400000 1.400000 &lt;NA&gt; ## 69 6.200000 2.200000 4.500000 1.178169 versicolor ## 78 6.700000 3.052174 5.000000 1.700000 versicolor ## 103 7.100000 3.052174 5.900000 2.100000 virginica ## 109 6.700000 3.052174 5.800000 1.800000 virginica ## 112 6.400000 2.700000 3.680142 1.900000 virginica ## 117 6.500000 NA 5.500000 1.800000 &lt;NA&gt; ## 122 5.600000 2.800000 4.900000 1.178169 virginica ## 123 7.700000 2.800000 6.700000 2.000000 &lt;NA&gt; ## 133 6.400000 2.800000 5.600000 NA &lt;NA&gt; ## 134 6.300000 2.800000 3.680142 1.500000 virginica ## 135 6.100000 2.600000 5.600000 1.400000 &lt;NA&gt; ## 138 5.818881 3.100000 5.500000 1.800000 virginica ## 146 6.700000 3.000000 3.680142 2.300000 virginica Dans la colonne Sepal.Length, toutes les valeurs manquantes ont Ã©tÃ© remplacÃ©es par ~5.862. Exercice. Pourquoi la prÃ©diction diffÃ¨re-t-elle de la moyenne? ğŸ˜± Attention. Lorsque les valeurs sont systÃ©matiquement manquantes chez une catÃ©gorie, les estimateurs seront biaisÃ©s. iris_NA_biais_1 &lt;- tibble(Sepal.Length = c(5.3, NA, 4.9, NA, 4.7, NA), Species = c(&quot;setosa&quot;, &quot;versicolor&quot;, &quot;setosa&quot;, &quot;versicolor&quot;, &quot;setosa&quot;, &quot;versicolor&quot;)) mean(iris_NA_biais_1$Sepal.Length, na.rm = TRUE) ## [1] 4.966667 iris_NA_biais_2 &lt;- tibble(Sepal.Length = c(5.3, 7.0, 4.6, 6.4, 4.8, 6.9), Species = c(&quot;setosa&quot;, &quot;versicolor&quot;, &quot;setosa&quot;, &quot;versicolor&quot;, &quot;setosa&quot;, &quot;versicolor&quot;)) mean(iris_NA_biais_2$Sepal.Length, na.rm = TRUE) ## [1] 5.833333 Dans lâ€™exemple prÃ©cÃ©dent, les donnÃ©es sont systÃ©matiquement manquantes chez lâ€™espÃ¨ce versicolor. La moyenne de la longueur des sÃ©pales est donc biaisÃ©e, et lâ€™imputation par la moyenne de sera tout autant. Lâ€™imputation par la moyenne est jugÃ©e non recommandable par plusieurs statisticiens. Dans la mesure du possible, lâ€™imputation multiple devrait Ãªtre favorisÃ©e Ã  lâ€™imputation univariÃ©e. 10.1.4.2.2 Lâ€™imputation multiple Lâ€™imputation multiple consiste Ã  imputer plusieurs fois les valeurs manquantes et Ã  combiner les rÃ©sultats pour diminuer lâ€™erreur causÃ©e par la complÃ©tion (Davey et al., 2001). Les valeurs manquantes sont remplacÃ©es par \\(M\\) (\\(M &gt; 1\\)) ensembles de valeurs simulÃ©es donnant lieu Ã  \\(M\\) versions plausibles mais diffÃ©rentes des donnÃ©es complÃ¨tes (Collins et al., 2001; Taylor et al., 2002). En pratique, seulement \\(M\\) allant de 5 Ã  10 (imputations) est suffisant pour produire des bonnes infÃ©rences (Collins et al., 2001; DonzÃ©, 2001). Chacun des \\(M\\) ensembles de donnÃ©es est analysÃ© de la mÃªme maniÃ¨re par des mÃ©thodes standards dâ€™analyse de donnÃ©es complÃ¨tes, et les rÃ©sultats sont combinÃ©s en utilisant une arithmÃ©tique simple: les moyennes des paramÃ¨tres estimÃ©s sont calculÃ©es, les erreurs standards sont combinÃ©es pour refleter lâ€™incertitude des donnÃ©es manquantes et lâ€™erreur dâ€™Ã©chantillonnage. Lâ€™imputation multiple est une procÃ©dure basÃ©e sur un modÃ¨le (model-based). Lâ€™utilisateur doit spÃ©cifier un modÃ¨le de probabilitÃ© conjointe pour les donnÃ©es observÃ©es et manquantes (Collins et al., 2001; Taylor et al., 2002). Le module mice donne accÃ¨s Ã  plusieurs types de modÃ¨les (argument method). Les modÃ¨les cart et rf tombent la la catÃ©gorie de lâ€™autoapprentissage (couvert au chapitre 12). Ils ont lâ€™avantage important dâ€™Ãªtre applicables autant pour tout type de variable. iris_mice &lt;- mice(iris_NA, method = &quot;rf&quot;) iris_imp &lt;- complete(iris_mice) De mÃªme que prÃ©cÃ©demment, le tableau original peut Ãªtre comparÃ© au tableau imputÃ©. iris_NA[!complete.cases(iris_NA), ] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 7 4.6 3.4 1.4 0.3 &lt;NA&gt; ## 16 5.7 NA 1.5 0.4 setosa ## 39 4.4 NA 1.3 0.2 setosa ## 45 5.1 NA 1.9 0.4 setosa ## 66 6.7 3.1 4.4 1.4 &lt;NA&gt; ## 69 6.2 2.2 4.5 NA versicolor ## 78 6.7 NA 5.0 1.7 versicolor ## 103 7.1 NA 5.9 2.1 virginica ## 109 6.7 NA 5.8 1.8 virginica ## 112 6.4 2.7 NA 1.9 virginica ## 117 6.5 NA 5.5 1.8 &lt;NA&gt; ## 122 5.6 2.8 4.9 NA virginica ## 123 7.7 2.8 6.7 2.0 &lt;NA&gt; ## 133 6.4 2.8 5.6 NA &lt;NA&gt; ## 134 6.3 2.8 NA 1.5 virginica ## 135 6.1 2.6 5.6 1.4 &lt;NA&gt; ## 138 NA 3.1 5.5 1.8 virginica ## 146 6.7 3.0 NA 2.3 virginica iris[!complete.cases(iris_NA), ] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 7 4.6 3.4 1.4 0.3 setosa ## 16 5.7 4.4 1.5 0.4 setosa ## 39 4.4 3.0 1.3 0.2 setosa ## 45 5.1 3.8 1.9 0.4 setosa ## 66 6.7 3.1 4.4 1.4 versicolor ## 69 6.2 2.2 4.5 1.5 versicolor ## 78 6.7 3.0 5.0 1.7 versicolor ## 103 7.1 3.0 5.9 2.1 virginica ## 109 6.7 2.5 5.8 1.8 virginica ## 112 6.4 2.7 5.3 1.9 virginica ## 117 6.5 3.0 5.5 1.8 virginica ## 122 5.6 2.8 4.9 2.0 virginica ## 123 7.7 2.8 6.7 2.0 virginica ## 133 6.4 2.8 5.6 2.2 virginica ## 134 6.3 2.8 5.1 1.5 virginica ## 135 6.1 2.6 5.6 1.4 virginica ## 138 6.4 3.1 5.5 1.8 virginica ## 146 6.7 3.0 5.2 2.3 virginica iris_imp[!complete.cases(iris_NA), ] ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 7 4.6 3.4 1.4 0.3 setosa ## 16 5.7 3.8 1.5 0.4 setosa ## 39 4.4 3.3 1.3 0.2 setosa ## 45 5.1 2.8 1.9 0.4 setosa ## 66 6.7 3.1 4.4 1.4 versicolor ## 69 6.2 2.2 4.5 1.4 versicolor ## 78 6.7 3.0 5.0 1.7 versicolor ## 103 7.1 3.3 5.9 2.1 virginica ## 109 6.7 3.3 5.8 1.8 virginica ## 112 6.4 2.7 4.9 1.9 virginica ## 117 6.5 3.0 5.5 1.8 virginica ## 122 5.6 2.8 4.9 1.7 virginica ## 123 7.7 2.8 6.7 2.0 versicolor ## 133 6.4 2.8 5.6 1.9 virginica ## 134 6.3 2.8 5.1 1.5 virginica ## 135 6.1 2.6 5.6 1.4 versicolor ## 138 6.3 3.1 5.5 1.8 virginica ## 146 6.7 3.0 5.5 2.3 virginica Mieux vauit Ã©viter dâ€™imputer des donnÃ©es compositionnelles transformÃ©es (alr, clr ou ilr), car lâ€™imputation dâ€™une dimension transformÃ©e aura un impact sur tout le vecteur. Dans ce cas, vous pourriez prÃ©fÃ©rablemen utiliser la fonction robCompositions::impCoda. 10.2 Valeurs et Ã©chantillons aberrants: dÃ©finition, origines, mÃ©thodes de dÃ©tection et traitement 10.2.1 DÃ©finitions En analyse univariÃ©e, une valeur aberrante est une â€œdonnÃ©e observÃ©eâ€ pour une variable qui semble anormale au regard des valeurs dont on dispose pour les autres observations de lâ€™Ã©chantillon (Planchon, 2005). En analyse multivariÃ©e, lâ€™Ã©chantillon aberrant rÃ©sulte dâ€™une erreur importante se trouvant dans un des composants du vecteur de rÃ©ponse, ou de petites erreurs systÃ©matiques dans chacun de ses composants, et qui de ce fait, ne partage pas les relations entre les variables de la population (Planchon, 2005). La valeur ou lâ€™observation aberrante est statistiquement discordante dans le contexte dâ€™un modÃ¨le de probabilitÃ© supposÃ© connu (Barnett et Lewis, 1994; Grubbs, 1969; Munoz-Garcia et al., 1990; Pires et Santos-Pereira, 2005). Leur prÃ©sence dans les donnÃ©es peut conduire Ã  des estimateurs de paramÃ¨tres biaisÃ©s et, suite Ã  la rÃ©alisation de tests statistiques, Ã  une interprÃ©tation des rÃ©sultats erronÃ©e (Planchon, 2005). 10.2.2 Origines Dans une collecte de donnÃ©es, plusieurs sources de variabilitÃ© peuvent mener Ã  des donnÃ©es aberrantes: la variabilitÃ© inhÃ©rente mais inusitÃ©e ou erreur systÃ©matique, lâ€™erreur de mesure et lâ€™erreur dâ€™exÃ©cution (figure 10.2) (Barnett et Lewis, 1994; Planchon, 2005). Figure 10.2: SchÃ©ma gÃ©nÃ©ral de traitement des valeurs aberrantes - adaptÃ© de Barnett et Lewis, 1994 La variabilitÃ© inhÃ©rente est celle par laquelle les observations varient naturellement de maniÃ¨re alÃ©atoire Ã  travers la population. Lâ€™erreur de mesure renferme les inadÃ©quations au niveau de la mÃ©thode de mesure, des instruments de mesure, lâ€™arrondi des valeurs obtenues ou les erreurs dâ€™enregistrement. Cette erreur est donc liÃ©e Ã  des circonstances bien dÃ©terminÃ©es. Les erreurs dâ€™exÃ©cution interviennent Ã©galement dans des circonstances bien dÃ©terminÃ©es. Ce sont les erreurs de manipulation, les erreurs commises dans lâ€™assemblage des donnÃ©es, ou lors du traitement informatique. Lâ€™examen des valeurs aberrantes dans une base de donnÃ©es a pour objectif de les identifier pour soit les supprimer, soit les conserver, ou les corriger avant dâ€™ajuster des modÃ¨les non robustes (Filzmoser et al., 2008; Planchon, 2005). La valeur extrÃªme peut Ãªtre liÃ©e Ã  un Ã©vÃ©nement atypique, mais nÃ©anmoins connu et intÃ©ressant Ã  Ã©tudier. Dans ce cas elle est importante Ã  conserver. La correction (ou accommodation) Ã©vite le rejet des observations aberrantes et consiste Ã  estimer les valeurs des paramÃ¨tres de la distribution de base de faÃ§on relativement libre sans dÃ©formation des rÃ©sultats liÃ©s Ã  leur prÃ©sence (Barnett et Lewis, 1994). 10.2.3 DÃ©tection et traitement des Ã©chantillons aberrants multivariÃ©s Lâ€™approche dâ€™identification des observations aberrantes selon Davies et Gather (1993) est de supposer quâ€™elles ont une distribution diffÃ©rente de celle du reste des observations. Reimann et al. (2005) les distinguent ainsi des valeurs extrÃªmes qui, bien quâ€™Ã©loignÃ©es du centre du nuage, appartiennent Ã  la mÃªme distribution que les autres observations. En analyse univariÃ©e, les mÃ©thodes graphiques telles que le diagramme de dispersion des observations classÃ©es en fonction de leur rang, les boxplots, les graphiques des quantiles de valeurs brutes ou des rÃ©sidus, permettent de signaler la prÃ©sence de valeurs aberrantes (Planchon, 2005). En analyse multivariÃ©e, il existe deux approches fondamentales dâ€™identification des valeurs aberrantes: celles basÃ©es sur le calcul de distances et les mÃ©thodes par projection (Filzmoser et al., 2008; Hadi et al., 2009). 10.2.3.1 Approches basÃ©es sur les distances 10.2.3.1.1 La distance de Mahalanobis Les mÃ©thodes basÃ©es sur la distance dÃ©tectent les valeurs aberrantes en calculant la distance, gÃ©nÃ©ralement la distance de Mahalanobis (vue au chapitre 9) entre un point particulier et le centre des donnÃ©es (Filzmoser et al., 2008; Pires et Santos-Pereira, 2005). Pour un Ã©chantillon \\(x\\) multivariÃ©, la distance de Mahalanobis est calculÃ©e comme: \\[ \\mathscr{M} = \\sqrt{(\\vec{x}-\\vec{\\mu})^T S^{-1} (\\vec{x}-\\vec{\\mu})}.\\ \\] oÃ¹ \\(\\vec{\\mu}\\) est la moyenne arithmÃ©tique multivariÃ©e (le centroÃ¯de) et \\(S\\) la matrice de variance-covariances de lâ€™Ã©chantillon, qui doit Ãªtre inversÃ©e. Cette distance indique Ã  quel point chaque observation est Ã©loignÃ©e du centre du nuage multivariÃ© crÃ©Ã© par les donnÃ©es (Alameddine et al., 2010; Davies et Gather, 1993). Dâ€™aprÃ¨s Alameddine et al. (2010), lorsque les donnÃ©es sont supposÃ©es suivre une distribution normale, les carrÃ©s des distances \\(\\mathscr{M}\\) calculÃ©es peuvent Ãªtre considÃ©rÃ©s comme suivant une distribution du \\(\\chi^2\\). Par convention, tout point qui a une dÃ©passant un quantile donnÃ© de la distribution du \\(\\chi^2\\) (par exemple, \\(\\chi^2_{df = p ; 0.975}\\), le quantile 97,5% avec \\(p\\) (le nombre de variables) degrÃ©s de libertÃ©), est considÃ©rÃ© comme atypique et identifiÃ© comme une valeur aberrante (Filzmoser et al., 2005). Les observations aberrantes multivariÃ©es peuvent ainsi Ãªtre dÃ©finies comme des observations ayant une grande distance de Mahalanobis (\\(\\mathscr{M}^2\\)). Lâ€™inconvÃ©nient avec les mÃ©thodes basÃ©es sur les distances rÃ©side dans la difficultÃ© dâ€™obtenir des estimÃ©s robuste de la moyenne \\(\\mu\\) et de la matrice de variance-covariances \\(S\\), puisque la distance de Mahalanobis est elle-mÃªme sensible aux donnÃ©es extrÃªmes. De plus, il serait difficile de fixer la valeur critique idÃ©ale de \\(\\mathscr{M}\\) permettant de sÃ©parer les valeurs aberrantes des points rÃ©guliers (Filzmoser et al., 2005; Filzmoser et al., 2008). La fonction sign1 du module mvoutlier dÃ©tecte les valeurs aberrantes selon un seuil du \\(\\chi^2_{df = 3 ; 0.975}\\) pour les transformations en log-ratio isomÃ©triques de Al, Fe et K dans un humus (lâ€™inverse de la matrice de covariance des les log-ratio centrÃ©s est singuliÃ¨re). library(&quot;mvoutlier&quot;) library(&quot;compositions&quot;) data(&quot;humus&quot;) sbp &lt;- matrix(c(1, 1,-1,-1, 1,-1, 0, 0, 0, 0, 1,-1), ncol = 4, byrow = TRUE) ilr_elements &lt;- humus %&gt;% dplyr::select(Al, Fe, K, Na) %&gt;% ilr(., V = gsi.buildilrBase(t(sbp))) %&gt;% as_tibble(.) %&gt;% dplyr::rename(AlFe_KNa = V1, Al_Fe = V2, K_Na = V3) is_out &lt;- sign1(ilr_elements, qcrit = 0.975)$wfinal01 plot(ilr_elements, col = is_out + 2) La proportion de valeurs aberrantes: sum(is_out == 0) / length(is_out) ## [1] 0.089141 DiffÃ©rentes mÃ©thodes robustes (qui sâ€™accommodent de la prÃ©sence de points extrÃªmes) de dÃ©tection des valeurs aberrantes sont prÃ©sentÃ©es dans la littÃ©rature telles que la mÃ©thode du volume minimum de lâ€™ellipsoÃ¯de (MVE, minimum volume ellipsoid), du dÃ©terminant minimum de la matrice de covariance (MCD, minimum Covariance matrix determinant), et les estimateurs de type maximum de vraisemblance (M-estimators) (Alameddine et al., 2010; Filzmoser et al., 2008). Ces mÃ©thodes calculent des distances robustes similaires aux distances de Mahalanobis, mais remplacent les matrices des moyennes et des covariances respectivement par un seuil critique multivariÃ© robuste (sur \\(\\mu\\)) et un estimateur dâ€™Ã©chelle (sur \\(S\\)) qui ne sont pas influencÃ©s par les valeurs aberrantes (Alameddine et al., 2010). 10.2.3.1.2 La mÃ©thode du volume minimum de lâ€™ellipsoÃ¯de (MVE) Le volume minimum de lâ€™ellipsoÃ¯de est le plus petit ellipsoÃ¯de rÃ©gulier couvrant au moins \\(h\\) Ã©lÃ©ments de lâ€™ensemble des donnÃ©es \\(X = \\{x_1, x_2, ..., x_n \\}\\) oÃ¹ lâ€™estimateur de localisation est le centre de cet ellipsoÃ¯de et lâ€™estimateur de dispersion correspond Ã  sa matrice de covariance. \\(h\\) est fixÃ© Ã  priori supÃ©rieur ou Ã©gal Ã  \\(\\frac{n}{2}+1\\), oÃ¹ \\(n\\) est le nombre total de points du nuage de donnÃ©es. Le seuil de dÃ©tection qui est la fraction des valeurs aberrantes qui, lorsquâ€™elle est dÃ©passÃ©e entraÃ®ne des estimÃ©s totalement biaisÃ©s est de lâ€™ordre de 50% Ã  mesure que \\(n\\) augmente (Alameddine et al., 2010; Croux et al., 2002; Filzmoser et al., 2005; Van Aelst et Rousseeuw, 2009). Lâ€™algorithme MVE est initiÃ© en choisissant au hasard un ensemble de \\(p+1\\) points de donnÃ©es pour estimer le modÃ¨le majoritaire, oÃ¹ \\(p\\) est le nombre de variables. Cet ensemble initial est alors augmentÃ© pour contenir les \\(h\\) points de donnÃ©es. Lâ€™algorithme passe par plusieurs itÃ©rations avant de converger sur lâ€™ensemble des points les plus rapprochÃ©s qui auront le plus petit volume dâ€™ellipsoÃ¯de (Alameddine et al., 2010). Le module MASS comprend la fonction cov.mve Ã  cet effet. Cette fonction demande le nombre minimal de points que lâ€™on dÃ©sire conserver, en absolu. Il sâ€™agit dâ€™un nombre entier, alors si lâ€™on dÃ©sire en utiliser une fraction (ici, 90%), il faut lâ€™arrondir. Parmi les sorties de la fonction cov.mve, on retrouve les numÃ©ros de ligne qui se trouvent Ã  lâ€™intÃ©rieur de lâ€™ellipsoide. library(&quot;MASS&quot;) select &lt;- dplyr::select # pour Ã©viter que la fonction select du module MASS remplace celle de dplyr min_in &lt;- round(0.9 * nrow(ilr_elements)) # le minimum de points Ã  garder, 90% du total id_in &lt;- cov.mve(ilr_elements, quantile.used = min_in)$best is_in &lt;- 1:nrow(ilr_elements) %in% id_in plot(ilr_elements, col = is_in + 2) La proportion de valeurs aberrantes: sum(!is_in) / length(is_in) ## [1] 0.1004862 10.2.3.1.3 La mÃ©thode du dÃ©terminant minimum de la matrice de covariance (MCD) La mÃ©thode du dÃ©terminant minimum de la matrice de covariance a pour objectif de trouver \\(h\\) (\\(h &gt; n\\)) observations de lâ€™ensemble de donnÃ©es \\(X = \\{x_1, x_2, ..., x_n \\}\\), dont la matrice de covariance a le plus petit dÃ©terminant. Comme avec la mÃ©thode MVE, lâ€™estimateur de localisation est la moyenne de ces \\(h\\) points et celui de la dispersion est proportionnel Ã  la matrice de covariance (Filzmoser et al., 2005; Hubert et al., 2018; Rousseeuw et Van Driessen, 1999). id_in &lt;- cov.mcd(ilr_elements, quantile.used = min_in)$best is_in &lt;- 1:nrow(ilr_elements) %in% id_in plot(ilr_elements, col = is_in + 2) La proportion de valeurs aberrantes: sum(!is_in) / length(is_in) ## [1] 0.1004862 Mais en cas de dissymÃ©trie des donnÃ©es, ces tests (MVE, MCD) ne seraient pas applicables (Planchon, 2005). 10.2.3.2 Les mÃ©thodes par projection Ces mÃ©thodes de dÃ©tection des observations aberrantes trouvent des projections appropriÃ©es des donnÃ©es dans lesquelles les observations aberrantes sont facilement apparentes. Ces observations sont ensuite pondÃ©rÃ©s pour produire un estimateur robuste pouvant Ãªtre utilisÃ© pour identifier les observations aberrantes (Filzmoser et al., 2008). Ces mÃ©thodes nâ€™assument pas une distribution particuliÃ¨re des donnÃ©es mais cherchent des projections utiles. Elles ne sont donc pas affectÃ©es par la non-normalitÃ© et sâ€™appliquent sur divers types de distributions (Filzmoser et al., 2008; Hadi et al., 2009). Le but de cette projection exploratoire est dâ€™utiliser les donnÃ©es pour trouver des projections minimales (Ã  une, deux ou trois dimensions) qui fournissent les vues les plus rÃ©vÃ©latrices des donnÃ©es complÃ¨tes (Friedman, 1987). La mÃ©thode attribue un indice numÃ©rique Ã  chaque projection en fonction de la densitÃ© des donnÃ©es projetÃ©e pour capturer le degrÃ© de structure non linÃ©aire prÃ©sent dans la distribution projetÃ©e (Friedman, 1987; Hadi et al., 2009). En R, nous revenons au module mvoutlier, mais cette fois-ci avec la fonction sign2. is_out &lt;- sign2(ilr_elements, qcrit = 0.975)$wfinal01 plot(ilr_elements, col = is_out + 2) La proportion de valeurs aberrantes: sum(is_out == 0) / length(is_out) ## [1] 0.102107 "],
["chapitre-temps.html", "11 Les sÃ©ries temporelles 11.1 OpÃ©rations sur les donnÃ©es temporelles 11.2 Analyse de sÃ©ries temporelles 11.3 ModÃ©lisation de sÃ©ries temporelles 11.4 Pour terminerâ€¦", " 11 Les sÃ©ries temporelles ï¸Â Objectifs spÃ©cifiques: Ã€ la fin de ce chapitre, vous saurez comment importer et manipuler des donnÃ©es temporelles (utiliser le format de date, filtrer, effectuer des sommaires, agrÃ©ger des donnÃ©es, etc.) effectuer une rÃ©gression sur une sÃ©rie temporelle Les sÃ©ries temporelles (ou chronologiques) sont des donnÃ©es associÃ©es Ã  des indices temporels de tout ordre de grandeur: seconde, minute, heure, jour, mois, annÃ©e, etc. En analyse de sÃ©rie temporelle, le temps est une variable explicative (ou dÃ©pendante) incontournable. Lâ€™Ã©mergence de cycles est une particularitÃ© des sÃ©ries temporelles. Ceux-ci peuvent Ãªtre analysÃ©s en vue dâ€™en dÃ©terminer la tendance. Les sÃ©ries temporelles peuvent Ã©galement Ãªtre modÃ©lisÃ©s en vue dâ€™effectuer des prÃ©visions. Source: ScÃ¨ne de Back to the future, Robert Zemeckis et and Bob Gale, 1985 Nous allons couvrir les concepts de base en analyse et modÃ©lisation de sÃ©ries temporelles. Mais avant cela, voyons comment les donnÃ©es temporelles sont manipulÃ©es en R. Cette section est basÃ©e sur le livre Forecasting: Principles and Practice, de Rob J. Hyndman et George Athanasopoulos, qui peut Ãªtre entiÃ¨rement consultÃ© gratuitement en ligne, ainsi que le cours associÃ© sur la plateforme dâ€™apprentissage DataCamp. Figure 11.1: Forecasting: Principles and Practice, de Rob J. Hyndman et George Athanasopoulos. 11.1 OpÃ©rations sur les donnÃ©es temporelles Le dÃ©bit de la riviÃ¨re ChaudiÃ¨re, dont lâ€™exutoire se situe prÃ¨s de QuÃ©bec, sur la rive Sud du fleuve Saint-Laurent, est mesurÃ© depuis 1915. library(&quot;tidyverse&quot;) hydro &lt;- read_csv(&quot;data/023402_Q.csv&quot;) ## Parsed with column specification: ## cols( ## Station = col_double(), ## Date = col_date(format = &quot;&quot;), ## DÃ©bit = col_double(), ## Remarque = col_character() ## ) La fonction read_csv() dÃ©tecte automatiquement que la colonne Date est une date. glimpse(hydro) ## Observations: 34,700 ## Variables: 4 ## $ Station &lt;dbl&gt; 23402, 23402, 23402, 23402, 23402, 23402, 23402, 23402, 23402, 23402, 23402, 23402, 23402, 23402, 23402, 2â€¦ ## $ Date &lt;date&gt; 1915-02-27, 1915-02-28, 1915-03-01, 1915-03-02, 1915-03-03, 1915-03-04, 1915-03-05, 1915-03-06, 1915-03-0â€¦ ## $ DÃ©bit &lt;dbl&gt; 538.0, 377.0, 269.0, 345.0, 269.0, 334.0, 269.0, 269.0, 269.0, 269.0, 269.0, 248.0, 215.0, 193.0, 193.0, 2â€¦ ## $ Remarque &lt;chr&gt; &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;, &quot;MC&quot;â€¦ Le dÃ©bit de la riviÃ¨re ChaudiÃ¨re peut Ãªtre explorÃ© graphiquement. hydro %&gt;% ggplot(aes(x = Date, y = `DÃ©bit`)) + geom_line() On observe des donnÃ©es sont manquantes de la fin des annÃ©es 1920 Ã  la fin des annÃ©es 1930. Autrement, il est difficile de visualiser la structure du dÃ©bit en fonction du temps, notamment si le dÃ©bit suit des cycles rÃ©guliers. On pourra isoler les donnÃ©es depuis 2014. hydro %&gt;% filter(Date &gt;= as.Date(&quot;2014-01-01&quot;)) %&gt;% ggplot(aes(x = Date, y = `DÃ©bit`)) + geom_line() R comprend la fonction as.Date(), oÃ¹ lâ€™argument format dÃ©crit la maniÃ¨re avec laquelle la date est exprimÃ©e. as.Date(x = &quot;1999/03/29&quot;, format = &quot;%Y/%m/%d&quot;) ## [1] &quot;1999-03-29&quot; Lâ€™argument x peut aussi bien Ãªtre une chaÃ®ne de caractÃ¨res quâ€™un vecteur oÃ¹ lâ€™on retrouve plusieurs chaÃ®nes de caractÃ¨res exprimant un format de date commun. La fonction as.Date() permet ainsi de transformer des caractÃ¨res en date si read_csv() ne le dÃ©tecte pas automatiquement. Ce format peut prendre la forme dÃ©sirÃ©e, dont les paramÃ¨tres sont listÃ©s sur la page dâ€™aide de la fonction strptime(). Toutefois, le plus petit incrÃ©ment de temps acceptÃ© par as.Date() est le jour: as.Date() exclut les heures, minutes et secondes. Le module lubridate, issu du tidyverse, permet quant Ã  lui de manipuler avec plus de grÃ¢ce les formats de date standards, incluant les dates et les heures: lubridate sera prÃ©fÃ©rÃ© dans ce chapitre. ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:data.table&#39;: ## ## hour, isoweek, mday, minute, month, quarter, second, wday, week, yday, year ## The following object is masked from &#39;package:plyr&#39;: ## ## here ## The following object is masked from &#39;package:base&#39;: ## ## date ## [1] &quot;2011-02-19 09:14:00 UTC&quot; Plusieurs autres formats standards sont prÃ©sentÃ©s sur un aide-mÃ©noire de lubridate. Si vos donnÃ©es comprennent des formats de date non standard, vous pourrez utiliser la fonction as.POSIXlt(), mais il pourrait Ãªtre prÃ©fÃ©rable de standardiser les dates a priori. Figure 11.2: Aide-mÃ©moire du module lubridate. Le module lubridate rend possible lâ€™extraction de la date (date()), lâ€™annÃ©e (year()), le mois (month()), le jour de la semaine (wday()), le jour julien (yday()), etc. pour plus dâ€™options, voir [lâ€™aide-mÃ©moire de lubridate])(https://rawgit.com/rstudio/cheatsheets/master/lubridate.pdf). date_1 &lt;- ymd_hms(&quot;2019-03-14 09:14:00&quot;) date_1 %&gt;% date() ## [1] &quot;2019-03-14&quot; date_1 %&gt;% month() ## [1] 3 date_1 %&gt;% yday() ## [1] 73 date_1 %&gt;% wday() ## [1] 5 date_1 %&gt;% seconds() ## [1] &quot;1552554840S&quot; Ces extractions peuvent Ãªtre utilisÃ©es dans des suites dâ€™opÃ©ration (pipelines). Par exemple, si nous dÃ©sirons obtenir le dÃ©bit mensuel moyen de la riviÃ¨re ChaudiÃ¨re depuis 1990, nous pouvons crÃ©er une nouvelle colonne Year et une autre Month avec la fonction mutate(), effectuer un filtre sur lâ€™annÃ©e, regrouper par mois pour obtenir le sommaire en terme de moyenne, puis lancer le graphique. hydro_month &lt;- hydro %&gt;% mutate(Year = Date %&gt;% year(), Month = Date %&gt;% month()) %&gt;% filter(Year &gt;= 1990) %&gt;% group_by(Month) %&gt;% dplyr::summarise(MeanFlow = mean(`DÃ©bit`, na.rm = TRUE)) hydro_month %&gt;% ggplot(aes(x=Month, y=MeanFlow)) + geom_line() + scale_x_continuous(breaks = 1:12) + expand_limits(y = 0) On pourra aussi agrÃ©ger par moyenne mensuelle en gardant lâ€™annÃ©e respective en crÃ©ant une nouvelle colonne de date YearMonth qui permettra le regroupement avec group_by(), puis crÃ©er plusieurs facettes. hydro %&gt;% mutate(Year = Date %&gt;% year(), Month = Date %&gt;% month(), YearMonth = ymd(paste0(Year, &quot;-&quot;, Month, &quot;-01&quot;))) %&gt;% filter(Year &gt;= 2010 &amp; Year &lt; 2018) %&gt;% group_by(Year, YearMonth) %&gt;% dplyr::summarise(`DÃ©bit` = mean(`DÃ©bit`, na.rm = TRUE)) %&gt;% ggplot(aes(x=YearMonth, y=`DÃ©bit`)) + facet_wrap(~Year, scales = &quot;free_x&quot;, ncol = 4) + geom_line() + expand_limits(y = 0) Il est possible dâ€™effectuer des opÃ©rations mathÃ©matiques sur des donnÃ©es temporelles. Par exemple, ajouter 10 jours Ã  chaque date. hydro %&gt;% head(5) %&gt;% mutate(DateOffset = Date + days(10)) ## # A tibble: 5 x 5 ## Station Date DÃ©bit Remarque DateOffset ## &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; &lt;date&gt; ## 1 23402 1915-02-27 538 MC 1915-03-09 ## 2 23402 1915-02-28 377 MC 1915-03-10 ## 3 23402 1915-03-01 269 MC 1915-03-11 ## 4 23402 1915-03-02 345 MC 1915-03-12 ## 5 23402 1915-03-03 269 MC 1915-03-13 Pour effectuer des opÃ©rations sur des incrÃ©ments infÃ©rieurs aux jours, il faut sâ€™assurer que le type des donnÃ©es temporelles soit bien POSIXct, et non pas Date. hydro %&gt;% pull(Date) %&gt;% class() ## [1] &quot;Date&quot; hydro &lt;- hydro %&gt;% mutate(Date = as_datetime(Date)) hydro %&gt;% pull(Date) %&gt;% class() ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; hydro %&gt;% head(5) %&gt;% mutate(DateOffset = Date + seconds(10)) ## # A tibble: 5 x 5 ## Station Date DÃ©bit Remarque DateOffset ## &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dttm&gt; ## 1 23402 1915-02-27 00:00:00 538 MC 1915-02-27 00:00:10 ## 2 23402 1915-02-28 00:00:00 377 MC 1915-02-28 00:00:10 ## 3 23402 1915-03-01 00:00:00 269 MC 1915-03-01 00:00:10 ## 4 23402 1915-03-02 00:00:00 345 MC 1915-03-02 00:00:10 ## 5 23402 1915-03-03 00:00:00 269 MC 1915-03-03 00:00:10 11.2 Analyse de sÃ©ries temporelles Tout comme câ€™est le cas de nombreux sujet couverts lors de ce cours, lâ€™analyse et modÃ©lisation de sÃ©ries temporelles est un domaine dâ€™Ã©tude en soi. Nous allons nous restreindre ici aux sÃ©ries temporelles consignÃ©es Ã  frÃ©quence rÃ©guliÃ¨re. Les exemples dâ€™analyses et modÃ©lisation de sÃ©ries temporelles sont typiquement des donnÃ©es Ã©conomiques, bien que les principes qui les guident sont les mÃªmes quâ€™en dâ€™autres domaines. Cette section est vouÃ©e Ã  lâ€™analyse, alors que la prochaine est vouÃ©e Ã  la modÃ©lisation. Par exemple, voici une sÃ©rie temporelle Ã©conomique typique, qui exprime les dÃ©penses mensuelles en restauration en Australie. library(&quot;forecast&quot;) ## Registered S3 method overwritten by &#39;xts&#39;: ## method from ## as.zoo.xts zoo ## Registered S3 method overwritten by &#39;quantmod&#39;: ## method from ## as.zoo.data.frame zoo ## Registered S3 methods overwritten by &#39;forecast&#39;: ## method from ## fitted.fracdiff fracdiff ## residuals.fracdiff fracdiff ## ## Attaching package: &#39;forecast&#39; ## The following object is masked from &#39;package:nlme&#39;: ## ## getResponse library(&quot;fpp2&quot;) ## Loading required package: fma ## ## Attaching package: &#39;fma&#39; ## The following object is masked from &#39;package:plyr&#39;: ## ## ozone ## The following objects are masked from &#39;package:MASS&#39;: ## ## cement, housing, petrol ## The following object is masked from &#39;package:robustbase&#39;: ## ## milk ## Loading required package: expsmooth ## ## Attaching package: &#39;fpp2&#39; ## The following object is masked from &#39;package:pls&#39;: ## ## gasoline data(&quot;auscafe&quot;) autoplot(auscafe) On y dÃ©tecte une tendance gÃ©nÃ©rale, probablement propulsÃ©e par la croissance de la dÃ©mographie et des revenus, ainsi que des tendances cycliques. On verra plus loin comment prÃ©dire des occurrences futures, ainsi que lâ€™incertitude de ces prÃ©dictions, Ã  partir des donnÃ©es consignÃ©es. Jusquâ€™Ã  prÃ©sent, nous avons travaillÃ© avec des tableaux de donnÃ©es incluant une colonne en format date. Nous allons maintenant travailler avec des sÃ©ries temporelles telles que reprÃ©sentÃ©es en R. 11.2.1 CrÃ©er et visualiser des sÃ©ries temporelles Lâ€™information consignÃ©e dans une sÃ©rie temporelle inclut nÃ©cessairement un indice temporel associÃ© Ã  au moins une variable. En R, cette information est consignÃ©e dans un objet de type ts, pour time series. Prenons une mesure quelconque prise Ã  chaque trimestre de lâ€™annÃ©e 2018. set.seed(96683) date &lt;- ymd(c(&quot;2018-01-01&quot;, &quot;2018-04-01&quot;, &quot;2018-07-01&quot;, &quot;2018-10-01&quot;)) mesure &lt;- runif(length(date), 1, 10) mesure_ts &lt;- ts(mesure, start = date[1], frequency = 4) mesure_ts ## Qtr1 Qtr2 Qtr3 Qtr4 ## 17532 7.175836 3.646285 6.631606 8.648371 Lâ€™argument start est la date de la premiÃ¨re observation et frequency est le nombre dâ€™observations par unitÃ© temporelle, ici lâ€™annÃ©e. Jâ€™ai auparavant recueilli des donnÃ©es mÃ©tÃ©o avec weathercan (dispobibles seulement depuis 1998) et fusionnÃ© avec le tableau hydro. Pour accÃ©lÃ©rer la procÃ©dure, jâ€™ai enregistrÃ© les donnÃ©es dans un fichier RData. De facto, ne gardons que les donnÃ©es disponibles entre 1998 et 2008, ainsi que les colonnes dÃ©signant la date, le dÃ©bit, les prÃ©cipitations totales et la tempÃ©rature. ## Observations: 3,653 ## Variables: 4 ## $ Date &lt;date&gt; 1998-01-01, 1998-01-02, 1998-01-03, 1998-01-04, 1998-01-05, 1998-01-06, 1998-01-07, 1998-01-08, 1998-â€¦ ## $ DÃ©bit &lt;dbl&gt; 15.70, 16.00, 17.40, 19.30, 23.20, 29.00, 58.85, 65.80, 73.40, 76.40, 76.70, 74.20, 69.80, 65.30, 61.0â€¦ ## $ total_precip &lt;dbl&gt; 1.6, 2.8, 2.2, 0.0, 5.8, 11.8, 2.4, 19.2, 11.6, 2.6, 0.0, 0.0, 11.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.2, 1.2â€¦ ## $ mean_temp &lt;dbl&gt; -21.1, -8.9, 1.9, -3.2, -8.7, -8.0, -7.4, -6.3, -5.4, -1.8, -6.4, -12.1, -7.9, -16.5, -17.2, -12.1, -9â€¦ Pour crÃ©er une sÃ©rie temporelle de type ts, jâ€™enlÃ¨ve la date, je dÃ©marre au premier Ã©vÃ©nement de 1998, et chaque incrÃ©ment a une frÃ©quence de 1/365.25 unitÃ©s depuis 1998 (il y a en moyenne 365.25 jours par an). hydrometeo_ts &lt;- ts(hydrometeo %&gt;% dplyr::select(-Date), start = c(hydrometeo$Date[1] %&gt;% year(), 1), frequency = 365.25) Le module ggplot2 comprend la fonction autoplot(), pratique pour visualiser les sÃ©ries temporelles. autoplot(hydrometeo_ts, facets = TRUE) + scale_x_continuous(breaks = 1998:2008) Il est possible de filtrer des sÃ©ries temporelles en mode tidyverse. Toutefois, il est plus simple dâ€™utiliser la fonction de base windows(). Disons, les 10 premiers jours de lâ€™an 2000. ## Time Series: ## Start = 2000.00136892539 ## End = 2000.02600958248 ## Frequency = 365.25 ## DÃ©bit total_precip mean_temp ## 2000.001 42.40 9.4 -5.6 ## 2000.004 40.70 0.0 -5.5 ## 2000.007 43.60 23.5 -0.9 ## 2000.010 49.04 0.0 -8.8 ## 2000.012 58.90 0.0 -12.9 ## 2000.015 49.10 1.2 -4.6 ## 2000.018 44.40 3.8 -10.5 ## 2000.021 40.60 6.8 -4.9 ## 2000.023 38.10 7.0 -2.3 ## 2000.026 36.50 12.9 -0.3 Voyons lâ€™Ã©volution des dÃ©bits mensuelles. hydrometeo_monthly &lt;- hydrometeo %&gt;% mutate(Year = Date %&gt;% year(), Month = Date %&gt;% month(), YearMonth = ymd(paste0(Year, &quot;-&quot;, Month, &quot;-01&quot;))) %&gt;% group_by(Year, YearMonth) %&gt;% dplyr::summarise(`DÃ©bit` = mean(`DÃ©bit`, na.rm = TRUE), total_precip = sum(total_precip, na.rm = TRUE), # somme mean_temp = mean(mean_temp, na.rm = TRUE)) # moyenne hydrometeo_monthly_ts &lt;- ts(hydrometeo_monthly %&gt;% ungroup() %&gt;% dplyr::select(`DÃ©bit`, total_precip, mean_temp), start = c(1998, 1), frequency = 12) Contraignons la pÃ©riode grÃ¢ce Ã  window(), puis visualisons les tendances cycliques avec forecast::ggseasonplot() et forecast::ggsubseriesplot(). Notez que jâ€™utilise la fonction cowplot::plot_grid() pour arranger diffÃ©rents graphiques ggplot2 en une grille. library(&quot;cowplot&quot;) ## ## ******************************************************** ## Note: As of version 1.0.0, cowplot does not change the ## default ggplot2 theme anymore. To recover the previous ## behavior, execute: ## theme_set(theme_cowplot()) ## ******************************************************** ## ## Attaching package: &#39;cowplot&#39; ## The following object is masked from &#39;package:lubridate&#39;: ## ## stamp theme_set(theme_grey()) # cowplot change le theme ggA &lt;- ggseasonplot(window(hydrometeo_monthly_ts[, 1], 1998, 2004-1/365.25)) + ggtitle(&quot;&quot;) ggB &lt;- ggseasonplot(window(hydrometeo_monthly_ts[, 1], 1998, 2004-1/365.25), polar = TRUE) + ggtitle(&quot;&quot;) ggC &lt;- ggsubseriesplot(window(hydrometeo_monthly_ts[, 1], 1998, 2004-1/365.25), polar = TRUE) + ggtitle(&quot;&quot;) + labs(y=&quot;Flow&quot;) plot_grid(ggA, ggB, ggC, ncol = 3, labels = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;)) 11.2.2 Structures dans les sÃ©ries temporelles Les sÃ©ries temporelles sont susceptibles dâ€™Ãªtre caractÃ©risÃ©es par des structures communÃ©ment observÃ©es. La tendance est une structure dÃ©crivant la hausse ou la baisse Ã  long terme dâ€™une variable numÃ©rique. La fluctuation saisonniÃ¨re est une structure pÃ©riodique, qui oscille autour de la tendance gÃ©nÃ©rale de maniÃ¨re rÃ©guliÃ¨re selon le calendrier. La fluctuation cyclique est aussi une structure pÃ©riodique, mais irrÃ©guliÃ¨re (par exemple, les oscillations peuvent durer parfois 2 ans, parfois 3). Les fluctuations cycliques sont souvent de plus longue frÃ©quence que les fluctuations saisonniÃ¨res, et leur irrÃ©gularitÃ© rend les prÃ©dictions plus difficiles. Note. Une tendance dÃ©tectÃ©e sur une pÃ©riode de temps trop courte peut sâ€™avÃ©rer Ãªtre une fluctuation. La figure 11.3 montre diffÃ©rentes structures. La figure 11.3A montre une tendance croissante des dÃ©penses mensuelles en restauration en Australie, ainsi que des fluctuations saisonniÃ¨res. La figure 11.3B montre des fluctuations saisonniÃ¨res des tempÃ©ratures quotidiennes moyennes Ã  lâ€™UniversitÃ© Laval, sans prÃ©senter de tendance claire. La figure 11.3C montre des fluctuations cycliques du nombre de lynx trappÃ©s par annÃ©e au Canada de 1821 Ã  1934, sans non plus prÃ©senter de tendance claire. Les cycles sont consÃ©quents des mÃ©canismes de dynamique des populations (plus de proie entraÃ®ne plus de prÃ©dateur, plus de prÃ©dateur entraÃ®ne moins de proie, moins de proie entraÃ®ne moins de prÃ©dateur, moins de prÃ©dateur entraÃ®ne plus de proie, etc.), que nous couvrirons au chapitre 14. data(&quot;lynx&quot;) plot_grid(autoplot(auscafe), autoplot(hydrometeo_ts[, 3]) + labs(y=&quot;Mean temperature&quot;), autoplot(lynx), ncol = 3, labels = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;)) Figure 11.3: Identification des tendances et fluctuations dans des sÃ©ries temporelles Il est possible que lâ€™on retrouve une hiÃ©rarchie dans les fluctuations, câ€™est-Ã -dire que de grandes fluctuations (saisonniÃ¨res ou cycliques) peuvent contenir des fluctuations sur des incrÃ©ments de temps plus petits. 11.2.3 Lâ€™autocorrÃ©lation Lorsque les donnÃ©es prÃ©sentes des fluctuations (saisonniÃ¨res ou cycliques), le graphique dâ€™autocorrÃ©lation montrera un sommet aux Ã©tapes des cycles ou des saisons. Le graphique dâ€™autocorrÃ©lation de donnÃ©es alÃ©atoires (aussi appelÃ©es bruit blanc) montera des sommets sans signification. Un graphique de retardement (lag plot) met successivement en relation \\(y_t\\) avec \\(y_{t-p}\\). Un graphique dâ€™autocorrÃ©lation est la corrÃ©lation entre \\(y_t\\), \\(y_{t-1}\\), \\(y_{t-2}\\), etc. Une graphique de retardement donne un aperÃ§u de la dÃ©pendance dâ€™une variable selon ses valeurs passÃ©es. Les graphiques de retardement de donnÃ©es ayant une forte tendance prÃ©senteront des points prÃ¨s de la diagonale, tandis que ceux montrant des donnÃ©es fluctuantes de type sinusoÃ¯dal prÃ©senteront des points disposÃ©s de maniÃ¨re circulaire. Des donnÃ©es alÃ©atoires, quant Ã  elles, ne prÃ©senteront pas de structure de retardement facilement identifiable. set.seed(64301) bruit_blanc &lt;- ts(runif(114, 0, 6000), start = c(1821, 1), frequency = 1) plot_grid(autoplot(lynx) + ggtitle(&quot;Lynx: SÃ©rie temporelle&quot;), ggAcf(lynx) + ggtitle(&quot;Lynx: AutocorrÃ©lation&quot;), gglagplot(lynx) + ggtitle(&quot;Lynx: Lag plot&quot;), autoplot(bruit_blanc) + ggtitle(&quot;Bruit blanc: SÃ©rie temporelle&quot;), ggAcf(bruit_blanc) + ggtitle(&quot;Bruit blanc: AutocorrÃ©lation&quot;), gglagplot(bruit_blanc) + ggtitle(&quot;Bruit blanc: Lag plot&quot;), ncol = 3) Exercice. CrÃ©ez, puis interprÃ©tez des graphiques autoplot(), ggAcf() et gglagplot() pour les donnÃ©es auscafe. Exercice. Trouvez le graphique dâ€™autocorrÃ©lation et le graphique de retardement correspondant Ã  chaque sÃ©rie temporelle. Figure 11.4: Exercice: Trouvez le graphique dâ€™autocorrÃ©lation et le graphique de retardement correspondant Ã  chaque sÃ©rie temporelle. RÃ©ponse, voir source(&quot;lib/09_exercice-hydrometeo.R&quot;): - DÃ©bit: A-B-C - total_precip: B-A-A - mean_temp: C-C-B 11.2.4 Signification statistique dâ€™une sÃ©rie temporelle Jâ€™ai prÃ©cÃ©demment introduit la notion de bruit blanc, qui est un signal ne contenant pas de structure, comme le grÃ©sillement dâ€™une radio mal syntonisÃ©e. Nous avons vu au chapitre 6 que les tests dâ€™hypothÃ¨se en statistiques frÃ©quentielles visent entre autre Ã  dÃ©tecter la probabilitÃ© que les donnÃ©es soient gÃ©nÃ©rÃ©es par une distribution dont la tendance centrale est nulle. De mÃªme, pour les sÃ©ries temporelles, il est possible de calculer la probabilitÃ© quâ€™un signal soit un bruit blanc. Deux outils peuvent nous aider Ã  effectuer ce test: lâ€™un visuel, lâ€™autre sous forme de calcul. Le graphique dâ€™autocorrÃ©lation est Ã  mÃªme dâ€™inclure des seuils pour lesquels la corrÃ©lation est significative (lignes pointillÃ©es bleues). ggAcf(lynx, ci = 0.95) + ggtitle(&quot;Lynx: AutocorrÃ©lation&quot;) Lâ€™analyse des seuils de signification de lâ€™autocorrÃ©lation indique sur la possibilitÃ© de conduire la sÃ©rie temporelle vers un processus de modÃ©lisation prÃ©dictive. Dans lâ€™exemple ci-dessus, on remarque quâ€™il existe des corrÃ©lations significatives pour un dÃ©calage de 4 Ã  6 donnÃ©es, mais que les donnÃ©es situÃ©es prÃ¨s les unes des autres pourraient Ãªtre plus difficiles Ã  modÃ©liser. Le test de Ljung-Box permet quant Ã  lui de tester si la sÃ©rie temporelle entiÃ¨re peut Ãªtre diffÃ©renciÃ©e dâ€™un bruit blanc. Box.test(lynx, lag = 20, type = &quot;Ljung-Box&quot;) ## ## Box-Ljung test ## ## data: lynx ## X-squared = 365.54, df = 20, p-value &lt; 2.2e-16 La probabilitÃ© que la sÃ©rie soit un bruit blanc est presque nulle. Notons que les tests statistiques sont aussi valides sur les dÃ©rivÃ©es des sÃ©ries temporelles. En outre, une dÃ©rivÃ©e premiÃ¨re de la sÃ©rie temporelle sur les dÃ©penses devient une sÃ©rie temporelle de la variation des dÃ©penses en restauration. plot_grid(autoplot(diff(auscafe)) + ggtitle(&quot;Restauration: SÃ©rie temporelle&quot;), ggAcf(diff(auscafe)) + ggtitle(&quot;Restauration: AutocorrÃ©lation&quot;), gglagplot(diff(auscafe)) + ggtitle(&quot;Restauration: Lag plot&quot;), ncol = 3) Box.test(diff(auscafe), lag = 16, type = &quot;Ljung-Box&quot;) ## ## Box-Ljung test ## ## data: diff(auscafe) ## X-squared = 647.11, df = 16, p-value &lt; 2.2e-16 Jusquâ€™Ã  prÃ©sent, nous nous sommes contentÃ©s dâ€™observer des sÃ©ries temporelles. LanÃ§ons-nous maintenant dans un domaine plus excitant. Source: ScÃ¨ne de Back to the future, Robert Zemeckis et and Bob Gale, 1985 11.3 ModÃ©lisation de sÃ©ries temporelles Lâ€™objectif gÃ©nÃ©ral de la modÃ©lisation de sÃ©rie temporelle est la prÃ©vision (forecast). La majoritÃ© des modÃ¨les se base sur des simulations de futurs possibles, desquels on pourra dÃ©duire une tendance centrale (point forecast) ainsi que des intervalles prÃ©visionnels. Il est important dâ€™insister sur le fait que la tendance centrale ne signifie pas que les donnÃ©es futures suivront cette tendance, mais que, selon les donnÃ©es et le modÃ¨le, la moitiÃ© des donnÃ©es devrait se retrouver sous la ligne, et lâ€™autre moitiÃ© au-dessus. De plus, la rÃ©gion de confiance dÃ©finie par les intervalles prÃ©visionnels signifient que par exemple 95% des points devraient se situer dans cette rÃ©gion. Une maniÃ¨re dâ€™Ã©valuer la performance dâ€™une prÃ©vision est de prÃ©voir des donnÃ©es auparavant observÃ©es Ã  partir des donnÃ©es qui les prÃ©cÃ¨dent. Ces valeurs sont dites lissÃ©es. Tout comme câ€™est le cas en rÃ©gression statistique, il est possible de dÃ©duire les rÃ©sidus du modÃ¨le. Pour les rÃ©gressions couvertes au chapitre 6, nous vÃ©rifions la validitÃ© du modÃ¨le en vÃ©rifiant si les rÃ©sidus Ã©taient distribuÃ©es normalement. Pour une sÃ©rie temporelle, on tend plutÃ´t Ã  vÃ©rifier si les rÃ©sidus forment un bruit blanc, câ€™est-Ã -dire quâ€™ils ne sont pas corrÃ©lÃ©s. De plus, pour Ã©viter dâ€™Ãªtre biaisÃ©es, leur moyenne doit Ãªtre de 0. De maniÃ¨re complÃ©mentaire pour la validitÃ© des intervalles prÃ©visionnels, mais non essentielle Ã  la validitÃ© du modÃ¨le, les rÃ©sidus devraient Ãªtre distribuÃ©s normalement et leur variance devrait Ãªtre constante (Hyndman et Athanasopoulos, 2018). Il est possible quâ€™un modÃ¨le remplisse toutes ces conditions, mais que sa prÃ©vision soit mÃ©diocre. Comme nous le verrons Ã©galement au chapitre 12, une prÃ©diction ou une prÃ©vision issue dâ€™un modÃ¨le ne peut pas Ãªtre Ã©valuÃ©e sur des donnÃ©es qui ont servies Ã  lisser le modÃ¨le. Pour vÃ©rifier une prÃ©vision temporelle, il faut sÃ©parer les donnÃ©es en deux sÃ©ries: une sÃ©rie dâ€™entraÃ®nement et une sÃ©rie de test (figure 11.5). Figure 11.5: Les points bleus dÃ©signe la sÃ©rie dâ€™entraÃ®nement et les points rouges, la sÃ©rie de test. Source de lâ€™image: Hyndman et Athanasopoulos, 1998. La sÃ©paration dans le temps entre la sÃ©rie dâ€™entraÃ®nement et la sÃ©rie de test se fait Ã  votre convenance, selon la disponibilitÃ© des donnÃ©es. Vous aurez toutefois avantage Ã  conserver davantage de donnÃ©es en entraÃ®nement (typiquement, 70%), et Ã  tout le moins, sÃ©parer au moins une fluctuation saisonniÃ¨re ou cyclique. La sÃ©rie dâ€™entraÃ®nement servira Ã  lisser le modÃ¨le pour en dÃ©couvrir les possibles structures. La sÃ©rie de test servira Ã  Ã©valuer sa performance sur des donnÃ©es obtenues, mais inconnues du modÃ¨le pour vÃ©rifier les structures dÃ©couvertes par le modÃ¨le. Lâ€™erreur prÃ©visionnelle est la diffÃ©rence entre une donnÃ©e observÃ©e en test et sa prÃ©vision (lâ€™Ã©quivalent des rÃ©sidus, mais appliquÃ©s sur des donnÃ©es indÃ©pendantes du modÃ¨le). La performance dâ€™une prÃ©vision peut Ãªtre Ã©valuÃ©e de diffÃ©rentes maniÃ¨res, mais lâ€™erreur moyenne absolue Ã©chelonnÃ©e (mean absolute scaled error, MASE) est conseillÃ©e puisquâ€™elle ne dÃ©pend pas de la mÃ©trique de la quantitÃ© produite: plus la MASE se rapproche de zÃ©ro, meilleure est la prÃ©vision. Plusieurs mÃ©thodes de prÃ©vision sont possibles. Nous en couvrirons 3 dans ce chapitre: la mÃ©thode naÃ¯ve, la mÃ©thode SES et la mÃ©thode ARIMA. Nous allons couvrir les diffÃ©rents aspects de la modÃ©lisation des sÃ©ries temporelles Ã  travers lâ€™utilisation de ces mÃ©thodes. 11.3.1 MÃ©thode naÃ¯ve La mÃ©thode naÃ¯ve dÃ©finit la valeur suivante selon la valeur prÃ©cÃ©dente (fonction forecast::naive()), ou la valeur de la saison prÃ©cÃ©dente (fonction forecast::snaive()). Ces fonctions du module forecast incluent un composante alÃ©atoire pour simuler des occurrences futures selon des marches alÃ©atoires (random walks), oÃ¹ chaque valeur suivante est simulÃ©e alÃ©atoirement, considÃ©rant la valeur prÃ©cÃ©dente. Nous tenterons de prÃ©voir les dÃ©bits de la riviÃ¨re ChaudiÃ¨re. Ceux-ci Ã©tant caractÃ©risÃ© par des fluctuations saisonniÃ¨res, mieux vaut utiliser snaive(). Mais auparavant, sÃ©parons la sÃ©rie en sÃ©rie dâ€™entraÃ®nement et sÃ©rie de test. flow_ts &lt;- hydrometeo_monthly_ts[, 1] flow_ts_train &lt;- window(flow_ts, start = 1998, end = 2005.999) flow_ts_test &lt;- window(flow_ts, start = 2006) LanÃ§ons la modÃ©lisation sur les donnÃ©es dâ€™entraÃ®nement. hm_naive &lt;- snaive(flow_ts_train, h = 24) autoplot(hm_naive) + autolayer(fitted(hm_naive)) + autolayer(flow_ts_test, color = rgb(0, 0, 0, 0.6)) + labs(x = &quot;AnnÃ©e&quot;, y = &quot;DÃ©bit&quot;) ## Warning: Removed 12 rows containing missing values (geom_path). Le graphique prÃ©cÃ©dent montre que la prÃ©vision naÃ¯ve (en rose) prend bien la valeur observÃ©e au cycle prÃ©cÃ©dent (en noir). Les donnÃ©es de test sont en gris transparent. Notons que la prÃ©sence de dÃ©bit nÃ©gatifs pourrait Ãªtre Ã©vitÃ©e en utilisant une transformation logarithmique du dÃ©bit prÃ©alablement Ã  la modÃ©lisation. Voyons maintenant lâ€™analyse des rÃ©sidus avec la fonction forecast::checkresiduals(). checkresiduals(hm_naive) ## ## Ljung-Box test ## ## data: Residuals from Seasonal naive method ## Q* = 34.903, df = 19, p-value = 0.01435 ## ## Model df: 0. Total lags used: 19 La p-value Ã©tant de 0.01546, il est peu probable que les rÃ©sidus forment un bruit blanc. Les rÃ©sidus contiennent de lâ€™autocorrÃ©lation, ce qui devrait Ãªtre Ã©vitÃ©. Ceci est toutefois dÃ» Ã  un seul point allant au-delÃ  du seuil de 0.05, que lâ€™on peut observer sur le graphique dâ€™autocorrÃ©lation. Le graphique de la distribution des rÃ©sidus montre des valeurs aberrantes, ainsi quâ€™une distribution plutÃ´t pointue, qui donnerait un test de Kurtosis probablement Ã©levÃ©. shapiro.test(residuals(hm_naive)) # non-normal si p-value &lt; seuil (0.05) ## ## Shapiro-Wilk normality test ## ## data: residuals(hm_naive) ## W = 0.93698, p-value = 0.0004733 library(&quot;e1071&quot;) kurtosis(residuals(hm_naive), na.rm = TRUE) # le rÃ©sultat d&#39;un test de kurtosis sur une distribution normale devrait Ãªtre de 0. ## [1] 2.909277 Pas de panique, les prÃ©dictions peuvent nÃ©anmoins Ãªtre valides: seulement, les intervalles prÃ©visionnels pourraient Ãªtre trop vagues ou trop restreintes: Ã  prendre avec des pincettes. Lâ€™Ã©valuation du modÃ¨le peut Ãªtre effectuÃ©e avec la fonction forecast::accuracy(), qui dÃ©tecte automatiquement la sÃ©rie dâ€™entraÃ®nement et la sÃ©rie de test si on lui fournit la sÃ©rie entiÃ¨re (ici lâ€™objet flow_ts). accuracy(hm_naive, flow_ts) ## ME RMSE MAE MPE MAPE MASE ACF1 Theil&#39;s U ## Training set 0.5952819 80.24256 54.53431 -57.045415 95.60936 1.000000 0.1669526053 NA ## Test set -1.0165543 75.18877 57.10499 -2.645333 59.10844 1.047139 -0.0006850245 0.3877041 La mÃ©thode naÃ¯ve est rarement utilisÃ©e en pratique autrement que comme standard par rapport auquel la performance dâ€™autres modÃ¨les est Ã©valuÃ©e. 11.3.2 MÃ©thode SES Alors que la mÃ©thode naÃ¯ve donne une crÃ©dibilitÃ© complÃ¨te Ã  la valeur prÃ©cÃ©dente (ou au cycle prÃ©cÃ©dent), la mÃ©thode SES (simple exponential smoothing) donne aux valeurs prÃ©cÃ©dentes des poids exponentiellement dÃ©croissants selon leur anciennetÃ©. La prÃ©vision par SES sera une moyenne pondÃ©rÃ©e des derniÃ¨res observations, en donnant plus de poids sur les observations plus rapprochÃ©es. MathÃ©matiquement, la mÃ©thode SES est dÃ©crite ainsi. \\[\\hat{y}_{t + h|t} = \\alpha y_t + \\alpha\\left( 1-\\alpha \\right) y_{t-1} + \\alpha\\left( 1-\\alpha \\right)^2 y_{t-2} + ...\\] oÃ¹ \\(\\hat{y}_{t + h|t}\\) est la prÃ©vision de \\(y\\) au temps \\(t + h|t\\), qui est le dÃ©calage de \\(h\\) Ã  partir de la derniÃ¨re mesure au temps \\(t\\). Le paramÃ¨tre \\(\\alpha\\) prend une valeur de 0 Ã  1, et dÃ©crit la distribution des poids. Une valeur de \\(\\alpha\\) Ã©levÃ© donnera davantage de poids aux Ã©vÃ©nements rÃ©cents. La somme de tous poids \\(\\alpha\\) tend vers 1 lorsque les pas de temps prÃ©cÃ©dents tendent vers lâ€™\\(\\infty\\). Une autre maniÃ¨re dâ€™exprimer lâ€™Ã©quation est de la segmenter en deux: une pour la prÃ©vision en fonction du niveau (level, le modÃ¨le), une autre pour dÃ©crire comment le niveau change au fil du temps. Description Ã‰quation PrÃ©vision \\(\\hat{y}_{t + h|t} = l_t\\) Niveau \\(l_t = \\alpha y_t + \\alpha\\left( 1-\\alpha \\right) l_{t-1}\\) ExprimÃ©e ainsi, la prÃ©vision nâ€™exprimera aucune tendance ni fluctuation. Il sâ€™agira dâ€™une projection jusquâ€™Ã  lâ€™infini de la moyenne des observations prÃ©cÃ©dentes pondÃ©rÃ©e par leur dÃ©calage. 11.3.2.1 SES de base Prenons les donnÃ©es de la NASA sur lâ€™indice de tempÃ©rature terre-ocÃ©an, qui dÃ©crit un dÃ©calage par rapport Ã  la moyenne des tempÃ©ratures globales observÃ©es entre de 1951 Ã  1980. La mÃ©thode SES est appelÃ©e par la fonction forecast::ses(), de la mÃªme maniÃ¨re quâ€™on lâ€™a fait prÃ©cÃ©demment avec la mÃ©thode naÃ¯ve. loti_ts &lt;- read_csv(&quot;data/09_nasa.csv&quot;) %&gt;% pull(LOTI) %&gt;% ts(., start = 1880, frequency = 1) #loti_ts &lt;- window(loti_ts, start = 1950) loti_ts_tr &lt;- window(loti_ts, end = 2004) loti_ses &lt;- ses(loti_ts_tr, h = 20, alpha = 0.5) autoplot(loti_ses) + autolayer(fitted(loti_ses)) Note. Les prÃ©visions climatiques sont effectuÃ©es par des modÃ¨les bien plus complexes que ce que nous voyons ici. Les prÃ©visions du GIEC agrÃ¨gent des tendances localisÃ©es et incluent une batterie de covariables, dont la plus Ã©vidente est la concentration en CO2 dans lâ€™atmosphÃ¨re. Il sâ€™agit seulement dâ€™un exemple dâ€™application. 11.3.2.2 SES avec tendance La prÃ©vision a peu dâ€™intÃ©rÃªt, Ã©tant donnÃ©e quâ€™elle nâ€™inclut pas de tendance. Or, nous pouvons en ajouter une Ã  lâ€™Ã©quation. Ainsi exprimÃ©e, la tendance changera aussi au fil du temps. Description Ã‰quation PrÃ©vision \\(\\hat{y}_{t + h|t} = l_t + \\left( \\phi + \\phi^2 + ... + \\phi^h \\right) \\times b_t\\) Niveau $l_t = y_t + ( 1-) ( l_{t-1} + b_{t-1} ) $ Tendance \\(b_t = \\beta^* \\left( l_t - l_{t-1} \\right) + (1-\\beta^*) \\phi b_{t-1}\\) Le paramÃ¨tre \\(\\beta^*\\) dÃ©crit la vitesse Ã  laquelle la tendance peut changer, de 0 oÃ¹ la pente ne change pas Ã  1 oÃ¹ la pente change rapidement. Le paramÃ¨tre \\(\\phi\\) adouci la pente en sâ€™Ã©loignant de la derniÃ¨re mesure. Un tendant vers 0 gÃ©nÃ©rera un fort adoucissement, alors quâ€™un tendant vers 1 ne gÃ©nÃ©rera pas dâ€™adoucissement. Il peut Ãªtre difficile de dÃ©terminer les paramÃ¨tres de lissage \\(\\alpha\\), \\(\\beta^*\\) et \\(\\phi\\), ainsi que les paramÃ¨tres dâ€™Ã©tat \\(l_0\\) et \\(b_0\\). La fonction de forecast::holt() permet de les estimer automatiquement. loti_holt_dF &lt;- holt(loti_ts_tr, damped = FALSE, h = 100) loti_holt_dT &lt;- holt(loti_ts_tr, damped = TRUE, h = 100) plot_grid(autoplot(loti_holt_dF), autoplot(loti_holt_dT)) loti_holt_dF$model$par ## alpha beta l b ## 0.4052862918 0.0001000051 -0.2170967540 0.0052550677 loti_holt_dT$model$par ## alpha beta phi l b ## 0.4843654438 0.0001000061 0.8286096396 -0.1096315698 -0.0332523695 Dans ce cas, lâ€™optimisation de \\(\\phi\\) lui donne une valeur de 0.8, une valeur suffisamment faible pour que lâ€™adoucissement soit fort. Vous obtiendrez une valeur de \\(\\phi\\) plus Ã©levÃ©e en ne considÃ©rant que les donnÃ©es obtenues depuis 1950 (en dÃ©commentant loti_ts &lt;- window(loti_ts, start = 1950), plus haut). 11.3.2.3 SES avec fluctuation saisonniÃ¨re Dâ€™autres paramÃ¨tres peuvent Ãªtre ajoutÃ©s pour de tenir compte des fluctuations saisonniÃ¨res (les fluctuations cycliques sont plus difficiles Ã  modÃ©liser) de maniÃ¨re additive ou multiplicative. Voici la modification apportÃ©e pour la modÃ©lisation additive, en laissant tomber lâ€™adoucissement. Description Ã‰quation PrÃ©vision \\(\\hat{y}_{t + h|t} = l_t + h \\times b_t + s_{t-m+h_m^+}\\) Niveau $l_t = (y_t - s_{t-m} ) + ( 1-) ( l_{t-1} + b_{t-1} ) $ Tendance \\(b_t = \\beta^* \\left( l_t - l_{t-1} \\right) + (1-\\beta^*) b_{t-1}\\) Saison \\(s_t = \\gamma \\left( y_t - l_{t-1} - b_{t-1} \\right) + (1-\\gamma) s_{t-m}\\) oÃ¹ \\(m\\) est la pÃ©riodicitÃ© des fluctuations saisonniÃ¨re, par exemple 4 pour quatre saisons annuelles et \\(\\gamma\\) est un paramÃ¨tre de la portion saisonniÃ¨re, qui, tout comme un effet alÃ©atoire en biostatistiques, fluctue autour de zÃ©ro. La variante multiplicative multiplie la prÃ©vision par un facteur plutÃ´t que dâ€™imposer un dÃ©calage. La mathÃ©matique nâ€™est pas prÃ©sentÃ©e ici pour plus de simplicitÃ© (consulter Hyndman et Athanasopoulos (2018), chapitre 7.3 pour plus de dÃ©tails). Dans le cas multiplicatif, lâ€™effet saisonnier fluctue autour de 1. Si lâ€™amplitude de la fluctuation sâ€™accroÃ®t au fil de la sÃ©rie temporelle, la mÃ©thode multiplicative donnera probablement de meilleurs rÃ©sultats. La fonction que nous utiliserons pour les SES-saisonniers est forecast::hw(). Les donnÃ©es de la NASA ne sont pas saisonniÃ¨res (frequency(loti_ts) donne 1). flow_hw &lt;- hw(flow_ts_train, damped = TRUE, h = 12*3, seasonal = &quot;additive&quot;) autoplot(flow_hw) + autolayer(fitted(flow_hw)) 11.3.2.4 Automatiser la prÃ©vision avec les SES Lâ€™erreur du modÃ¨le peut aussi Ãªtre calculÃ©e de sorte quâ€™elle soit constante ou augmente selon le niveau (ou dÃ©calage). Nous avons donc plusieurs types de modÃ¨les de la famille SES. Tendance: [sans tendance, tendance additive, tendance adoucie] Saison: [sans saison, saison additive, saison multiplicative] Erreur: [erreur additive, erreur multiplicative] Lequel choisir? Encore une fois, on peut laisser R optimiser notre choix avec un modÃ¨le ETS (error, tend and seasonnal). Lâ€™optimisation est lancÃ©e avec la fonction forecast::ets(). flow_model &lt;- ets(flow_ts_train) flow_model ## ETS(M,N,M) ## ## Call: ## ets(y = flow_ts_train) ## ## Smoothing parameters: ## alpha = 0.0104 ## gamma = 1e-04 ## ## Initial states: ## l = 127.508 ## s = 0.6511 0.9399 0.7981 0.4272 0.6253 0.8872 ## 0.6594 1.2741 3.7317 1.5211 0.1878 0.2971 ## ## sigma: 0.6065 ## ## AIC AICc BIC ## 1222.469 1228.469 1260.935 Le modÃ¨le retenu est un ETS(M,N,M), dÃ©finissant dans lâ€™ordre le type dâ€™erreur, de tendance et de saison selon A pour additif, M pour multiplicatif et N pour lâ€™absence. Nous avons une erreur de type M (multiplicative), une tendance de type N (sans tendance) et une saison de type M (multiplicative). Lâ€™absence de valeur pour phi indique que lâ€™adoucissement nâ€™est probablement pas nÃ©cessaire. Nous pouvons visualiser lâ€™Ã©volution des diffÃ©rentes composantes. autoplot(flow_model) Dans un modÃ¨le sans tendance, avec saisonnalitÃ© multiplicative, les donnÃ©es levels sont multipliÃ©es par les donnÃ©es season pour obtenir la prÃ©vision. MalgrÃ© lâ€™absence de tendance dans le modÃ¨le, il semble que le dÃ©bit a diminuÃ© de 2000 Ã  2003 entre deux Ã©tats stables de 1998 Ã  2000 et de 2003 Ã  2006. La fonction forecast::ets() gÃ©nÃ¨re un modÃ¨le, mais pas de prÃ©diction. Pour obtenir une prÃ©diction, nous devons utiliser la fonction forecast::forecast(), que jâ€™utiliserai ici en mode tidyverse. flow_ets &lt;- flow_ts_train %&gt;% ets() flow_fc &lt;- flow_ets %&gt;% forecast() flow_fc %&gt;% autoplot() Lâ€™analyse dâ€™exactitude et celle des rÃ©sidus sont toutes aussi pertinentes. La premiÃ¨re est effectuÃ©e sur la prÃ©vision, et la seconde sur le modÃ¨le. accuracy(flow_fc, flow_ts) ## ME RMSE MAE MPE MAPE MASE ACF1 Theil&#39;s U ## Training set -13.346098 59.33195 44.55719 -90.26010 110.68741 0.8170487 0.1612394 NA ## Test set 3.768712 69.95479 55.55668 -24.65164 62.94846 1.0187472 0.2452020 0.6990499 checkresiduals(flow_ets) ## ## Ljung-Box test ## ## data: Residuals from ETS(M,N,M) ## Q* = 27.068, df = 5, p-value = 5.533e-05 ## ## Model df: 14. Total lags used: 19 Il est peu probable que les rÃ©sidus aient Ã©tÃ© gÃ©nÃ©rÃ©s par un bruit blanc, indiquant quâ€™il existe une structure dans les donnÃ©es qui nâ€™a pas Ã©tÃ© capturÃ©e par le modÃ¨le. Exercice. ModÃ©liser la sÃ©rie temporelle lynx avec forecast::ets(). Que se passe-t-il? 11.3.2.5 PrÃ©traitement des donnÃ©es Jâ€™ai spÃ©cifiÃ© plus haut que les donnÃ©es de dÃ©bit pourraient avantageusement Ãªtre transformÃ©es avec un logarithme pour Ã©viter les prÃ©dictions de dÃ©bits nÃ©gatifs. Dâ€™autres types de transformation peuvent Ãªtre utilisÃ©es, comme la racine carrÃ©e ou cubique, lâ€™opposÃ©e de lâ€™inverse (\\(-1/x\\)) ou les transformations compositionnelles (chapitre 8). La transformation Box-Cox est aussi largement utilisÃ©e pour sa polyvalence. \\[ w = \\begin{cases} ln(y_t) &amp;\\text{if } \\lambda = 0 \\\\ \\frac{y_t - 1}{\\lambda} &amp;\\text{if } \\lambda \\neq 0 \\end{cases} \\] \\(\\lambda = 1\\): pas de transformation \\(\\lambda = 1/2\\): ressemble Ã  \\(\\sqrt{y_t}\\) \\(\\lambda = 1/3\\): ressemble Ã  \\(\\sqrt[3]{y_t}\\) \\(\\lambda = 0\\): log naturel \\(\\lambda = -1\\): ressemble Ã  \\(1/y_t\\) La fonction forecast::BoxCox.lambda() estime la valeur optimale de \\(\\lambda\\). BoxCox.lambda(flow_ts_train) ## [1] 0.784101 Cette valeur peut Ãªtre imputÃ©e Ã  lâ€™argument lambda de la fonction forecast::ets(). Dans notre cas, nous dÃ©sirions plutÃ´t une transformation logarithmique. ConsÃ©quemment, nous utilisons lambda = 0. flow_ts_train %&gt;% ets(lambda = 0) %&gt;% forecast() %&gt;% autoplot() Les erreurs ne franchissent pas le 0, mais sont vraisemblablement surestimÃ©es lors des sommets. Notez que R sâ€™occupe de la transformation retour. La diffÃ©renciation est aussi une forme de prÃ©traitement. La diffÃ©renciation (fonction base::diff()) consiste en la soustraction de la valeur prÃ©cÃ©dente Ã  la valeur suivante. La valeur prÃ©cÃ©dente peut Ãªtre dÃ©calÃ©e Ã  la valeur de la pÃ©riode de lâ€™unitÃ© temporelle prÃ©cÃ©dente, par exemple le mois de mars de lâ€™annÃ©e prÃ©cÃ©dente. Un objectif de la diffÃ©renciation est de rendre la sÃ©rie temporelle stationnaire en termes de tendance et de fluctuation saisonniÃ¨re, de sorte que la sÃ©rie diffÃ©renciÃ©e se comporte comme un bruit blanc. plot_grid(flow_ts_train %&gt;% autoplot() + ggtitle(&quot;DÃ©bit&quot;), loti_ts_tr %&gt;% autoplot() + ggtitle(&quot;LOTI&quot;), flow_ts_train %&gt;% diff(., lag = 12) %&gt;% autoplot() + ggtitle(&quot;DÃ©bit avec diffÃ©renciation saisonniÃ¨re&quot;), loti_ts_tr %&gt;% diff(., lag = 1) %&gt;% autoplot() + ggtitle(&quot;LOTI avec diffÃ©renciation d&#39;ordre 1&quot;)) 11.3.3 La mÃ©thode ARIMA Un modÃ¨le ARIMA, lâ€™acronyme de lâ€™anglais auto-regressive integrated moving average, est une combinaison de trois parties: AR-I-MA. Lâ€™autorÃ©gression consiste en une rÃ©gression linÃ©aire dont la variable rÃ©ponse \\(y_t\\) est la variable Ã  lâ€™instant \\(t\\) et les variables explicatives sont les variables aux instants prÃ©cÃ©dents. Pour un nombre \\(p\\) de pÃ©riodes prÃ©cÃ©dentes, nous obtenons une rÃ©gression linÃ©aire typique. \\[y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + ... + \\phi_p y_{t-p} + \\epsilon_t\\] oÃ¹ \\(\\epsilon_t\\) est lâ€™erreur sur la prÃ©diction. La partie concernant la moyenne mobile est une rÃ©gression non pas sur les observations, mais sur les erreurs. ConsidÃ©rant les \\(q\\) erreurs prÃ©cÃ©dentes, nous obtenons \\[y_t = c + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + ... + \\theta_q \\epsilon_{t-q} + \\epsilon_t\\] La somme de lâ€™autorÃ©gression et de la moyenne mobile donne un modÃ¨le ARMA. Le I de ARIMA, mis pour integrated, est le contraire de la diffÃ©renciation, que jâ€™ai prÃ©sentÃ© Ã  la fin de la section sur les SES. Puisque la sÃ©rie temporelle doit Ãªtre stationnaire pour effectuer lâ€™ARMA, nous devons diffÃ©rencier la sÃ©rie un nom \\(d\\) de fois avant de procÃ©der Ã  lâ€™autorÃ©gression et au calcul de la moyenne mobile. Nous obtenons ainsi une ARIMA dâ€™ordres \\(p\\), \\(d\\) et \\(q\\), notÃ©e \\(ARIMA(p,d,q)\\). Nous devons aussi statuer si \\(c\\) (lâ€™intercept ou le drift) doit Ãªtre ou non considÃ©rÃ© come nul. Ces ordres peuvent Ãªtre spÃ©cifiÃ©s dans la fonction telle que forecast::Arima(order = c(0, 1, 1), include.constant = TRUE). Toutefois, il est possible de les optimiser grÃ¢ce Ã  la fonction forecast::auto.arima(). Tout comme les sorties de forecast::ets(), forecast::auto.arima() fourni le modÃ¨le, mais pas les prÃ©dictions: la fonction forecast::forecast() doit Ãªtre lancÃ©e pour obteir la prÃ©diction. loti_arima &lt;- loti_ts_tr %&gt;% auto.arima() loti_arima %&gt;% forecast(h = 30) %&gt;% autoplot() summary(loti_arima) ## Series: . ## ARIMA(1,1,3) with drift ## ## Coefficients: ## ar1 ma1 ma2 ma3 drift ## -0.9405 0.6260 -0.6019 -0.3716 0.0060 ## s.e. 0.0522 0.0976 0.0847 0.0846 0.0031 ## ## sigma^2 estimated as 0.01036: log likelihood=109.53 ## AIC=-207.06 AICc=-206.35 BIC=-190.14 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set -0.0007946749 0.09932376 0.0824145 22.62973 65.41129 0.8925238 -0.0221428 Le sommaire du modÃ¨le spÃ©cifie une \\(ARIMA(1,1,3)\\) en utilisant lâ€™intercept \\(c\\) (with drift). Lorsque lâ€™on compte prÃ©dire des sÃ©ries saisonniÃ¨res, nous devons ajouter un nouveau jeu dâ€™ordres \\((P,D,Q)m\\), oÃ¹ \\(P\\), \\(D\\) et \\(Q\\) sont Ã©quivalents Ã  leurs minuscules, mais portent sur des dÃ©calages saisonniers et non pas des dÃ©calages dâ€™unitÃ©s de temps. Lâ€™ordre \\(m\\) est le nombre de pÃ©riodes Ã  considÃ©rer par unitÃ© temporelle, par exemple 12 mois par an. Bonne nouvelle: forecast::auto.arima() automatise le tout. Nous pouvons utiliser lambda = 0 pour effectuer une transformation logarithmique. flow_arima &lt;- flow_ts_train %&gt;% auto.arima(lambda = 0) flow_arima %&gt;% forecast(h = 36) %&gt;% autoplot() summary(flow_arima) ## Series: . ## ARIMA(1,0,0)(2,1,0)[12] ## Box Cox transformation: lambda= 0 ## ## Coefficients: ## ar1 sar1 sar2 ## 0.3029 -0.4913 -0.2687 ## s.e. 0.1074 0.1196 0.1186 ## ## sigma^2 estimated as 0.6561: log likelihood=-101.88 ## AIC=211.77 AICc=212.27 BIC=221.49 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set 9.038874 71.47442 40.46424 -34.75026 64.84494 0.7419961 -0.1823408 Le sommaire du modÃ¨le, ARIMA(1,0,0)(2,1,0)[12] sous le format \\(ARIMA(p,d,q)(P,D,Q)m\\), retourne automatiquement une pÃ©riode de 12 mois avec une diffÃ©renciation saisonniÃ¨re mais sans diffÃ©renciation ordinaire, excluant la moyenne mobile dans les deux cas. Exercice. Il est toujours pertinent dâ€™effectuer lâ€™analyse des rÃ©sidusâ€¦ 11.3.4 Les modÃ¨les dynamiques Jâ€™ai notÃ© prÃ©cÃ©demment que lâ€™Ã©volution du climat tient compte dâ€™une sÃ©rie de covariables explicatives. De mÃªme, le dÃ©bit dans la riviÃ¨re ChaudiÃ¨re nâ€™est pas un effet de la saison, mais de son environnement (climat, changements dans la morphologie du paysage, utilisation de lâ€™eau, etc.). La prÃ©vision du dÃ©bit aura avantage Ã  considÃ©rer ces covariables. Lâ€™ARIMA peut accueillir des covariables en modÃ©lisant le terme dâ€™erreur, \\(\\epsilon_t\\) en fonction de sÃ©ries temporelles conjointes. Le dÃ©bit mensuel de la riviÃ¨re ChaudiÃ¨re peut Ãªtre modÃ©lisÃ© en fonction de la tempÃ©rature moyenne mensuelle et des prÃ©cipitations totales mensuelles en ajoutant lâ€™argument xreg Ã  la fonction forecast::auto.arima(). Lâ€™argument consiste en la matrice temporelle des variables explicatives. Notez que forecast::auto.arima() ne fonctionne pas (encore?) avec lâ€™interface-formule de R, mais que lâ€™on peut se dÃ©brouiller en transformant en sÃ©rie temporelle la sortie de la fonction base::model.matrix(), qui elle peut accueillir une formule. hm_tr &lt;- window(hydrometeo_monthly_ts, end = c(2004, 12)) hm_te &lt;- window(hydrometeo_monthly_ts, start = c(2005, 1)) flow_darima &lt;- auto.arima(y = hm_tr[, &quot;DÃ©bit&quot;], xreg = hm_tr[, c(&quot;total_precip&quot;, &quot;mean_temp&quot;)], lambda = 0) summary(flow_darima) ## Series: hm_tr[, &quot;DÃ©bit&quot;] ## Regression with ARIMA(1,0,0)(0,1,1)[12] errors ## Box Cox transformation: lambda= 0 ## ## Coefficients: ## ar1 sma1 total_precip mean_temp ## 0.3213 -0.6236 0.0028 -0.0215 ## s.e. 0.1148 0.1828 0.0017 0.0451 ## ## sigma^2 estimated as 0.5392: log likelihood=-80.87 ## AIC=171.74 AICc=172.65 BIC=183.12 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set 3.046006 64.76333 37.60995 -31.06861 57.3032 0.7149168 -0.3068227 Note. Pour obtenir des rÃ©sultats plus prÃ©cis, mais dont les rÃ©sultats seront plus longs Ã  venir, spÃ©cifiez lâ€™argument stepwise = FALSE dans la fonction forecast::auto.arima(). Les coefficients sur les covariables sont interprÃ©tables dans lâ€™Ã©chelle de la prÃ©vision transformÃ©e. Ainsi, 1 mm de prÃ©cipitation par mois augmentera le logarithme naturel du dÃ©bit augmente de 0.0028. De mÃªme, 1 Â°C de tempÃ©rature moyenne diminuera le logarithme naturel du dÃ©bit augmente de 0.0215: notez que lâ€™erreur standard sur ce coefficient Ã©tant trÃ¨s Ã©levÃ©e, le coefficient nâ€™est Ã  premiÃ¨re vue pas diffÃ©rent de 0. La tempÃ©rature moyenne aurait avantage Ã  Ãªtre remplacÃ©e par un meilleur indicateur incluant les pÃ©riodes dâ€™accumulation de neige et de leur fonte. Pas mal doc? Source: ScÃ¨ne de Back to the future, Robert Zemeckis et and Bob Gale, 1985 La prÃ©vision dâ€™un modÃ¨le dynamique demandera les sÃ©ries temporelles des covariables, qui peuvent elles-mÃªmes Ãªtre modÃ©lisÃ©es ou Ãªtre issues de simulations. Dans notre cas, nous pouvons utiliser la sÃ©rie de test. flow_darima %&gt;% forecast(xreg = hm_te[, c(&quot;total_precip&quot;, &quot;mean_temp&quot;)]) %&gt;% autoplot() checkresiduals(flow_darima) ## ## Ljung-Box test ## ## data: Residuals from Regression with ARIMA(1,0,0)(0,1,1)[12] errors ## Q* = 16.853, df = 13, p-value = 0.2061 ## ## Model df: 4. Total lags used: 17 11.3.5 Les modÃ¨les TBATS Les modÃ¨les TBATS (Hyndman et Athanasopoulos, 2018) combinent tout ce que lâ€™on a vu jusquâ€™Ã  prÃ©sent, Ã  lâ€™exception notable des covariables, dans une interface automatisÃ©e. Lâ€™automatisation a lâ€™avantage dâ€™une utilisation rapide, mais donne parfois des prÃ©dictions erronÃ©es. lynx_tbats &lt;- lynx %&gt;% tbats() lynx_tbats_f &lt;- lynx_tbats %&gt;% forecast() lynx_tbats_f %&gt;% autoplot() summary(lynx_tbats_f) ## ## Forecast method: BATS(0.159, {5,1}, 0.833, -) ## ## Model Information: ## BATS(0.159, {5,1}, 0.833, -) ## ## Call: tbats(y = .) ## ## Parameters ## Lambda: 0.159431 ## Alpha: 0.4096962 ## Beta: 0.06308072 ## Damping Parameter: 0.833291 ## AR coefficients: 1.166918 -0.79967 0.178396 -0.165326 -0.174982 ## MA coefficients: -0.391172 ## ## Seed States: ## [,1] ## [1,] 15.0399753 ## [2,] 0.4123534 ## [3,] 0.0000000 ## [4,] 0.0000000 ## [5,] 0.0000000 ## [6,] 0.0000000 ## [7,] 0.0000000 ## [8,] 0.0000000 ## attr(,&quot;lambda&quot;) ## [1] 0.1594313 ## ## Sigma: 1.542751 ## AIC: 1956.137 ## ## Error measures: ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set 52.88873 827.169 496.3222 -21.21278 50.42654 0.5973607 -0.04569254 ## ## Forecasts: ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 1935 3144.2286 1772.9365 5314.924 1279.84707 6900.473 ## 1936 2272.7187 842.6749 5351.691 464.08249 8064.647 ## 1937 1466.3925 410.3827 4224.271 185.16011 6932.120 ## 1938 981.1883 227.2540 3210.040 88.86649 5547.989 ## 1939 807.6476 170.9933 2800.598 62.30711 4954.037 ## 1940 902.9068 196.5571 3075.407 73.22121 5401.634 ## 1941 1286.8205 307.5223 4124.040 123.20099 7068.774 ## 1942 1964.6976 520.9678 5872.628 225.83614 9784.659 ## 1943 2722.5747 765.2794 7819.328 346.39977 12815.672 ## 1944 3100.2748 837.7743 9148.331 368.32463 15162.079 Le sommaire du modÃ¨le le type de modÃ¨le sÃ©lectionnÃ© ainsi que ses paramÃ¨tres. Le titre du graphique en donne aussi un aperÃ§u: BATS(0.159, {5,1}, 0.833, -): Le coefficient lambda utilisÃ© est 0.159. Le {5, 1} signifie que p = 5 et et q = 1. Lâ€™adoucissement est de 0.833 Aucune pÃ©riode nâ€™est incluse Lâ€™approche TBATS performe bien pour les donnÃ©es Ã  fluctuations cycliques. Toutefois, les intervalles prÃ©visionnels son souvent trop larges et lâ€™optimisation peut Ãªtre longue. 11.4 Pour terminerâ€¦ Nous avons vu comment manipuler des sÃ©ries temporelles avec dplyr et le format de base ts. Le nouveau format de sÃ©rie temporelle du module tsibble permet des manipulations dans un flux de travail plus conforme au tidyverse. De mÃªme, la nouvelle mouture du module forecast nommÃ©e fable, rÃ©Ã©crite vers lâ€™approche tidyverse, offrira des fonctions permettant notamment une hiÃ©rarchisation dans les fluctuations saisonniÃ¨res, par exemple des cycles journaliers enchÃ¢ssÃ©s dans des cycles hebdomadaires, enchÃ¢ssÃ©s dans des cycles trimestriels. Le module prophet, distribuÃ© par Facebook en mode open source, gagne en popularitÃ©. Bien quâ€™il soit rÃ©putÃ© pour offrir des prÃ©visions fiables, ses bases mathÃ©matiques me semblent insuffisamment documentÃ©es. Fait intÃ©ressant: prophet est en mesure dâ€™effectuer des prÃ©visions en mode dynamique. Quoi quâ€™il en soit, jâ€™ai favorisÃ© des modules plus matures et mieux documentÃ©s, qui pourront vous servir de tremplin vers les nouveaux modules. Maintenant, le futur vous appartient. "],
["chapitre-ml.html", "12 Introduction Ã  lâ€™autoapprentissage 12.1 Lexique 12.2 DÃ©marche 12.3 Lâ€™autoapprentissage en R 12.4 Algorithmes", " 12 Introduction Ã  lâ€™autoapprentissage ï¸Â Objectifs spÃ©cifiques: Ã€ la fin de ce chapitre, vous saurez Ã©tablir un plan de modÃ©lisation par autoapprentissage saurez dÃ©finir le sous-apprentissage et le surapprentissage serez en mesure dâ€™effectuer un autoapprentissage avec les techniques des k-proches voisins, les arbres de dÃ©cision, les forÃªts alÃ©atoires, les rÃ©seaux neuronnaux et les processus gaussiens Plusieurs cas dâ€™espÃ¨ces en sciences et gÃ©nies peuvent Ãªtre approchÃ©s en liant un variable avec une ou plusieurs autres Ã  lâ€™aide de rÃ©gressions linÃ©aires, polynomiales, sinusoÃ¯dales, exponentielle, sigmoÃ¯dales, etc. Encore faut-il sâ€™assurer que ces formes prÃ©Ã©tablies reprÃ©sentent le phÃ©nomÃ¨ne de maniÃ¨re fiable. Lorsque la forme de la rÃ©ponse est difficile Ã  envisager, en particulier dans des cas non-linÃ©aires ou impliquant plusieurs variables, on pourra faire appel Ã  des modÃ¨les dont la structure nâ€™est pas contrÃ´lÃ©e par une Ã©quation rigide gouvernÃ©e par des paramÃ¨tres (comme la pente ou lâ€™intercept). Lâ€™autoapprentissage, apprentissage automatique, ou machine learning, vise Ã  dÃ©tecter des structures complexes Ã©mergeant dâ€™ensembles de donnÃ©es Ã  lâ€™aide des mathÃ©matiques et de processus automatisÃ©s afin de prÃ©dire lâ€™Ã©mergence de futures occurrences. Comme ensemble de techniques empiriques, lâ€™autoapprentissage est un cas particulier de lâ€™intelligence artificielle, qui elle inclut aussi les mÃ©canismes dÃ©terministes et des ensembles dâ€™opÃ©rations logiques. Par exemple, les premiers ordinateurs Ã  compÃ©titionner aux Ã©checs se basaient sur des rÃ¨gles de logique (si la reine noire est positionnÃ©e en c3 et quâ€™un le fou blanc est en position f6 et que â€¦ alors bouge la tour en g5 - jâ€™Ã©cris nâ€™importe quoi). Un jeu simple dâ€™intelligence artificielle consiste Ã  lancer une marche alÃ©atoire, par exemple bouger Ã  chaque pas dâ€™une distance au hasard en x et y, puis de recalculer le pas sâ€™il arrive dans une boÃ®te dÃ©finie (figure 12.1). Dans les deux cas, il sâ€™agit dâ€™intelligence artificielle, mais pas dâ€™autoapprentissage. Figure 12.1: Petite tortue, nâ€™entre pas dans la boÃ®te! Lâ€™autoapprentissage passera davantage par la simulation de nombreuses parties et dÃ©gagera la structure optimale pour lâ€™emporter considÃ©rant les positions des piÃ¨ces sur lâ€™Ã©chiquier. 12.1 Lexique Lâ€™autoapprentissage possÃ¨de son jargon particulier. Puisque certains termes peuvent porter Ã  confusion, voici quelques dÃ©finitions de termes que jâ€™utiliserai dans ce chapitre. RÃ©ponse. La variable que lâ€™on cherche Ã  obtenir. Il peut sâ€™agir dâ€™une variable continue comme dâ€™une variable catÃ©gorielle. On la nomme aussi la cible. PrÃ©dicteur. Une variable utilisÃ©e pour prÃ©dire une rÃ©ponse. Les prÃ©dicteurs sont des variables continues. Les prÃ©dicteurs de type catÃ©goriel doivent prÃ©alablement Ãªtre dummifiÃ©s (voir chapitre 5). On nomme les prÃ©dicteurs les entrÃ©es. Apprentissage supervisÃ© et non-supervisÃ©. Si vous avez suivi le cours jusquâ€™ici, vous avez dÃ©jÃ  utilisÃ© des outils entrant dans la grande famille de lâ€™apprentissage automatique. La rÃ©gression linÃ©aire, par exemple, vise Ã  minimiser lâ€™erreur sur la rÃ©ponse en optimisant les coefficients de pente et lâ€™intercept. Un apprentissage supervisÃ© a une cible, comme câ€™est le cas de la rÃ©gression linÃ©aire. En revanche, un apprentissage non supervisÃ© nâ€™en a pas: on laisse lâ€™algorithme le soin de dÃ©tecter des structures intÃ©ressantes. Nous avons dÃ©jÃ  utilisÃ© cette approche. Pensez-y un peuâ€¦ lâ€™analyse en composante principale ou en coordonnÃ©es principales, ainsi que le partitionnement hiÃ©rarchique ou non, couverts au chapitre 9, sont des exemples dâ€™apprentissage non supervisÃ©. En revanche, lâ€™analyse de redondance a une rÃ©ponse. Lâ€™analyse discriminante aussi, bien que sa rÃ©ponse soit catÃ©gorielle. Lâ€™apprentissage non supervisÃ© ayant dÃ©jÃ  Ã©tÃ© couvert (sans le nommer) au chapitre 9, ce chapitre ne sâ€™intÃ©resse quâ€™Ã  lâ€™apprentissage supervisÃ©. RÃ©gression et Classification. Alors que la rÃ©gression est un type dâ€™apprentissage automatique pour les rÃ©ponses continues, la classification vise Ã  prÃ©dire une rÃ©ponse catÃ©gorielle. Il existe des algorithmes uniquement application Ã  la rÃ©gression, uniquement applicables Ã  la classification, et plusieurs autres adaptable aux deux situations. DonnÃ©es dâ€™entraÃ®nement et donnÃ©es de test. Lorsque lâ€™on gÃ©nÃ¨re un modÃ¨le, on dÃ©sire quâ€™il sache comment rÃ©agir Ã  ses prÃ©dicteurs. Cela se fait avec des donnÃ©es dâ€™entraÃ®nement, sur lesquelles on calibre et valide le modÃ¨le. Les donnÃ©es de test servent Ã  vÃ©rifier si le modÃ¨le est en mesure de prÃ©dire des rÃ©ponses sur lesquelles il nâ€™a pas Ã©tÃ© entraÃ®nÃ©. Fonction de perte. Une fonction qui mesure lâ€™erreur dâ€™un modÃ¨le. 12.2 DÃ©marche La premiÃ¨re tÃ¢che est dâ€™explorer les donnÃ©es, ce que nous avons couvert au chapitres 3 et 4. 12.2.1 PrÃ©traitement Pour la plupart des techniques dâ€™autoapprentissage, le choix de lâ€™Ã©chelle de mesure est dÃ©terminant sur la modÃ©lisation subsÃ©quente. Par exemple, un algorithme basÃ© sur la distance comme les k plus proches voisins ne mesurera pas les mÃªmes distances entre deux observations si lâ€™on change lâ€™unitÃ© de mesure dâ€™une variable du mÃ¨tre au kilomÃ¨tre. Il est donc important dâ€™effectuer, ou dâ€™envisager la possibilitÃ© dâ€™effectuer un prÃ©traitement sur les donnÃ©es. Je vous rÃ©fÃ¨re au chapitre 8 pour plus de dÃ©tails sur le prÃ©traitement. 12.2.2 EntraÃ®nement et test Vous connaissez peut-Ãªtre lâ€™expression sportive â€œavoir lâ€™avantage du terrainâ€. Il sâ€™agit dâ€™un principe prÃ©tendant que les athlÃ¨tes performent mieux en terrain connu. Idem pour les modÃ¨les phÃ©nomÃ©nologiques. Il est possible quâ€™un modÃ¨le fonctionne trÃ¨s bien sur les donnÃ©es avec lesquelles il a Ã©tÃ© entraÃ®nÃ©, mais trÃ¨s mal sur des donnÃ©es externes. De mauvaises prÃ©dictions effectuÃ©es Ã  partir dâ€™un modÃ¨le qui semblait bien se comporter peut mener Ã  des dÃ©cisions qui, pourtant prises de maniÃ¨re confiante, se rÃ©vÃ¨lent fallacieuses au point dâ€™aboutir Ã  de graves consÃ©quences. Câ€™est pourquoi, en mode prÃ©dictif, on doit Ã©valuer la prÃ©cision et la justesse dâ€™un modÃ¨le sur des donnÃ©es qui nâ€™ont pas Ã©tÃ© utilisÃ©s dans son entraÃ®nement. En pratique, il convient de sÃ©parer un tableau de donnÃ©es en deux: un tableau dâ€™entraÃ®nement et un tableau de test. Il nâ€™existe pas de standards sur la proportion Ã  utiliser dans lâ€™un et lâ€™autre. Cela dÃ©pend de la prudence de lâ€™analyse et de lâ€™ampleur de son tableau de donnÃ©es. Dans certains cas, nous prÃ©fÃ©rerons couper le tableau Ã  50%. Dand dâ€™autres, nous prÃ©fÃ©rerons rÃ©server le deux-tiers des donnÃ©es pour lâ€™entraÃ®nement, ou 70%, 75%. Rarement, toutefois, rÃ©servera-t-on moins plus de 50% et moins de 20% Ã  la phase de test. Si les donnÃ©es sont peu Ã©quilibrÃ©es (par exemple, on retrouve peu de donnÃ©es de lâ€™espÃ¨ce \\(A\\), que lâ€™on retrouve peu de donnÃ©es Ã  un pH infÃ©rieur Ã  5 ou que lâ€™on a peu de donnÃ©es croisÃ©es de lâ€™espÃ¨ce \\(A\\) Ã  ph infÃ©rieur Ã  5), il y a un danger quâ€™une trop grande part, voire toute les donnÃ©es, se retrouvent dans le tableau dâ€™entraÃ®nement (certaines situations ne seront ainsi pas testÃ©es) ou dans le tableau de test (certaines situations ne seront pas couvertes par le modÃ¨le). Lâ€™analyste doit sâ€™assurer de sÃ©parer le tableau au hasard, mais de maniÃ¨re consciencieuse. 12.2.3 Sousapprentissage et surapprentissage Une difficultÃ© en modÃ©lisation phÃ©nomÃ©nologique est ce qui tient de la structure et ce qui tient du bruit. Lorsque lâ€™on considÃ¨re une structure comme du bruit, on est dans un cas de sousapprentissage. Lorsque, au contraire, on interprÃ¨te du bruit comme une structure, on est en cas de surapprentissage. Les graphiques de la figure 12.2 prÃ©sentent ces deux cas, avec au centre un cas dâ€™apprentissage conforme. Figure 12.2: Cas de figure de mÃ©sapprentissage. Ã€ gauche, sous-apprentissage. Au centre, apprentissage valide. Ã€ droite, surapprentissage. Il est nÃ©anmoins difficile dâ€™inspecter un modÃ¨le comprenant plusieurs entrÃ©es. On dÃ©tectera le mÃ©sapprentissage lorsque la prÃ©cision dâ€™un modÃ¨le est lourdement altÃ©rÃ©e en phase de test. Une maniÃ¨re de limiter le mÃ©sapprentissage est dâ€™avoir recours Ã  la validation croisÃ©e. 12.2.4 Validation croisÃ©e Souvent confondue avec le fait de sÃ©parer le tableau en phases dâ€™entraÃ®nement et de test, la validation croisÃ©e est un principe incluant plusieurs algorithmes qui consiste Ã  entraÃ®ner le modÃ¨le sur un Ã©chantillonnage alÃ©atoire des donnÃ©es dâ€™entraÃ®nement. La technique la plus utilisÃ©e est le k-fold, oÃ¹ lâ€™on sÃ©pare alÃ©atoirement le tableau dâ€™entraÃ®nement en un nombre k de tableaux. Ã€ chaque Ã©tape de la validation croisÃ©e, on calibre le modÃ¨le sur tous les tableaux sauf un, puis on valide le modÃ¨le sur le tableau exclu. La performance du modÃ¨le en entraÃ®nement est jugÃ©e sur les validations. 12.2.5 Choix de lâ€™algorithme dâ€™apprentissage Face aux centaines dâ€™algorithmes dâ€™apprentissages qui vous sont offertes, choisir lâ€™algorithme (ou les algorithmes) adÃ©quats pour votre problÃ¨me nâ€™est pas une tÃ¢che facile. Ce choix sera motivÃ© par les tenants et aboutissants des algorithmes, votre expÃ©rience, lâ€™expÃ©rience de la littÃ©rature, lâ€™expÃ©rience de vos collÃ¨gues, etc. Ã€ moins dâ€™Ãªtre particuliÃ¨rement surdouÃ©.e, il vous sera pratiquement impossible de maÃ®triser la mathÃ©matique de chacun dâ€™eux. Une approche raisonnable est de tester plusieurs modÃ¨les, de retenir les modÃ¨les qui semblent les plus pertinents, et dâ€™approfondir si ce nâ€™est dÃ©jÃ  fait la mathÃ©matique des options retenues. Ajoutons quâ€™il existe des algorithmes gÃ©nÃ©tiques, qui ne sont pas couverts ici, qui permettent de sÃ©lectionner des modÃ¨les dâ€™autoapprentissages optimaux. Un de ces algorithmes est offert par le module Python tpot. 12.2.6 DÃ©ploiement Nous ne couvrirons pas la phase de dÃ©ploiement dâ€™un modÃ¨le. Notons seulement quâ€™il est possible, en R, dâ€™exporter un modÃ¨le dans un fichier .Rdata, qui pourra Ãªtre chargÃ© dans un autre environnement R. Cet environnement peut Ãªtre une feuille de calcul comme une interface visuelle montÃ©e, par exemple, avec Shiny (chapitre 8). En rÃ©sumÃ©, Explorer les donnÃ©es SÃ©lectionner des algorithmes Effectuer un prÃ©traitement CrÃ©er un ensemble dâ€™entraÃ®nement et un ensemble de test Lisser les donnÃ©es sur les donnÃ©es dâ€™entraÃ®nement avec validation croisÃ©e Tester le modÃ¨le DÃ©ployer le modÃ¨le 12.3 Lâ€™autoapprentissage en R Plusieurs options sont disponibles. Les modules que lâ€™on retrouve en R pour lâ€™autoapprentissage sont nombreux, et parfois spÃ©cialisÃ©s. Il est possible de les utiliser individuellement. Chacun de ces modules fonctionne Ã  sa faÃ§on. Le module caret de R a Ã©tÃ© conÃ§u pour donner accÃ¨s Ã  des centaines de fonctions dâ€™autoapprentissage via une interface commune. Le module mlr occupe sensiblement le mÃªme crÃ©neau que caret, mais utilise plutÃ´t une approche par objets connectÃ©s. Au moment dâ€™Ã©crire ces lignes, mlr est peu documentÃ©, donc a priori plus complexe Ã  prendre en main. En Python, le module scikit-learn offre un interface unique pour lâ€™utilisation de nombreuses techniques dâ€™autoapprentissage. Il est possible dâ€™appeler des fonctions de Python Ã  partir de R grÃ¢ce au module reticulate. Dans ce chapitre, nous verrons comment fonctionnent certains algorithmes sÃ©lectionnÃ©s, puis nous les appliquerons avec le module respectif qui mâ€™a semblÃ© le plus appropriÃ©. library(&quot;tidyverse&quot;) # Ã©videmment library(&quot;caret&quot;) ## ## Attaching package: &#39;caret&#39; ## The following object is masked from &#39;package:compositions&#39;: ## ## R2 ## The following object is masked from &#39;package:pls&#39;: ## ## R2 ## The following object is masked from &#39;package:vegan&#39;: ## ## tolerance ## The following object is masked from &#39;package:purrr&#39;: ## ## lift 12.4 Algorithmes Il existe des centaines dâ€™algorithmes dâ€™apprentissage. Je nâ€™en couvrirai que quatre, qui me semblent Ãªtre appropriÃ©s pour la modÃ©lisation phÃ©nomÃ©nologique en agroÃ©cologie, et utilisables autant pour la rÃ©gression et la classification. Les k plus proches voisins Les arbres de dÃ©cision Les rÃ©seaux neuronaux Les processus gaussiens 12.4.1 Les k plus proches voisins Figure 12.3: &lt;&lt; Leâ€¦ lâ€™idÃ©e en arriÃ¨re pour Ãªtreâ€¦ euhâ€¦ simpliste, lÃ  câ€™est que câ€™est un peu deâ€¦ euhmmâ€¦ de la vitamine de vinyle.&gt;&gt; - Georges (Les voisins, une piÃ¨ce de Claude Meunier). Pour dire comme Georges, leâ€¦ lâ€™idÃ©e en arriÃ¨re des KNN pour Ãªtreâ€¦ euhâ€¦ simpliste, câ€™est quâ€™un objet va ressembler Ã  ce qui se trouve dans son voisinage. Les KNN se basent en effet sur une mÃ©trique de distance pour rechercher un nombre k de points situÃ©s Ã  proximitÃ© de la mesure. Les k points les plus proches sont retenus, k Ã©tant un entier non nul Ã  optimiser. Un autre paramÃ¨tre parfois utilisÃ© est la distance maximale des voisins Ã  considÃ©rer: un voisin trop Ã©loignÃ© pourra Ãªtre discartÃ©. La rÃ©ponse attribuÃ©e Ã  la mesure est calculÃ©e Ã  partir de la rÃ©ponse des k voisins retenus. Dans le cas dâ€™une rÃ©gression, on utiliser gÃ©nÃ©ralement la moyenne. Dans le cas de la classification, la mesure prendra la catÃ©gorie qui sera la plus prÃ©sente chez les k plus proches voisins. Lâ€™algorithme des k plus proches voisins est relativement simple Ã  comprendre. Certains piÃ¨ges sont, de mÃªme, peuvent Ãªtre contournÃ©s facilement. Imaginez que vous rechercher les points les plus rapprochÃ©s dans un systÃ¨me de coordonnÃ©es gÃ©ographiques oÃ¹ les coordonnÃ©es \\(x\\) sont exprimÃ©es en mÃ¨tres et les coordonnÃ©es \\(y\\), en centimÃ¨tres. Vous y projetez trois points (figure 12.4). Figure 12.4: Distances entre les points pour utilisation avec les KNN Techniquement la distance A-B est 100 plus Ã©levÃ©e que la distance A-C, mais lâ€™algorithme ne se soucie pas de la mÃ©trique que vous utilisez (figure 12.4). Il est primordial dans ce cas dâ€™utiliser la mÃªme mÃ©trique. Cette stratÃ©gie est Ã©vidente lorsque les variables sont comparables. Câ€™est rarement le cas, que ce soit lorsque lâ€™on compare des dimensions physionomiques (la longueur dâ€™une phalange ou celle dâ€™un fÃ©mur) mais lorsque les variables incluent des mÃ©langes de longueurs, des pH, des dÃ©comptes, etc., il est important de bien identifier la mÃ©trique et le type de distance quâ€™il convient le mieux dâ€™utiliser. En outre, la standardisation des donnÃ©es Ã  une moyenne de zÃ©ro et Ã  un Ã©cart-type de 1 est une approche courrament utilisÃ©e. 12.4.1.1 Exemple dâ€™application Pour ce premier exemple, je prÃ©senterai un cheminement dâ€™autoapprentissage, du prÃ©traitement au test. Nous allons essayer de classer les espÃ¨ces de dragon selon leurs dimensions. Figure 12.5: Dimensions mesurÃ©s sur les dragons capturÃ©s. dragons &lt;- read_csv(&quot;data/11_dragons.csv&quot;) ## Parsed with column specification: ## cols( ## V1 = col_double(), ## V2 = col_double(), ## V3 = col_double(), ## V4 = col_double(), ## V5 = col_double(), ## V6 = col_double(), ## V7 = col_double(), ## V8 = col_double(), ## V9 = col_double(), ## V10 = col_double(), ## V11 = col_double(), ## ID = col_double(), ## Species = col_character() ## ) Assurons-nous que les donnÃ©es sont toutes Ã  lâ€™Ã©chelle. Nous pourrions utiliser la fonction scale(). Toutefois, si je capture un nouveau dragon, je nâ€™aurai pas lâ€™information pour convertir mes nouvelles dimensions dans la mÃªme mÃ©trique que celle utilisÃ©e pour lisser mon modÃ¨le. Prenez donc soin de conserver la moyenne et lâ€™Ã©cart-type pour subsÃ©quemment calculer des mises Ã  lâ€™Ã©chelle. dim_means &lt;- dragons %&gt;% dplyr::select(starts_with(&quot;V&quot;)) %&gt;% summarise_all(mean, na.rm = TRUE) dim_sds &lt;- dragons %&gt;% dplyr::select(starts_with(&quot;V&quot;)) %&gt;% summarise_all(sd, na.rm = TRUE) dragons_sc &lt;- dragons %&gt;% dplyr::select(starts_with(&quot;V&quot;)) %&gt;% scale(.) %&gt;% as_tibble() %&gt;% mutate(Species = dragons$Species) SÃ©parons les donnÃ©es en entraÃ®nement (_tr) et en test (_te) avec une proportion 70/30 (p = 0.7). Il est essentiel dâ€™utiliser set.seed() pour sâ€™assurer que la partition soit la mÃªme Ã  chaque session de code (pour la reproductibilitÃ©) - jâ€™ai lâ€™habitude de taper nâ€™importe quel numÃ©ro Ã  environ 6 chiffres, mais lors de publications, je vais sur random.org et je gÃ©nÃ¨re un numÃ©ro au hasard, sans biais. set.seed(68017) id_tr &lt;- createDataPartition(dragons_sc$Species, p = 0.7, list = FALSE) dragons_tr &lt;- dragons_sc[id_tr, ] dragons_te &lt;- dragons_sc[-id_tr, ] Avant de lancer nos calculs, allons vois sur la page de caret les modules qui effectuent des KNN pour la classification. Nous trouvons knn et kknn. Si les modules nÃ©cessaires aux calculs ne sont pas installÃ©s sur votre ordinateur, caret vous demandera de les installer. Prenons le module kknn, qui demande le paramÃ¨tre kmax, soit le nombre de voisins Ã  considÃ©rer, ainsi quâ€™un paramÃ¨tre de distance (spÃ©cifiez 1 pour la distance de Mahattan et 2 pour la distance euclidienne), et un kernel, qui est une fonction pour mesurer la distance. Comment choisir les bons paramÃ¨tres? Une maniÃ¨re de procÃ©der est de crÃ©er une grille de paramÃ¨tres. kknn_grid &lt;- expand.grid(kmax = 3:6, distance = 1:2, kernel = c(&quot;rectangular&quot;, &quot;gaussian&quot;, &quot;optimal&quot;)) Les noms des colonnes de la grille doivent correspondre aux noms des paramÃ¨tres du modÃ¨le. Nous allons modÃ©liser avec une validation croisÃ©e Ã  5 plis. ctrl &lt;- trainControl(method=&quot;repeatedcv&quot;, repeats = 5) Pour finalement lisser le modÃ¨le. set.seed(8961704) clf &lt;- train(Species ~ ., data = dragons_tr, method = &quot;kknn&quot;, tuneGrid = kknn_grid, trainControl = ctrl) clf ## k-Nearest Neighbors ## ## 34 samples ## 11 predictors ## 5 classes: &#39;Dragon de caverne&#39;, &#39;Dragon de feu&#39;, &#39;Dragon de mer&#39;, &#39;Dragon de pierre&#39;, &#39;Dragon de saturne&#39; ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 34, 34, 34, 34, 34, 34, ... ## Resampling results across tuning parameters: ## ## kmax distance kernel Accuracy Kappa ## 3 1 rectangular 0.8914566 0.8572135 ## 3 1 gaussian 0.8914566 0.8572135 ## 3 1 optimal 0.8914566 0.8572135 ## 3 2 rectangular 0.8601133 0.8164023 ## 3 2 gaussian 0.8601133 0.8164023 ## 3 2 optimal 0.8601133 0.8164023 ## 4 1 rectangular 0.8914566 0.8572135 ## 4 1 gaussian 0.8914566 0.8572135 ## 4 1 optimal 0.8914566 0.8572135 ## 4 2 rectangular 0.8528406 0.8070909 ## 4 2 gaussian 0.8601133 0.8164023 ## 4 2 optimal 0.8601133 0.8164023 ## 5 1 rectangular 0.8881233 0.8533711 ## 5 1 gaussian 0.8881233 0.8533711 ## 5 1 optimal 0.8914566 0.8572135 ## 5 2 rectangular 0.8528406 0.8071905 ## 5 2 gaussian 0.8601133 0.8165018 ## 5 2 optimal 0.8601133 0.8164023 ## 6 1 rectangular 0.8881233 0.8533711 ## 6 1 gaussian 0.8881233 0.8533711 ## 6 1 optimal 0.8914566 0.8572135 ## 6 2 rectangular 0.8528406 0.8071905 ## 6 2 gaussian 0.8601133 0.8165018 ## 6 2 optimal 0.8601133 0.8164023 ## ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were kmax = 6, distance = 1 and kernel = optimal. Nous obtenons les paramÃ¨tres du modÃ¨le optimal. PrÃ©disons lâ€™espÃ¨ce de dragons selon ses dimensions pour chacun des tableaux. pred_tr &lt;- predict(clf) pred_te &lt;- predict(clf, newdata = dragons_te) Une maniÃ¨re dâ€™Ã©valuer la prÃ©diction est dâ€™afficher un tableau de contingence. table(dragons_tr$Species, pred_tr) ## pred_tr ## Dragon de caverne Dragon de feu Dragon de mer Dragon de pierre Dragon de saturne ## Dragon de caverne 6 0 0 0 0 ## Dragon de feu 0 7 0 0 0 ## Dragon de mer 0 0 7 0 0 ## Dragon de pierre 0 0 0 7 0 ## Dragon de saturne 0 0 0 0 7 table(dragons_te$Species, pred_te) ## pred_te ## Dragon de caverne Dragon de feu Dragon de mer Dragon de pierre Dragon de saturne ## Dragon de caverne 2 0 0 0 0 ## Dragon de feu 0 2 0 0 0 ## Dragon de mer 0 0 3 0 0 ## Dragon de pierre 0 0 0 3 0 ## Dragon de saturne 0 0 0 1 2 Les espÃ¨ces de dragon sont toutes bien classÃ©es tant entraÃ®nement quâ€™en test (câ€™est rarement le cas dans les situations rÃ©elles). 12.4.2 Les arbres dÃ©cisionnels Figure 12.6: Les Ents, tirÃ© du film le Seigneur des anneaux, qui prennent trop de temps avant de se dÃ©cider - paradoxalement, les abrbres de dÃ©cisions sont dotÃ©s dâ€™algorithmes rapides. Un arbre dÃ©cisionnel est une collection hiÃ©rarchisÃ©e de dÃ©cisions, le plus souvent binaires. Chaque embranchement est un test Ã  vrai ou faux sur une variable. La rÃ©ponse, que ce soit une catÃ©gorie ou une valeur numÃ©rique, se trouve au bout de la derniÃ¨re branche. Les suites de dÃ©cisions sont organisÃ©es de maniÃ¨re Ã  ce que la prÃ©cision de la rÃ©ponse soit optimisÃ©e. Ils ont lâ€™avantage de pouvoir Ãªtre exprimÃ©s en un schÃ©ma simple et imprimable. Figure 12.7: Exemple dâ€™arbre de dÃ©cision, tirÃ© du blogue de Jeremy Jordon. Les arbres sont notamment paramÃ©trÃ©s par le nombre maximum dâ€™embranchements, qui sâ€™il est trop Ã©levÃ© peut mener Ã  du surapprentissage. Il existe de nombreux algorithmes dâ€™arbres de dÃ©cision. Une collection dâ€™arbres devient une forÃªt. Les forÃªts alÃ©atoires (random forest) sont une catÃ©gorie dâ€™algorithmes composÃ©s de plusieurs arbres de dÃ©cision optimisÃ©s sur des donnÃ©es rÃ©pliquÃ©es alÃ©atoirement par bagging. Allons-y par Ã©tape. Ã€ partir des donnÃ©es existantes composÃ©es de n observations (donc n lignes) sÃ©lectionnÃ©es pour lâ€™entraÃ®nement, Ã©chantillonnons au hasard avec remplacement un nombre n de nouvelles observations. Le remplacement implique quâ€™on retrouvera fort probablement dans notre nouveau tableau des lignes identiques. Lissons un arbre sur notre tableau alÃ©atoire. Effectuons un nouveau tirage, puis un autre arbre. Puis encore, et encore, disons 10 fois. Nous obtiendrons une forÃªt de 10 arbres. Pour une nouvelle observation Ã  prÃ©dire, nous obtenons donc 10 prÃ©dictions, sur lesquelles nous pouvons effectuer un moyenne sâ€™il sâ€™agit dâ€™une variable numÃ©rique, ou bien prenons la catÃ©gorie la plus souvent prÃ©dite dans le cas dâ€™une classification. Les forÃªts alÃ©atoires peuvent Ãªtre constituÃ©s de 10, 100, 1000 arbres: autant quâ€™il en est nÃ©cessaire. 12.4.2.1 Exemple dâ€™application Utilisons toujours nos donnÃ©es de dimensions de dragons. Bien quâ€™il en existe plusieurs, le module conventionnel pour effectuer un arbre de dÃ©cision est rpart2. Sur la page de caret, nous trouvons rpart2, apte pour les classifications et les rÃ©gressions, qui nâ€™a besoin que du paramÃ¨tre maxdepth. rpart2_grid &lt;- expand.grid(maxdepth = 3:10) # expand_grid n&#39;est pas nÃ©cessaire ici Prenons 5 plis encore une fois. ctrl &lt;- trainControl(method=&quot;repeatedcv&quot;, repeats = 5) Pour finalement lisser le modÃ¨le. set.seed(3468973) clf &lt;- train(Species ~ ., data = dragons_tr, method = &quot;rpart2&quot;, tuneGrid = rpart2_grid) clf ## CART ## ## 34 samples ## 11 predictors ## 5 classes: &#39;Dragon de caverne&#39;, &#39;Dragon de feu&#39;, &#39;Dragon de mer&#39;, &#39;Dragon de pierre&#39;, &#39;Dragon de saturne&#39; ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 34, 34, 34, 34, 34, 34, ... ## Resampling results across tuning parameters: ## ## maxdepth Accuracy Kappa ## 3 0.4162868 0.2872798 ## 4 0.4162868 0.2872798 ## 5 0.4162868 0.2872798 ## 6 0.4162868 0.2872798 ## 7 0.4162868 0.2872798 ## 8 0.4162868 0.2872798 ## 9 0.4162868 0.2872798 ## 10 0.4162868 0.2872798 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was maxdepth = 3. Nous obtenons les paramÃ¨tres du modÃ¨le optimal: maxdepth = 3 - puisque câ€™est Ã  la limite infÃ©rieure de la grille, mieux vaudrait Ã©tendre la grille, mais passons pour lâ€™exemple. Comme je lâ€™ai mentionnÃ©, un arbre de dÃ©cision est un outil convivial Ã  visualiser. plot(clf$finalModel) text(clf$finalModel) Ou en plus beau, je vous laisse essayer. library(&quot;rattle&quot;) fancyRpartPlot(clf$finalModel) Tout comme pour les KNN, prÃ©disons lâ€™espÃ¨ce de dragons selon ses dimensions pour chacun des tableaux. pred_tr &lt;- predict(clf) pred_te &lt;- predict(clf, newdata = dragons_te) En ce qui a trait aux tableaux de contigenceâ€¦ table(dragons_tr$Species, pred_tr) ## pred_tr ## Dragon de caverne Dragon de feu Dragon de mer Dragon de pierre Dragon de saturne ## Dragon de caverne 0 0 0 0 6 ## Dragon de feu 0 7 0 0 0 ## Dragon de mer 0 0 7 0 0 ## Dragon de pierre 0 0 0 7 0 ## Dragon de saturne 0 0 0 0 7 table(dragons_te$Species, pred_te) ## pred_te ## Dragon de caverne Dragon de feu Dragon de mer Dragon de pierre Dragon de saturne ## Dragon de caverne 0 1 0 0 1 ## Dragon de feu 0 2 0 0 0 ## Dragon de mer 0 0 3 0 0 ## Dragon de pierre 0 0 0 3 0 ## Dragon de saturne 0 0 0 0 3 Les espÃ¨ces de dragon sont toutes bien classÃ©es en entraÃ®nement et en testâ€¦ sauf pour les dragons de caverne, qui (lâ€™avez-vous remarquez?) nâ€™apparaissent pas dans lâ€™arbre de dÃ©cision! Le module caret vient avec la fonction varImp() qui offre une apprÃ©ciation de lâ€™importance des variables dans le modÃ¨le final. La notion dâ€™importance varie dâ€™un modÃ¨le Ã  lâ€™autre, et reste Ã  ce jour mal documentÃ©. Mieux vaut en examiner les tenants et aboutissants avant dâ€™interprÃ©ter exessivement la sortie de cette fonction. varImp(clf) %&gt;% plot(.) On pourra effectuer de la mÃªme maniÃ¨re une forÃªt alÃ©atoire, mais cette fois-ci avec le module rf. set.seed(3468973) ctrl &lt;- trainControl(method=&quot;repeatedcv&quot;, repeats = 5) clf &lt;- train(Species ~ ., data = dragons_tr, method = &quot;rf&quot;) clf ## Random Forest ## ## 34 samples ## 11 predictors ## 5 classes: &#39;Dragon de caverne&#39;, &#39;Dragon de feu&#39;, &#39;Dragon de mer&#39;, &#39;Dragon de pierre&#39;, &#39;Dragon de saturne&#39; ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 34, 34, 34, 34, 34, 34, ... ## Resampling results across tuning parameters: ## ## mtry Accuracy Kappa ## 2 0.9018266 0.8776655 ## 6 0.9555287 0.9441745 ## 11 0.9524518 0.9401765 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was mtry = 6. Et les rÃ©sultats. pred_tr &lt;- predict(clf) pred_te &lt;- predict(clf, newdata = dragons_te) table(dragons_te$Species, pred_te) ## pred_te ## Dragon de caverne Dragon de feu Dragon de mer Dragon de pierre Dragon de saturne ## Dragon de caverne 2 0 0 0 0 ## Dragon de feu 0 2 0 0 0 ## Dragon de mer 0 0 3 0 0 ## Dragon de pierre 0 0 0 3 0 ## Dragon de saturne 0 0 0 0 3 table(dragons_tr$Species, pred_tr) ## pred_tr ## Dragon de caverne Dragon de feu Dragon de mer Dragon de pierre Dragon de saturne ## Dragon de caverne 6 0 0 0 0 ## Dragon de feu 0 7 0 0 0 ## Dragon de mer 0 0 7 0 0 ## Dragon de pierre 0 0 0 7 0 ## Dragon de saturne 0 0 0 0 7 Notez que les forÃªts alÃ©atoires ne gÃ©nÃ¨re par de visuel. 12.4.3 Les rÃ©seaux neuronaux AprÃ¨s les KNN et les random forests, nous passons au domaine plus complexe des rÃ©seaux neuronaux. Le terme rÃ©seau neuronal est une mÃ©taphore liÃ©e Ã  une perception que lâ€™on avait du fonctionnement du cerveau humain lorsque la technique des rÃ©seaux neuronaux a Ã©tÃ© dÃ©veloppÃ©e dans les annÃ©es 1950. Un rÃ©seau neuronal comprend une sÃ©rie de boÃ®tes dâ€™entrÃ©es liÃ©e Ã  des fonctions qui transforment et acheminent successivement lâ€™information jusquâ€™Ã  la sortie dâ€™une ou plusieurs rÃ©ponse. Il existe plusieurs formes de rÃ©seaux neuronnaux, dont la plus simple manifestation est le perceptron multicouche. Dans lâ€™exemple de la figure 12.8, on retrouve 4 variables dâ€™entrÃ©e et trois variables de sortie entre lesquelles on retrouve 5 couches dont le nombre de neurones varient entre 3 et 6. Figure 12.8: RÃ©seau neuronal schÃ©matisÃ©, Source: Neural designer. Entre la premiÃ¨re couche de neurones (les variables prÃ©dictives) et la derniÃ¨re couche (les variables rÃ©ponse), on retrouve des couches cachÃ©es. Chaque neurone est reliÃ© Ã  tous les neurones de la couche suivante. Les liens sont des poids, qui peuvent prendre des valeurs dans lâ€™ensemble des nombres rÃ©els. Ã€ chaque neurone suivant la premiÃ¨re couche, on fait la somme des poids multipliÃ©s par la sortie du neurone. Le nombre obtenu entre dans chaque neurone de la couche. Le neurone est une fonction, souvent trÃ¨s simple, qui transforme le nombre. La fonction plus utilisÃ©e est probablement la fonction ReLU, pour rectified linear unit, qui expulse le mÃªme nombre aux neurones de la prochaine couche sâ€™il est positif: sinon, il expulse un zÃ©ro. Exercice. Si tous les neurones sont des fonctions ReLU, calculez la sortie de ce petit rÃ©seau neuronal. Vous trouverez la rÃ©ponse sur lâ€™image images/11_nn_ex1_R.jpg. Il est aussi possible dâ€™ajouter un biais Ã  chaque neurone, qui est un nombre rÃ©el additionnÃ© Ã  la somme des neurones pondÃ©rÃ©e par les poids. Lâ€™optimisation les poids pour chaque lien et les biais pour chaque neurone (grÃ¢ce Ã  des algorithmes dont le fonctionnement sort du cadre de ce cours) constitue le processus dâ€™apprentissage. Avec lâ€™aide de logiciels et de modules spÃ©cialisÃ©s, la construction de rÃ©seaux de centaines de neurones organisÃ©s en centaines de couches vous permettra de capter des patrons complexes dans des ensembles de donnÃ©es. Vous avez peut-Ãªtre dÃ©jÃ  entendu parler dâ€™apprentissage profond (ou deep learning). Il sâ€™agit simplement dâ€™une appellation des rÃ©seaux neuronaux modernisÃ© pour insister sur la prÃ©sence de nombreuses couches de neurones. Câ€™est un terme Ã  la mode. 12.4.3.1 Les rÃ©seaux neuronaux sur R avec neuralnet Plusieurs modules sont disponibles sur R pour lâ€™apprentissage profond. Certains utilisent le module H2O.ia, propulsÃ© en Java, dâ€™autres utilisent plutÃ´t Keras, propulsÃ© en Python par lâ€™intermÃ©diaire de Tensorflow. Jâ€™ai une prÃ©fÃ©rence pour Keras, puisquâ€™il supporte les rÃ©seaux neuronaux classiques (perceptrons multicouche) autant que convolutifs ou rÃ©currents. Keras pourrait nÃ©anmoins Ãªtre difficile Ã  installer sur Windows, oÃ¹ Python ne vient pas par dÃ©faut. Sur Windows, Keras ne fonctionne quâ€™avec Anaconda: vous devez donc installez Anaconda ou Miniconda (Miniconda offre une installation minimaliste). Donc, pour ce cours, nous utiliserons le module neuralnet. Il est possible de lâ€™utilser grÃ¢ce Ã  lâ€™interface de caret, mais son utilisation directe permet davantage de flexibilitÃ©. Chargeons les donnÃ©es dâ€™iris. library(&quot;neuralnet&quot;) ## ## Attaching package: &#39;neuralnet&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## compute data(&quot;iris&quot;) Prenons soin de segmenter nos donnÃ©es en entraÃ®nement et en test. set.seed(8453668) iris_tr_index &lt;- createDataPartition(y=iris$Species, p = 0.75, list = FALSE) Nous pouvons ainsi crÃ©er nos tableaux dâ€™entraÃ®nement et de test pour les variables prÃ©dictives. Les rÃ©seaux neuronnaux sont aptes Ã  gÃ©nÃ©rer des sorties multiples. Nous dÃ©sirons prÃ©dire une catÃ©gorie, et neuralnet ne sâ€™occupe pas de les transformer de facto. Lors de la prÃ©diction dâ€™une catÃ©gorie, nous devons gÃ©nÃ©rÃ©e des sorties multiples qui permettront de dÃ©cider de lâ€™appartenance exclusive Ã  une catÃ©gorie ou une autre. Nous avons abordÃ© lâ€™encodage catÃ©goriel aux chapitres 6 et 8. Câ€™est ce que nous ferons ici. species_oh &lt;- model.matrix(~ 0 + Species, iris) colnames(species_oh) &lt;- levels(iris$Species) iris_oh &lt;- iris %&gt;% cbind(species_oh) iris_tr &lt;- iris_oh[iris_tr_index, ] iris_te &lt;- iris_oh[-iris_tr_index, ] LanÃ§ons le rÃ©seau neuronnal avec lâ€™interface-formule de R (neuralnet nâ€™accepte pas le . pour indiquer prend toutes les variables Ã  lâ€™exeption de celles utilisÃ©es en y): nous allons les inclure Ã  la main. Lâ€™argument hidden est un vecteur qui indique le nombre de neuronnes pour chaque couche. Lâ€™argument linear.input indique si lâ€™on dÃ©sire travailler en rÃ©gression (linear.output = TRUE) ou en classification (linear.output = FALSE). Lorsque les donnÃ©es sont nombreuses, patience, le calcul prend pas mal de temps. Dans ce cas-ci, nous avons un tout petit tableau. nn &lt;- neuralnet(setosa + versicolor + virginica ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data = iris_tr, hidden = c(5, 5), act.fct = &quot;tanh&quot;, linear.output = FALSE) Un rÃ©seau neuronnal peu complexe peut Ãªtre lisible. plot(nn) Il nâ€™existe pas de rÃ¨gle stricte sur le nombre de couche et le nombre de noeud par couche. Il est nÃ©anmoins conseillÃ© de gÃ©nÃ©rer dâ€™abord un modÃ¨le simple, puis au besoin de le complexifier graduellement en terme de nombre de noeuds, puis de nombre de couches. Si vous dÃ©sirez aller plus loin et utiliser keras, le module autokeras, disponible seulement en Python, est conÃ§u pour optimiser un modÃ¨le Keras. La sortie du rÃ©seau neuronal est une valeur prÃ¨s de 1 ou une valeur prÃ¨s de 0. Voici une maniÃ¨re de gÃ©nÃ©rer un vecteur catÃ©goriel. compute_te &lt;- compute(nn, iris_te) pred_te &lt;- compute_te$net.result %&gt;% as_tibble() %&gt;% apply(., 1, which.max) %&gt;% levels(iris$Species)[.] %&gt;% as.factor() ## Warning: `as_tibble.matrix()` requires a matrix with column names or a `.name_repair` argument. Using compatibility `.name_repair`. ## This warning is displayed once per session. La fonction caret::confusionMatrix() permet de gÃ©nÃ©rer les statistiques du modÃ¨le. confusionMatrix(iris_te$Species, pred_te) ## Confusion Matrix and Statistics ## ## Reference ## Prediction setosa versicolor virginica ## setosa 12 0 0 ## versicolor 1 10 1 ## virginica 0 0 12 ## ## Overall Statistics ## ## Accuracy : 0.9444 ## 95% CI : (0.8134, 0.9932) ## No Information Rate : 0.3611 ## P-Value [Acc &gt; NIR] : 2.421e-13 ## ## Kappa : 0.9167 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: setosa Class: versicolor Class: virginica ## Sensitivity 0.9231 1.0000 0.9231 ## Specificity 1.0000 0.9231 1.0000 ## Pos Pred Value 1.0000 0.8333 1.0000 ## Neg Pred Value 0.9583 1.0000 0.9583 ## Prevalence 0.3611 0.2778 0.3611 ## Detection Rate 0.3333 0.2778 0.3333 ## Detection Prevalence 0.3333 0.3333 0.3333 ## Balanced Accuracy 0.9615 0.9615 0.9615 Encore une fois, câ€™est rarement le cas mais nous obtenons une classification parfaite. 12.4.3.2 Pour aller plus loin En une heure divisÃ©e en 4 vidÃ©os, Grant Sanderson explique les rÃ©seaux neuronaux de maniÃ¨re intuitive. En ce qui a trait Ã  Keras, je recommande le livre Deep learning with R, de FranÃ§ois Allaire, auquel vous avez accÃ¨s avec un IDUL de lâ€™UniversitÃ© Laval. Si vous vous sentez Ã  lâ€™aise Ã  utiliser Keras avec le langage Python, je vous recommande le cours gratuit en ligne Applications of deep neural networks, de Jeff Heaton. Des types de rÃ©seaux neuronaux spÃ©cialisÃ©s ont Ã©tÃ© dÃ©veloppÃ©s. Je les prÃ©sente sans aller dans les dÃ©tails. RÃ©seaux neuronaux convolutif. Ce type de rÃ©seau neuronal est surtout utilisÃ© en reconnaissance dâ€™image. Les couches de neurones convolutifs possÃ¨dent, en plus des fonctions des perceptrons classiques, des filtres permettant dâ€™intÃ©grer les variables descriptives connexes Ã  lâ€™observation: dans le cas dâ€™une image, il sâ€™agit de scanner les pixels au pourtour du pixel traitÃ©. Une brÃ¨ve introduction sur Youtube. RÃ©seaux neuronaux rÃ©currents. PrÃ©dire des occurrences futures Ã  partir de sÃ©ries temporelles implique que la rÃ©ponse au temps t dÃ©pend non seulement de conditions externes, mais aussi le la rÃ©ponse au temps t-1. Les rÃ©seaux neuronaux rÃ©currents. Vous devrez ajouter des neurones particuliers pour cette tÃ¢che, qui pourra Ãªtre pris en charge par Keras grÃ¢ce aux couches de type Long Short-Term Memory network, ou LSTM. RÃ©seaux neuronaux probabilistes. Les rÃ©seaux neuronaux non-probabilistes offre une estimation de la variable rÃ©ponse. Mais quelle est la crÃ©dibilitÃ© de la rÃ©ponse selon les variables descriptives? Question qui pourrait se rÃ©vÃ©ler cruciale en mÃ©decine ou en ingÃ©nierie, Ã  la laquelle on pourra rÃ©pondre en mode probabiliste. Pour ce faire, on pose des distributions a priori sur les poids du rÃ©seau neuronal. Le module edward, programmÃ© et distribuÃ© en Python, offre cette possibilitÃ©. Vous pourrez accÃ©der Ã  edward grÃ¢ce au module reticulate, mais Ã  ce stade mieux vaudra basculer en Python. Pour en savoir davantage, considÃ©rez cette confÃ©rence de Andrew Rowan. 12.4.4 Les processus gaussiens Les sorties des techniques que sont les KNN, les arbres ou les forÃªts ainsi que les rÃ©seaux neuronaux sont (classiquement) des nombres rÃ©els ou des catÃ©gories. Dans les cas oÃ¹ la crÃ©dibilitÃ© de la rÃ©ponse est importante, il devient pertinent que la sortie soit probabiliste: les prÃ©dictions seront alors prÃ©sentÃ©es sous forme de distributions de probabilitÃ©. Dans le cas dâ€™une classification, la sortie du modÃ¨le sera un vecteur de probabilitÃ© quâ€™une observation appartienne Ã  une classe ou Ã  une autre. Dans celui dâ€™une rÃ©gression, on obtiendra une distribution continue. Les processus gaussiens tirent profit des statistiques bayÃ©siennes pour effectuer des prÃ©dictions probabilistes. Dâ€™autres techniques peuvent Ãªtre utilisÃ©es pour effectuer des prÃ©dictions probabilistes, comme les rÃ©seaux neuronaux probabilistes, que jâ€™ai introduits prÃ©cÃ©demment. Bien que les processus gaussiens peuvent Ãªtre utilisÃ©s pour la classification, son fonctionnement sâ€™explique favorablement, de maniÃ¨re intuitive, pas la rÃ©gression. 12.4.4.1 Un approche intuitive Ayant acquis de lâ€™expÃ©rience en enseignement des processus gaussiens, John Cunningham a dÃ©veloppÃ© une approche intuitive permettant de saisir les mÃ©canismes des processus gaussiens. lors de confÃ©rences disponible sur YouTube (1, 2), il aborde le sujet par la nÃ©cessitÃ© dâ€™effectuer une rÃ©gression non-linÃ©aire. GÃ©nÃ©rons dâ€™abord une variable prÃ©dictive x, lâ€™heure, et une variable rÃ©ponse y, le rythme cardiaque dâ€™un individu en battements par minute (bpm). x &lt;- c(7, 8, 10, 14, 17) y &lt;- c(61, 74, 69, 67, 78) plot(x, y, xlab=&quot;Heure&quot;, ylab=&quot;Rythme cardiaque (bpm)&quot;) abline(v=12, lty=3, col=&#39;gray50&#39;);text(12, 67, &#39;?&#39;, cex=2) abline(v=16, lty=3, col=&#39;gray50&#39;);text(16, 72, &#39;?&#39;, cex=2) Poser un problÃ¨me par un processus gaussien, câ€™est se demander les valeurs crÃ©dibles qui pourraient Ãªtre obtenues hors du domaine dâ€™observations (par exemple, dans la figure ci-dessus, Ã  x=12 et x=16)? Ou bien, de maniÃ¨re plus gÃ©nÃ©rale, quelles fonctions ont pu gÃ©nÃ©rer les variables rÃ©ponse Ã  partir dâ€™une structure dans les variables prÃ©dictives? Les distributions normales, que nous appellerons gaussiennes dans cette section par concordance avec le terme processus gaussien, sont particuliÃ¨rement utiles pour rÃ©pondre Ã  cette question. Nous avons vu prÃ©cÃ©demment ce que sont les distributions de probabilitÃ©: des outils mathÃ©matiques permettant dâ€™apprÃ©hender la structure des processus alÃ©atoires. Une distribution gaussienne reprÃ©sente une situation oÃ¹ lâ€™on tire au hasard des valeurs continues. Une distribution gaussienne de la variable alÃ©atoire \\(X\\) de moyenne \\(0\\) et de variance de \\(1\\) est notÃ©e ainsi: \\[ X \\sim \\mathcal{N} \\left( 0, 1\\right)\\] Par exemple, une courbe de distribution gaussienne du rythme cardiaque Ã  7:00 pourrait prendre la forme suivante. \\[ bpm \\sim \\mathcal{N} \\left( 65, 5\\right)\\] En R: x_sequence &lt;- seq(50, 80, length=100) plot(x_sequence, dnorm(x_sequence, mean=65, sd=5), type=&quot;l&quot;, xlab=&quot;Rythme cardiaque (bpm)&quot;, ylab=&quot;DensitÃ©&quot;) Une distribution binormale, un cas particulier de la distribution multinormale, comprendra deux vecteurs, \\(x_1\\) et \\(x_2\\). Elle aura donc deux moyennes. Puisquâ€™il sâ€™agit dâ€™une distribution binormale, et non pas deux distributions normales, les deux variables ne sont pas indÃ©pendantes et lâ€™on utilisera une matrice de covariance au lieu de deux variances indÃ©pendantes. \\[ \\binom{x_1}{x_2} \\sim \\mathcal{N} \\Bigg( \\binom{\\mu_1}{\\mu_2}, \\left[ {\\begin{array}{cc} \\Sigma_{x_1} &amp; \\Sigma_{x_1,x_2} \\\\ \\Sigma_{x_1,x_2}^T &amp; \\Sigma_{x_2} \\\\ \\end{array} } \\right] \\Bigg) \\] La matrice \\(\\Sigma\\), dite de variance-covariance, indique sur sa diagonale les variances des variables (\\(\\Sigma_{x_1}\\) et \\(\\Sigma_{x_2}\\)). Les covariances \\(\\Sigma_{x_1,x_2}\\) et \\(\\Sigma_{x_1,x_2}^T\\) sont symÃ©triques et indiquent le lien entre les variables. On pourrait supposer que le rythme cardiaque Ã  8:00 soit corrÃ©lÃ© avec celui Ã  7:00. Mises ensembles, les distributions gaussiennes Ã  7:00 et Ã  8:00 formeraient une distribution gaussienne binormale. \\[ \\binom{bpm_7}{bpm_8} \\sim \\mathcal{N} \\Bigg( \\binom{65}{75}, \\left[ {\\begin{array}{cc} 10 &amp; 6 \\\\ 6 &amp; 15 \\\\ \\end{array} } \\right] \\Bigg) \\] En R: library(&quot;ellipse&quot;) ## ## Attaching package: &#39;ellipse&#39; ## The following object is masked from &#39;package:car&#39;: ## ## ellipse ## The following object is masked from &#39;package:graphics&#39;: ## ## pairs means_vec &lt;- c(65, 75) covariance_mat &lt;- matrix(c(10, 6, 6, 15), ncol=2) par(pty=&#39;s&#39;) plot(ellipse(x=covariance_mat, centre=means_vec, levels=0.95), type=&#39;l&#39;, xlab=&quot;Rythme cardiaque Ã  7:00 (bpm)&quot;, ylab=&quot;Rythme cardiaque Ã  8:00 (bpm)&quot;) #lines(ellipse(x=covariance_mat, centre=means_vec, level=0.8)) On peut se poser la question: Ã©tant donnÃ©e que \\(x_1 = 68\\), quelle serait la distribution de \\(x_2\\)? Dans ce cas bivariÃ©e, la distribution marginale serait univariÃ©e, mais dans le cas multivariÃ© en \\(D\\) dimensions, la distribution marginale oÃ¹ lâ€™on spÃ©cifie \\(m\\) variables serait de \\(D-m\\). de Une propriÃ©tÃ© fondamentale dâ€™une distribution gaussienne est que peu importe lâ€™endroit oÃ¹ lâ€™angle selon lequel on la tranche, la distribution marginale sera aussi gaussienne. Lorsque lâ€™on retranche une ou plusieurs variables en spÃ©cifiant la valeur quâ€™elles prennent, on applique un conditionnement Ã  la distribution. Les points sur lâ€™axe (symbole x) conditionnÃ©s sont des Ã©chantillons tirÃ©s au hasard dans la distribution conditionnÃ©e. Une autre maniÃ¨re de visualiser la distribution gaussienne binormale est de placer \\(x_1\\) et \\(x_2\\) cÃ´te Ã  cÃ´te en abscisse, avec leur valeur en ordonnÃ©e. Le bloc de code suivant peut sembler lourd au premier coup dâ€™Å“il: pas de panique, il sâ€™agit surtout dâ€™instructions graphiques. Vous pouvez vous amuser Ã  changer les paramÃ¨tres de la distribution binormale (section 1) ainsi que la valeur de \\(x_1\\) Ã  laquelle est conditionnÃ©e la distribution de \\(x_2\\) (section 2). Les valeurs que peuvent prendre le rythme cardiaque en \\(x_2\\) sont tirÃ©es alÃ©atoirement dâ€™une distribution conditionnÃ©e. Sautons maintenant au cas multinormal, incluant 6 variables (hexanormal!). Afin dâ€™Ã©viter de composer une matrice de covariance Ã  la mitaine, je me permets de la gÃ©nÃ©rer avec une fonction. Cette fonction particuliÃ¨re est nommÃ©e fonction de base radiale ou exponentiel de la racine. \\[K_{RBF} \\left( x_i, x_j \\right) = \\sigma^2 exp \\left( -\\frac{\\left( x_i - x_j \\right)^2}{2 l^2} \\right) \\] RBF_kernel &lt;- function(x, sigma, l) { n &lt;- length(x) k &lt;- matrix(ncol = n, nrow = n) for (i in 1:n) { for (j in 1:n) { k[i, j] = sigma^2 * exp(-1/(2*l^2) * (x[i] - x[j])^2) } } colnames(k) &lt;- paste0(&#39;x&#39;, 1:n) rownames(k) &lt;- colnames(k) return(k) } Dans la fonction RBF_kernel, x dÃ©signe les dimensions, sigma dÃ©signe un Ã©cart-type commun Ã  chacune des dimensions et l est la longueur dÃ©signant lâ€™amplification de la covariance entre des dimensions Ã©loignÃ©es (dans le sens que la premiÃ¨re dimension est Ã©loignÃ©e de la derniÃ¨re). Pour 6 dimensions, avec un Ã©cart-type de 4 et une longueur de 2. covariance_6 &lt;- RBF_kernel(1:6, sigma=4, l=2) round(covariance_6, 2) ## x1 x2 x3 x4 x5 x6 ## x1 16.00 14.12 9.70 5.19 2.17 0.70 ## x2 14.12 16.00 14.12 9.70 5.19 2.17 ## x3 9.70 14.12 16.00 14.12 9.70 5.19 ## x4 5.19 9.70 14.12 16.00 14.12 9.70 ## x5 2.17 5.19 9.70 14.12 16.00 14.12 ## x6 0.70 2.17 5.19 9.70 14.12 16.00 Changez la valeur de l permet de bien saisir son influence sur la matrice de covariance. Avec un l de 1, la covariance entre \\(x_1\\) et \\(x_6\\) est pratiquement nulle: elle est un peut plus Ã©levÃ©e avec l=2. Pour reprendre lâ€™exemple du rythme cardiaque, on devrait en effet sâ€™attendre Ã  retrouver une plus grande corrÃ©lation entre celles mesurÃ©es aux temps 4 et 5 quâ€™entre les temps 1 et 6. De mÃªme que dans la situation oÃ¹ nous avions une distribution binormale, nous pouvons conditionner une distribution multinormale. Dans lâ€™exemple suivant, je conditionne la distribution multinormale de 6 dimensions en spÃ©cifiant les valeurs prises par les deux premiÃ¨res dimensions. Le rÃ©sultat du conditionnement est une distribution en 4 dimensions. Puisquâ€™il est difficile de prÃ©senter une distribution en 6D, le graphique en haut Ã  gauche ne comprend que les dimensions 1 et 6. Remarquez que la corrÃ©lation entre les dimensions 1 et 6 est faible, en concordance avec la matrice de covariance gÃ©nÃ©rÃ©e par la fonction RBF_kernel. Lancez plusieurs fois le code et voyez ce qui advient des Ã©chantillonnages dans les dimensions 3 Ã  6 selon le conditionnement en 1 et 2. La structure de la covariance assure que les dimensions proches prennent des valeurs similaires, assurant une courbe lisse et non en dents de scie. Pourquoi sâ€™arrÃªter Ã  6 dimensions? Prenons-en plusieurs, puis gÃ©nÃ©rons plus dâ€™un Ã©chantillon. Ensuite, utilisons ces simulations pour de calculer la moyenne et lâ€™Ã©cart-type de chacune des dimensions. Revenons au rythme cardiaque. On pourra utiliser le conditionnement aux temps observÃ©s, soit 7:00, 8:00, 10:00, 14:00 et 17:00 pour estimer la distribution Ã  12:00 et 16:00, oÃ¹ Ã  des dimensions artificielles quelconques ici fixÃ©es aux demi-heures. Comme on devrait sâ€™y attendre, la rÃ©gression rÃ©sultant de la mise en indices de la distribution est prÃ©cise aux mesures, et imprÃ©cise aux indices peu garnis en mesures. Nous avions utilisÃ© 21 dimensions. Lorsque lâ€™on gÃ©nÃ©ralise la procÃ©dure Ã  une quantitÃ© infinie de dimensions, on obtient un processus gaussien. Lâ€™indice de la variable devient ainsi une valeur rÃ©elle. Un processus gaussien, \\(\\mathcal{GP}\\), est dÃ©fini par une fonction de la moyenne, \\(m \\left( x \\right)\\), et une autre de la covariance que lâ€™on nomme noyau (ou kernel), \\(K \\left( x, x&#39; \\right)\\). Un processus gaussien est notÃ© de la maniÃ¨re suivante: \\[\\mathcal{GP} \\sim \\left( m \\left( x \\right), K \\left( x, x&#39; \\right) \\right)\\] La fonction dÃ©finissant la moyenne peut Ãªtre facilement Ã©cartÃ©e en sâ€™assurant de centrer la variable rÃ©ponse Ã  zÃ©ro (\\(y_{centrÃ©} = y - \\hat{y}\\)). Ainsi, par convention, on spÃ©cifie une fonction de moyenne comme retournant toujours un zÃ©ro. Quant au noyau, il peut prendre diffÃ©rentes fonctions de covariance ou combinaisons de fonctions de covariance. RÃ¨gle gÃ©nÃ©rale, on utilisera un noyau permettant de dÃ©finir deux paramÃ¨tres: la hauteur (\\(\\sigma\\)) et la longueur de lâ€™ondulation (\\(l\\)) (figure 12.9). Figure 12.9: HyperparamÃ¨tres dâ€™un noyau RBF. On pourra ajouter Ã  ce noyau un bruit blanc, câ€™est-Ã -dire une variation purement alÃ©atoire, sans covariance (noyau gÃ©nÃ©rant une matrice diagonale). Le noyau devient ainsi un a priori, et le processus gaussien conditionnÃ© aux donnÃ©es devient un a posteriori probabiliste. Finalement, les processus gaussiens peuvent Ãªtre extrapolÃ©s Ã  plusieurs variables descriptives. 12.4.5 Les processus gaussiens en R Pas de souci, vous nâ€™aurez pas Ã  programmer vos propres fonctions pour lancer des processus gaussiens. Vous pourrez passer par caret. Vous pourriez, comme câ€™est le cas avec les rÃ©seaux neuronnaux, obtenir davantage de contrÃ´le sur lâ€™autoapprentissage en utilisant directement la fonction gausspr() du package kernlab. library(&quot;kernlab&quot;) ## ## Attaching package: &#39;kernlab&#39; ## The following object is masked from &#39;package:permute&#39;: ## ## how ## The following object is masked from &#39;package:purrr&#39;: ## ## cross ## The following object is masked from &#39;package:ggplot2&#39;: ## ## alpha x &lt;- c(7, 8, 10, 14, 17) y &lt;- c(61, 74, 69, 67, 78) y_sc &lt;- (y - mean(y)) / sd(y) m &lt;- gausspr(x, y_sc, kernel = &#39;rbfdot&#39;, # le noyau: diffÃ©rents types disponibles (?gausspr) kpar = list(sigma = 4), # hyperparamÃ¨tre du noyau (l est optimisÃ©) variance.model = TRUE, # pour pouvoir gÃ©nÃ©rer les Ã©carts-type scaled = TRUE, # mettre Ã  l&#39;Ã©chelle des variables var = 0.01, # bruit blanc cross = 2) # nombre de plis de la validation croisÃ©e xtest &lt;- seq(6, 18, by = 0.1) y_sc_pred_mean &lt;- predict(m, xtest, type=&quot;response&quot;) y_pred_mean &lt;- y_sc_pred_mean * sd(y) + mean(y) y_sc_pred_sd &lt;- predict(m, xtest, type=&quot;sdeviation&quot;) # &quot;sdeviation&quot; en rÃ©gression et &quot;probabilities&quot; pour la classification y_pred_sd &lt;- y_sc_pred_sd * sd(y) plot(x, y, xlim = c(6, 18), ylim = c(45, 90)) lines(xtest, y_pred_mean) lines(xtest, y_pred_mean + y_pred_sd, col=&quot;red&quot;) lines(xtest, y_pred_mean - y_pred_sd, col=&quot;red&quot;) abline(v=12, lty=3, col=&#39;gray50&#39;);text(12, 67, &#39;?&#39;, cex=2) abline(v=16, lty=3, col=&#39;gray50&#39;);text(16, 72, &#39;?&#39;, cex=2) 12.4.5.1 Application pratique Les processus gaussiens sont utiles pour effectuer des prÃ©dictions sur des phÃ©nomÃ¨ne sur lesquels on dÃ©sire Ã©viter de se commettre sur la structure. Les sÃ©ries temporelles ou les signaux spectraux en sont des exemples. Aussi, jâ€™ai utilisÃ© les processus gaussiens pour modÃ©liser des courbes de rÃ©ponse aux fertilisants. Prenons ces donnÃ©es gÃ©nÃ©rÃ©es au hasard, comprenant lâ€™identifiant de la mesure, le bloc du test, la dose de fertilisant, trois variables environnementales ainsi que la performance de la culture en terme de rendement. fert &lt;- read_csv(&quot;data/11_response_fert.csv&quot;) fert %&gt;% ggplot(aes(x = Dose, y = Yield)) + geom_line(aes(group = Block), colour = rgb(0, 0, 0, 0.5)) Les blocs 1 Ã  30 serviront dâ€™entraÃ®nement, les autres de test. Le rendement est mis Ã  lâ€™Ã©chelle pour la modÃ©lisation. environment &lt;- fert %&gt;% dplyr::select(Dose, var1, var2, var3) yield_sc &lt;- (fert$Yield - mean(fert$Yield)) / sd(fert$Yield) environment_tr &lt;- environment[fert$Block &lt;= 30, ] environment_te &lt;- environment[fert$Block &gt; 30, ] yield_tr &lt;- yield_sc[fert$Block &lt;= 30] yield_te &lt;- yield_sc[fert$Block &gt; 30] Je pourrais optimiser les hyperparamÃ¨tres en crÃ©ant une grille puis en lanÃ§ant plusieurs processus gaussiens en boucle. Mais pour lâ€™exemple jâ€™utilise des hyperparamÃ¨tres quelconque. yield_gp &lt;- gausspr(environment_tr, yield_tr, kernel = &#39;rbfdot&#39;, kpar = list(sigma = 0.1), variance.model = TRUE, scaled = TRUE, var = 0.1, cross = 10) # rendements prÃ©dits dans l&#39;Ã©chelle originale gp_pred_tr &lt;- predict(yield_gp, environment_tr, type=&quot;response&quot;) * sd(fert$Yield) + mean(fert$Yield) gp_pred_te &lt;- predict(yield_gp, environment_te, type=&quot;response&quot;) * sd(fert$Yield) + mean(fert$Yield) # rendements rÃ©els dans l&#39;Ã©chelle originale yield_tr_os &lt;- yield_tr * sd(fert$Yield) + mean(fert$Yield) yield_te_os &lt;- yield_te * sd(fert$Yield) + mean(fert$Yield) par(mfrow = c(1, 2)) plot(yield_tr_os, gp_pred_tr, main = &quot;train&quot;) abline(0, 1, col = &quot;red&quot;) plot(yield_te_os, gp_pred_te, main = &quot;test&quot;) abline(0, 1, col = &quot;red&quot;) La prÃ©diction semble bien fonctionner en entraÃ®nement comme en test. Pour une application Ã  un cas dâ€™Ã©tude, disons que pour mon site jâ€™ai des variables environnementales de valeurs du bloc 50, et que je cherche la dose optmale. fert %&gt;% dplyr::filter(Block == 50) %&gt;% dplyr::select(var1, var2, var3) %&gt;% dplyr::slice(1) ## # A tibble: 1 x 3 ## var1 var2 var3 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.57 101. -10.7 Je peux crÃ©er un tableau comprenant des environnements Ã©gaux pour chaque ligne, mais comprenant des incrÃ©ments de dose, puis prÃ©dire la courbe de rÃ©ponse ainsi que son incertitude. Et puisque câ€™est un cas documentÃ©, je peux afficher les rÃ©sultats de lâ€™essai pour vÃ©rifier si le modÃ¨le est crÃ©dible. environment_appl &lt;- data.frame(Dose = seq(0, 200, 5), var1 = 1.57, var2 = 101.5, var3 = -10.7) yield_appl_sc &lt;- predict(yield_gp, environment_appl, type=&quot;response&quot;) y_sc_pred_sd_sc &lt;- predict(yield_gp, environment_appl, type=&quot;sdeviation&quot;) yield_appl &lt;- yield_appl_sc * sd(fert$Yield) + mean(fert$Yield) yield_appl_sd &lt;- y_sc_pred_sd_sc * sd(fert$Yield) plot(environment_appl$Dose, yield_appl, type = &quot;l&quot;, ylim = c(0, 35)) points(x = fert[fert$Block == 50, ]$Dose, y = fert[fert$Block == 50, ]$Yield) lines(environment_appl$Dose, yield_appl + yield_appl_sd, col = &quot;red&quot;) lines(environment_appl$Dose, yield_appl - yield_appl_sd, col = &quot;red&quot;) &lt;our chaque incrÃ©ment de dose de la courbe de rponse, il est possible de calculer un rendement Ã©conomique et/ou Ã©cologique en fonction du prix de la dose pondÃ©rÃ© par un coÃ»t environnemental, puis de soutirer une performance optimale en terme de fertilisation. Exercice. Changez les valeurs des variables environnementales pour gÃ©nÃ©rer le tableau environment_appl avec des valeurs qui sortent du lot (voir figure 12.10). Quâ€™observez-vous? Pourquoi? Figure 12.10: Vairables environnementales du tableau fictif fert. Exercice. Effectuer la prÃ©diction du rendement avec dâ€™autres techniques, comme des rÃ©seaux neuronaux. Comment les modÃ¨les se comportent-ils? "],
["chapitre-geo.html", "13 Les donnÃ©es gÃ©ospatiales 13.1 Les donnÃ©es spatiales 13.2 Cartographier avec le module ggmap 13.3 Types gÃ©nÃ©riques de donnÃ©es spatiales 13.4 Les choroplÃ¨the 13.5 Les rasters 13.6 Autoapprentissage spatial 13.7 Les objets spatialisÃ©s en R 13.8 Les systÃ¨mes de coordonnÃ©es 13.9 Manipuler des tableaux sf 13.10 Manipuler des objets raster 13.11 Graphiques dâ€™objets spatialisÃ©s 13.12 Ressources complÃ©mentaires", " 13 Les donnÃ©es gÃ©ospatiales ï¸Â Objectifs spÃ©cifiques: Ã€ la fin de ce chapitre, vous saurez cartographier des donnÃ©es gÃ©orÃ©fÃ©rencÃ©es avec ggplot serez en mesure dâ€™effectuer un autoapprentissage spatial saurez utiliser R comme outil dâ€™analyse spatiale (donnÃ©e associÃ©es Ã  des points, lignes, polygones et rasters) 13.1 Les donnÃ©es spatiales Des donnÃ©es associÃ©es Ã  un endroit sont spatiales. Puisque ce cours ne traite pas dâ€™Ã©cologie exoplanÃ©taire, nous traiterons en particulier de donnÃ©es gÃ©ospatiales, mot que lâ€™on utilise pour dÃ©signer des donnÃ©es avec rÃ©fÃ©rence spatiale sur la planÃ¨te Terre. Lorsque nous avons abordÃ© les sÃ©ries temporelles, jâ€™ai pris pour acquis que nous utilisions le calendrier grÃ©gorien comme rÃ©fÃ©rence temporelle. Les donnÃ©es gÃ©ospatiales, quant Ã  elles, sont souvent exprimÃ©es en termes dâ€™angles donnant une position Ã  la surface dâ€™une rÃ©fÃ©rence dont la forme est un ellipsoide de rÃ©volution (le systÃ¨me gÃ©odÃ©sique): la longitude dÃ©crivant lâ€™angle de part et dâ€™autre (entre 0Â° et 180Â° Ouest ou Est) du mÃ©ridien de rÃ©fÃ©rence (le premier mÃ©ridien, prÃ¨s de Greenwich) et la latitude dÃ©crivant lâ€™angle entre lâ€™Ã©quateur et lâ€™un des pÃ´les (entre 0Â° et 90Â° Nord ou Sud). Les angles sont parfois exprimÃ©es sous forme degrÃ©Â° minute' seconde'' Sens cardinal, par exemple 46Â° 53' 21.659'' S. Toutefois, il est plus commun (et plus pratique) dâ€™exprimer les angles de maniÃ¨re dÃ©cimale, accompagnÃ©e dâ€™un signe pour indiquer le sens cardinal (par convention positif au Nord et Ã  lâ€™Est). Par exemple, on exprimerait 46Â° 53' 21.659'' S sous forme dÃ©cimale par les opÃ©rations suivantes. \\[ secondes = \\frac{21.659&#39;&#39;}{3600&#39;&#39; \\cdot Â°^{-1}} = 0.00602Â° \\] \\[ minutes = \\frac{53&#39;}{60&#39; \\cdot Â°^{-1}} = 0.883Â° \\] \\[ - \\left( 46Â° + 0.883Â° + 0.00602Â° \\right) = -46.889Â° \\] La rÃ©fÃ©rence de lâ€™altitude est gÃ©nÃ©ralement donnÃ©e par rapport Ã  un gÃ©oÃ¯de, qui est une Ã©lÃ©vation thÃ©orique du niveau de la mer ainsi quâ€™une direction de la gravitÃ© Ã©valuÃ©e sur toute la surface du globe. Toutefois, les angles ne sont pas pratiques pour Ã©valuer des distances, ce que lâ€™on fera bien mieux sur une carte. Pour prÃ©senter la Terre sous forme de carte, on crÃ© des reprÃ©sentations applaties du globe sous forme de carte avec lâ€™aide dâ€™Ã©quations de projection. Or, il existe diffÃ©rents systÃ¨mes gÃ©odÃ©siques, diffÃ©rents gÃ©oides et de nombreuses maniÃ¨res de calculer les projections. Ainsi, il est important de spÃ©cifier les rÃ©fÃ©rences utilisÃ©es lorsque lâ€™on donne dans la prÃ©cision. Pour cette sÃ©ance, je prendrai pour acquis que vous possÃ©dez certaines bases en positionnement, qui sont par ailleurs essentielles pour pratiquer adÃ©quatement un mÃ©tier scientifique. library(&quot;tidyverse&quot;) Dans ce chapitre, jâ€™utiliserai notamment comme exemple dâ€™application des donnÃ©es mÃ©tÃ©orologiques soutirÃ©es dâ€™Environnement Canada grÃ¢ce au module weathercan, obtenues entre les longitudes -60Â° et -80Â° et entre les latitudes 45Â° et 50Â° en mai 2018. Jâ€™ai effectuÃ© quelques opÃ©rations pour obtenir des indicateurs mÃ©tÃ©o: degrÃ©s-jour (somme des degrÃ©s de tempÃ©rature moyenne &gt; 5 Â°C, degree_days), prÃ©cipitations totales (cumul_precip) et indice de diversitÃ© des prÃ©cipitations (plus lâ€™indice sdi sâ€™approche de 1, plus les tempÃ©ratures sont uniformÃ©ment distribuÃ©es pendant la pÃ©riode). Les calculs sont consignÃ©s dans le fichier lib/12_weather-fetch.R, mais Ã©tant donnÃ© que le tÃ©lÃ©chargement prend pas mal de temps, jâ€™ai crÃ©Ã© un csv. Les coordonnÃ©es se trouvent dans les colonnes de latitude (lat) et longitude (lon). source(&quot;lib/12_weather-fetch.R&quot;) weather &lt;- read_csv(&quot;data/12_weather.csv&quot;) weather %&gt;% head() ## # A tibble: 6 x 8 ## station_id station_name prov lat lon degree_days cumul_precip sdi ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10247 ILE AUX GRUES QC 47.1 -70.5 NA NA NA ## 2 10661 NORTHEAST MARGAREE (AUT) NS 46.4 -61.0 372. 222. 0.904 ## 3 10732 NICOLET QC 46.2 -72.7 823. 78.4 0.704 ## 4 10761 MCTAVISH QC 45.5 -73.6 915. 107 0.740 ## 5 10762 STE-CLOTILDE QC 45.2 -73.7 860. 85.2 0.820 ## 6 10763 ILES DE LA MADELEINE QC 47.4 -61.8 326. 167. 0.853 Dans le tableau weather, chaque observation est liÃ©e Ã  un point dans lâ€™espace. Dans ce cas, nous avons tous les outils nÃ©cessaires pour afficher nos points dans lâ€™espace (figure 13.1). weather %&gt;% ggplot(mapping = aes(x = lon, y = lat)) + geom_point() Figure 13.1: Position des stations mÃ©tÃ©o du tableau weather Si vous avez lâ€™oeil averti, vous avez peut-Ãªtre repÃ©rÃ© le QuÃ©bec, le Nouveau-Brunswick et la frontiÃ¨re avec les Ã‰tats-Unis. Lâ€™absence de repÃ¨re rend nÃ©anmoins difficle lâ€™interprÃ©tation de cette carte. 13.2 Cartographier avec le module ggmap Le module ggmap ajoute des couches dâ€™images tÃ©lÃ©chargÃ©es depuis des services de cartorgaphie en ligne. Dans cette section, nous allons utiliser le service de carte Stamen, qui ne demande pas de frais dâ€™utilisation ou dâ€™enregistrement particulier. La fonction get_stamenmap() demande une boÃ®te de coordonnÃ©es dÃ©limitant la carte Ã  produire, un paramÃ¨tre de zoom (plus le zoom est Ã©levÃ©, plus la carte incluera de dÃ©tails: un zoom de 2 est suffisant pour une carte du monde, mais pour lâ€™Est du Canada, on prendra plutÃ´t un zoom de 6 - un bon zoom est obtenu par tÃ¢tonnement) et accessoirement un type de carte. library(&quot;ggmap&quot;) east_canada &lt;- get_stamenmap(bbox = c(left=-81, right = -59, bottom = 44, top = 51), zoom = 6, maptype = &quot;terrain&quot;) Pour afficher la carte, nous enchÃ¢ssons notre objet dans une fonction ggmap(), Ã  laquelle nous pouvons ajouter une couche. ggmap(east_canada) + geom_point(data = weather, mapping = aes(x = lon, y = lat)) Une approche plus gÃ©nÃ©raliste consiste Ã  spÃ©cifier dans la fonction ggmap() lâ€™agument de base utilisÃ© pour lancer un graphique ggplot, comme Ã  la figure figure 13.2. En outre, lâ€™utiliation de lâ€™argument base_layer permet dâ€™effectuer des fecettes et dâ€™Ã©viter de spÃ©cifier la source des donnÃ©es dans toutes les couches subsÃ©quentes. ggmap(east_canada, base_layer = ggplot(weather, aes(x = lon, y = lat))) + geom_point() Figure 13.2: Position des stations mÃ©tÃ©o du tableau weather superposÃ© Ã  une carte importÃ©e par ggmap La carte que nous avons crÃ©Ã©e est de type terrain, un type dâ€™affichage efficace mais peu appropriÃ© pour une publication visant Ã  Ãªtre imprimÃ©e. Le type toner-lite est davantage vouÃ© Ã  lâ€™impression, alors que le type watercolor est plus joli pour le web. Les types offerts sont listÃ©s dans la ficher dâ€™aide ?get_stamenmap. maptype = c(&quot;terrain&quot;, &quot;terrain-background&quot;, &quot;terrain-labels&quot;, &quot;terrain-lines&quot;, &quot;toner&quot;, &quot;toner-2010&quot;, &quot;toner-2011&quot;, &quot;toner-background&quot;, &quot;toner-hybrid&quot;, &quot;toner-labels&quot;, &quot;toner-lines&quot;, &quot;toner-lite&quot;, &quot;watercolor&quot;) 13.3 Types gÃ©nÃ©riques de donnÃ©es spatiales Nous avons jusquâ€™Ã  prÃ©sent utilisÃ© des donnÃ©es spatiales attachÃ©es Ã  un point. Ce ne sont pas les seuls. DonnÃ©es ponctuelles: associÃ©es Ã  un point. Exemple: mesure Ã  un endroit prÃ©cis. DonnÃ©es linÃ©aires: associÃ©es Ã  une sÃ©rie de point. Exemple: mesure associÃ©e Ã  une route ou une riviÃ¨re. DonnÃ©es de polygone: associÃ©es Ã  une aire dÃ©limitÃ©e par des points. Exemples: DonnÃ©es associÃ©es Ã  un champ, une unitÃ© administrative, un bassin versant, etc. DonnÃ©es raster: associÃ©es Ã  une grille. Exemple: une image satellite oÃ¹ chaque pixel est associÃ© Ã  un recouvrement foliaire. Lâ€™enregistrement des donnÃ©es ponctuelles ne posent pas de dÃ©fi particulier. Les donnÃ©es associÃ©es Ã  un ligne, toutefois posent un problÃ¨me dâ€™organisation, puisquâ€™une ligne elle-mÃªme contient des informations sur les coordonnÃ©es de ses points ainsi que lâ€™ordre dans lequel les points sont connectÃ©s. On pourra soit crÃ©er un tableau de donnÃ©es ayant une colonne oÃ¹ lâ€™identifiant de la ligne est consignÃ©, renvoyant Ã  un autre tableau oÃ¹ chaque ligne dÃ©crit un point en terme dâ€™identifiant de ligne Ã  laquelle il appartient, ses coordonnÃ©es, ainsi que sont ordre de rattachement dans la ligne. Les informations de la ligne pourraient aussi Ãªtre enchÃ¢ssÃ©es dans une cellule de tableau de donnÃ©es, en tant que sous-tableau. Ou bien, on pourrait crÃ©er un tableau sous forme de jointure entre le tableau des donnÃ©es et le tableau des lignes. Un dÃ©fi similaire pourrait subvenir avec des polygones, qui demandent davantage dâ€™information Ã©tant donnÃ©e quâ€™ils peuvent Ãªtre trouÃ©s (par exemple un lac) ou sÃ©parÃ©s en diffÃ©rents morceaux (un archipel, par exemple). Enfin, il existe des formats de donnÃ©es spatiales gÃ©nÃ©riques (shapefiles et geojson) ou spÃ©cialement conÃ§us pour R (module sf), que nous couvrirons plus loin dans ce chapitre. 13.4 Les choroplÃ¨the Les cartes de type choroplÃ¨the se prÃ©sentent sous forme de polygones dÃ©crits par un groupe et un ordre, dont un la couleur de remplissage dÃ©pend dâ€™une variable. Les pays, par exemple, forment des polygones. world &lt;- map_data(map = &quot;world&quot;) # jeu de donneÃ©s de ggplot2 head(world) ## long lat group order region subregion ## 1 -69.89912 12.45200 1 1 Aruba &lt;NA&gt; ## 2 -69.89571 12.42300 1 2 Aruba &lt;NA&gt; ## 3 -69.94219 12.43853 1 3 Aruba &lt;NA&gt; ## 4 -70.00415 12.50049 1 4 Aruba &lt;NA&gt; ## 5 -70.06612 12.54697 1 5 Aruba &lt;NA&gt; ## 6 -70.05088 12.59707 1 6 Aruba &lt;NA&gt; La fonction map_data() de ggplot2 permet de soutirer des polygone de certaines cartes. Pour les zones gÃ©ographiques prÃ©dÃ©finies, il est prÃ©fÃ©rable de soutirer les polygones dÃ©sirÃ©es de donnÃ©es existantes plutÃ´t que les crÃ©er soi-mÃªme. Souvent, ces polygones ne sont pas directement disponibles en R. Dans ce cas, il faudra trouver des fichiers de carte auprÃ¨s de Statistique Canada, DonnÃ©es QuÃ©bec, etc., ce que nous verrons plus loin. especes_menacees &lt;- read_csv(&#39;data/WILD_LIFE_14012020030114795.csv&#39;) iucn_oecd &lt;- especes_menacees %&gt;% dplyr::filter(IUCN == &#39;THREATENED&#39;) %&gt;% dplyr::select(Country, Value) %&gt;% dplyr::group_by(Country) %&gt;% dplyr::summarise(n_threatened_species = sum(Value)) %&gt;% dplyr::arrange(desc(n_threatened_species)) Les noms des pays doivent correspondre exactement, et la colonne des pays doit porter le mÃªme nom (jâ€™ai inspectÃ© les vecteurs iucn_oecd30$Country et unique(world$region)). iucn_oecd &lt;- iucn_oecd %&gt;% replace(.==&quot;United States&quot;, &quot;USA&quot;) %&gt;% replace(.==&quot;Slovak Republic&quot;, &quot;Slovakia&quot;) %&gt;% replace(.==&quot;United Kingdom&quot;, &quot;UK&quot;) %&gt;% dplyr::rename(&quot;region&quot; = &quot;Country&quot;) Les espÃ¨ces sont jointes au tableau contenant les polygones. world_iucn &lt;- world %&gt;% left_join(iucn_oecd, by = &quot;region&quot;) Pour le graphique de la figure 13.3, La stratÃ©gie est de crÃ©er des polygones groupÃ©s par groupes de polygones (group = group), dont la couleur de remplissage correspond Ã  au nombre dâ€™espÃ¨ce. Jâ€™ajoute coord_map() en spÃ©cifiant une projection de type Mercator (essayez projection = &quot;ortho&quot;). Le reste est de la dÃ©coration. ggplot(world_iucn, aes(long, lat)) + geom_polygon(aes(group = group, fill = n_threatened_species), colour = &quot;grey50&quot;, lwd = 0.1) + coord_map(projection = &quot;mercator&quot;, xlim = c(-180, 180), ylim = c(-90, 90)) + scale_fill_gradient(low = &quot;#8CBFE6&quot;, high = &quot;#FF0099&quot;, na.value = &quot;grey80&quot;) + labs(title = &quot;Number of threatened species in OECD countries&quot;, subtitle = &quot;Source: OCDE, 2019&quot;) + theme(panel.background = element_rect(fill = &quot;grey97&quot;), plot.background = element_rect(fill = &quot;white&quot;), axis.text = element_blank(), axis.ticks = element_blank(), axis.title = element_blank(), panel.grid = element_blank()) + guides(fill = guide_legend(title = &quot;Number of\\nthreatened\\nspecies&quot;)) Figure 13.3: Nombre dâ€™espÃ¨ces en danger dans les pays de lâ€™OCDE Comme câ€™est le cas des points de la figure 13.2, on peut superposer des choroplÃ¨thes Ã  des cartes tÃ©lÃ©chargÃ©es (figure 13.4). worldmap &lt;- get_stamenmap(bbox = c(left=-170, right = 170, bottom = -80, top = 80), zoom = 2, maptype = &quot;watercolor&quot;) world_iucn_oecd &lt;- world_iucn %&gt;% filter(!is.na(n_threatened_species)) ggmap(worldmap, base_layer = ggplot(world_iucn_oecd, aes(long, lat))) + geom_polygon(aes(group = group, fill = n_threatened_species), colour = &quot;black&quot;, lwd = 0.2) + coord_map(projection = &quot;mercator&quot;, xlim = c(-180, 180), ylim = c(-90, 90)) + scale_fill_gradient(low = &quot;blue&quot;, high = &quot;red&quot;, na.value = &quot;grey80&quot;) + labs(title = &quot;Number of threatened species in OECD countries&quot;, subtitle = &quot;Source: OCDE, 2019&quot;) + theme(axis.text = element_blank(), axis.ticks = element_blank(), axis.title = element_blank(), panel.grid = element_blank()) + guides(fill = guide_legend(title = &quot;Number of\\nthreatened\\nspecies&quot;)) Figure 13.4: Nombre dâ€™espÃ¨ces en danger dans les pays de lâ€™OCDE superposÃ© Ã  une carte importÃ©e par ggmap Si vous savez crÃ©er des polygones, vous saurez crÃ©er des lignes de maniÃ¨re similaire avec la couche graphique geom_path(). Pour les rasters, câ€™est moins Ã©vident. 13.5 Les rasters Les rasters sont des donnÃ©es associÃ©es Ã  un grille. Nous avons introduit la fonction expand.grid() au chapitre 12 lorsque nous dÃ©sirions crÃ©er un tableau oÃ¹ chaque ligne dÃ©signe une des combinaisons possibles dâ€™hyperparamÃ¨tres pour ajuster un modÃ¨le dâ€™autoapprentissage. De mÃªme, nous pouvons crÃ©er une grille comprenant les combinaisons de longitudes et de latitudes, puis crÃ©er une variable spatialisÃ©e \\(z = 10\\times sin \\left( xy \\right) - 0.01x^2 + 0.05y^2\\). grid &lt;- expand.grid(lon = seq(from = -80, to = -60, by = 0.25), lat = seq(from = 45, to = 50, by = 0.25)) grid &lt;- grid %&gt;% mutate(z = 10*sin(lon*lat) - 0.01*lon^2 + 0.05*lat^2) # crÃ©er une variable spatialisÃ©e skimr::skim(grid) # vu au chapitre sur les tableaux Table 13.1: Data summary Name grid Number of rows 1701 Number of columns 3 _______________________ Column type frequency: numeric 3 ________________________ Group variables None Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist lon 0 1 -70.00 5.85 -80.00 -75.00 -70.00 -65.00 -60.00 â–‡â–‡â–‡â–‡â–‡ lat 0 1 47.50 1.51 45.00 46.25 47.50 48.75 50.00 â–‡â–†â–†â–†â–† z 0 1 63.61 12.85 28.97 54.51 63.97 72.76 95.81 â–â–…â–‡â–†â–‚ Pour visualiser une grille avec ggplot2, on peut avoir recourt Ã  la couche graphique geom_tile(), dont la couleur remplissage est associÃ©e Ã  la colonne z du tableau. Ce type de graphique est appelÃ©e heatmap (figure ??). ggplot(grid, aes(lon, lat)) + geom_tile(aes(fill = z)) Remarquez que jâ€™ai crÃ©Ã© une fonction pour gÃ©nÃ©rer une variable spatialisÃ©e. Une telle fonction nâ€™a pas besoin dâ€™Ãªtre inventÃ©e: on peut en crÃ©er une en utilisant les outils que nous avons appris jusquâ€™Ã  prÃ©sent, en particulier avec lâ€™autoapprentissage. 13.6 Autoapprentissage spatial La gÃ©ostatistique est lâ€™Ã©tude statistique des variables spatiales, un sujet complexe qui sort du cadre de ce cours - que vous pourrez creuser dans le livre Applied Spatial Data Analysis with R. Ici, nous allons projeter des variables spatialisÃ©es Ã  lâ€™aide de lâ€™autoapprentissage, oÃ¹ la position (coordonnÃ©es en longitude et latitude, par exemple) peut servir de variable prÃ©dictive (ainsi que, Ã©ventuellement, des variables spatialisÃ©es concernant lâ€™altitude, lâ€™hydrologie, la gÃ©omorphologie, lâ€™Ã©cologie, la sociologie, la gestion du territoire, etc.). Pour ce faire, vous pourrez utiliser algorithme qui convient Ã  vos donnÃ©es et Ã  votre domaine dâ€™Ã©tude. Nous allons utiliser les processus gaussiens, qui sont particuliÃ¨rement utiles pour Ã©valuer lâ€™incertitude des prÃ©dictions. Par exemple, nous allons prÃ©dire sur une grille les donnÃ©es de degrÃ©s-jour du tableau weather avec un processus gaussien. Pour Ã©valuer la performance dâ€™une prÃ©diction, nâ€™oublions de sÃ©parer nos donnÃ©es en jeux dâ€™entraÃ®nement et de test (avec la fonction caret::createDataPartition())! library(&quot;caret&quot;) weather_dd &lt;- weather %&gt;% dplyr::select(lon, lat, degree_days) %&gt;% drop_na() weather_dd_sc &lt;- weather_dd %&gt;% mutate(degree_days = (degree_days - mean(degree_days))/sd(degree_days)) train_id &lt;- createDataPartition(y = weather_dd_sc$degree_days, p = 0.7, list = FALSE) Utilisons la fonction kernlab::gausspr(), vue au chapitre 12. library(&quot;kernlab&quot;) dd_gp &lt;- gausspr(x = weather_dd_sc[train_id, c(&quot;lon&quot;, &quot;lat&quot;)], y = weather_dd_sc[train_id, &quot;degree_days&quot;], kernel = &quot;rbfdot&quot;, #kpar = list(sigma = 01), # laisser optimiser variance.model = TRUE, scale = TRUE, var = 0.1, cross = 5) ## Using automatic sigma estimation (sigest) for RBF or laplace kernel Ã‰valuons visuellement al performance de la prÃ©diction (figure 13.5). pred_dd_tr &lt;- predict(dd_gp) pred_dd_te &lt;- predict(dd_gp, newdata = weather_dd_sc[-train_id, c(&quot;lon&quot;, &quot;lat&quot;)]) par(mfrow = c(1, 2)) plot(weather_dd_sc$degree_days[train_id], pred_dd_tr, main = &quot;Train prediction&quot;, xlab = &quot;mesurÃ©&quot;, ylab = &quot;prÃ©dit&quot;) abline(0, 1, col=&quot;red&quot;) plot(weather_dd_sc$degree_days[-train_id], pred_dd_te, main = &quot;Test prediction&quot;, xlab = &quot;mesurÃ©&quot;, ylab = &quot;prÃ©dit&quot;) abline(0, 1, col=&quot;red&quot;) Figure 13.5: Performance du processus gaussien en entraÃ®nement et en test. La prÃ©diction nâ€™est pas extraordinaire, mais gardons-la pour lâ€™exemple (jâ€™ai essayÃ© avec des rÃ©seaux neuronaux sans plus de succÃ¨s). La prochaine Ã©tape est de crÃ©er une gille oÃ¹ chaque point [longitude, latitude] servira de variable explicative pour calculer les degrÃ©s-jour. grid &lt;- expand.grid(lon = seq(from = -80, to = -60, by = 0.25), lat = seq(from = 45, to = 50, by = 0.25)) grid &lt;- grid %&gt;% mutate(pred_dd_mean = predict(dd_gp, newdata = ., type = &quot;response&quot;) * sd(weather_dd$degree_days) + mean(weather_dd$degree_days), pred_dd_sd = predict(dd_gp, newdata = ., type = &quot;sdeviation&quot;) * sd(weather_dd$degree_days)) head(grid) ## lon lat pred_dd_mean pred_dd_sd ## 1 -80.00 45 654.5782 70.82743 ## 2 -79.75 45 647.5643 62.33121 ## 3 -79.50 45 637.1303 54.88782 ## 4 -79.25 45 623.4572 48.73097 ## 5 -79.00 45 606.9355 43.98920 ## 6 -78.75 45 588.1704 40.62892 Utilisons les polygones de la carte du monde zoomÃ©e Ã  lâ€™endroit qui nous intÃ©resse, et ajoutons-y notre prÃ©diction superposÃ©e par les localisations des stations mÃ©tÃ©o. Jâ€™ajoute des contours ainsi que des Ã©tiquettes de contours (ce qui nÃ©cessite le module metR). Les processus gaussiens permettent de juxtaposer une carte des Ã©cart-type des prÃ©dictions, donnant une apprÃ©ciation de la prÃ©cision du modÃ¨le (figure 13.6). Cette juxtaposition est effectuÃ©e avec la fonction plot_grid() le module cowplot. library(&quot;metR&quot;) ## ## Attaching package: &#39;metR&#39; ## The following object is masked from &#39;package:kernlab&#39;: ## ## cross ## The following object is masked from &#39;package:aqp&#39;: ## ## denormalize ## The following object is masked from &#39;package:greta&#39;: ## ## f ## The following object is masked from &#39;package:purrr&#39;: ## ## cross gg_mean &lt;- ggplot(grid, aes(x = lon, y = lat)) + xlim(c(-80, -60)) + ylim(c(45, 50)) + coord_equal() + geom_tile(aes(fill = pred_dd_mean)) + geom_contour(data = grid, mapping = aes(x = lon, y = lat, z = pred_dd_mean), binwidth = 50, colour = &quot;black&quot;, lwd = 0.2) + geom_label_contour(aes(z = pred_dd_mean)) + geom_path(data = world, aes(x = long, y = lat, group = group)) + geom_point(data = weather, mapping = aes(x = lon, y = lat), size = 0.1) + scale_fill_gradient(low = &quot;#8CBFE6&quot;, high = &quot;#FF0099&quot;, na.value = &quot;grey80&quot;) gg_sd &lt;- ggplot(grid, aes(x = lon, y = lat)) + xlim(c(-80, -60)) + ylim(c(45, 50)) + coord_equal() + geom_tile(aes(fill = pred_dd_sd)) + geom_contour(data = grid, mapping = aes(x = lon, y = lat, z = pred_dd_sd), binwidth = 50, colour = &quot;black&quot;, lwd = 0.2) + geom_label_contour(aes(z = pred_dd_sd)) + geom_path(data = world, aes(x = long, y = lat, group = group)) + geom_point(data = weather, mapping = aes(x = lon, y = lat), size = 0.1) + scale_fill_gradient(low = &quot;#8CBFE6&quot;, high = &quot;#FF0099&quot;, na.value = &quot;grey80&quot;) cowplot::plot_grid(gg_mean, gg_sd, labels = c(&quot;A&quot;, &quot;B&quot;), nrow = 2) Figure 13.6: PrÃ©diction des degrÃ©s-jour dans lâ€™espace avec les processus gaussiens 13.7 Les objets spatialisÃ©s en R Au chapitre 11, nous avons couvert le type dâ€™objet ts, spÃ©cialisÃ© pour les sÃ©ries temporelles. De mÃªme, le type dâ€™objet sf est spÃ©cialisÃ© pour les objets georÃ©fÃ©rencÃ©s. Les formats de donnÃ©es spatiales conventionnellement utilisÃ©s en R depuis 2003 sont offerts par le module sp. Ce format hÃ©ritait de difficultÃ©s, rÃ©cemment surmontÃ©es par le module sf, plus convivial et mieux adaptÃ© au tidyverse. Bien que sp soit plus largement documentÃ©, sf est suffisamment mature pour une utilisation professionnelle. Ã‰videmment, un aide-mÃ©moire a Ã©tÃ© crÃ©Ã© (figure 13.7). Figure 13.7: Aide-mÃ©moire du module sf, crÃ©Ã© par RStudio Nous avons couvert quatre types de donnÃ©es spatiales. Nous allons maintenant les traiter en deux catÃ©gories: les donnÃ©es vectorielle, comprenant les points, lignes et polygones et les donnÃ©es raster, comprenant les grilles de donnÃ©es. 13.7.1 DonnÃ©es vectorielles (points, lignes et polygones) Un cas typique consiste Ã  importer un tableau de donnÃ©es localisÃ©es en un point, que lâ€™on dÃ©sire localiser en format sf avec la fonction st_as_sf(). Le tableau weather, par exemple, comporte une latitude (colonne lat) et une longitude (colonne lon), spÃ©cifiÃ©es dans lâ€™argument coord. Puisquâ€™il sâ€™agit de donnÃ©es canadiennes, je suppose que les coordonnÃ©es sont projetÃ©es en format NAD83, tel quâ€™utilisÃ© par Statistique Canada et Ressources naturelles Canada (Ã  dÃ©faut de trouver la bonne info en ce moment). Le code PROJ4, spÃ©cifiÃ© sous lâ€™argument crs, dÃ©crit lâ€™ellipsoÃ¯de utilisÃ© pour calculer les longitudes et latitudes ainsi que, sâ€™il y a lieu, la projection (dÃ©tails plus loin dans la section 13.8). library(&quot;sf&quot;) ## Linking to GEOS 3.6.2, GDAL 2.2.3, PROJ 4.9.3 weather_geo &lt;- weather %&gt;% st_as_sf(coords = c(&quot;lon&quot;, &quot;lat&quot;), crs = &quot;+proj=longlat +datum=NAD83&quot;) weather_geo ## Simple feature collection with 260 features and 6 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: -79.85 ymin: 45.02 xmax: -60.04 ymax: 49.84 ## epsg (SRID): 4269 ## proj4string: +proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs ## # A tibble: 260 x 7 ## station_id station_name prov degree_days cumul_precip sdi geometry ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;POINT [Â°]&gt; ## 1 10247 ILE AUX GRUES QC NA NA NA (-70.53 47.07) ## 2 10661 NORTHEAST MARGAREE (AUT) NS 372. 222. 0.904 (-60.98 46.37) ## 3 10732 NICOLET QC 823. 78.4 0.704 (-72.66 46.23) ## 4 10761 MCTAVISH QC 915. 107 0.740 (-73.58 45.5) ## 5 10762 STE-CLOTILDE QC 860. 85.2 0.820 (-73.68 45.17) ## 6 10763 ILES DE LA MADELEINE QC 326. 167. 0.853 (-61.77 47.43) ## 7 10764 TROIS-RIVIERES QC 753. 72.2 0.731 (-72.52 46.35) ## 8 10791 MATAGAMI QC 343. 72.4 0.745 (-77.79 49.76) ## 9 10792 GRAND ETANG NS 268. NA NA (-61.05 46.55) ## 10 10797 MISTOOK QC 426. NA NA (-71.72 48.6) ## # â€¦ with 250 more rows Notre objet sf comprend des mÃ©tadonnÃ©es sur le type de gÃ©omÃ©trie (geometry type: POINT), les limites des objets (bbox: ...), le systÃ¨me de rÃ©fÃ©rence (epsg ou proj4string: ...) ainsi que le tableau descriptif. En format sf, la colonne geometry (qui elle est de type sfc) comprend dans chacune des cellules, exprimÃ©e sous forme de liste, toute lâ€™information nÃ©cessaire Ã  la construction de la gÃ©omÃ©trie, que ce soit un point, une ligne ou un polygone. Pour ce qui est des polygones et des lignes, il est plus commun de les importer depuis des sources institutionnelles. On pourra tÃ©lÃ©charger des donnÃ©es gÃ©ographiques, puis dÃ©zipper les fichier manuellement. Mais on peut aussi copier un lien, le coller dans R et dÃ©zipper automatiquement. Le block de code suivant tÃ©lÃ©charge un dossier de shapefiles dÃ©crivant les polygones des rÃ©gions administratives du QuÃ©bec. download.file(&quot;ftp://ftp.mrnf.gouv.qc.ca/public/dgig/produits/bdga5m/vectoriel/region_admin_SHP.zip&quot;, destfile=&quot;data/12_quebec/12_region_admin_SHP.zip&quot;) unzip(&quot;data/12_quebec/12_region_admin_SHP.zip&quot;, exdir = &quot;data/12_quebec/&quot;) Pour charger dans R des shapefiles en format sf, nous utilisons la fonction st_read() pointant vers le fichier .shp. quebec &lt;- st_read(&quot;data/12_quebec/region_admin_polygone.shp&quot;) ## Reading layer `region_admin_polygone&#39; from data source `/home/essi/Git/ecologie-mathematique-R/data/12_quebec/region_admin_polygone.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 21 features and 10 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: -79.7625 ymin: 44.99136 xmax: -56.93495 ymax: 62.58217 ## epsg (SRID): 4269 ## proj4string: +proj=longlat +datum=NAD83 +no_defs head(quebec) ## Simple feature collection with 6 features and 10 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: -79.7625 ymin: 48.04944 xmax: -56.93495 ymax: 62.58217 ## epsg (SRID): 4269 ## proj4string: +proj=longlat +datum=NAD83 +no_defs ## AREA PERIMETER REGIO_S_ REGIO_S_ID RES_NO_IND RES_DE_IND RES_CO_REG RES_NM_REG RES_CO_REF ## 1 1.240869e+02 94.6754067 2 1 50 02 0100 000 RÃ©gion administrative 10 Nord-du-QuÃ©bec BDGA5M ## 2 7.530857e-04 0.1519356 3 2 50 02 0100 000 RÃ©gion administrative 09 CÃ´te-Nord BDGA5M ## 3 4.460926e+01 55.8047474 4 3 50 02 0100 000 RÃ©gion administrative 09 CÃ´te-Nord BDGA5M ## 4 5.028808e-01 8.8274338 5 4 50 02 0100 000 RÃ©gion administrative 09 CÃ´te-Nord BDGA5M ## 5 8.779790e-02 2.0259021 6 5 50 02 0100 000 RÃ©gion administrative 09 CÃ´te-Nord BDGA5M ## 6 3.813094e+00 22.6943449 7 2 50 02 0100 000 RÃ©gion administrative 09 CÃ´te-Nord BDGA5M ## RES_CO_VER geometry ## 1 V2017-06 POLYGON ((-77.80893 62.4465... ## 2 V2017-06 POLYGON ((-66.6878 55.00005... ## 3 V2017-06 POLYGON ((-70.02987 55.0000... ## 4 V2017-06 POLYGON ((-66.25978 55.0000... ## 5 V2017-06 POLYGON ((-67.2192 55.00003... ## 6 V2017-06 POLYGON ((-63.60572 52.8760... Nous avons vu au chapitre 3 quâ€™il est prÃ©fÃ©rable dâ€™Ã©viter la rÃ©pÃ©tition de lâ€™information. Dans le format tibble que nous avons utilisÃ© pour dÃ©crire les polygones, lâ€™information attachÃ©e Ã  un polygone est rÃ©pÃ©tÃ©e pour chaque point qui le compose: forcer une information hiÃ©rarchisÃ©e Ã  se conformer Ã  une structure rectangulaire multiplie la quantitÃ© dâ€™information. Le format sf Ã©vite cette mutiplication dâ€™information en hiÃ©rachisant les polygones dans la colonne geometry. En guise dâ€™exploration rapide, la fonction plot() affichera les choroplÃ¨thes. plot(quebec) Si nous ne dÃ©sirons que la gÃ©omÃ©trie, quebec %&gt;% st_geometry() %&gt;% plot() # ou bien plot(st_geometry(quebec)), ou bien plot(quebec %&gt;% select(geometry)) Exercice. Explorer lâ€™objet quebec, en particulier la colonne geometry, notamment en utilisant la fonction str(). Vous pourrez soutirer les informations du systÃ¨me de coordonnÃ©es avec la fonction st_crs(). Il est possible de calculer des attributs des gÃ©omÃ©tries Ã  lâ€™aide des fonctions st_area() pour les polygones, ou st_length() pour les lignes et les polygones et la fonction st_centroid() pour trouver le centroÃ¯de dâ€™un polygone - Ã  cette Ã©tape, il se pourrait que R vous demande dâ€™installer le module lwgeom: suivez ses consignes! quebec_point &lt;- quebec %&gt;% mutate(st_area = st_area(quebec), st_length = st_length(quebec)) %&gt;% st_centroid() ## Warning in st_centroid.sf(.): st_centroid assumes attributes are constant over geometries of x ## Warning in st_centroid.sfc(st_geometry(x), of_largest_polygon = of_largest_polygon): st_centroid does not give correct ## centroids for longitude/latitude data plot(quebec_point) ## Warning: plotting the first 9 out of 12 attributes; use max.plot = 12 to plot all Les aires et les pÃ©rimÃ¨tres calculÃ©s ne correspondent pas tout Ã  fait Ã  ceux des variables AREA et PERIMETER, probablement calculÃ©s sur une autre base. La fonction st_centroid() crÃ©e un nouveau tableau dont la gÃ©omÃ©trie est le POINT, elle doit donc Ãªtre passÃ©e aprÃ¨s les opÃ©rations sur les polygones. La fonction st_simplify() permet de simplifier les polygones en un nombre rÃ©duit de points, ce qui peut Ãªtre utile pour accÃ©lÃ©rer les calculs. La fonction st_buffer() permet de crÃ©er un rayon dâ€™une longueur donnÃ©e autour dâ€™un point, procÃ©dure souvent utilisÃ©e pour visualiser un rayon dâ€™influence. Mais pour calculer des distances, les donnÃ©es doivent projetÃ©es. Nous pouvons les projeter avec st_transform() avec le code EPSG dÃ©sirÃ©. quebec_point %&gt;% st_transform(3348) %&gt;% st_buffer(50000) %&gt;% # 50 km du centre de la rÃ©gion plot() ## Warning: plotting the first 10 out of 12 attributes; use max.plot = 12 to plot all Dâ€™autres opÃ©rations sur les vecteurs sont offertes et documentÃ©es sous la fiche dâ€™aire sf::geos_unary(). Enfin, pour exporter un tableau sf en format csv incluant la gÃ©omÃ©trie, utilisez st_write(obj = tableau,dsn = &quot;tableau.csv&quot;, layer_options = &quot;GEOMETRY=AS_XY&quot;). Toutefois, si la gÃ©omÃ©trie nâ€™est pas consituÃ©e de points, il faudra prÃ©alablement transformer les polygones en points avec st_cast(). st_write(obj = quebec %&gt;% dplyr::filter(AREA &lt; 1) %&gt;% # ne retenir que quelques rÃ©gions pour crÃ©er un fichier moins volumineux st_cast(&quot;POINT&quot;), dsn = &quot;data/12_quebec_export.csv&quot;, layer_options = &quot;GEOMETRY=AS_XY&quot;) 13.7.2 DonnÃ©es raster Les donnÃ©es rasters sont des grilles, souvent enchÃ¢ssÃ©es dans des images tif gÃ©orÃ©fÃ©rencÃ©es. Ces images peuvent comprendre plusieurs variables, que lâ€™on nomme des bandes, en rÃ©fÃ©rence aux bandes spectrales des images satÃ©litaires (rouge, vert et bleu). Les donnÃ©es raster peuvent Ãªtre importÃ©es dans votre session grÃ¢ce Ã  deux fonctions du module raster : raster() importera des donnÃ©es raster Ã  une bande et brick(), des donnÃ©es raster Ã  plusieurs bandes. library(&quot;raster&quot;) canopy &lt;- raster(&quot;data/12_nytrees/canopy.tif&quot;) # source: https://assets.datacamp.com/production/repositories/738/datasets/79cb56df0fa27272e16b366a697aba8ac1d3e923/canopy.zip canopy ## class : RasterLayer ## dimensions : 230, 253, 58190 (nrow, ncol, ncell) ## resolution : 300, 300 (x, y) ## extent : 1793685, 1869585, 2141805, 2210805 (xmin, xmax, ymin, ymax) ## crs : +proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs ## source : /home/essi/Git/ecologie-mathematique-R/data/12_nytrees/canopy.tif ## names : canopy ## values : 0, 255 (min, max) manhattan &lt;- brick(&quot;data/12_nytrees/manhattan/manhattan.tif&quot;) # source: https://assets.datacamp.com/production/repositories/738/datasets/30830f8ba4a60aa1711f41e9a842b22cba3204f3/manhattan.zip manhattan ## class : RasterBrick ## dimensions : 773, 801, 619173, 3 (nrow, ncol, ncell, nlayers) ## resolution : 29.98979, 30.00062 (x, y) ## extent : 575667.9, 599689.7, 4503277, 4526468 (xmin, xmax, ymin, ymax) ## crs : +proj=utm +zone=18 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 ## source : /home/essi/Git/ecologie-mathematique-R/data/12_nytrees/manhattan/manhattan.tif ## names : manhattan.1, manhattan.2, manhattan.3 ## min values : 0, 0, 0 ## max values : 255, 255, 255 Les informations des objets RasterLayer et RasterBrick peuvent Ãªtre extraites par les fonctions extent(), ncell(), nlayers() et crs(). La fonction plot() permet dâ€™explorer les donnÃ©es en crÃ©ant un graphique par bande. plot(manhattan) Les fichiers raster, en format qui viennent souvent en format tif, sont typiquement trÃ¨s volumineux. Si une plus faible rÃ©solution convient Ã  une analyse spatiale, on pourra simplifier un raster avec la fonction raster::aggregate() (jâ€™utilise la notation module::fonction() pour Ã©viter la confusion avec la fonction dplyr::aggregate()). Lâ€™argument fact est le facteur de conversion et lâ€™argument fun est la fonction dâ€™aggrÃ©gation (typiquement mean ou median). manhattan_lowres &lt;- raster::aggregate(manhattan, fact = 20, fun = median) manhattan_lowres ## class : RasterBrick ## dimensions : 39, 41, 1599, 3 (nrow, ncol, ncell, nlayers) ## resolution : 599.7958, 600.0124 (x, y) ## extent : 575667.9, 600259.5, 4503067, 4526468 (xmin, xmax, ymin, ymax) ## crs : +proj=utm +zone=18 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 ## source : memory ## names : manhattan.1, manhattan.2, manhattan.3 ## min values : 13, 30, 47 ## max values : 186, 194, 185 plot(manhattan_lowres) Avec un facteur de conversion de 20, nous sommes passÃ©s dâ€™une grille de 773 \\(\\times\\) 801 Ã  39 \\(\\times\\) 41. Lâ€™exemple utilisÃ© est volontairement exagÃ©rÃ© pour montrer lâ€™effet de la perte de rÃ©solution, et gÃ©nÃ©ralement le facteur de conversion utilisÃ© est plus faible que 20. La fonction reclassify() est lâ€™Ã©quivalent de cut() pour les rasters. Lâ€™argument demandÃ©, en plus de lâ€™objet raster, est une matrice de classification Ã  trois colonnes. Les deux premiÃ¨res colonnes spÃ©cifient la plage de valeur Ã  classifier et la troisiÃ¨me colonne spÃ©cifie la valeur de remplacement (qui peut Ãªtre NA). La classification sâ€™applique Ã  toutes les couches sâ€™il sâ€™agit dâ€™un RasterBrick. manhattan_rcl &lt;- reclassify(manhattan_lowres, rcl = matrix(c(0, 50, 1, 50, 100, 2, 100, 1000, NA), ncol = 3, byrow = TRUE)) plot(manhattan_rcl) 13.8 Les systÃ¨mes de coordonnÃ©es Les longitudes et latitudes sont des angles sur un ellipsoÃ¯de de rÃ©volution. DiffÃ©rentes institutions utilisent diffÃ©rentes formes dâ€™ellipsoÃ¯de portant leur nom particulier: NAD83, WGS84, ETRS89, etc. Les projections servent Ã  aplanir des coordonnÃ©es gÃ©odÃ©siques obtenues selon un ellipsoÃ¯de donnÃ© en vue de crÃ©er des reprÃ©sentations 2D, comme la projection Mercator universelle ou de nombreuses autres. Le systÃ¨me de coordonnÃ©es peut Ãªtre projetÃ© ou non. SystÃ¨me de coordonnÃ©es non-projetÃ©es: caractÃ©risÃ© par des angles de longitude et de latitudes sur un systÃ¨me gÃ©odÃ©sique 3D reprÃ©sentÃ© par un ellipsoÃ¯de de rÃ©volution. SystÃ¨me de coordonnÃ©es projetÃ©es: caractÃ©risÃ© par des distances X et Y sur une systÃ¨me gÃ©odÃ©sique reprÃ©sentÃ© en 2D. Lorsque vous utilisez des shapefiles, les informations du systÃ¨me de coordonnÃ©es seront incluses dans le fichier ayant une extension prj. Examinons le systÃ¨me de coordonnÃ©es du tableau quebec avec la fonction st_crs(). st_crs(quebec) ## Coordinate Reference System: ## EPSG: 4269 ## proj4string: &quot;+proj=longlat +datum=NAD83 +no_defs&quot; Dâ€™emblÃ©e, la mention +proj=longlat retrouvÃ©e dans proj4string (une reprÃ©sentation de PROJ4) indique que le systÃ¨me nâ€™est pas projetÃ©, et que le systÃ¨me gÃ©odÃ©sique est le GRS80, pratiquement identique au WGS84. Le code EPSG contient la mÃªme information que le proj4string, traduite de maniÃ¨re succincte par un code Ã  4 chiffres. Dans certains cas, les informations du systÃ¨me de coordonnÃ©es ne sont pas disponibles: il vous faudra creuser si elles sont essentielles Ã  vos travaux. Pour assigner un systÃ¨me de coordonnÃ©es, vous pourrez soit assigner lâ€™EPSG par st_crs(objet_sg) &lt;- 4269 ou objet_sg &lt;- objet_sg %&gt;% st_set_crs(), ou bien le PROJ4 par st_crs(objet_sg) &lt;- &quot;+proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs&quot;. La procÃ©dure est la mÃªme pour les rasters, mais avec la fonction crs() au lieu de st_crs(). Pour passer dâ€™un systÃ¨me de coordonnÃ©es Ã  un autre, utilisez st_transform() pour les vecteurs et projectRasters() pour les rasters. Pour les rasters, si vos donnÃ©es sont catÃ©gorielles, et non pas numÃ©rique, utilisez method = &quot;ngb&quot; plutÃ´t que la valeur par dÃ©faut, method = &quot;bilinear&quot;, conÃ§ue pour les variables numÃ©riques. 13.9 Manipuler des tableaux sf Vous avez peut-Ãªtre remarquÃ© que jâ€™ai prÃ©cÃ©demment effectuÃ© une opÃ©ration mutate() en mode pipeline (%&gt;%) sur un tableau sf. Eh oui, les sf sont compatibles avec le mode tidyverse. Vous pourrez filtrer avec filter(), sÃ©lectionner avec select() et manipuler des colonnes avec mutate(). Reprenons les donnÃ©es des espÃ¨ces en danger, mais cette fois-ci nous allons travailler avec des donnÃ©es spatialisÃ©es avec sf. Dâ€™abord, allons chercher une carte du monde: le format geojson peut Ãªtre importÃ© de la mÃªme maniÃ¨re que des shape files. Puisquâ€™un geojson consigne lâ€™information gÃ©ographique en un seul fichier (les shapefiles en contiennent plusieurs), on peut lâ€™importer directement dâ€™Internet. world_gj &lt;- st_read(&quot;https://raw.githubusercontent.com/johan/world.geo.json/master/countries.geo.json&quot;) ## Reading layer `countries.geo&#39; from data source `https://raw.githubusercontent.com/johan/world.geo.json/master/countries.geo.json&#39; using driver `GeoJSON&#39; ## Simple feature collection with 180 features and 2 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -180 ymin: -85.60904 xmax: 180 ymax: 83.64513 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs world_gj ## Simple feature collection with 180 features and 2 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -180 ymin: -85.60904 xmax: 180 ymax: 83.64513 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs ## First 10 features: ## id name geometry ## 1 AFG Afghanistan MULTIPOLYGON (((61.21082 35... ## 2 AGO Angola MULTIPOLYGON (((16.32653 -5... ## 3 ALB Albania MULTIPOLYGON (((20.59025 41... ## 4 ARE United Arab Emirates MULTIPOLYGON (((51.57952 24... ## 5 ARG Argentina MULTIPOLYGON (((-65.5 -55.2... ## 6 ARM Armenia MULTIPOLYGON (((43.58275 41... ## 7 ATA Antarctica MULTIPOLYGON (((-59.57209 -... ## 8 ATF French Southern and Antarctic Lands MULTIPOLYGON (((68.935 -48.... ## 9 AUS Australia MULTIPOLYGON (((145.398 -40... ## 10 AUT Austria MULTIPOLYGON (((16.97967 48... Des multipolygones sont formÃ©s lorsque plusieurs polygones forment une seule entitÃ©, par exemple un pays constituÃ© de plusieurs Ã®les. Nous allons effectuÃ© la jointure entre le tableau world_gj et les donnÃ©es de lâ€™IUCN. De mÃªme que prÃ©cÃ©demment, les noms des pays doivent correspondre exactement. iucn_oecd &lt;- iucn_oecd %&gt;% replace(.==&quot;USA&quot;, &quot;United States of America&quot;) %&gt;% replace(.==&quot;UK&quot;, &quot;United Kingdom&quot;) %&gt;% dplyr::rename(&quot;name&quot; = &quot;region&quot;) Pour cette jointure, je dÃ©sire ne conserver que les donnÃ©es gÃ©ographiques des pays de lâ€™OCDE. Je peux effectuer une jointure Ã  gauche sur le tableau iucn_oecd ou une joiture Ã  droite sur le tableau world_gj. oecd_gj &lt;- world_gj %&gt;% right_join(iucn_oecd, by = &quot;name&quot;) ## Warning: Column `name` joining factor and character vector, coercing into character vector Contrairement aux tableaux tibble, le format sf conserve la gÃ©omÃ©trie. oecd_gj %&gt;% dplyr::select(name, n_threatened_species) ## Simple feature collection with 39 features and 2 fields (with 1 geometry empty) ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -180 ymin: -55.61183 xmax: 180 ymax: 83.23324 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs ## First 10 features: ## name n_threatened_species geometry ## 1 Czech Republic 7285 MULTIPOLYGON (((16.96029 48... ## 2 Germany 6275 MULTIPOLYGON (((9.921906 54... ## 3 United States of America 6005 MULTIPOLYGON (((-155.5421 1... ## 4 Canada 3208 MULTIPOLYGON (((-63.6645 46... ## 5 Slovakia 2927 MULTIPOLYGON (((18.85314 49... ## 6 Austria 2826 MULTIPOLYGON (((16.97967 48... ## 7 Japan 2719 MULTIPOLYGON (((134.6384 34... ## 8 Switzerland 2583 MULTIPOLYGON (((9.594226 47... ## 9 Poland 2308 MULTIPOLYGON (((15.017 51.1... ## 10 United Kingdom 2186 MULTIPOLYGON (((-5.661949 5... Pour une raison ou une autre, si vous dÃ©sirex retirer la gÃ©omÃ©trie, oecd_gj %&gt;% dplyr::select(name, n_threatened_species) %&gt;% st_set_geometry(NULL) %&gt;% top_n(4) ## Selecting by n_threatened_species ## name n_threatened_species ## 1 Czech Republic 7285 ## 2 Germany 6275 ## 3 United States of America 6005 ## 4 Canada 3208 On pourra explorer notre tableau avec la fonction plot(). plot(oecd_gj) Tout comme on effectue des jointures entre des tableaux, on peut effectuerd es jointures spatiales sur des sf. On pourra trouver des intersections entre polygones, effectuer des unions, des diffÃ©rences, etc. Par exemple, en modÃ©lisation, il est commun dâ€™extrapoler des rÃ©sultats sur une grille. Ici, nous crÃ©ons une grille couvrant tout le quÃ©bec (figure ??). Nous crÃ©ons une grille constituÃ©e des centroÃ¯des, %&gt;% # (par dÃ©faut, il sâ€™agit dâ€™une grille de polygones rectangulaires) nous transformons le rÃ©sultat en format sf (au lieu de sfc), %&gt;% nous effectuons une jointure sous forme dâ€™intersection et %&gt;% nous retirons les occurences hors de la jointure. quebec_grid &lt;- quebec %&gt;% st_make_grid(n = 80, what = &quot;centers&quot;) %&gt;% st_sf() %&gt;% # transformer en objet sf st_join(quebec, join = st_intersects) %&gt;% drop_na() ## although coordinates are longitude/latitude, st_intersects assumes that they are planar ## although coordinates are longitude/latitude, st_intersects assumes that they are planar par(mfrow = c(1, 2)) plot(quebec_grid %&gt;% st_geometry(), pch = 16, cex = 0.2, main = &quot;Grille QuÃ©bec&quot;) plot(quebec_grid %&gt;% filter(RES_NM_REG == &quot;MontÃ©rÃ©gie&quot;) %&gt;% st_geometry(), main = &quot;Grille MontÃ©rÃ©gie&quot;) Pour soutirer la grille en vue de modÃ©liser, quebec_grid %&gt;% st_coordinates() %&gt;% as_tibble() %&gt;% dplyr::rename(&quot;lon&quot; = &quot;X&quot;, &quot;lat&quot; = &quot;Y&quot;) ## # A tibble: 3,636 x 2 ## lon lat ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -74.2 45.1 ## 2 -73.9 45.1 ## 3 -73.6 45.1 ## 4 -73.3 45.1 ## 5 -73.1 45.1 ## 6 -72.8 45.1 ## 7 -72.5 45.1 ## 8 -72.2 45.1 ## 9 -71.9 45.1 ## 10 -71.6 45.1 ## # â€¦ with 3,626 more rows 13.10 Manipuler des objets raster Comme les objets vectoriels, les objets raster peuvent subir diffÃ©rents types dâ€™opÃ©rations. Nous en couvrirons trois. Masque (mask()): lâ€™intersection entre un polygone et un raster. DÃ©couper (crop()): dÃ©coupe rectangulaire selon les limites de lâ€™objet. Extraction (extract()): extrait, et accessoirement effectue un sommaire, des rasters dans un polygone donnÃ© CrÃ©ons dâ€™abord un polygone ayant le mÃªme systÃ¨me de coordonnÃ©es que le raster canopy. poly &lt;- st_sfc(st_polygon(list(cbind(c(1800000, 1830000, 1820000, 1800000), c(2160000, 2200000, 2150000, 2160000))))) %&gt;% st_set_crs(as.character(crs(canopy))) %&gt;% st_cast(&quot;POLYGON&quot;) En ce moment, le module raster nâ€™est pas adaptÃ© au format sf, quâ€™il faudrait prÃ©alablement convertir vers lâ€™ancien format sp. poly_sp &lt;- as(poly, &quot;Spatial&quot;) Appliquons un masque, puis un crop, puis les deux. canopy_mask &lt;- mask(canopy, mask = poly_sp) canopy_crop &lt;- crop(canopy, y = poly_sp) canopy_mc &lt;- crop(canopy_mask, y = poly_sp) par(mfrow = c(1, 4)) plot(canopy, main = &quot;Original&quot;) plot(poly_sp, add = TRUE) plot(canopy_mask, main = &quot;mask()&quot;) plot(poly_sp, add = TRUE) plot(canopy_crop, main = &quot;crop()&quot;) plot(poly_sp, add = TRUE) plot(canopy_mc, main = &quot;mask() &amp; crop()&quot;) plot(poly_sp, add = TRUE) Pour effectuer un calcul sur lâ€™intÃ©rieur du polygone avec extract()â€¦ on spÃ©cifie le raster, le polygone et la fonction! extract(canopy, poly_sp, fun = mean) ## [,1] ## [1,] 10.51405 13.11 Graphiques dâ€™objets spatialisÃ©s Pour afficher les objets sf et raster, nous avons utilisÃ© les fonctions de base Ã  titre exploratoire. Mais lorsque vient le temps de publier une carte, la trousse de ggplot2 est toute indiquÃ©e, en y ajoutant lâ€™outil geom_sf(). ggplot(quebec) + geom_sf(aes(fill = RES_NM_REG)) + geom_sf(data = quebec_point) Les coordonnÃ©es peuvent Ãªtre manipulÃ©es avec coord_sf(). world_gj_iucn &lt;- world_gj %&gt;% full_join(iucn_oecd, by = &quot;name&quot;) ## Warning: Column `name` joining factor and character vector, coercing into character vector ggplot(world_gj_iucn) + geom_sf(aes(fill = n_threatened_species), colour = &quot;gray50&quot;) + coord_sf(xlim = c(-170, -40), ylim = c(10, 80)) + scale_fill_gradient(low = &quot;#8CBFE6&quot;, high = &quot;#FF0099&quot;, na.value = &quot;grey80&quot;) + labs(title = &quot;Number of threatened species in OECD countries&quot;, subtitle = &quot;Source: OCDE, 2019&quot;) + guides(fill = guide_legend(title = &quot;Number of\\nthreatened\\nspecies&quot;)) Les cartes thÃ©matiques de tmap (thematic maps) sont construites sensiblement de la mÃªme maniÃ¨re que ggplot2. DiffÃ©rents types de projections sont disponibles avec diffÃ©rentes palettes de couleurs (palette_explorer()). library(&quot;tmap&quot;) library(&quot;tmaptools&quot;) tm_shape(set_projection(world_gj_iucn, &quot;wintri&quot;)) + tm_polygons(&quot;n_threatened_species&quot;, palette = &quot;viridis&quot;) ## OGR: Corrupt data ## Warning: The shape set_projection(world_gj_iucn, &quot;wintri&quot;) contains empty units. ## OGR: Corrupt data ## OGR: Corrupt data ## OGR: Corrupt data Si vous dÃ©sirez crÃ©er des cartes intÃ©ractives, passez en mode leaflet en spÃ©cifiant tmap_mode(&quot;view&quot;) (pour revenir en mode statique, tmap_mode(&quot;plot&quot;)) tmap_mode(&quot;view&quot;) ## tmap mode set to interactive viewing tm_shape(world_gj_iucn) + tm_polygons(&quot;n_threatened_species&quot;, palette = &quot;viridis&quot;) ## Warning: The shape world_gj_iucn is invalid. See sf::st_is_valid ## Warning: The shape world_gj_iucn contains empty units. Petit exemple avec des facettes synchronisÃ©es. quebec_tmap &lt;- tm_shape(quebec) + tm_polygons(c(&quot;AREA&quot;, &quot;PERIMETER&quot;)) + tm_facets(sync = TRUE, ncol = 2) quebec_tmap Les cartes tmap peuvent Ãªtre exportÃ©es sous forme dâ€™image. tmap_save(tm = quebec_tmap, filename = &quot;images/12_quebec_tmap.png&quot;, height=7) ## Map saved to /home/essi/Git/ecologie-mathematique-R/images/12_quebec_tmap.png ## Resolution: 3220.011 by 2100 pixels ## Size: 10.73337 by 7 inches (300 dpi) Toutefois, au moment dâ€™Ã©crire ces lignes, lâ€™exportation en format dynamique ne fonctionne pas pour les facettes. De plus, il semble y avoir un bogue avec les chemins relatifs. Il faudra donc coller (paste0) le rÃ©pertoire de travail (getwd()) au chemin relatif (&quot;/images/12_quebec_tmap_widget/12_quebec_tmap.html&quot;). Enfin, les fichiers html font souvent rÃ©fÃ©rence Ã  des fichiers externes: mieux vaut les enregistrer dans des dossiers indÃ©pendants (dans ce cas dans &quot;12_quebec_tmap_widget&quot;, que vous pourrez zipper avant de partager, ou placer sur un site web) plutÃ´t que dans un dossier comprenant plusieurs images. Pas si compliquÃ©â€¦ quand on le sait. quebec_tmap_area &lt;- tm_shape(quebec) + tm_polygons(&quot;AREA&quot;) tmap_save(tm = quebec_tmap_area, filename = paste0(getwd(),&quot;/images/12_quebec_tmap_widget/12_quebec_tmap.html&quot;)) ## Interactive map saved to /home/essi/Git/ecologie-mathematique-R/images/12_quebec_tmap_widget/12_quebec_tmap.html 13.12 Ressources complÃ©mentaires Geocomputation with R, de Robin Lovelace, Jakub Nowosad et Jannes Muenchow (2019). Spatial Data Science, de Robert J. Hijmans, du Geospatial and Farming Systems Research Consortium (GFC), University of California (2019) Simple Features for R, de Edzer Pebesma (2019) "],
["chapitre-ode.html", "14 ModÃ©lisation de mÃ©canismes Ã©cologiques 14.1 Ã‰quations diffÃ©rentielles 14.2 Les Ã©quations diffÃ©rentielles ordinaires en modÃ©lisation Ã©cologique", " 14 ModÃ©lisation de mÃ©canismes Ã©cologiques ï¸Â Objectifs spÃ©cifiques: Ã€ la fin de ce chapitre, vous saurez dÃ©finir une Ã©quation diffÃ©rentielle ordinaire et une Ã©quation diffÃ©rentielle partielle saurez aptes Ã  dÃ©tecter un problÃ¨me impliquant le besoin dâ€™utiliser des Ã©quations diffÃ©rentielles serez en mesure dâ€™effectuer une modÃ©lisation impliquant un systÃ¨me dâ€™Ã‰DO en contexte Ã©cologique On se rÃ©fÃ¨re Ã  la modÃ©lisation mÃ©canistique lorsque des principes thÃ©oriques guident une modÃ©lisation, Ã  lâ€™inverse de la modÃ©lisation phÃ©nomÃ©nologique, qui est guidÃ©e par les donnÃ©es. Il existe de nombreuses techniques de modÃ©lisation mÃ©canistique, mais la plupart sont guidÃ©es par les Ã©quations diffÃ©rentielles. 14.1 Ã‰quations diffÃ©rentielles Les Ã©quations diffÃ©rentielles permettent la rÃ©solution de problÃ¨mes impliquant des gradients dans le temps et dans lâ€™espace. On les utilise pour modÃ©liser la dynamique des populations, la thermodynamique, lâ€™Ã©coulement de lâ€™eau dans les sols, le transport des solutÃ©s, etc. On en distingue deux grandes catÃ©gories: les Ã©quations diffÃ©rentielles ordinaires et partielles. Ã‰quations diffÃ©rentielles ordinaires (Ã‰DO). Les Ã©quations diffÃ©rentielles ordinaires sâ€™appliquent sur des fonctions sâ€™appliquant Ã  une seule variables, qui est souvent le temps. On pourra suivre, par exemple, lâ€™Ã©volution de la tempÃ©rature en un point, en fonction du temps Ã  partir dâ€™une condition initiale. Parfois, plusieurs Ã‰DO sont utilisÃ©es conjointement pour crÃ©er un systÃ¨me dâ€™Ã‰DO que lâ€™on pourra nommÃ© un systÃ¨me dynamique. Les solutions analytiques des Ã‰DO sont parfois relativement faciles Ã  rÃ©soudre, mais les ordinateurs permettent des rÃ©solutions numÃ©riques en quelques lignes de code. Ã‰quations diffÃ©rentielles partielles (Ã‰DP). Dans ce cas, ce sont plusieurs variables qui sont diffÃ©renciÃ©es dans la mÃªme fonction. Il peut sâ€™agir des coordonnÃ©es dans lâ€™espace \\([x, y, z]\\) (rÃ©gime permanent), qui peuvent aussi Ãªtre appliquÃ©s Ã  diffÃ©rents pas de temps (rÃ©gime transitoire). Le problÃ¨me sera dÃ©limitÃ© non pas seulement par des conditions initiales, mais aussi par des conditions aux frontiÃ¨res du modÃ¨le. Puisque que les solutions analytiques des EDP peuvent rarement Ãªtre dÃ©veloppÃ©es, on utilisera pratiquement toujours des approches numÃ©riques que sont principalement les mÃ©thodes de rÃ©solution par diffÃ©rences finies ou par Ã©lÃ©ments finis. La prÃ©sente mouture de ce manuel ne comprend pas la rÃ©solution dâ€™Ã‰DP. 14.2 Les Ã©quations diffÃ©rentielles ordinaires en modÃ©lisation Ã©cologique Lâ€™Ã©volution des populations dans le temps peut Ãªtre abordÃ©e Ã  lâ€™aide de systÃ¨mes dâ€™Ã©quations diffÃ©rentielles. Une simple Ã©quation dÃ©crivant la croissance dâ€™une population peut Ãªtre couplÃ©e Ã  des schÃ©mas dâ€™exploitation de cette population, que ce soit une exploitation forestiÃ¨re, une terre fourragÃ¨re ou un territoire de chasse. On pourra aussi faire interagir des populations dans des schÃ©mas de relations biologiques. Ces processus peuvent Ãªtre implÃ©mentÃ©s avec des processus alÃ©atoires pour gÃ©nÃ©rer des schÃ©mas probabilistes. De plus, les biostatistiques et lâ€™autoapprentissage peuvent Ãªtre mis Ã  contribution afin de calibrer les modÃ¨les. 14.2.1 Ã‰volution dâ€™une seule population en fonction du temps La croissance dâ€™une population (ou de sa densitÃ©) isolÃ©e en fonction du temps dÃ©pend des conditions qui lui offre son environnement. Dans le cas de la biomasse dâ€™une culture Ã  croissance constante, le taux de croissance est toujours le mÃªme. \\[ \\frac{d ğŸŒ¿ }{dt} = c \\] \\[ \\int_0^t c dt = \\int_{ğŸŒ¿_0}^{ğŸŒ¿(t)} ~dğŸŒ¿ \\] \\[ ct = ğŸŒ¿(t) - ğŸŒ¿_0\\] \\[ ğŸŒ¿(t) = ğŸŒ¿_0 + ct \\] library(&quot;tidyverse&quot;) y0 &lt;- 2 c &lt;- 2 # exprimÃ© en individu / pas de temps time &lt;- seq(0, 6, 0.1) y &lt;- y0 + c * time tibble(time, y) %&gt;% ggplot(aes(x = time, y = y)) + geom_line() + geom_label(x = max(time), y = max(y), label = round(max(y))) + expand_limits(y = 0) Dans le cas dâ€™une population qui se reproduit, une formulation simple modÃ©lise une Ã©volution linÃ©aire associÃ©e Ã  un taux de natalitÃ© \\(n\\) et un taux de mortalitÃ© \\(m\\), oÃ¹ \\(r = n-m\\) est le taux de croissance de la population dâ€™une population de lapins ğŸ° en fonction du temps \\(t\\). \\[ \\frac{dğŸ°}{dt} = nğŸ° - mğŸ° = rğŸ° \\] \\[ \\int_0^t dt = \\int_{ğŸ°_0}^{ğŸ°(t)} \\frac{1}{rğŸ°} ~dğŸ° \\] \\[ t = \\frac{1}{r} ln(ğŸ°) \\bigg\\rvert_{ğŸ°_0}^{ğŸ°(t)} \\] \\[ rt = ln \\left( \\frac{ğŸ°(t)}{ğŸ°_0} \\right) \\] \\[ ğŸ°(t) = ğŸ°_0 exp(rt) \\] La vitesse de croissance est constante pour une population constante, mais la croissance de la population est exponentielle Ã©tant donnÃ©e que chaque nouvel individu se reproduit. y0 &lt;- 10 r &lt;- 0.2 # exprimÃ© en individu / pas de temps time &lt;- seq(0, 10, 0.1) y &lt;- y0 * exp(r*time) tibble(time, y) %&gt;% ggplot(aes(x = time, y = y)) + geom_line() + geom_label(x = max(time), y = max(y), label = round(max(y))) + expand_limits(y = 0) De 10 lapins au dÃ©part, nous en avons un peu plus de 75 aprÃ¨s 10 ansâ€¦ et prÃ¨s de 5 milliards aprÃ¨s 100 ans! En fait, la capacitÃ© de support dâ€™une population Ã©tant gÃ©nÃ©ralement limitÃ©e, on peut supposer que le taux de natalitÃ© dÃ©croit et que le taux de mortalitÃ© croit linÃ©airement avec lâ€™effectif. \\[ n(ğŸ°) = \\alpha - \\beta ğŸ° \\] \\[ m(ğŸ°) = \\gamma + \\delta ğŸ° \\] On aura donc \\[ \\frac{dğŸ°}{dt} = ğŸ° \\left( \\alpha - \\beta ğŸ° \\right) - ğŸ° \\left( \\gamma + \\delta ğŸ° \\right) = rğŸ° \\left( 1 - \\frac{ğŸ°}{K} \\right) \\] oÃ¹ \\(r = \\alpha - \\gamma\\) est lâ€™ordonnÃ©e Ã  lâ€™origine du taux de croissance (thÃ©orique, lorsque la population est nulle) et \\(K = \\frac{\\alpha-\\gamma}{\\beta + \\delta}\\) est la capacitÃ© limite du milieu de subsistance. On pourra sâ€™aider dâ€™un logiciel de calcul symbolique comme sympy ou maxima pour en tirer une solution analytique. Mais Ã  ce point, nous utiliserons une approximation numÃ©rique. Nous utiliserons le module deSolve. library(&quot;deSolve&quot;) deSolve demande de dÃ©finir les paramÃ¨tres de lâ€™Ã‰DO ou du systÃ¨me dâ€™Ã‰DO. Nous devons dâ€™abord spÃ©cifier Ã  quels pas de temps notre Ã‰DO doit Ãªtre approximÃ©e. Jâ€™Ã©tends la plage de temps Ã  30 ans pour bien visualiser la courbe de croissance. time &lt;- seq(0, 30, by = 0.5) Les conditions initiales du systÃ¨me dâ€™Ã‰DO sont aussi dÃ©finies dans un vecteur. La seule condition initiale de notre Ã‰DO est le nombre initial de lapin. y0 &lt;- c(lapin = 10) On dÃ©finira les paramÃ¨tres dans un vecteur p. Dans notre cas, nous avons \\(r\\), le taux de croissance Ã  lâ€™origine et \\(K\\), la capacitÃ© de support de lâ€™Ã©cosystÃ¨me. Il est prÃ©fÃ©rable de nommer les paramÃ¨tres du vecteur pour Ã©viter les erreurs. p &lt;- c(r = 0.2, K = 40) Enfin, une fonction dÃ©finit lâ€™Ã‰DO avec, comme entrÃ©es, les pas de temps, les conditions initiales et les paramÃ¨tres. La sortie de la fonction est un vecteur des dÃ©rivÃ©es emboÃ®tÃ©s dans une liste (lisez le fichier dâ€™aide de la fonction ode pour les dÃ©tails en lanÃ§ant ?ode). model_logistic &lt;- function(t, y, p) { lapin &lt;- y[1] dlapin_dt &lt;- p[1] * lapin * (1 - lapin/p[2]) return(list(c(dlapin_dt))) } Une fois que les pas de temps, les conditions initiales, les paramÃ¨tres et le modÃ¨le sont dÃ©finis, on les spÃ©cifie comme arguments dans la fonction ode. La sortie de la fonction ode est une matrice dont la premiÃ¨re colonne comprend les pas de temps imposÃ©s, et les autres colonnes sont les dÃ©rivÃ©es spÃ©cifiÃ©es Ã  la sortie de la fonction ode. lapin_t &lt;- ode(y = y0, times = time, model_logistic, p) head(lapin_t) ## time lapin ## [1,] 0.0 10.00000 ## [2,] 0.5 10.76856 ## [3,] 1.0 11.57342 ## [4,] 1.5 12.41288 ## [5,] 2.0 13.28478 ## [6,] 2.5 14.18643 lapin_t %&gt;% as_tibble() %&gt;% ggplot(aes(x = time, y = lapin)) + geom_line() + expand_limits(y = 0) Exercice. Que ce passerait-il si le taux de croissance Ã©tait nÃ©gatif? Profitez-en pour changer les paramÃ¨tres r et K. Exercice. Dâ€™autres formulations existent pour exprimer des taux de croissance (Gompertz, Allee, etc.). En outre la formulation de Gompertz sâ€™Ã©crit comme suit. \\[ \\frac{dğŸ°}{dt} = rğŸ° \\left( ln \\frac{K}{ğŸ°} \\right) \\] Entrer cet Ã‰DO dans R avec deSolve. 14.2.2 Population exploitÃ©e Lâ€™exploitation dâ€™une population peut Ãªtre effectuÃ©e de diffÃ©rentes maniÃ¨res. Dâ€™abord, le prÃ©lÃ¨vement peut Ãªtre effectuÃ© de maniÃ¨re constante, par exemple dans un Ã©levage ou par la chasse ou la cueillette. Ajoutons un prÃ©lÃ¨vement constant dans une courbe de croissance logistique. \\[ \\frac{dğŸ°}{dt} = rğŸ° \\left( 1 - \\frac{ğŸ°}{K} \\right) - Q \\] oÃ¹ \\(Q\\) est le quota, ou le prÃ©lÃ¨vement constant. On pourra aussi effectuer un prÃ©lÃ¨vement proportionnel Ã  la population. \\[ \\frac{dğŸ°}{dt} = rğŸ° \\left( 1 - \\frac{ğŸ°}{K} \\right) - EğŸ° \\] oÃ¹ \\(E\\) est lâ€™effort dâ€™exploitation. Ou bien effectuer une sÃ©rie de prÃ©lÃ¨vement ponctuels, comme la rÃ©colte de plantes fourragÃ¨res. \\[ \\frac{dğŸŒ¿}{dt} = c - \\left[ ğŸŒ¿ - \\gamma \\right] \\bigg\\rvert_{t=a, b, c, d, e, ...} \\] oÃ¹ \\(\\gamma\\) est le reste de la biomasse aprÃ¨s la rÃ©colte et \\(t=a, b, c, d, e, ...\\) sont les pas de temps oÃ¹ le bloc entre les crochets est actif, câ€™est-Ã -dire la pÃ©riode de rÃ©colte. La solution analytique dâ€™une culture Ã  croissance constante est plutÃ´t facile Ã  dÃ©duire. Les fonctions de prÃ©lÃ¨vement peuvent Ãªtre modulÃ©es Ã  votre guise. Prenons pour lâ€™exemple un prÃ©lÃ¨vement constant et une croissance logistique. p &lt;- c(r = 0.2, K = 40, Q = 1) model_logistic_expl &lt;- function(t, y, p) { lapin &lt;- y[1] dlapin_dt &lt;- p[1] * lapin * (1 - lapin/p[2]) - p[3] return(list(c(dlapin_dt))) } lapin_t &lt;- ode(y = y0, times = time, model_logistic_expl, p) lapin_t %&gt;% as_tibble() %&gt;% ggplot(aes(x = time, y = lapin)) + geom_line() + expand_limits(y = 0) Exercice. ModÃ©liser avec un prÃ©lÃ¨vement proportionnel. Quâ€™arrive-t-il lorsque le prÃ©lÃ¨vement est trop Ã©levÃ©? Lâ€™exploitation ponctuelle, comme la rÃ©colte ou lâ€™administration dâ€™une sÃ©rie de traitements, implique lâ€™utilisation dâ€™approches intermittentes. Bien que deSolve ignore les changements dans les variables dâ€™Ã©tat (y) tels que dÃ©finis dans les dÃ©rivÃ©s, nous pouvons avoir recours Ã  des Ã©vÃ¨nements dans le jargon de deSolve. Ces Ã©vÃ¨nements doivent Ãªtre spÃ©cifiÃ©s dans un data.frame ou une liste. Il est difficile de trouver un exemple gÃ©nÃ©rique pour modÃ©liser des Ã©vÃ¨nements. Pour en savoir davantage, je vous invite donc Ã  consulter la fiche dâ€™aide ?events. Dans notre cas, nous allons modÃ©liser une rÃ©colte de plantes fourragÃ¨res. La rÃ©colte est dÃ©clenchÃ©e lorsque le rendement atteint 2 t/ha, et laisser 0.3 t/ha au sol pour assurer le renouvellement pour les coupes subsÃ©quentes. DÃ©finissons dâ€™abord les entrÃ©es du modÃ¨les. time &lt;- seq(0, 120, 0.1) p &lt;- c(r = 0.1, K = 2.5) y0 &lt;- c(champ = 0.1) Nous devons dÃ©finir une fonction root (racine), comprenant tous les arguments de la fonction dâ€™Ã‰DO, dont la sortie est une valeur qui dÃ©clenchera un Ã©vÃ¨nement lorsque la valeur sera nulle. Dans notre cas, la valeur correspond simplement au rendement moins 2, la quantitÃ© au champ y[1]. Notez que dâ€™autres stratÃ©gies peuvent Ãªtre utilisÃ©es pour dÃ©clencher une rÃ©colte, par exemple le pourcentage de floraison qui demanderait des simulations plus poussÃ©es. recolte_root &lt;- function(t, y, p) y[1]-2 Puis, lorsque la fonction root est dÃ©clenchÃ©e, lâ€™Ã©vÃ¨nement ramÃ¨ne la quantitÃ© au champs Ã  0.3 t/ha, une quantitÃ© qui permet de relancer la croissance. recolte_event &lt;- function(t, y, p) { y[1] &lt;- 0.3 return(y) } La fonction du modÃ¨le est telle quâ€™utilisÃ©e auparavant: une fonction logistique. recolte &lt;- function(t, y, p) { champ &lt;- y[1] dchamp_dt &lt;- p[1] * champ * (1 - champ/p[2]) return(list(c(dchamp_dt))) } La fonction ode est lancÃ©e en entrant les fonction root et events. out &lt;- ode(times = time, y = y0, func = recolte, parms = p, rootfun = recolte_root, events = list(func = recolte_event, root = TRUE), method=&quot;impAdams&quot;) plot(out) Nous pourrons organiser deux rÃ©coltes de 1.7 t/ha et une de 2 t/ha pour terminer la saison. Exercice. Quâ€™adviendrait-il si vous laissiez 0.15 t/ha au champ au lieu de 0.3? Ou si vous laissiez 1 t/ha? Ou si vous dÃ©clenchiez une rÃ©colte Ã  2.3 t/ha? DÃ©fi. Pouvez-vous modÃ©liser lâ€™ensilage? 14.2.3 Interactions biologiques Les interactions biologiques entre deux espÃ¨ces Ã  un stade de croissance dÃ©fini peuvent prendre diffÃ©rentes formes, du mutualisme (les deux espÃ¨ces bÃ©nÃ©ficient de la relation) Ã  la compÃ©tition (les deux espÃ¨ces se nuisent) en passant par la prÃ©dation ou le parasitisme (une espÃ¨ce bÃ©nÃ©ficie de lâ€™autre en lui nuisant) ou le neutralisme (aucun effet). Ces effets sont dÃ©crits dans Pringle (2016) en un tableau synthÃ¨se. Figure 14.1: Interactions biologiques, Pringle, E.G. 2016. Orienting the Interaction Compass: Resource Availability as a Major Driver of Context Dependence. Plos Biology. https://doi.org/10.1371/journal.pbio.2000891. Ces interactions peuvent Ãªtre dÃ©crite mathÃ©matiquement dans des systÃ¨mes dâ€™Ã‰DO, ou Ã‰DO couplÃ©es. Le cas dâ€™Ã©tude le plus courant reprend le systÃ¨me dâ€™Ã©quation prÃ©dateur-proie de Lotka-Volterra, deux auteurs ayant dÃ©veloppÃ© de maniÃ¨re indÃ©pendante des Ã©quations similaires respectivement en 1925 et 1926. Les Ã©quations de Lotka-Volterra supposent une croissance illimitÃ©e des deux espÃ¨ces: les proies ğŸ° se reproduisent par elles-mÃªmes (\\(\\alpha ğŸ°\\)), tandis que les prÃ©dateurs ğŸ¦Š croissent selon la disponibilitÃ© des proies (\\(\\delta ğŸ°ğŸ¦Š\\)). Ã€ lâ€™inverse, la mortalitÃ© des proies dÃ©pend du nombre de prÃ©dateurs (\\(- \\beta ğŸ°ğŸ¦Š\\)), mais la mortalitÃ© des prÃ©dateurs est indÃ©pendante des proies (\\(- \\gamma ğŸ¦Š\\)). On obtient ainsi un systÃ¨me dâ€™Ã©quation. \\[\\frac{dğŸ°}{dt} = \\alpha ğŸ° - \\beta ğŸ°ğŸ¦Š = ğŸ° \\left( \\alpha - \\beta ğŸ¦Š \\right)\\] \\[\\frac{dğŸ¦Š}{dt} = \\delta ğŸ°ğŸ¦Š - \\gamma ğŸ¦Š = ğŸ¦Š \\left( \\delta ğŸ° - \\gamma \\right) \\] Ã€ lâ€™Ã©quilibre de ğŸ°, câ€™est-Ã -dire oÃ¹ \\(\\frac{dğŸ°}{dt} = 0\\), on retrouve \\(ğŸ°=0\\) ou \\(ğŸ¦Š = \\frac{\\alpha}{\\beta}\\). De mÃªme, Ã  lâ€™Ã©quilibre de ğŸ¦Š, on retrouve \\(ğŸ¦Š=0\\) ou \\(ğŸ° = \\frac{\\gamma}{\\delta}\\). En termes mathÃ©matiques, ces Ã©quilibre sont des isoclines, des points dâ€™inflexion dans le systÃ¨me dâ€™Ã‰DO. Nous allons rÃ©soudre les Ã©quations de Lotka-Volterra avec deSolve. Rappelons-nous que nous devons dÃ©finir des pas de temps oÃ¹ approximer les populations (times), des conditions initiales (y0) et des paramÃ¨tres (p). time &lt;- seq(0, 30, by = 0.1) y0 &lt;- c(lapin = 3, renard = 1) p &lt;- c(alpha = 2, # taux de croissance des lapins (naissance - mortalitÃ©, 1/an) beta = 0.8, # taux de prÃ©dation des lapins (renard / an) delta = 0.1, # taux de conversion lors de la prÃ©dation (lapin / renard) gamma = 0.2) # mortalitÃ© naturelle des renards (1/an) On peut calculer dâ€™emblÃ©e les isoclines. lapin_iso &lt;- p[4]/p[3] renard_iso &lt;- p[1]/p[2] Nous devons ensuite crÃ©er notre modÃ¨le. modele_LV &lt;- function(t, y, p) { lapin = y[1] renard = y[2] dlapin_dt = p[1] * lapin - p[2] * lapin * renard drenard_dt = p[3] * lapin * renard - p[4] * renard return(list(c(dlapin_dt, drenard_dt))) } LanÃ§ons lâ€™approximation. effectifs_t = ode(y = y0, times = time, modele_LV, p) head(effectifs_t) ## time lapin renard ## [1,] 0.0 3.000000 1.000000 ## [2,] 0.1 3.380961 1.011940 ## [3,] 0.2 3.806028 1.028156 ## [4,] 0.3 4.278154 1.049326 ## [5,] 0.4 4.799633 1.076263 ## [6,] 0.5 5.371673 1.109943 effectifs_t %&gt;% as_tibble() %&gt;% gather(key=&quot;espece&quot;, value = &quot;value&quot;, -time) %&gt;% ggplot(aes(x=time, y=value)) + geom_line(aes(colour=espece)) + expand_limits(y = 0) Lorsque la population de lapins croit, celle des renards croit Ã  retardement jusquâ€™Ã  ce que la population de lapin diminue jusquâ€™Ã  Ãªtre presque Ã©teinte. Dans ces conditions, la population de renard ne peut plus Ãªtre soutenue, et dÃ©croit, ce qui en retour donne lâ€™opportunitÃ© de la population de lapins de resurgir. effectifs_t %&gt;% as_tibble() %&gt;% ggplot(aes(x = lapin, y = renard)) + geom_path() + geom_hline(yintercept = lapin_iso, linetype = 2) + geom_vline(xintercept = renard_iso, linetype = 2) Les conditions initiales sont responsables de lâ€™amplitude des cycles. Excercice. VÃ©rifier lâ€™effet des paramÃ¨tres sur les cycles. Quâ€™adviendrait-il des populations si lâ€™on prenait plutÃ´t un profil de croissance logistique chez les lapins? \\[\\frac{dğŸ°}{dt} = \\alphağŸ° \\left( 1-\\frac{ğŸ°}{K} \\right) - \\beta ğŸ°ğŸ¦Š \\] \\[\\frac{dğŸ¦Š}{dt} = \\delta ğŸ°ğŸ¦Š - \\gamma ğŸ¦Š \\] Pour les isoclines, Ã  lâ€™Ã©quilibre oÃ¹ \\(\\frac{dğŸ°}{dt} = 0\\), on retrouve \\(ğŸ¦Š=\\frac{\\alpha}{\\beta} \\left( 1-\\frac{ğŸ°}{K} \\right)\\) ou \\(ğŸ°=0\\). De mÃªme que prÃ©cÃ©demment, Ã  lâ€™Ã©quilibre de ğŸ¦Š, on retrouve \\(ğŸ¦Š=0\\) ou \\(ğŸ° = \\frac{\\gamma}{\\delta}\\). Reprenons nos paramÃ¨tres, mais en ajoutant la capacitÃ© de support des lapins, Ã  \\(K = 40\\). time &lt;- seq(0, 60, by = 0.1) y0 &lt;- c(lapin = 3, renard = 1) p &lt;- c(alpha = 2, # taux de croissance des lapins (naissance - mortalitÃ©, 1/an) beta = 0.8, # taux de prÃ©dation des lapins (renard / an) delta = 0.1, # taux de conversion lors de la prÃ©dation (lapin / renard) gamma = 0.2, # mortalitÃ© naturelle des renards (1/an) K = 40) # capacitÃ© de support de l&#39;Ã©cosystÃ¨me Calculons les isoclines, en tenant compte que, cette fois-ci, lâ€™isocline des renards est une fonction du nombre de lapins. lapin_iso &lt;- p[4] / p[3] renard_iso &lt;- tibble(lapin = seq(from = 0, to = 40, by = 1)) %&gt;% # acec une sÃ©quence de lapins ... mutate(renard = p[1] / p[2] * (1 - lapin/p[5])) # ... calculer les renards Le modÃ¨le logistique diffÃ¨re peu du modÃ¨le classique de Lotka-Volterra. modele_LV_logist &lt;- function(t, y, p) { lapin = y[1] renard = y[2] dlapin_dt = p[1] * lapin * (1-y[1]/p[5]) - p[2] * lapin * renard drenard_dt = p[3] * lapin * renard - p[4] * renard return(list(c(dlapin_dt, drenard_dt))) } LanÃ§ons la modÃ©lisation, puis affichons les rÃ©sultats. effectifs_t &lt;- ode(y = y0, times = time, modele_LV_logist, p) gg_time &lt;- effectifs_t %&gt;% as_tibble() %&gt;% gather(key=&quot;espece&quot;, value = &quot;value&quot;, -time) %&gt;% ggplot(aes(x=time, y=value)) + geom_line(aes(colour=espece)) + expand_limits(y = 0) gg_cycle &lt;- effectifs_t %&gt;% as_tibble() %&gt;% ggplot(aes(x = lapin, y = renard)) + geom_path() + geom_vline(xintercept = lapin_iso, linetype = 2) + geom_line(data = renard_iso, linetype = 2) + xlim(c(0, 10)) cowplot::plot_grid(gg_time, gg_cycle) Ainsi conÃ§u, le systÃ¨me tant vers des effectifs constants aux isoclines. Dans les cycles Ã©tudiÃ©s jusquâ€™ici, les effectifs atteignent systÃ©matiquement un Ã©tat critique, mais se recouvrent sans cesse. Il serait toutefois Ã©tonnant que les paramÃ¨tres des Ã©quations (reproduction, mortalitÃ©, prÃ©dation, support des Ã©cosystÃ¨mes) soient constants. On peut admettre que les paramÃ¨tres peuvent varier en fonction de dâ€™autres paramÃ¨tres, ou simplement au hasard. Justement, il est possible dâ€™ajouter de la stochastique (processus alÃ©atoire) dans nos fonctions. En outre, plusieurs simulations pourront nous indiquer un risque dâ€™effondrement dâ€™un Ã©cosystÃ¨me. Mais adviendra la possibilitÃ© que les effectifs des populations prennent des valeurs nÃ©gatives, ce qui nâ€™est pas admissible. Une solution est de reformuler nos Ã©quations pour faire en sorte de modÃ©liser le logarithme des effectifs, qui pourront Ãªtre recalculÃ©es par lâ€™exponentielle dans la base du log. Un log nÃ©gatif retransformÃ© par lâ€™exponentiel devient une fraction de 1 (si \\(log_{10}(x) = -1\\), \\(x = 0.1\\)). Une autre approche est dâ€™utiliser un Ã©vÃ©nement ramenant lâ€™effectif nÃ©gatif Ã  zÃ©ro, dÃ©clanchÃ© lorsquâ€™un des effectifs est infrieur ou Ã©gal Ã  zÃ©ro. Câ€™est ce que nous allons faire, avec les mÃªmes time, y0 et p que prÃ©cÃ©demment. La fonction root est un moyen de dÃ©clencher lâ€™Ã©vÃ©nement. Elle prend la valeur de zÃ©ro si lâ€™un des deux effectifs est nul. zero_root &lt;- function(t, y, p) { x1 &lt;- y[1] &gt;= 0 x2 &lt;- y[2] &gt;= 0 xnum &lt;- as.numeric(x1 &amp; x2) return(xnum) } zero_event &lt;- function(t, y, p) { if (y[1] &lt;= 0) y[1] &lt;- 0 if (y[2] &lt;= 0) y[2] &lt;- 0 return(y) } Reprenons la fonction logistique, mais en ajoutant un effet alÃ©atoire Ã  chacun des paramÃ¨tres. modele_LV_alea &lt;- function(t, y, p) { lapin = y[1] renard = y[2] alpha &lt;- rnorm(1, p[1], 0.0005) beta &lt;- rnorm(1, p[2], 0.0005) delta &lt;- rnorm(1, p[3], 0.001) gamma &lt;- rnorm(1, p[4], 0.001) K &lt;- rnorm(1, p[5], 1) dlapin_dt &lt;- alpha * lapin * (1-lapin/K) - beta * lapin * renard drenard_dt &lt;- delta * lapin * renard - gamma * renard return(list(c(dlapin_dt, drenard_dt))) } La modÃ©lisation prend en compte lâ€™Ã©vÃ©nement. set.seed(14389) effectifs_t = ode(y = y0, times = time, func = modele_LV_alea, parms = p, rootfun = zero_root, events = list(func = zero_event, root = TRUE), method=&quot;impAdams&quot;) effectifs_tibble &lt;- effectifs_t %&gt;% unclass() %&gt;% as_tibble() On lance ensuite les mÃªmes fonctions de visualisation que prÃ©cÃ©demment. gg_time &lt;- effectifs_tibble %&gt;% gather(key=&quot;espece&quot;, value = &quot;value&quot;, -time) %&gt;% ggplot(aes(x=time, y=value)) + geom_line(aes(colour=espece)) + expand_limits(y = 0) gg_cycle &lt;- effectifs_tibble %&gt;% ggplot(aes(x = lapin, y = renard)) + geom_path(aes(colour = time)) + geom_vline(xintercept = lapin_iso, linetype = 2) + geom_line(data = renard_iso, linetype = 2) + expand_limits(x = 0, y = 0) cowplot::plot_grid(gg_time, gg_cycle) Une trÃ¨s faible variance sur les paramÃ¨tres peu grandement perturber le systÃ¨me. Il est possible, en effectuant plusieur simulations en boucle, dâ€™Ã©valuer le risque dâ€™effondrement des effectifs dâ€™une espÃ¨ce, ce qui arrive pour le cas simulÃ© pour les lapins, puis pour les renards. Nous avons modÃ©lisÃ© une relation biologique de prÃ©dation. Il existe dans la littÃ©rature une panoplie de modÃ¨les dâ€™Ã‰DO pour dÃ©crire les relations biologiques, qui peuvent Ãªtre modÃ©lisÃ©s entre plusieurs espÃ¨ces pour crÃ©er des rÃ©seaux trophiques complexes. Toutefois, la difficultÃ© de collecter des donnÃ©es en quantitÃ© et en qualitÃ© suffisante rendent ces modÃ¨les difficiles Ã  apprÃ©hender. Exercice. ModÃ©liser une compÃ©tition interspÃ©cifique oÃ¹ chaque population croit de maniÃ¨re logistique. \\[\\frac{dğŸ}{dt} = r_1 ğŸ \\left( 1-\\frac{ğŸ}{K_1} -\\alpha \\frac{ğŸ€}{K_2} \\right) \\] \\[\\frac{dğŸ€}{dt} = r_2 ğŸ€ \\left( 1-\\frac{ğŸ€}{K_2} -\\beta \\frac{ğŸ}{K_1} \\right) \\] oÃ¹ \\(r_1\\) et \\(r_2\\) sont les taux de croissances respectifs des ğŸ et des ğŸ€, ainsi que \\(K_1\\) et que \\(K_2\\) sont les capacitÃ©s de support des ğŸ et des ğŸ€. Le coefficient \\(\\alpha\\) dÃ©crit lâ€™ampleur de la compÃ©tition de ğŸ€ sur ğŸ et le coefficient \\(\\beta\\) dÃ©crit lâ€™ampleur de la compÃ©tition de ğŸ sur ğŸ€ (\\(\\alpha\\) et \\(\\beta\\) sont &gt;= 0). Exercice. Les interactions biologiques forment une bonne introduction aux systÃ¨mes dâ€™Ã©quations diffÃ©rentielles ordinaires. On fait nÃ©anmoins souvent rÃ©fÃ©rence aux Ã©quations de Lorenz (1963), qui a dÃ©veloppÃ© un systÃ¨me dâ€™Ã‰DO chaotique depuis trois Ã©quations, \\[ X&#39; = aX + YZ, \\] \\[ Y&#39; = b \\left(Y-Z\\right), \\] \\[ Z&#39; = -XY + cY - Z, \\] oÃ¹ \\(X\\) est la tempÃ©rature horizontale, \\(Y\\) est la tempÃ©rature verticale, \\(Z\\) est le flux de chaleur convectif, et oÃ¹ lâ€™on retrouve les paramÃ¨tres \\(a = -8/3\\), \\(b=-10\\) et \\(c=28\\). RÃ©soudre les Ã©quations de Lorents avec deSolve. Porter graphiquement les relations entre X, Y et Z. "]
]
