[
["chapitre-ordination.html", "8 Association, partitionnement et ordination 8.1 Espaces dâ€™analyse 8.2 Analyse dâ€™association 8.3 Partitionnement 8.4 Ordination", " 8 Association, partitionnement et ordination ï¸Â Objectifs spÃ©cifiques: Ã€ la fin de ce chapitre, vous serez en mesure dâ€™effectuer des calculs permettant de mesurer des diffÃ©rence entre des observations, des groupes dâ€™observation ou des variables observÃ©es serez en mesure dâ€™effection des analyses de partitionnement hiÃ©rarchiques et non-hiÃ©rarchiques serez en mesure dâ€™effectuer des calculs dâ€™ordination Ã  lâ€™aide des techniques de rÃ©duction dâ€™axe communes: analyse en composante principale, lâ€™analyse de correspondance, lâ€™analyse en coordonnÃ©es principales, analyse discriminante linÃ©aire, lâ€™analyse de redondance et lâ€™analyse canonique des correspondances. Les donnÃ©es Ã©cologiques incluent gÃ©nÃ©ralement plusieurs variables qui doivent Ãªtre analysÃ©es conjointement. Les techniques pour lâ€™analyse multivariÃ©e de donnÃ©es Ã©cologiques ont grandi en nombre et en complexitÃ©, laissant Ã©merger lâ€™Ã©cologie numÃ©rique, un nouveau domaine dâ€™Ã©tude scientifique initiÃ© par Pierre Legendre et Louis Legendre dont lâ€™ouvrage Numerical Ecology, aujourdâ€™hui Ã  sa troisiÃ¨me Ã©dition, reste un incontournable pour qui sâ€™intÃ©resse aux mathÃ©matiques sous-jacentes au domaine. Pour la rÃ©daction de ces notes, câ€™est toutefois le livre Numerical ecology with R, Ã©crit par Borcard et al. (2011) pour offrir un guide Ã  qui voudrait une approche plus appliquÃ©e. Lâ€™Ã©cologie numÃ©rique sera effleurÃ©e dans ce chapitre, qui introduit Ã  trois concepts. Les associations permettent de quantifier la ressemblance ou la diffÃ©rence entre deux observation (Ã©chantillons) ou variables (descripteurs). Lorsque lâ€™on a plus de deux variables ou plus de deux site, nous obtenons des matrices dâ€™association. Le partitionnement permet de regrouper des observations ou des variables selon des mÃ©triques dâ€™association. Lâ€™ordination vise par lâ€™intermÃ©diaire de techniques de rÃ©duction dâ€™axe Ã  mettre de lâ€™ordre dans des donnÃ©es dont le nombre Ã©levÃ© de variables peut amener Ã  des difficultÃ©s dâ€™apprÃ©ciation et dâ€™interprÃ©taion. library(&quot;tidyverse&quot;) ## â”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 1.2.1 â”€â”€ ## âœ” ggplot2 3.1.0 âœ” purrr 0.3.0 ## âœ” tibble 2.0.1 âœ” dplyr 0.8.0.1 ## âœ” tidyr 0.8.2 âœ” stringr 1.4.0 ## âœ” readr 1.3.1 âœ” forcats 0.4.0 ## â”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€ ## âœ– dplyr::filter() masks stats::filter() ## âœ– dplyr::lag() masks stats::lag() 8.1 Espaces dâ€™analyse 8.1.1 Abondance et occurence Lâ€™abondance est le dÃ©compte dâ€™espÃ¨ces observÃ©es, tandis que lâ€™occurence est la prÃ©sence ou lâ€™absence dâ€™une espÃ¨ce. Le tableau suivant contient des donnÃ©es dâ€™abondance. abundance &lt;- tibble(&#39;Bruant familier&#39; = c(1, 0, 0, 3), &#39;Citelle Ã  poitrine rousse&#39; = c(1, 0, 0, 0), &#39;Colibri Ã  gorge rubis&#39; = c(0, 1, 0, 0), &#39;Geai bleu&#39; = c(3, 2, 0, 0), &#39;Bruant chanteur&#39; = c(1, 0, 5, 2), &#39;Chardonneret&#39; = c(0, 9, 6, 0), &#39;Bruant Ã  gorge blanche&#39; = c(1, 0, 0, 0), &#39;MÃ©sange Ã  tÃªte noire&#39; = c(20, 1, 1, 0), &#39;Jaseur borÃ©al&#39; = c(66, 0, 0, 0)) Ce tableau peut Ãªtre rapidement transformÃ© en donnÃ©es dâ€™occurence, qui ne comprennent que lâ€™information boolÃ©enne de prÃ©sence (notÃ© 1) et dâ€™absence (notÃ© 0). occurence &lt;- abundance %&gt;% transmute_all(~if_else(. &gt; 0, 1, 0)) Lâ€™espace des espÃ¨ces (ou des variables ou descripteurs) est celui oÃ¹ les espÃ¨ces forment les axes et oÃ¹ les sites sont positionnÃ©s dans cet espace. Il sâ€™agit dâ€™une perspective en mode R, qui permet principalement dâ€™identifier quels espÃ¨ces se retrouvent plus courrament ensemble. abundance %&gt;% select(&quot;Bruant chanteur&quot;, &quot;Chardonneret&quot;, &quot;MÃ©sange Ã  tÃªte noire&quot;) ## # A tibble: 4 x 3 ## `Bruant chanteur` Chardonneret `MÃ©sange Ã  tÃªte noire` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 20 ## 2 0 9 1 ## 3 5 6 1 ## 4 2 0 0 Dans lâ€™espace des sites (ou les Ã©chantillons ou objets), on transpose la matrice dâ€™abondance. On passe ici en mode Q, oÃ¹ chaque point est une espÃ¨ce, et oÃ¹ lâ€™on peut observer quels Ã©chantillons sont similaires. abundance %&gt;% t() ## [,1] [,2] [,3] [,4] ## Bruant familier 1 0 0 3 ## Citelle Ã  poitrine rousse 1 0 0 0 ## Colibri Ã  gorge rubis 0 1 0 0 ## Geai bleu 3 2 0 0 ## Bruant chanteur 1 0 5 2 ## Chardonneret 0 9 6 0 ## Bruant Ã  gorge blanche 1 0 0 0 ## MÃ©sange Ã  tÃªte noire 20 1 1 0 ## Jaseur borÃ©al 66 0 0 0 8.1.2 Environnement Lâ€™espace de lâ€™environnement comprend souvent un autre tableau contenant lâ€™information sur lâ€™environnement oÃ¹ se trouve les espÃ¨ces: les coordonnÃ©es et lâ€™Ã©lÃ©vation, la pente, le pH du sol, la pluviomÃ©trie, etc. 8.2 Analyse dâ€™association Nous utiliserons le terme association comme une mesure pour quantifier la ressemblance ou la diffÃ©rence entre deux objets (Ã©chantillons) ou variables (descripteurs). Alors que la corrÃ©lation et la covariance sont des mesures dâ€™association entre des variables (analyse en mode R), la similaritÃ© et la distance sont deux types de une mesure dâ€™association entre des objets (analyse en mode Q). Une distance de 0 est mesurÃ©e chez deux objets identiques. La distance augmente au fur et Ã  mesure que les objets sont dissociÃ©s. Une similaritÃ© ayant une valeur de 0 indique aucune association, tandis quâ€™une valeur de 1 indique une association parfaite. Ã€ lâ€™opposÃ©, la dissimilaritÃ© est Ã©gale Ã  1-similaritÃ©. La distance peut Ãªtre liÃ©e Ã  la similaritÃ© par la relation: \\[distance=\\sqrt{1-similaritÃ©}\\] ou \\[distance=\\sqrt{dissimilaritÃ©}\\] La racine carrÃ©e permet, pour certains indices de similaritÃ©, dâ€™obtenir des propriÃ©tÃ©s euclÃ©diennes. Pour plus de dÃ©tails, voyez le tableau 7.2 de Legendre et Legendre (2012). Les matrices dâ€™association sont gÃ©nÃ©ralement prÃ©sentÃ©es comme des matrices carrÃ©es, dont les dimensions sont Ã©gales au nombre dâ€™objets (mode Q) ou de vrariables (mode R) dans le tableau. Chaque Ã©lÃ©ment (â€œcelluleâ€) de la matrice est un indice dâ€™association entre un objet (ou une variable) et un autre. Ainsi, la diagonale de la matrice est un vecteur nul (distance ou dissimilaritÃ©) ou unitaire (similaritÃ©), car elle correspond Ã  lâ€™association entre un objet et lui-mÃªme. Puisque lâ€™association entre A et B est la mÃªme quâ€™entre B et A, et puisque la diagonale retourne une valeur convenue, il est possible dâ€™exprimer une matrice dâ€™association en mode â€œcompactâ€, sous forme de vecteur. Le vecteur dâ€™association entre des objets A, B et C contiendra toute lâ€™information nÃ©cessaire en un vecteur de trois chiffres, [AB, AC, BC], plutÃ´t quâ€™une matrice de dimension \\(3 \\times 3\\). Lâ€™impact sur la mÃ©moire vive peut Ãªtre considÃ©rable pour les calculs comprenant de nombreuses dimensions. En R, les calculs de similaritÃ© et de distances peuvent Ãªtre effectuÃ©s avec le module vegan. La fonction vegdist permet de calculer les indices dâ€™association en forme carrÃ©e. Nous verons plus tard les mÃ©thodes de mesure de similaritÃ© et de distance plus loin. Pour lâ€™instant, utilisons la mÃ©thode de Jaccard pour une dÃ©monstration sur des donnÃ©es dâ€™occurence. library(&quot;vegan&quot;) ## Loading required package: permute ## Loading required package: lattice ## This is vegan 2.5-4 vegdist(occurence, method = &quot;jaccard&quot;, diag = TRUE, upper = TRUE) ## 1 2 3 4 ## 1 0.0000000 0.7777778 0.7500000 0.7142857 ## 2 0.7777778 0.0000000 0.6000000 1.0000000 ## 3 0.7500000 0.6000000 0.0000000 0.7500000 ## 4 0.7142857 1.0000000 0.7500000 0.0000000 Remarquez que vegdist retourne une matrice dont la diagonale est de 0 (on lâ€™affiche en spÃ©cifiant diag = TRUE). La diagonale est lâ€™association dâ€™un objet avec lui-mÃªme. Or la similaritÃ© dâ€™un objet avec lui-mÃªme devrait Ãªtre de 1! En fait, par convention vegdist retourne des dissimilaritÃ©s, non pas des similaritÃ©s. La matrice de distance serait donc calculÃ©e en extrayant la racine carrÃ©e des Ã©lÃ©ments de la matrice de dissimilaritÃ©: dissimilarity &lt;- vegdist(occurence, method = &quot;jaccard&quot;, diag = TRUE, upper = TRUE) distance &lt;- sqrt(dissimilarity) distance ## 1 2 3 4 ## 1 0.0000000 0.8819171 0.8660254 0.8451543 ## 2 0.8819171 0.0000000 0.7745967 1.0000000 ## 3 0.8660254 0.7745967 0.0000000 0.8660254 ## 4 0.8451543 1.0000000 0.8660254 0.0000000 Dans le chapitre sur lâ€™analyse compositionnelle, nous avons abordÃ© les significations diffÃ©rentes que peuvent prendre le zÃ©ro. Lâ€™information fournie par un zÃ©ro peut Ãªtre diffÃ©rente selon les circonstances. Dans le cas dâ€™une variable continue, un zÃ©ro signifie gÃ©nÃ©ralement une mesure sous le seuil de dÃ©tection. Deux tissus dont la concentration en cuivre est nulle ont une afinitÃ© sous la perspective de la concentration en cuivre. Dans le cas de mesures dâ€™abondance (dÃ©compte) ou dâ€™occurence (prÃ©sence-absence), on pourra dÃ©crire comme similaires deux niches Ã©cologiques oÃ¹ lâ€™on retrouve une espÃ¨ce en particulier. Mais deux sites oÃ¹ lâ€™on de retouve pas dâ€™ours polaires ne correspondent pas nÃ©cessairement Ã  des niches similaires! En effet, il peut exister de nombreuses raisons Ã©cologiques et mÃ©thodologiques pour lesquelles lâ€™espÃ¨ces ou les espÃ¨ces nâ€™ont pas Ã©tÃ© observÃ©es. Câ€™est le problÃ¨me des double-zÃ©ros (espÃ¨ces non observÃ©es Ã  deux sites), problÃ¨me qui est amplifiÃ© avec les grilles comprenant des espÃ¨ces rares. La ressemblance entre des objets comprenant des donnÃ©es continues devrait Ãªtre calculÃ©e grÃ¢ce Ã  des indicateurs symÃ©triques. Inversement, les affinitÃ©s entre les objets dÃ©crits par des donnÃ©es dâ€™abondance ou dâ€™occurence susceptibles de gÃ©nÃ©rer des problÃ¨mes de double-zÃ©ros devraient Ãªtre Ã©valuÃ©es grÃ¢ce Ã  des indicateurs asymÃ©triques. Un dÃ©fi supplÃ©mentaire arrive lorsque les donnÃ©es sont de type mixte. Nous utiliserons la convention de vegan et nous calculerons la dissimilaritÃ©, non pas la similaritÃ©. Les mesures de dissimilaritÃ© sont calculÃ©es sur des donnÃ©es dâ€™abondance ou des donnÃ©es dâ€™occurence. Notons quâ€™il existe beaucoup de confusion dans la littÃ©rature sur la maniÃ¨re de nommer les dissimilaritÃ©s (ce qui nâ€™est pas le cas des distances, dont les noms sont reconnus). Dans les sections suivantes, nous noterons la dissimilaritÃ© avec un \\(d\\) minuscule et la distance avec un \\(D\\) majuscule. 8.2.1 Association entre objets (mode Q) 8.2.1.1 Objets: Abondance La dissimilaritÃ© de Bray-Curtis est asymÃ©trique. Elle est aussi appelÃ©e lâ€™indice de Steinhaus, de Czekanowski ou de SÃ¸rensen. Il est important de sâ€™assurer de bien sâ€™entendre la mÃ©thode Ã  laquelle on fait rÃ©fÃ©rence. Lâ€™Ã©quation enlÃ¨ve toute ambiguitÃ©. La dissimilaritÃ© de Bray-Curtis entre les points A et B est calculÃ©e comme suit. \\[d_{AB} = \\frac {\\sum \\left| A_{i} - B_{i} \\right| }{\\sum \\left(A_{i}+B_{i}\\right)}\\] Utilisons vegdist pour gÃ©nÃ©rer les matrices dâ€™association. Le format â€œlisteâ€ de R est pratique pour enregistrer la collection dâ€™objets, dont les matrice dâ€™association que nous allons crÃ©er dans cette section. associations_abund &lt;- list() associations_abund[[&#39;BrayCurtis&#39;]] &lt;- vegdist(abundance, method = &quot;bray&quot;) associations_abund[[&#39;BrayCurtis&#39;]] ## 1 2 3 ## 2 0.9433962 ## 3 0.9619048 0.4400000 ## 4 0.9591837 1.0000000 0.7647059 La dissimilaritÃ© de Bray-Curtis est souvent utilisÃ©e dans la littÃ©rature. Toutefois, la version originale de Bray-Curtis nâ€™est pas tout Ã  fait mÃ©trique (semimÃ©trique). ConsÃ©quemment, la dissimilaritÃ© de Ruzicka (une variante de la dissimilaritÃ© de Jaccard pour les donnÃ©es dâ€™abondance) est mÃ©trique, et devrait probablement Ãªtre prÃ©fÃ©rÃ© Ã  Bary-Curtis (Oksanen, 2006). \\[d_{AB, Ruzicka} = \\frac { 2 \\times d_{AB, Bray-Curtis} }{1 + d_{AB, Bray-Curtis}}\\] associations_abund[[&#39;Ruzicka&#39;]] &lt;- associations_abund[[&#39;BrayCurtis&#39;]] * 2 / (1 + associations_abund[[&#39;BrayCurtis&#39;]]) La dissimilaritÃ© de Kulczynski (aussi Ã©crit Kulsinski) est asymÃ©trique et semimÃ©trique, tout comme celle de Bray-Curtis. Elle est calculÃ©e comme suit. \\[d_{AB} = 1-\\frac{1}{2} \\times \\left[ \\frac{\\sum min(A_i, B_i)}{\\sum A_i} + \\frac{\\sum min(A_i, B_i)}{\\sum B_i} \\right]\\] associations_abund[[&#39;Kulczynski&#39;]] &lt;- vegdist(abundance, method = &quot;kulczynski&quot;) Une approche commune pour mesurer lâ€™association entre sites dÃ©crits par des donnÃ©es dâ€™abondance est la distance de Hellinger. Notez quâ€™il sâ€™agit ici dâ€™une distance, non pas dâ€™une dissimilaritÃ©. Pour lâ€™obtenir, on doit dâ€™abord diviser chaque donnÃ©e dâ€™abondance par lâ€™abondance totale pour chaque site pour obtenir les espÃ¨ces en tant que proportions, puis on extrait la racine carrÃ©e de chaque Ã©lÃ©ment. Enfin, on calcule la distance euclidienne entre les proportions de chaque site. Pour rappel, une distance euclidienne est la gÃ©nÃ©ralisation en plusieurs dimensions du thÃ©orÃ¨me de Pythagore, \\(c = \\sqrt{a^2 + b^2}\\). \\[D_{AB} = \\sqrt {\\sum \\left( \\frac{A_i}{\\sum A_i} - \\frac{B_i}{\\sum B_i} \\right)^2}\\] ğŸ˜±Â Attention La distance dâ€™Hellinger hÃ©rite des biais liÃ©es aux donnÃ©es compositionnelles. Elle peut Ãªtre substitiÃ©e par une matrice de distances dâ€™Aitchison. associations_abund[[&#39;Hellinger&#39;]] &lt;- dist(decostand(abundance, method=&quot;hellinger&quot;)) Toute comme la distance dâ€™Hellinger, la distance de chord est calculÃ©e par une distance euclidienne sur des donnÃ©es dâ€™abondance transformÃ©es de sorte que chaque ligne ait une longueur (norme) de 1. associations_abund[[&#39;Chord&#39;]] &lt;- dist(decostand(abundance, method=&quot;normalize&quot;)) La mÃ©trique du chi-carrÃ©, ou \\(\\chi\\)-carrÃ©, ou chi-square, donne davantage de poids aux espÃ¨ces rares quâ€™aux espÃ¨ces communes. Son utilisation est recommandÃ©e lorsque les espÃ¨ces rares sont de bons indicateurs de conditions Ã©cologiques particuliÃ¨res (Legendre et Legendre, 2012, p.Â 308). \\[ d_{AB} = \\sqrt{\\sum _j \\frac{1}{\\sum y_j} \\left( \\frac{A_j}{\\sum A} - \\frac{B_j}{\\sum B} \\right)^2 } \\] La mÃ©trique peut Ãªtre transformÃ©e en distance en la multipliant par la racine carrÃ©e de la somme totale des espÃ¨ces dans la matric dâ€™abondance (\\(X\\)). \\[ D_{AB} = \\sqrt{\\sum X} \\times d_{AB} \\] associations_abund[[&#39;ChiSquare&#39;]] &lt;- dist(decostand(abundance, method=&quot;chi.square&quot;)) Une manniÃ¨re visuellement plus intÃ©ressante de prÃ©senter une matrice dâ€™association est un graphique de type heatmap. associations_abund_df &lt;- list() for (i in 1:length(associations_abund)) { associations_abund_df[[i]] &lt;- data.frame(as.matrix(associations_abund[[i]])) colnames(associations_abund_df[[i]]) &lt;- rownames(associations_abund_df[[i]]) associations_abund_df[[i]]$row &lt;- rownames(associations_abund_df[[i]]) associations_abund_df[[i]] &lt;- associations_abund_df[[i]] %&gt;% gather(key=row) associations_abund_df[[i]]$column = rep(1:4, 4) associations_abund_df[[i]]$dist &lt;- names(associations_abund)[i] } associations_abund_df &lt;- do.call(rbind, associations_abund_df) ggplot(associations_abund_df, aes(x=row, y=column)) + facet_wrap(. ~ dist, nrow = 2) + geom_tile(aes(fill = value)) + geom_text(aes(label = round(value, 2))) + scale_fill_gradient2(low = &quot;#00ccff&quot;, mid = &quot;#aad400&quot;, high = &quot;#ff0066&quot;, midpoint = 2) + labs(x=&quot;Site&quot;, y=&quot;Site&quot;) Peu importe le type dâ€™association utilisÃ©e, les heatmaps montrent les mÃªmes tendances. Les assocaitions de dissimilaritÃ© (Bray-Curtis, Kulczynski et Ruzicka) sâ€™Ã©talent de 0 Ã  1, tandis que les distances (Chi-Square, Chord et Hellinger) partent de zÃ©ro, mais nâ€™ont pas de limite supÃ©rieure. On note les plus grandes diffÃ©rences entre les sites 2 et 4, tandis que les sites 2 et 3 sont les plus semblables pour toutes les mesures dâ€™association Ã  lâ€™exception de la dissimilaritÃ© de Kulczynski. 8.2.1.2 Objets: Occurence (prÃ©sence-absence) Des indices dâ€™association diffÃ©rents devraient Ãªtre utilisÃ©s lorsque des donnÃ©es sont compilÃ©es sous forme boolÃ©enne. En gÃ©nÃ©ral, les tableaux de donnÃ©es dâ€™occurence seront compilÃ©s avec des 1 (prÃ©sence) et des 0 (absence). La similaritÃ© de Jaccard entre le site A et le site B est la proportion de double 1 (prÃ©sences de 1 dans A et B) parmi les espÃ¨ces. La dissimilariÃ© est la proportion complÃ©mentaire (comprenant [1, 0], [0, 1] et [0, 0]). La distance de Jaccard est la racine carrÃ©e de la dissimilaritÃ©. associations_occ &lt;- list() associations_occ[[&#39;Jaccard&#39;]] &lt;- vegdist(occurence, method = &quot;jaccard&quot;) Les distances dâ€™Hellinger, de chord et de chi-carrÃ© sont aussi appropriÃ©es pour les calculs de distances sur des tableaux dâ€™occurence. associations_occ[[&#39;Hellinger&#39;]] &lt;- dist(decostand(occurence, method=&quot;hellinger&quot;)) associations_occ[[&#39;Chord&#39;]] &lt;- dist(decostand(occurence, method=&quot;normalize&quot;)) associations_occ[[&#39;ChiSquare&#39;]] &lt;- dist(decostand(occurence, method=&quot;chi.square&quot;)) Graphiquement, associations_occ_df &lt;- list() for (i in 1:length(associations_occ)) { associations_occ_df[[i]] &lt;- data.frame(as.matrix(associations_occ[[i]])) colnames(associations_occ_df[[i]]) &lt;- rownames(associations_occ_df[[i]]) associations_occ_df[[i]]$row &lt;- rownames(associations_occ_df[[i]]) associations_occ_df[[i]] &lt;- associations_occ_df[[i]] %&gt;% gather(key=row) associations_occ_df[[i]]$column = rep(1:4, 4) associations_occ_df[[i]]$dist &lt;- names(associations_occ)[i] } associations_occ_df &lt;- do.call(rbind, associations_occ_df) ggplot(associations_occ_df, aes(x=row, y=column)) + facet_wrap(. ~ dist) + geom_tile(aes(fill = value)) + geom_text(aes(label = round(value, 2))) + scale_fill_gradient2(low = &quot;#00ccff&quot;, mid = &quot;#aad400&quot;, high = &quot;#ff0066&quot;, midpoint = 1) + labs(x=&quot;Site&quot;, y=&quot;Site&quot;) Il est attendu que les matrices dâ€™association sur lâ€™occurence sont semblables Ã  celles sur lâ€™abondance. Dans ce cas-ci, la distance dâ€™Hellinger donne des rÃ©sultats semblables Ã  la dissimilaritÃ© de Jaccard. 8.2.1.3 Objets: DonnÃ©es quantitatives Les donnÃ©es quantitative en Ã©cologie peuvent dÃ©crire lâ€™Ã©tat de lâ€™environnement: le climat, lâ€™hydrologie, lâ€™hydrogÃ©ochimie, la pÃ©dologie, etc. En rÃ¨gle gÃ©nÃ©rale, les coordonnÃ©es des sites ne sot pas des variables environnementales, Ã  que lâ€™on soupÃ§onne la coordonnÃ©e elle-mÃªme dâ€™Ãªtre responsable dâ€™effets sur notre systÃ¨me: mais il sâ€™agira la plupart du temps dâ€™effets confondants (par exemple, on peut mesurer un effet de lattitude sur le rendement des agrumes, mais il sâ€™agira probablement avant tout dâ€™effets dus aux conditions climatiques, qui elles changent en fonction de la lattitude). Dâ€™autre types de donnÃ©es quantitative pouvant Ãªtre apprÃ©hendÃ©es par des distances sont les traits phÃ©nologiques, les ionomes, les gÃ©nomes, etc. La distance euclidienne est la racine carrÃ©e de la somme des carrÃ©s des distances sur tous les axes. Il sâ€™agit dâ€™une application multidimensionnelle du thÃ©orÃ¨me de Pythagore. La distance dâ€™Aitchison, couverte dans le chapitre 7, est une distance euclidienne calculÃ©e sur des donnÃ©es compositionnelles prÃ©alablement transformÃ©es. La distance euclidienne est sensible aux unitÃ©s utilisÃ©s: utiliser des milimÃ¨tres plutÃ´t que des mÃ¨tres enflera la distance euclidienne. Il est recommandÃ© de porter une attention particuliÃ¨re aux unitÃ©s, et de standardiser les donnÃ©es au besoin (par exemple, en centrant la moyenne Ã  zÃ©ro et en fixant lâ€™Ã©cart-type Ã  1). On pourrait, par exemple, mesurer la distance entre des observations des dimensions de diffÃ©rentes espÃ¨ces dâ€™iris. Ce tableau est inclu dans R par dÃ©faut. data(iris) iris %&gt;% sample_n(5) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 6.7 3.3 5.7 2.5 virginica ## 2 5.4 3.9 1.3 0.4 setosa ## 3 6.4 2.8 5.6 2.2 virginica ## 4 5.5 2.6 4.4 1.2 versicolor ## 5 5.5 2.5 4.0 1.3 versicolor Les mesures du tableau sont en centimÃ¨tres. Pour Ã©viter de donner davantage de poids aux longueur des sÃ©pales et en mÃªme temps de nÃ©gliger la largeur des pÃ©tales, nous allons standardiser le tableau. iris_sc &lt;- iris %&gt;% select(-Species) %&gt;% scale(.)%&gt;% as_tibble(.) %&gt;% mutate(Species = iris$Species) iris_sc ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 -0.898 1.02 -1.34 -1.31 setosa ## 2 -1.14 -0.132 -1.34 -1.31 setosa ## 3 -1.38 0.327 -1.39 -1.31 setosa ## 4 -1.50 0.0979 -1.28 -1.31 setosa ## 5 -1.02 1.25 -1.34 -1.31 setosa ## 6 -0.535 1.93 -1.17 -1.05 setosa ## 7 -1.50 0.786 -1.34 -1.18 setosa ## 8 -1.02 0.786 -1.28 -1.31 setosa ## 9 -1.74 -0.361 -1.34 -1.31 setosa ## 10 -1.14 0.0979 -1.28 -1.44 setosa ## # â€¦ with 140 more rows Pour les comparaisons des dimensions, prenons la moyenne des dimensions (mises Ã  lâ€™Ã©chelle) par espÃ¨ce. iris_means &lt;- iris_sc %&gt;% group_by(Species) %&gt;% summarise_all(mean) %&gt;% select(-Species) iris_means ## # A tibble: 3 x 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -1.01 0.850 -1.30 -1.25 ## 2 0.112 -0.659 0.284 0.166 ## 3 0.899 -0.191 1.02 1.08 Nous pouvons utiliser la distance euclidienne, commune en gÃ©omÃ©trie, pour comparer les espÃ¨ces. La distance euclidienne est calculÃ©e comme suit. \\[ \\mathcal{E} = \\sqrt{\\Sigma_i \\left( A_i - B_i \\right) ^2 } \\] associations_cont = list() associations_cont[[&#39;Euclidean&#39;]] &lt;- dist(iris_sc %&gt;% select(-Species), method=&quot;euclidean&quot;) La distance de Mahalanobis est semblable Ã  la distance euclidienne, mais qui tient compte de la covariance de la matrice des objets. Cette covariance peut Ãªtre utilisÃ©e pour dÃ©crire la structure dâ€™un nuage de points. La diastance de Mahalanobis se calcule comme suit. \\[\\mathcal{M} = \\sqrt{(A - B)^T S^{-1} (A-B)}\\] Notez quâ€™il sâ€™agit dâ€™une gÃ©nÃ©ralisation de la distance euclidienne, qui Ã©quivaut Ã  une distance de Mahalanobis dont la matrice de covariance est une matrice identitÃ©. La distance de Mahalanobis permet de reprÃ©senter des distances dans un espace fortement corrÃ©lÃ©. Elle est courramment utilisÃ©e pour dÃ©tecter les valeurs aberrantes selon des critÃ¨res de distance Ã  partir du centre dâ€™un jeu de donnÃ©es multivariÃ©es. associations_cont[[&#39;Mahalanobis&#39;]] &lt;- vegdist(iris_sc %&gt;% select(-Species), &#39;mahalanobis&#39;) La distance de Manhattan porte aussi le nom de distance de cityblock ou de taxi. Câ€™est la distance que vous devrez parcourir pour vous rendre du point A au point B Ã  Manhattan, câ€™est-Ã -dire selon une sÃ©quence de tronÃ§ons perpendiculaires. \\[ D_{AB} = \\sum _i \\left| A_i - B_i \\right| \\] La distance de Manhattan est appropriÃ©e lorsque les gradients (changements dâ€™un Ã©tat Ã  lâ€™autre ou dâ€™une rÃ©gion Ã  lâ€™autre) ne permettent pas des changements simultanÃ©s. Mieux vaut standardiser les variables pour Ã©viter quâ€™une dimension soit prÃ©pondÃ©rante. associations_cont[[&#39;Manhattan&#39;]] &lt;- vegdist(iris_sc %&gt;% select(-Species), &#39;manhattan&#39;) Avant de prÃ©senter les rÃ©sultats des espÃ¨ces dâ€™iris, voici une reprÃ©sentation des distances euclidiennes (rouge), de Mahalanobis (bleu) et de Manhattan (vert), chacune de 1 et 2 unitÃ©s Ã  partir du centre et, pour ce qui est de la distance de Mahalanobis, selon la covariance. library(&quot;car&quot;) ## Loading required package: carData ## ## Attaching package: &#39;car&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## recode ## The following object is masked from &#39;package:purrr&#39;: ## ## some library(&quot;MASS&quot;) ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## select select &lt;- dplyr::select # Ã©viter les conflits de fonctions entre MASS et dplyr filter &lt;- dplyr::filter sigma &lt;- matrix(c(1, 0.6, 0.6, 1), ncol = 2) # matrice de covariance mu &lt;- c(0, 0) # centre data &lt;- mvrnorm(n = 100, mu, sigma) # gÃ©nÃ©rer des donnÃ©es plot(data, ylim = c(-2, 2), xlim = c(-2, 2), asp = 1) ## cercles t &lt;- seq(0,2*pi,length=100) c1 &lt;- t(rbind(mu[2] + sin(t)*1, mu[1] + cos(t)*1)) c2 &lt;- t(rbind(mu[2] + sin(t)*2, mu[1] + cos(t)*2)) lines(c1, lwd = 2, col = &quot;red&quot;) lines(c2, lwd = 2, col = &quot;red&quot;) ## ellipses e1 &lt;- ellipse(mu, sigma, radius=1, add=TRUE) e2 &lt;- ellipse(mu, sigma, radius=2, add=TRUE) ## carrÃ©s lines(c(1, 0, -1, 0, 1), c(0, 1, 0, -1, 0), lwd = 2, col = &quot;green&quot;) lines(c(2, 0, -2, 0, 2), c(0, 2, 0, -2, 0), lwd = 2, col = &quot;green&quot;) Et, graphiquement, les rÃ©sultats des distances des iris. associations_cont_df &lt;- list() for (i in 1:length(associations_cont)) { associations_cont_df[[i]] &lt;- data.frame(as.matrix(associations_cont[[i]])) colnames(associations_cont_df[[i]]) &lt;- rownames(associations_cont_df[[i]]) associations_cont_df[[i]]$row &lt;- rownames(associations_cont_df[[i]]) associations_cont_df[[i]] &lt;- associations_cont_df[[i]] %&gt;% gather(key=row) associations_cont_df[[i]]$column = rep(1:nrow(iris), nrow(iris)) associations_cont_df[[i]]$dist &lt;- names(associations_cont)[i] } associations_cont_df &lt;- do.call(rbind, associations_cont_df) ggplot(associations_cont_df, aes(x=row, y=column)) + facet_wrap(. ~ dist) + geom_tile(aes(fill = value), colour = NA) + #geom_text(aes(label = round(value, 2))) + scale_fill_gradient2(low = &quot;#00ccff&quot;, mid = &quot;#aad400&quot;, high = &quot;#ff0066&quot;, midpoint = 5) + labs(x=&quot;Site&quot;, y=&quot;Site&quot;) Le tableau iris est ordonnÃ© par espÃ¨ce. Les distances euclidienne et de Manhattan permettent aisÃ©ment de distinguer les espÃ¨ces selon les dimensions des pÃ©tales et des sÃ©pales. Toutefois, lâ€™utilsation de la covariance avec la distance de Mahalanobis crÃ©e des distinction moins tranchÃ©es. 8.2.1.4 Objets: DonnÃ©es mixtes Les donnÃ©es catÃ©gorielles ordinales peuvent Ãªtre transformÃ©es en donnÃ©es continues par gradations linÃ©aires ou quadratiques. Les donnÃ©es catÃ©gorielles nominales, quant Ã  elles, peuvent Ãªtre encodÃ©es (encodage catÃ©goriel) en donnÃ©es similaires Ã  des occurences. Attention toutefois: contrairement Ã  la rÃ©gression linÃ©aire qui demande dâ€™exclure une catÃ©gorie, lâ€™encodage catÃ©goriel doit inclure toutes les catÃ©gories. Le comportement par dÃ©faut de la fonction model.matrix est dâ€™exclure la catÃ©gorie de rÃ©fÃ©rence: on doit spÃ©cifier que lâ€™intercept est de zÃ©ro, câ€™est-Ã -dire model.matrix(~ + categorie). La similaritÃ© de Gower a Ã©tÃ© dÃ©veloppÃ©e pour mesurer des associations entre des objets dont les donnÃ©es sont mixtes: boolÃ©ennes, catÃ©gorielles et continues. La similaritÃ© de Gower est calculÃ©e en additionnant les distances calculÃ©es par colonne, individuellement. Si la colonne est boolÃ©enne, on utilise les distances de Jaccard (qui exclue les double-zÃ©ro) de maniÃ¨re univariÃ©e: une variable Ã  la fois. Pour les variables continues, on utilise la distance de Manhattan divisÃ©e par la plage de valeurs de la variable (pour fin de standardisation). Puisquâ€™elle hÃ©rite de la particularitÃ© de la distance de Manhattan et de la similaritÃ© de Jaccard univariÃ©e, la similaritÃ© de Gower reste une combinaison linÃ©aire de distances univariÃ©es. X &lt;- tibble(ID = 1:8, age = c(21, 21, 19, 30, 21, 21, 19, 30), gender = c(&#39;M&#39;,&#39;M&#39;,&#39;N&#39;,&#39;M&#39;,&#39;F&#39;,&#39;F&#39;,&#39;F&#39;,&#39;F&#39;), civil_status = c(&#39;MARRIED&#39;,&#39;SINGLE&#39;,&#39;SINGLE&#39;,&#39;SINGLE&#39;,&#39;MARRIED&#39;,&#39;SINGLE&#39;,&#39;WIDOW&#39;,&#39;DIVORCED&#39;), salary = c(3000.0,1200.0 ,32000.0,1800.0 ,2900.0 ,1100.0 ,10000.0,1500.0), children = c(TRUE, FALSE, TRUE, TRUE, TRUE, TRUE, FALSE, TRUE), available_credit = c(2200,100,22000,1100,2000,100,6000,2200)) X ## # A tibble: 8 x 7 ## ID age gender civil_status salary children available_credit ## &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; ## 1 1 21 M MARRIED 3000 TRUE 2200 ## 2 2 21 M SINGLE 1200 FALSE 100 ## 3 3 19 N SINGLE 32000 TRUE 22000 ## 4 4 30 M SINGLE 1800 TRUE 1100 ## 5 5 21 F MARRIED 2900 TRUE 2000 ## 6 6 21 F SINGLE 1100 TRUE 100 ## 7 7 19 F WIDOW 10000 FALSE 6000 ## 8 8 30 F DIVORCED 1500 TRUE 2200 Il faut prÃ©alablement procÃ©der Ã  lâ€™encodage catÃ©goriel pour les variables catÃ©gorielles nominales. X_dum &lt;- model.matrix(~ 0 + ., X[, -1]) X_dum ## age genderF genderM genderN civil_statusMARRIED civil_statusSINGLE ## 1 21 0 1 0 1 0 ## 2 21 0 1 0 0 1 ## 3 19 0 0 1 0 1 ## 4 30 0 1 0 0 1 ## 5 21 1 0 0 1 0 ## 6 21 1 0 0 0 1 ## 7 19 1 0 0 0 0 ## 8 30 1 0 0 0 0 ## civil_statusWIDOW salary childrenTRUE available_credit ## 1 0 3000 1 2200 ## 2 0 1200 0 100 ## 3 0 32000 1 22000 ## 4 0 1800 1 1100 ## 5 0 2900 1 2000 ## 6 0 1100 1 100 ## 7 1 10000 0 6000 ## 8 0 1500 1 2200 ## attr(,&quot;assign&quot;) ## [1] 1 2 2 2 3 3 3 4 5 6 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$gender ## [1] &quot;contr.treatment&quot; ## ## attr(,&quot;contrasts&quot;)$civil_status ## [1] &quot;contr.treatment&quot; ## ## attr(,&quot;contrasts&quot;)$children ## [1] &quot;contr.treatment&quot; Calculons la dissimilaritÃ© de Gower (cette fois le graphique est fait avec pheatmap). library(&quot;pheatmap&quot;) d_gow &lt;- as.matrix(vegdist(X_dum, &#39;gower&#39;)) colnames(d_gow) &lt;- rownames(d_gow) &lt;- X$ID pheatmap(d_gow) Les dendrogrammes apparaissants sur les axes du graphique sont issus dâ€™un processus de partitionnement basÃ© sur la distance, que nous verrons plus loin dans ce chapiter. Les profils des clients 4 et 7, ainsi que ceux des clients 3 et 7 diffÃ¨rent le plus. Les profils 3 et 4 sont nÃ©anmoins plutÃ´t diffÃ©rents. 8.2.2 Associations entre variables (mode R) Il existe de nombreuses approches pour mesurer les associations entre variables. La plus connue est la corrÃ©lation. Mais les donnÃ©es dâ€™abondance et dâ€™occurence demandent des approches diffÃ©rentes. 8.2.2.1 Variables: Abondance La distance du chi-carrÃ© est suggÃ©rÃ©e par Borcard et al. (2011). abundance_r &lt;- t(abundance) D_chisq_R &lt;- as.matrix(dist(decostand(abundance_r, method=&quot;chi.square&quot;))) pheatmap(D_chisq_R, display_numbers = round(D_chisq_R, 2)) Des coabondances sont notables pour la mÃ©sange Ã  tÃªte noire, le jaseur borÃ©al, la citelle Ã  poitrine rousse et le bruant Ã  gorge blanche (tache bleu au centre). 8.2.2.2 Variables: Occurence La dissimilaritÃ© de Jaccard peut Ãªtre utilisÃ©e. occurence_r &lt;- t(occurence) D_jacc_R &lt;- as.matrix(vegdist(occurence_r, method = &quot;jaccard&quot;)) pheatmap(D_jacc_R, display_numbers = round(D_jacc_R, 2)) Des cooccurences sont notables pour le jaseur borÃ©al, la citelle Ã  poitrine rousse et le bruant Ã  gorge blanche (tache bleu au centre). 8.2.2.3 Variables: QuantitÃ©s La matrice des corrÃ©lations de Pearson peut Ãªtre utilisÃ©e pour les donnÃ©es continues. Quant aux variables ordinales, elles devraient idÃ©alement Ãªtre liÃ©es linÃ©airement ou quadratiquement. Si ce nâ€™est pas le cas, câ€™est-Ã -dire que les catÃ©gories sont ordonnÃ©es par rang seulement, vous pourrez avoir recours aux coefficients de corrÃ©lation de Spearman ou de Kendall. iris_cor &lt;- iris %&gt;% select(-Species) %&gt;% cor(.) pheatmap(cor(iris[, -5]), cluster_rows = FALSE, cluster_cols = FALSE, display_numbers = round(iris_cor, 2)) 8.2.3 Conclusion sur les associations Il nâ€™existe pas de rÃ¨gle claire pour dÃ©terminer quelle technique dâ€™association utiliser. Cela dÃ©pend en premier lieu de vos donnÃ©es. Vous sÃ©lectionnerez votre mÃ©thode dâ€™association selon le type de donnÃ©es que vous abordez, la question Ã  laquelle vous dÃ©sirez rÃ©pondre ainsi lâ€™expÃ©rience dans la littÃ©rature comme celle de vos collÃ¨gues scientifiques. Sâ€™il nâ€™existe pas de rÃ¨gle clair, câ€™est quâ€™il existe des dizaines de mÃ©thodes diffÃ©rentes, et la plupart dâ€™entre elles vous donneront une perspective juste et valide. Il faut nÃ©anmoins faire attention pour Ã©viter de sÃ©lectionner les mÃ©thodes qui ne sont pas appropriÃ©es. 8.3 Partitionnement Les donnÃ©es suivantes ont Ã©tÃ© gÃ©nÃ©rÃ©es par Leland McInnes (Tutte institute of mathematics, Ottawa). ÃŠtes-vous en mesure dâ€™identifier des groupes? Combien en trouvez-vous? df_mcinnes &lt;- read_csv(&quot;data/clusterable_data.csv&quot;, col_names = c(&quot;x&quot;, &quot;y&quot;), skip = 1) ## Parsed with column specification: ## cols( ## x = col_double(), ## y = col_double() ## ) ggplot(df_mcinnes, aes(x=x, y=y)) + geom_point() + coord_fixed() En 2D, lâ€™oeil humain peut facilement dÃ©tecter les groupes. En 3D, câ€™est toujours possible, mais au-delÃ  de 3D, le partitionnement cognitive devient rapidement maladroite. Les algorithmes sont alors dâ€™une aide prÃ©cieuse. Mais ils transportent en pratique tout un baggage de limitations. Quel est le critÃ¨re dâ€™association entre les groupes? Combien de groupe devrions-nous crÃ©er? Comment distinguer une donnÃ©e trop bruitÃ©e pour Ãªtre classifiÃ©e? Le partitionnement de donnÃ©es (clustering en anglais), et inversement leur regroupement, permet de crÃ©er des ensembles selon des critÃ¨res dâ€™association. On suppose donc que Le partitionnement permet de crÃ©er des groupes selon lâ€™information que lâ€™on fait Ã©merger des donnÃ©es. Il est consÃ©quemment entendu que les donnÃ©es ne sont pas catÃ©gorisÃ©es Ã  priori: il ne sâ€™agit pas de prÃ©dire la catÃ©gorie dâ€™un objet, mais bien de crÃ©er des catÃ©gories Ã  partir des objets par exemple selon leurs dimensions, leurs couleurs, leurs signature chimique, leurs comportements, leurs gÃ¨nes, etc. Plusieurs mÃ©thodes sont aujourdâ€™hui offertes aux analystes pour partitionner leurs donnÃ©es. Dans le cadre de ce manuel, nous couvrirons ici deux grandes tendances dans les algorithmes. MÃ©thodes hiÃ©rarchique et non hiÃ©rarchiques. Dans un partitionnement hiÃ©rarchique, lâ€™ensemble des objets forme un groupe, comprenant des sous-regroupements, des sous-sous-regroupements, etc., dont les objets forment lâ€™ultime partitionnement. On pourra alors identifier comment se dÃ©cline un partitionnement. Ã€ lâ€™inverse, un partitionnement non-hiÃ©rarchique des algorhitmes permettent de crÃ©er les groupes non hiÃ©rarchisÃ©s les plus diffÃ©rents que possible. Membership exclusif ou flou. Certaines techniques attribuent Ã  chaque objet une classe unique: lâ€™appartenance sera indiquÃ©e par un 1 et la non appartenance par un 0. Dâ€™autres techniques vont attribuer un membership flou oÃ¹ le degrÃ© dâ€™appartenance est une variable continue de 0 Ã  1. Parmi les mÃ©thodes floues, on retrouve les mÃ©thodes probabilistes. 8.3.1 Ã‰valuation dâ€™un partitionnement Le choix dâ€™une technique de partitionnement parmi de nombreuses disponibles, ainsi que le choix des paramÃ¨tres gouvernant chacune dâ€™entre elles, est avant tout basÃ© sur ce que lâ€™on dÃ©sire dÃ©finir comme Ã©tant un groupe, ainsi que la maniÃ¨re dâ€™interprÃ©ter les groupes. En outre, le nombre de groupe Ã  dÃ©partager est toujours une dÃ©cision de lâ€™analyste. NÃ©anmoins, on peut se fier des indicateurs de performance de partitionnement. Parmis ceux-ci, retenons le score silouhette ainsi que lâ€™indice de Calinski-Harabaz. 8.3.1.1 Score silouhette En anglais, le h dans silouhette se trouve aprÃ¨s le l: on parle donc de silhouette coefficient pour dÃ©signer le score de chacun des objets dans le partitionnement. Pour chaque objet, on calcule la distance moyenne qui le sÃ©pare des autres points de son groupe (\\(a\\)) ainsi que la distance moyenne qui le sÃ©pare des points du groupe le plus rapprochÃ©. \\[s = \\frac{b-a}{max \\left(a, b \\right)}\\] Un coefficient de -1 indique le pire classement, tandis quâ€™un coefficient de 1 indique le meilleur classement. La moyenne des coefficients silouhette est le score silouhette. 8.3.1.2 Indice de Calinski-Harabaz Lâ€™indice de Calinski-Harabaz est proportionnel au ratio des dispersions intra-groupe et la moyenne des dispersions inter-groupes. Plus lâ€™indice est Ã©levÃ©, mieux les groupes sont dÃ©finis. La mathÃ©matique est dÃ©crite dans la documentation de scikit-learn, un module dâ€™analyse et autoapprentissage sur Python. Note. Les coefficients silouhette et lâ€™indice de Calinski-Harabaz sont plus appropriÃ©s pour les formes de groupes convexes (cercles, sphÃ¨res, hypersphÃ¨res) que pour les formes irrÃ©guliÃ¨res (notamment celles obtenues par la DBSCAN, discutÃ©e ci-desssous). 8.3.2 Partitionnement non hiÃ©rarchique Il peut arriver que vous nâ€™ayez pas besoin de comprendre la structure dâ€™agglomÃ©ration des objets (ou variables). Plusieurs techniques de partitionnement non hiÃ©rarchique sont disponibles sur R. On sâ€™intÃ©ressera en particulier aux k-means et au dbscan. 8.3.2.1 Kmeans Lâ€™objectif des kmeans est de minimiser la distance euclÃ©dienne entre un nombre prÃ©dÃ©fini de k groupes exclusifs. Lâ€™algorhitme commence par placer une nombre k de centroides au hasard dans lâ€™espace dâ€™un nombre p de variables (vous devez fixer k, et p est le nombre de colonnes de vos donnÃ©es). Ensuite, chaque objet est Ã©tiquettÃ© comme appartenant au groupe du centroid le plus prÃ¨s. La position du centroide est dÃ©placÃ©e Ã  la moyenne de chaque groupe. Recommencer Ã  partir de lâ€™Ã©tape 2 jusquâ€™Ã  ce que lâ€™assignation des objets aux groupes ne change plus. Source: David Sheehan La technique des kmeans suppose que les groupes ont des distributions multinormales - reprÃ©sentÃ©es par des cercles en 2D, des sphÃ¨res en 3D, des hypersphÃ¨res en plus de 3D. Cette limitation est problÃ©matique lorsque les groupes se prÃ©sentent sous des formes irrÃ©guliÃ¨res, comme celles du nuage de points de Leland McInnes, prÃ©sentÃ© plus haut. De plus, la technique classique des kmeans est basÃ©e sur des distances euclidiennes: lâ€™utilisation des kmeans nâ€™est appropriÃ©e pour les donnÃ©es comprenant beaucoup de zÃ©ros, comme les donnÃ©es dâ€™abondance, qui devraient prÃ©alablement Ãªtre transformÃ©es en variables centrÃ©es et rÃ©duites (Legendre et Legendre, 2012). La technique des mixtures gaussiennes (gaussian mixtures) est une gÃ©nÃ©ralisation des kmeans permettant dâ€™intÃ©grer la covariance des groupes. Les groupes ne sont plus des hyper-sphÃ¨res, mais des hyper-ellipsoÃ¯des. 8.3.2.1.1 Application Nous pouvons utilisÃ© la fonction kmeans de R. Toutefois, puisque lâ€™on dÃ©sire ici effectuer des tests de partitionnement pour plusieurs nombres de groupes, nous utiliserons cascadeKM, du module vegan. Notez que de nombreux paramÃ¨tres par dÃ©faut sont utilisÃ©s dans les exÃ©cutions ci-dessous. Ces notes de cours ne forment pas un travail de recherche scientifique. Lors de travaux de recherche, lâ€™utilsation dâ€™un argument ou dâ€™un autre dans une fonction doit Ãªtre justifiÃ©: quâ€™un paramÃ¨tre soit utilisÃ© par dÃ©faut dans une fonction nâ€™est a priori pas une justification convainquante. Pour les kmeans, on doit fixer le nombre de groupes. Le graphique des donnÃ©es de Leland McInnes montrent 6 groupes. Toutefois, il est rare que lâ€™on puisse visualiser des dÃ©marquations aussi tranchÃ©es que celles de lâ€™exemple, qui plus est dans des cas oÃ¹ lâ€™on doit traiter de plus de deux dimensions. Je vais donc lancer le partitionnement en boucle pour plusieurs nombres de groupes, de 3 Ã  10 et pour chaque groupe, Ã©valuer le score silouhette et de Calinski-Habaraz. Jâ€™utilise un argument random_state pour mâ€™assurer que les groupes seront les mÃªmes Ã  chaque fois que la cellule sera lancÃ©e. library(&quot;vegan&quot;) mcinnes_kmeans &lt;- cascadeKM(df_mcinnes, inf.gr = 3, sup.gr = 10, criterion = &quot;calinski&quot;) str(mcinnes_kmeans) ## List of 4 ## $ partition: int [1:2309, 1:8] 2 2 2 2 2 2 2 2 2 2 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:2309] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. ..$ : chr [1:8] &quot;3 groups&quot; &quot;4 groups&quot; &quot;5 groups&quot; &quot;6 groups&quot; ... ## $ results : num [1:2, 1:8] 85.1 2164.5 61.4 2294.6 51.4 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:2] &quot;SSE&quot; &quot;calinski&quot; ## .. ..$ : chr [1:8] &quot;3 groups&quot; &quot;4 groups&quot; &quot;5 groups&quot; &quot;6 groups&quot; ... ## $ criterion: chr &quot;calinski&quot; ## $ size : int [1:10, 1:8] 505 1243 561 NA NA NA NA NA NA NA ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:10] &quot;Group 1&quot; &quot;Group 2&quot; &quot;Group 3&quot; &quot;Group 4&quot; ... ## .. ..$ : chr [1:8] &quot;3 groups&quot; &quot;4 groups&quot; &quot;5 groups&quot; &quot;6 groups&quot; ... ## - attr(*, &quot;class&quot;)= chr &quot;cascadeKM&quot; Lâ€™objet mcinnes_kmeans, de type cascadeKM, peut Ãªtre visualisÃ© directement avec la fonction plot. plot(mcinnes_kmeans) On obtient un maximum de Calinski Ã  4 groupes, qui correspons Ã  la deuxiÃ¨me simulation effectuÃ©e de 3 Ã  10. Examinons les scores silouhette (module: cluster). library(&quot;cluster&quot;) asw &lt;- c() for (i in 1:ncol(mcinnes_kmeans$partition)) { mcinnes_kmeans_silhouette &lt;- silhouette(mcinnes_kmeans$partition[, i], dist = vegdist(df_mcinnes, method = &quot;euclidean&quot;)) asw[i] &lt;- summary(mcinnes_kmeans_silhouette)$avg.width } plot(3:10, asw, type = &#39;b&#39;) Le score silouhette maximum est Ã  3 groupes. La forme des groupes nâ€™Ã©tant pas convexe, il fallait sâ€™attendre Ã  ce que indicateurs maximaux pour les deux indicateurs soient diffÃ©rents. Câ€™est dâ€™ailleurs souvent le cas. Cet exemple supporte que le choix du nombre de groupe Ã  dÃ©partager repose sur lâ€™analyste, non pas uniquement sur les indicateurs de performance. Choisissons 6 groupes, puisque que câ€™est visuellement ce que lâ€™on devrait chercher pour ce cas dâ€™Ã©tude. kmeans_group &lt;- mcinnes_kmeans$partition[, 4] mcinnes_kmeans$partition %&gt;% head(3) ## 3 groups 4 groups 5 groups 6 groups 7 groups 8 groups 9 groups 10 groups ## 1 2 3 2 1 4 3 6 5 ## 2 2 3 5 3 4 7 6 5 ## 3 2 2 2 1 2 3 4 8 df_mcinnes %&gt;% mutate(kmeans_group = kmeans_group) %&gt;% # ajouter une colonne de regoupement ggplot(aes(x=x, y=y)) + geom_point(aes(colour = factor(kmeans_group))) + coord_fixed() Lâ€™algorithme kmeans est loin dâ€™Ãªtre statisfaisant. Cela est attendu, puisque les kmeans recherchent des distribution gaussiennes sur des groupes vraisemblablement non-gaussiens. Nous pouvons crÃ©er un graphique silouhette pour nos 6 groupes. Notez quâ€™Ã  cause dâ€™un bogue, il nâ€™est pas possible de prÃ©senter les donnÃ©es clairement lorsquâ€™elles sont nombreuses. sil &lt;- silhouette(mcinnes_kmeans$partition[, 6], dist = vegdist(df_mcinnes[, ], method = &quot;euclidean&quot;)) sil &lt;- sortSilhouette(sil) plot(sil, col = &#39;black&#39;) 8.3.2.2 DBSCAN La technique DBSCAN (* Density-Based Spatial Clustering of Applications with Noise) sousentend que les groupes sont composÃ©s de zones oÃ¹ lâ€™on retrouve plus de points (zones denses) sÃ©parÃ©es par des zones de faible densitÃ©. Pour lancer lâ€™algorithme, nous devons spÃ©cifier une mesure dâ€™association critique (distance ou dissimilaritÃ©) d* ainsi quâ€™un nombre de point critique k dans le voisinage de cette distance. Lâ€™algorithme commence par Ã©tiqueter chaque point selon lâ€™une de ces catÃ©gories: Noyau: le point a au moins k points dans son voisinage, câ€™est-Ã -dire Ã  une distance infÃ©rieure ou Ã©gale Ã  d. Bordure: le point a moins de k points dans son voisinage, mais lâ€™un de des points voisins est un noyau. Bruit: le cas Ã©chÃ©ant. Ces points sont considÃ©rÃ©s comme des outliers. Les noyaux distancÃ©s de d ou moins sont connectÃ©s entre eux en englobant les bordures. Le nombre de groupes est prescrit par lâ€™algorithme DBSCAN, qui permet du coup de dÃ©tecter des donnÃ©es trop bruitÃ©es pour Ãªtre classÃ©es. Damiani et al. (2014) a dÃ©veloppÃ© une approche utilisant la technique DBSCAN pour partitionner des zones dâ€™escale pour les flux de populations migratoires. 8.3.2.2.1 Application La technique DBSCAN nâ€™est pas basÃ©e sur le nombre de groupe, mais sur la densitÃ© des points. Lâ€™argument x ne constitue pas les donnÃ©es, mais une matrice dâ€™association. Lâ€™argument minPts spÃ©cifie le nombre minimal de points qui lâ€™on doit retrouver Ã  une distance critique d* pour la formation des *noyaux et la propagation des groupes, spÃ©cifiÃ©e dans lâ€™argument eps. La distance d peut Ãªtre estimÃ©e en prenant une fraction de la moyenne, mais on aura volontiers recours Ã  sont bon jugement. library(&quot;dbscan&quot;) mcinnes_dbscan &lt;- dbscan(x = vegdist(df_mcinnes[, ], method = &quot;euclidean&quot;), eps = 0.03, minPts = 10) dbscan_group &lt;- mcinnes_dbscan$cluster unique(dbscan_group) ## [1] 1 0 2 6 3 4 5 Les paramÃ¨tres spÃ©cifiÃ©s donnent 5 groupes (1, 2, ..., 5) et des points trop bruitÃ©s pour Ãªtre classifiÃ©s (Ã©tiquetÃ©s 0). Voyons comment les groupes ont Ã©tÃ© formÃ©s. df_mcinnes %&gt;% mutate(dbscan_group = dbscan_group) %&gt;% # ajouter une colonne de regoupement ggplot(aes(x=x, y=y)) + geom_point(aes(colour = factor(dbscan_group))) + coord_fixed() Le partitionnement semble plus conforme Ã  ce que lâ€™on recherche. NÃ©anmoins, DBSCAN crÃ© quelques petits groupes indÃ©sirables (groupe 6, en rose) ainsi quâ€™un grand groupe (violet) qui auraient lieu dâ€™Ãªtre partitionnÃ©. Ces dÃ©faut pourraient Ãªtre rÃ©glÃ©s en jouant sur les paramÃ¨tres eps et minPts. 8.3.3 Partitionnement hiÃ©rarchique Les techniques de partitionnement hiÃ©rarchique sont basÃ©es sur les matrices dâ€™association. La technique pour mesurer lâ€™association (entre objets ou variables) dÃ©terminera en grande partie le paritionnement des donnÃ©es. Les partitionnements hiÃ©rarchiques ont lâ€™avantage de pouvoir Ãªtre reprÃ©sentÃ©s sous forme de dendrogramme (ou arbre) de partition. Un tel dendrogramme prÃ©sente des sous-groupes qui se joignent en groupes jusquâ€™Ã  former un seul ensemble. Le partitionnement hiÃ©rarchique est abondamment utilisÃ© en phylogÃ©nie, pour Ã©tudier les relations de parentÃ© entre organismes vivants, populations dâ€™organismes et espÃ¨ces. La phÃ©nÃ©tique, branche empirique de la phylogÃ©nÃ¨se interspÃ©cifique, fait usage du partitionnement hiÃ©rarchique Ã  partir dâ€™associations gÃ©nÃ©tiques entre unitÃ©s taxonomiques. On retrouve de nombreuses ressources acadÃ©miques en phylogÃ©nÃ©tique ainsi que des outils pour R et Python. Toutefois, la phylogÃ©nÃ©tique en particulier ne fait pas partie de la prÃ©sente ittÃ©ration de ce manuel. 8.3.3.1 Techniques de partitionnement hiÃ©rarchique Le partitionnement hiÃ©rarchique est typiquement effectuÃ© avec une des quatres mÃ©thodes suivantes, dont chacune possÃ¨de ses particularitÃ©s, mais sont toutes agglomÃ©ratives: Ã  chaque Ã©tape dâ€™agglomÃ©ration, on fusionne les deux groupes ayant le plus dâ€™affinitÃ© sur la base des deux sous-groupes les plus rapprochÃ©s. Single link (single). Les groupes sont agglomÃ©rÃ©s sur la base des deux points parmi les groupes, qui sont les plus proches. Complete link (complete). Ã€ la diffÃ©rence de la mÃ©thode single, on considÃ¨re comme critÃ¨re dâ€™agglomÃ©ration les Ã©lÃ©ments les plus Ã©loignÃ©s de chaque groupe. AgglomÃ©ration centrale. Il sâ€™agit dâ€™une fammlle de mÃ©thode basÃ©es sur les diffÃ©rences entre les tendances centrales des objets ou des groupes. Average (average). AppelÃ©e UPGMA (Unweighted Pair-Group Method unsing Average), les groupes sont agglomÃ©rÃ©s selon un centre calculÃ©s par la moyenne et le nombre dâ€™objet pondÃ¨re lâ€™agglomÃ©ration (le poids des groupes est retirÃ©). Cette technique est historiquement utilisÃ©e en bioinformatique pour partitionner des groupes phylogÃ©nÃ©tiques (Sneath et Sokal, 1973). Weighted (weighted). La version de average, mais non pondÃ©rÃ©e (WPGMA). Centroid (centroid). Tout comme average, mais le centroÃ¯de (centre gÃ©omÃ©trique) est utilisÃ© au lieu de la moyenne. Accronyme: UPGMC. Median (median). AppelÃ©e WPGMC. Devinez! ;) Ward (ward). Lâ€™optimisation vise Ã  minimiser les sommes des carrÃ©s par regroupement. 8.3.3.2 Quel outil de partitionnement hiÃ©rarchique utiliser? Alors que le choix de la matrice dâ€™association dÃ©pend des donnÃ©es et de leur contexte, la technique de partitionnement hiÃ©rarchique peut, quant Ã  elle, Ãªtre basÃ©e sur un critÃ¨re numÃ©rique. Il en existe plusieurs, mais le critÃ¨re recommandÃ© pour le choix dâ€™une technique de partitionnement hiÃ©rarchique est la corrÃ©lation cophÃ©nÃ©tique. La distance cophÃ©nÃ©tique est la distance Ã  laquelle deux objets ou deux sous-groupes deviennent membres dâ€™un mÃªme groupe. La corrÃ©lation cophÃ©nÃ©tique est la corrÃ©lation de Pearson entre le vecteur dâ€™association des objets et le vecteur de distances cophÃ©nÃ©tiques. 8.3.3.3 Application Les techniques de partitionnement hiÃ©rarchique prÃ©sentÃ©es ci-dessus sont disponibles dans le module stats de R, qui est chargÃ© automatiquement lors de lâ€™ouversture de R. Nous allons classifier les dimensions des iris grÃ¢ce Ã  la distance de Manhattan. mcinnes_hclust_distmat &lt;- vegdist(df_mcinnes, method = &quot;manhattan&quot;) clustering_methods &lt;- c(&#39;single&#39;, &#39;complete&#39;, &#39;average&#39;, &#39;centroid&#39;, &#39;ward&#39;) clust_l &lt;- list() coph_corr_l &lt;- c() for (i in seq_along(clustering_methods)) { clust_l[[i]] &lt;- hclust(mcinnes_hclust_distmat, method = clustering_methods[i]) coph_corr_l[i] &lt;- cor(mcinnes_hclust_distmat, cophenetic(clust_l[[i]])) } ## The &quot;ward&quot; method has been renamed to &quot;ward.D&quot;; note new &quot;ward.D2&quot; tibble(clustering_methods, coph_corr = coph_corr_l) %&gt;% ggplot(aes(x = fct_reorder(clustering_methods, -coph_corr), y = coph_corr)) + geom_col() + labs(x = &quot;MÃ©thode de partitionnement&quot;, y = &quot;CorrÃ©lation cophÃ©nÃ©tique&quot;) La mÃ©thode average retourne la corrÃ©lation la plus Ã©levÃ©e. Pour plus de flexibilitÃ©, enchÃ¢ssons le nom de la mÃ©thode dans une variable. Ainsi, en chageant le nom de cette variable, le reste du code sera consÃ©quent. names(clust_l) &lt;- clustering_methods best_method &lt;- &quot;average&quot; Le partitionnement hiÃ©rarchique peut Ãªtre visualisÃ© par un dendrogramme. plot(clust_l[[best_method]]) 8.3.3.4 Combien de groupes utiliser? La longueur des lignes verticales est la distance sÃ©parant les groupes enfants. Bien que la sÃ©lection du nombre de groupe soit avant tout basÃ©e sur les besoins du problÃ¨me, nous pouvons nous appuyer sur certains outils. La hauteur totale peut servir de critÃ¨re pour dÃ©finir un nombre de groupes adÃ©quat. On pourra sÃ©lectionner le nombre de groupe oÃ¹ la hauteur se stabilise en fonction du nombre de groupe. On pourra aussi utiliser le graphique silhouette, comprenant une collection de largeurs de silouhette, reprÃ©sentant le degrÃ© dâ€™appartenance Ã  son groupe. La fonction sklearn.metrics.silhouette_score, du module scikit-learn, sâ€™en occupe. asw &lt;- c() num_groups &lt;- 3:10 for(i in seq_along(num_groups)) { sil &lt;- silhouette(cutree(clust_l[[best_method]], k = num_groups[i]), mcinnes_hclust_distmat) asw[i] &lt;- summary(sil)$avg.width } plot(num_groups, asw, type = &quot;b&quot;) Le nombre optimal de groupes serait de 5. Coupons le dendrorgamme Ã  la hauteur correspondant Ã  5 groupes avec la fonction cutree. k_opt &lt;- num_groups[which.max(asw)] hclust_group &lt;- cutree(clust_l[[best_method]], k = k_opt) plot(clust_l[[best_method]]) rect.hclust(clust_l[[best_method]], k = k_opt) La classification hiÃ©rarchique, uniquement basÃ©e sur la distance, peut Ãªtre inappropriÃ©e pour dÃ©finir des formes complexes. df_mcinnes %&gt;% mutate(hclust_group = hclust_group) %&gt;% # ajouter une colonne de regoupement ggplot(aes(x=x, y=y)) + geom_point(aes(colour = factor(hclust_group))) + coord_fixed() 8.3.4 Partitionnement hiÃ©rarchique basÃ©e sur la densitÃ© des points La tecchinque HDBSCAN, dont lâ€™algorithme est relativement rÃ©cent (Campello et al., 2013), permet une partitionnement hiÃ©rarchique sur le mÃªme principe des zones de densitÃ© de la technique DBSCAN. Le HDBSCAN a Ã©tÃ© utilisÃ©e pour partitionner les lieux dâ€™escale dâ€™oiseaux migrateurs en Chine (Xu et al., 2013). Avec DBSCAN, un rayon est fixÃ© dans une mÃ©trique appropriÃ©e. Pour chaque point, on compte le nombre de point voisins, câ€™est Ã  dire le nombre de point se situant Ã  une distance (ou une dissimilaritÃ©) Ã©gale ou infÃ©rieure au rayon fixÃ©. Avec HDBSCAN, on spÃ©cifie le nombre de points devant Ãªtre recouverts et on calcule le rayon nÃ©cessaire pour les recouvrir. Ainsi, chaque point est associÃ© Ã  un rayon critique que lâ€™on nommera \\(d_{noyau}\\). La mÃ©trique initiale est ensuite altÃ©rÃ©e: on remplace les associations entre deux objets A et B par la valeur maximale entre cette association, le rayon critique de A et le rayon critique de B. Cette nouvelle distance est appelÃ©e la distance dâ€™atteinte mutuelle: elle accentue les distances pour les points se trouvant dans des zones peu denses. On applique par la suite un algorithme semblable Ã  la partition hiÃ©rarchique single link: En sâ€™Ã©largissant, les rayons se superposent, chaque superposition de rayon forment graduellement des groupes qui sâ€™agglomÃ¨rent ainsi de maniÃ¨re hiÃ©rarchique. Au lieu dâ€™effectuer une tranche Ã  une hauteur donnÃ©e dans un dendrogramme de partitionnement, la technique HDBSCAN se base sur un dendrogramme condensÃ© qui discarte les sous-groupes comprenant moins de n objets (\\(n_{gr min}\\)). Dans nouveau dendrogramme, on recherche des groupes qui occupent bien lâ€™espace dâ€™analyse. Pour ce faitre, on utilise lâ€™inverse de la distance pour crÃ©er un indicateur de persistance (semblable Ã  la similaritÃ©), \\(\\lambda\\). Pour chaque groupe hiÃ©rarchique dans le dendrogramme condensÃ©, on peut calculer la persistance oÃ¹ le groupe prend naissance. De plus, pour chaque objet dâ€™un groupe, on peut aussi calculer une distance Ã  laquelle il quitte le groupe. La stabilitÃ© dâ€™un groupe est la domme des diffÃ©rences de persistance entre la persistance Ã  la naissance et les persistances des objets. On descend dans le dendrogramme. Si la somme des stabilitÃ© des groupes enfants est plus grande que la stabilitÃ© du groupe parent, on accepte la division. Sinon, le parent forme le groupe. La documentation du module hdbscan pour Python offre une description intuitive et plus exhaustive des principes et algorithme de HDBSCAN. 8.3.4.1 ParamÃ¨tres Outre la mÃ©trique dâ€™association dont nous avons discutÃ©, HDBSCAN demande dâ€™Ãªtre nourri avec quelques paramÃ¨tres importants. En particulier, le nombre minimum dâ€™objets par groupe, \\(n_{gr min}\\) dÃ©pend de la quantitÃ© de donnÃ©es que vous avez Ã  votre disposition, ainsi que de la quantitÃ© dâ€™objets que vous jugez suffisante pour crÃ©er des groupes. Nous utiliserons lâ€™implÃ©mentation de HDBSCAN du module dbscan. Si vous dÃ©sirez davantage dâ€™options, vous prÃ©fÃ©rerez probablement lâ€™implÃ©mentation du module largeVis. mcinnes_hdbscan &lt;- hdbscan(x = vegdist(df_mcinnes, method = &quot;euclidean&quot;), minPts = 20, gen_hdbscan_tree = TRUE, gen_simplified_tree = FALSE) hdbscan_group &lt;- mcinnes_hdbscan$cluster unique(hdbscan_group) ## [1] 6 0 4 3 5 1 2 Nous avons 6 groupes, numÃ©rotÃ©s de 1 Ã  6, ainsi que des Ã©tiquettes identifiant des objets dÃ©signÃ©s comme Ã©tant du bruit de fond, numÃ©rotÃ© 0. Le dendrogramme non condensÃ© peu Ãªtre produit. plot(mcinnes_hdbscan$hdbscan_tree) Difficile dâ€™y voir clair avec autant dâ€™objets. Lâ€™objet mcinnes_hdbscan a un nombre minimum dâ€™objets par groupe de 20. Ce qui permet de prÃ©senter le dendrogramme de maniÃ¨re condensÃ©e. plot(mcinnes_hdbscan) Enfin, un aperÃ§u des stratÃ©gies de partitionnement utilisÃ©s jusquâ€™ici. clustering_group &lt;- df_mcinnes %&gt;% mutate(kmeans_group, hclust_group, dbscan_group, hdbscan_group) %&gt;% gather(-x, -y, key = &quot;method&quot;, value = &quot;cluster&quot;) ## Warning: attributes are not identical across measure variables; ## they will be dropped clustering_group$cluster &lt;- factor(clustering_group$cluster) clustering_group %&gt;% ggplot(aes(x = x, y = y)) + geom_point(aes(colour = cluster)) + facet_wrap(~method, ncol = 2) + coord_equal() + theme_bw() Clairement, le partitionnement avec HDBSCAN donne les meilleurs rÃ©sultats. 8.3.5 Conclusion sur le partitionnement Au chapitre 4, nous avons vu avec le jeu de donnÃ©es â€œdatasaurusâ€ que la visualisation peut permettre de dÃ©tecter des structures en segmentant les donnÃ©es selon des groupes. Or, si les donnÃ©es nâ€™Ã©taient pas Ã©tiquetÃ©es, leur structure serait indÃ©tectable avec les algorithmes disponibles actuellement. Le partitionnement permet dâ€™explorer des donnÃ©es, de dÃ©tecter des tendances et de dÃ©gager des groupes permettant la prise de dÃ©cision. Plusieurs techniques de partitionnement ont Ã©tÃ© prÃ©sentÃ©es. Le choix de la technique sera dÃ©terminante sur la maniÃ¨re dont les groupes seront partitionnÃ©s. La dÃ©finition dâ€™un groupe variant dâ€™un cas Ã  lâ€™autre, il nâ€™existe pas de rÃ¨gle pour prescrire une mÃ©thode ou une autre. La partitionnement hiÃ©rarchique a lâ€™avantage de permetre de visualiser comment les groupes sâ€™agglomÃ¨rent. Parmi les mÃ©thodes de partitionnement hiÃ©rarchique disponibles, les mÃ©thodes basÃ©es sur la densitÃ© permettent une grande flexibilitÃ©, ainsi quâ€™une dÃ©tection dâ€™observations ne faisant partie dâ€™aucun goupe. 8.4 Ordination En Ã©cologie, biologie, agronommie comme en foresterie, la plupart des tableaux de donnÃ©es comprennent de nombreuses variables: pH, nutriments, climat, espÃ¨ces ou cultivars, etc. Lâ€™ordination vise Ã  mettre de lâ€™ordre dans des donnÃ©es dont le nombre Ã©levÃ© de variables peut amener Ã  des difficultÃ©s dâ€™apprÃ©ciation et dâ€™interprÃ©taion (Legendre et Legendre, 2012). Plus prÃ©cisÃ©ment, le terme ordination est utilisÃ© en Ã©cologie pour dÃ©signer les techniques de rÃ©duction dâ€™axe. Lâ€™analyse en composante principale est probablement la plus connue de ces techniques. Mais de nombreuses techniques dâ€™ordination ont Ã©tÃ© dÃ©veloppÃ©es au cours des derniÃ¨res annÃ©es, chacune ayant ses domaines dâ€™application. Les techniques de rÃ©duction dâ€™axe permettent de dÃ©gager lâ€™information la plus importante en projetant une synthÃ¨se des relations entre les observations et entre les variables. Les techniques ne supposant aucune structure a priori sont dites non-contraignantes: elles ne comprennent pas de tests statistiques. Ã€ lâ€™inverse, les ordinations contraignantes lient des variables descriptives avec une ou plusieurs variables prÃ©dictives. La rÃ©fÃ©rence en la matiÃ¨re est indiscutablement (Legendre et Legendre, 2012). Cette section en couvrira quelques unes et vous guidera vers la technique la plus appropriÃ©e pour vos donnÃ©es. 8.4.1 Ordination non contraignante Cette section couvrira lâ€™analyse en composantes principales (ACP), lâ€™analyse de correspondance (AC), lâ€™analyse factorielle (AF) ainsi que lâ€™analyse en coordonnÃ©es principales (ACoP). MÃ©thode Distance prÃ©servÃ©e Variables Analyse en composantes principales (ACP) Distance euclidienne DonnÃ©es quantitatives, relations linÃ©aires (attention aux double-zÃ©ros) Analyse de correspondance (AC) Distance de \\(\\chi^2\\) DonnÃ©es non-nÃ©gatives, dimentionnellement homogÃ¨nes ou binaires, abondance ou occurence Positionnement multidimensionnel (PoMd) Toute mesure de dissimilaritÃ© DonnÃ©es quantitatives, qualitatives nominales/ordinales ou mixtes Source: AdaptÃ© de (Legendre et Legendre, 2012, chapitre 9) 8.4.1.1 Analyse en composantes principales Lâ€™objectif dâ€™une ACP est de reprÃ©senter les donnÃ©es dans un nombre rÃ©duit de dimensions reprÃ©sentant le plus possible la variation dâ€™un tableau de donnÃ©es: elle permet de projetter les donnÃ©es dans un espace oÃ¹ les variables sont combinÃ©es en axes orthogonaux dont le premier axe capte le maximum de variance. Lâ€™ACP peut par exemple Ãªtre utilisÃ©e pour analyser des corrÃ©lations entre variables ou dÃ©gager lâ€™information la plus pertinente dâ€™un tableau de donnÃ©es mÃ©tÃ©o ou de signal en un nombre plus retreint de variables. Lâ€™ACP effectue une rotation des axes Ã  partir du centre (moyenne) du nuage de points effectuÃ©e de maniÃ¨re Ã  ce que le premier axe dÃ©finisse la direction oÃ¹ lâ€™on retrouve la variance maximale. Ce premier axe est une combinaison linÃ©aire des variables et forme la premiÃ¨re composante principale. Une fois cet axe dÃ©finit, on trouve de deuxiÃ¨me axe, orthogonal au premier, oÃ¹ lâ€™on retouve la variance maximale - cet axe forme la deuxiÃ¨me composante principale, et ainsi de suite jusquâ€™Ã  ce que le nombre dâ€™axe corresponde au nombre de variables. Les projections des observations sur ces axes principaux sont appelÃ©s les scores. Les projections des variables sur les axes principaux sont les vecteurs propres (eigenvectors, ou loadings). La variance des composantes principales diminue de la premiÃ¨re Ã  la derniÃ¨re, et peut Ãªtre calculÃ©e comme une proportion de la variance totale: câ€™est le pourcentage dâ€™inertie. Par convention, on utilise les valeurs propres (eigenvalues) pour mesurer lâ€™importance des axes. Si la premiÃ¨re composante principale a une inertie de 50% et la deuxiÃ¨me a une intertie de 30%, la reprÃ©sentation en 2D des projection reprÃ©sentera 80% de la variance du nuage de points. Lâ€™hÃ©tÃ©rogÃ©nÃ©itÃ© des Ã©chelles de mesure peut avoir une grande importance sur les rÃ©sultats dâ€™une ACP (les donnÃ©es doivent Ãªtre dimensionnellement homogÃ¨nes). En effet, la hauteur dâ€™un ceriser aura une variance plus grande que le diamÃ¨tre dâ€™une cerise exprimÃ© dans les mÃªmes unitÃ©s, et cette derniÃ¨re aura plus de variance que la teneur en cuivre dâ€™une feuille. Il est consÃ©quemment avisÃ© de mettre les donnÃ©es Ã  lâ€™Ã©chelle en centrant la moyenne Ã  zÃ©ro et lâ€™Ã©cart-type Ã  1 avant de procÃ©der Ã  une ACP. Lâ€™ACP a Ã©tÃ© conÃ§ue pour projetter en un nombre moindre de dimensions des observations dont les distributions sont multinormales. Bien que lâ€™ACP soit une technique robuste, il est prÃ©fÃ©rable de transformer prÃ©alablement les variables dont la distribution est particuliÃ¨rement asymÃ©triques (Legendre et Legendre, 2012, p.Â 450). Le cas Ã©chÃ©ant, les valeurs extrÃªmes pourraient faire dÃ©vier les vecteurs propres et biaiser lâ€™analyse. En particulier, les donnÃ©es ACP menÃ©es sur des donnÃ©es compositionnelles sont rÃ©putÃ©es pour gÃ©nÃ©rer des analyses biaisÃ©es (Pawlowsky-Glahn and Egozcue, 2006). Le test de Mardia (Korkmaz, 2014) peut Ãªtre utilisÃ© pour tester la multinormalitÃ©. Une distribution multinormale devrait gÃ©nÃ©rer des scores en forme dâ€™hypersphÃ¨re (en forme de cercle sur un biplot: voir plus loin). 8.4.1.1.1 Vecteurs propres et valeurs propres Une matrice carrÃ©e (comme une matrice de covariance \\(\\Sigma\\)) multipliÃ©e par un vecteur propre \\(e\\) est Ã©gale aux valeurs propres \\(\\lambda\\) multipliÃ©es par les vecteurs propres \\(e\\). \\[ \\Sigma e = \\lambda e \\] De maniÃ¨re intuitive, les vecteurs propres indiquent lâ€™orientation de la covariance, et les valeurs propres indique la longueur associÃ©e Ã  cette direction. Lâ€™ACP est basÃ©e sur le calcul des vecteurs propres et des valeurs propres de la matrice de covariance des variables. Pour dâ€™abord obtenir les valeurs propres \\(\\lambda\\), il faut rÃ©soudre lâ€™Ã©quation \\[ det(cov(X) - \\lambda I) = 0 \\], oÃ¹ \\(det\\) est lâ€™opÃ©ration permettant de calculer le dÃ©terminant, \\(cov\\) est lâ€™opÃ©ration pour calculer la covariance, \\(X\\) est la matrice de donnÃ©es, \\(\\lambda\\) sont les valeurs propres et \\(I\\) est une matrice dâ€™identitÃ©. Pour \\(p\\) variables dans votre tableau \\(X\\), vous obtiendrex \\(p\\) valeurs propres. Ensuite, on trouve les vecteurs propres en rÃ©solvant lâ€™Ã©quation $ e = e $. Bien quâ€™il soit possible dâ€™effectuer cette opÃ©ration Ã  la main pour des cas trÃ¨s simples, vous aurez avantage Ã  utiliser un langage de programmation. Chargeons les donnÃ©es dâ€™iris, puis isolons seulement les deux dimensions des sÃ©pales lâ€™espÃ¨ce setosa. data(&quot;iris&quot;) setosa_sepal &lt;- iris %&gt;% filter(Species == &quot;setosa&quot;) %&gt;% select(starts_with(&quot;Sepal&quot;)) setosa_sepal ## Sepal.Length Sepal.Width ## 1 5.1 3.5 ## 2 4.9 3.0 ## 3 4.7 3.2 ## 4 4.6 3.1 ## 5 5.0 3.6 ## 6 5.4 3.9 ## 7 4.6 3.4 ## 8 5.0 3.4 ## 9 4.4 2.9 ## 10 4.9 3.1 ## 11 5.4 3.7 ## 12 4.8 3.4 ## 13 4.8 3.0 ## 14 4.3 3.0 ## 15 5.8 4.0 ## 16 5.7 4.4 ## 17 5.4 3.9 ## 18 5.1 3.5 ## 19 5.7 3.8 ## 20 5.1 3.8 ## 21 5.4 3.4 ## 22 5.1 3.7 ## 23 4.6 3.6 ## 24 5.1 3.3 ## 25 4.8 3.4 ## 26 5.0 3.0 ## 27 5.0 3.4 ## 28 5.2 3.5 ## 29 5.2 3.4 ## 30 4.7 3.2 ## 31 4.8 3.1 ## 32 5.4 3.4 ## 33 5.2 4.1 ## 34 5.5 4.2 ## 35 4.9 3.1 ## 36 5.0 3.2 ## 37 5.5 3.5 ## 38 4.9 3.6 ## 39 4.4 3.0 ## 40 5.1 3.4 ## 41 5.0 3.5 ## 42 4.5 2.3 ## 43 4.4 3.2 ## 44 5.0 3.5 ## 45 5.1 3.8 ## 46 4.8 3.0 ## 47 5.1 3.8 ## 48 4.6 3.2 ## 49 5.3 3.7 ## 50 5.0 3.3 library(&quot;MVN&quot;) ## sROC 0.1-2 loaded setosa_sepal_mvn &lt;- mvn(setosa_sepal, mvnTest = &quot;mardia&quot;) setosa_sepal_mvn$multivariateNormality ## Test Statistic p value Result ## 1 Mardia Skewness 0.759503524380438 0.943793240544741 YES ## 2 Mardia Kurtosis 0.0934600553610254 0.925538081956867 YES ## 3 MVN &lt;NA&gt; &lt;NA&gt; YES Pour considÃ©rer la distribution comme multinormale, la p-value de la distortion (Mardia Skewness) et la statistique de Kurtosis (Mardia Kurtosis) doit Ãªtre Ã©gale ou plus Ã©levÃ©e que 0.05 (Kormaz, 2019, fiche dâ€™aide de la fonction mvn de R). Câ€™est bien le cas pour les donnÃ©es du tableau setosa_sepal. Retirons de la matrice de covariance les valeurs et vecteurs propres avec la fonction eigen. setosa_eigen &lt;- eigen(cov(setosa_sepal)) setosa_eigenval &lt;- setosa_eigen$values setosa_eigenvec &lt;- setosa_eigen$vectors Le premier vecteur propre correspond Ã  la premiÃ¨re colonne, et le second Ã  la deuxiÃ¨me. Les coordonnÃ©es x et y sont les premiÃ¨res et deuxiÃ¨mes lignes. Les vecteurs propres ont une longueur unitaire (norme de 1). Ils peuvent Ãªtre mis Ã  lâ€™Ã©chelles Ã  la racine carrÃ©e des valeurs propres. setosa_eigenvec_sc &lt;- setosa_eigenvec %*% diag(sqrt(setosa_eigen$values)) Pour effectuer une translation des vecteurs propres au centre du nuage de point, nous avons besoin du centroÃ¯de. centroid &lt;- setosa_sepal %&gt;% apply(., 2, mean) plot(setosa_sepal, asp = 1) # vecteurs propres brutes lines(x=c(centroid[1], centroid[1] + setosa_eigenvec[1, 1]), y=c(centroid[2], centroid[2] + setosa_eigenvec[2, 1]), col = &quot;green&quot;, lwd = 3) # vecteur propre 1 lines(x=c(centroid[1], centroid[1] + setosa_eigenvec[1, 2]), y=c(centroid[2], centroid[2] + setosa_eigenvec[2, 2]), col = &quot;green&quot;, lwd = 3) # vecteur propre 1 # vecteurs propres Ã  l&#39;Ã©chelle lines(x=c(centroid[1], centroid[1] + setosa_eigenvec_sc[1, 1]), y=c(centroid[2], centroid[2] + setosa_eigenvec_sc[2, 1]), col = &quot;red&quot;, lwd = 4) # vecteur propre 1 lines(x=c(centroid[1], centroid[1] + setosa_eigenvec_sc[1, 2]), y=c(centroid[2], centroid[2] + setosa_eigenvec_sc[2, 2]), col = &quot;red&quot;, lwd = 4) # vecteur propre 1 points(x=centroid[1], y=centroid[2], pch = 16, cex = 2, col =&quot;blue&quot;) # centroid On peut observer que, comme je lâ€™ai mentionnÃ© plus haut, les vecteurs propres indiquent lâ€™orientation de la covariance, et les valeurs propres indique la longueur associÃ©e Ã  cette direction. 8.4.1.1.2 Biplot Imaginez un nuage de points en 3D, axes y compris. Vous tournez votre nuage de points pour trouver la perspective en 2D qui fera en sorte que vos donnÃ©es soient les plus dispersÃ©es possibles. Avec une lampe de poche, vous illuminez votre nuage de points dans lâ€™axe de cette perspective: vous venez dâ€™effectuer une analyse en composantes principales, et lâ€™ombre des points et des axes sur le mur formera votre biplot. Pour crÃ©er un biplot, on juxtapose les descripteurs (variables) en tant que vecteurs propres, reprÃ©sentÃ©s par des flÃ¨ches, et les objets (observations) en tant que scores, reprÃ©sentÃ©s par des points. Les rÃ©sultats dâ€™une ordination peuvent Ãªtre prÃ©sentÃ©s selon deux types de biplots (Legendre et Legendre, 2012). Biplot de corrÃ©lation permettant de visualiser les corrÃ©lations entre des variables mÃ©tÃ©orologiques. Source: Parent, 2017 Deux types de projection sont courramment utilisÃ©s. Biplot de distance. Ce type de projection permet de visualiser la position des objets entre eux et par rapport aux descripteurs et dâ€™apprÃ©cier la contribution des descripteurs pour crÃ©er les composantes principales. Pour crÃ©er un biplot de distance, on projette directement les vecteurs propres (\\(U\\)) en guise de descripteurs. Pour ce qui est des objets, on utilise les scores de lâ€™ACP (\\(F\\)). De cette maniÃ¨re, les distances euclidiennes entre les scores sont des approximations des distances euclidiennes dans lâ€™espace multidimentionnel, la projection dâ€™un objet sur un descripteur perpendiculairement Ã  ce dernier est une approximation de la position de lâ€™objet sur le descripteur et la projection dâ€™un descripteur sur un axe principal est proportionnelle Ã  sa contribution pour gÃ©nÃ©rer lâ€™axe. Biplot de corrÃ©lation. Cette projection permet dâ€™apprÃ©cier les corrÃ©lations entre les descripteurs. Pour ce faire, les objets et les valeurs propres doivent Ãªtre transformÃ©s. Pour gÃ©nÃ©rer les descripteurs, les vecteurs propres (\\(U\\)) doivent Ãªtre multipliÃ©s par la matrice diagonalisÃ©e de la racine carrÃ©e des valeurs propres (\\(\\Lambda\\)), câ€™est-Ã -dire \\(U \\Lambda ^{\\frac{1}{2}}\\). En ce qui a trait aux objets, on multiplie les scores par (\\(F\\)) par la racine carrÃ©e nÃ©gative des valeurs propres diagonalisÃ©es, câ€™est-Ã -dire \\(F \\Lambda ^{- \\frac{1}{2}}\\). De cette maniÃ¨re, tout comme câ€™est le cas pour le biplot de distance, la projection dâ€™un objet sur un descripteur perpendiculairement Ã  ce dernier est une approximation de la position de lâ€™objet sur le descripteur, la projection dâ€™un descripteur sur un axe principal est proportionnelle Ã  son Ã©cart-type et les angles entre les descripteurs sont proportionnelles Ã  leur corrÃ©lation (et non pas leur proximitÃ©). En dâ€™autres mots, le bilot de distances devrait Ãªtre utilisÃ© pour apprÃ©cier la distance entre les objets et le biplot de corrÃ©lation devrait Ãªtre utilisÃ© pour apprÃ©cier les corrÃ©lations entre les descripteurs. Mais dans tous les cas, le type de biplot utilisÃ© doit Ãªtre indiquÃ©. Le triplot est une forme apparentÃ©e au biplot, auquel on ajoute des variables prÃ©dictives. Le triplot est utile pour reprÃ©senter les rÃ©sultats des ordinations contraignantes comme les analyses de redondance et les analyse de correspondance canoniques. 8.4.1.1.3 Application Bien que lâ€™ACP puisse Ãªtre effectuÃ©e grÃ¢ce Ã  des modules de base de R, nous utiliserons le module vegan. Le tableau varechem comprend des donnÃ©es issues dâ€™analyse de sols identifiÃ©s par leur composition chimique, leur pH, leur profondeur totale et la profondeur de lâ€™humus publiÃ©es dans VÃ¤re et al. (1995) et exportÃ©es du module vegan. library(&quot;vegan&quot;) data(&quot;varechem&quot;) varechem %&gt;% sample_n(5) ## N P K Ca Mg S Al Fe Mn Zn Mo Baresoil ## 1 22.8 40.9 171.4 691.8 151.4 40.8 104.8 17.6 43.6 9.1 0.4 40.5 ## 2 26.6 36.7 171.4 738.6 94.9 33.8 20.7 2.5 77.6 7.4 0.3 23.0 ## 3 31.1 32.3 73.7 219.0 52.5 25.5 304.6 204.4 14.2 2.6 0.5 3.6 ## 4 21.8 38.1 146.8 512.2 75.0 36.6 92.3 4.6 29.0 8.1 0.5 33.3 ## 5 33.1 22.7 43.6 240.3 25.7 14.9 39.0 8.4 26.8 8.4 0.2 8.1 ## Humdepth pH ## 1 3.8 2.7 ## 2 2.8 2.8 ## 3 1.8 3.3 ## 4 2.7 2.7 ## 5 1.0 3.1 Comme nous lâ€™avons vu prÃ©cdemment, les donnÃ©es de concentration sont de type compositionnelles. Les donnÃ©es compositionnelles du tableau varechem mÃ©riteraient dâ€™Ãªtre transformÃ©es (Aitchison et Greenacre, 2002). Utilisons les log-ratios centrÃ©s (clr). library(&quot;compositions&quot;) ## Loading required package: tensorA ## ## Attaching package: &#39;tensorA&#39; ## The following object is masked from &#39;package:base&#39;: ## ## norm ## Loading required package: robustbase ## Loading required package: energy ## Loading required package: bayesm ## Welcome to compositions, a package for compositional data analysis. ## Find an intro with &quot;? compositions&quot; ## ## Attaching package: &#39;compositions&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## cor, cov, dist, var ## The following objects are masked from &#39;package:base&#39;: ## ## %*%, scale, scale.default varecomp &lt;- varechem %&gt;% select(-Baresoil, -Humdepth, -pH) %&gt;% mutate(Fv = apply(., 1, function(x) 1e6 - sum(x))) vareclr &lt;- varecomp %&gt;% acomp(.) %&gt;% clr(.) %&gt;% as_tibble() %&gt;% bind_cols(varechem %&gt;% select(Baresoil, Humdepth, pH)) ## Warning: Calling `as_tibble()` on a vector is discouraged, because the behavior is likely to change in the future. Use `enframe(name = NULL)` instead. ## This warning is displayed once per session. vareclr %&gt;% sample_n(5) ## # A tibble: 5 x 15 ## N P K Ca Mg S Al Fe Mn Zn ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -1.48 -0.728 0.525 1.49 -0.423 -0.842 1.17 0.712 -1.25 -2.61 ## 2 -0.871 -0.623 0.872 1.92 -0.244 -0.757 0.250 -1.77 -0.862 -2.41 ## 3 -0.901 0.00174 1.27 2.32 0.361 -0.546 -1.41 -3.42 0.374 -2.07 ## 4 -0.773 -0.988 0.175 1.05 -0.174 -0.960 1.50 0.628 -1.88 -3.09 ## 5 -1.78 -0.613 0.354 1.72 -0.174 -0.645 0.932 -1.02 -0.935 -2.31 ## # â€¦ with 5 more variables: Mo &lt;dbl&gt;, Fv &lt;dbl&gt;, Baresoil &lt;dbl&gt;, ## # Humdepth &lt;dbl&gt;, pH &lt;dbl&gt; Effectuons lâ€™ACP. Pour cet exemple, nous standardiserons les donnÃ©es Ã©tant donnÃ©es que les colonnes Baresoil, Humedepth et pH ne sont pas Ã  la mÃªme Ã©chelle que les colonnes des clr. vareclr_sc &lt;- scale(vareclr) vare_pca &lt;- rda(vareclr_sc) # ou bien rda(vareclr, scale = TRUE, mais la mise Ã  l&#39;Ã©chelle prÃ©alable est plus explicite) Lâ€™objet vareclr_pca contient lâ€™information nÃ©cessaire pour mener notre ACP. summary(vare_pca, scaling = 2) # scaling = 2 pour obtenir les infos pour les biplots de corrÃ©lation ## ## Call: ## rda(X = vareclr_sc) ## ## Partitioning of variance: ## Inertia Proportion ## Total 15 1 ## Unconstrained 15 1 ## ## Eigenvalues, and their contribution to the variance ## ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Eigenvalue 7.1523 2.4763 2.1122 0.93015 0.57977 0.48786 0.36646 ## Proportion Explained 0.4768 0.1651 0.1408 0.06201 0.03865 0.03252 0.02443 ## Cumulative Proportion 0.4768 0.6419 0.7827 0.84473 0.88338 0.91590 0.94034 ## PC8 PC9 PC10 PC11 PC12 PC13 ## Eigenvalue 0.29432 0.19686 0.15434 0.107357 0.095635 0.042245 ## Proportion Explained 0.01962 0.01312 0.01029 0.007157 0.006376 0.002816 ## Cumulative Proportion 0.95996 0.97308 0.98337 0.990527 0.996902 0.999719 ## PC14 ## Eigenvalue 0.0042200 ## Proportion Explained 0.0002813 ## Cumulative Proportion 1.0000000 ## ## Scaling 2 for species and site scores ## * Species are scaled proportional to eigenvalues ## * Sites are unscaled: weighted dispersion equal on all dimensions ## * General scaling constant of scores: 4.309777 ## ## ## Species scores ## ## PC1 PC2 PC3 PC4 PC5 PC6 ## N 0.1437 0.7606 -0.6792 0.19837 0.1128526 -0.050149 ## P 0.8670 -0.3214 -0.2950 -0.22940 0.1437960 -0.042884 ## K 0.9122 -0.3857 0.2357 0.03469 0.2737020 0.075717 ## Ca 0.9649 -0.3362 -0.2147 0.17757 -0.2188717 0.008051 ## Mg 0.8263 -0.2723 0.1035 0.52135 -0.1495399 -0.342214 ## S 0.8825 -0.3169 0.3539 -0.21216 0.1176279 -0.191386 ## Al -1.0105 -0.2442 0.2146 0.02674 -0.1005560 -0.043569 ## Fe -1.0338 -0.2464 0.1492 0.13162 0.1512218 0.081571 ## Mn 0.9556 0.1041 -0.1256 -0.21300 0.2565831 0.275275 ## Zn 0.7763 -0.1031 -0.3123 -0.36493 -0.5665691 0.153089 ## Mo -0.2152 0.8717 0.4065 -0.33643 -0.2134335 -0.167725 ## Fv 0.2360 0.5776 -0.8112 0.12736 0.1280097 -0.109737 ## Baresoil 0.5147 0.4210 0.4472 0.54980 -0.1438570 0.463148 ## Humdepth 0.7455 0.4379 0.5194 0.16493 0.0004757 -0.273056 ## pH -0.5754 -0.5864 -0.5957 0.23408 -0.1517661 -0.056641 ## ## ## Site scores (weighted sums of species scores) ## ## PC1 PC2 PC3 PC4 PC5 PC6 ## sit1 0.16862 0.423777 0.46731 0.91175 1.10380 1.06421 ## sit2 -0.09705 -0.097482 0.61143 -0.29049 1.14916 0.40622 ## sit3 0.02831 -0.795737 0.74176 -0.19097 -2.43337 -0.81762 ## sit4 1.39081 -0.354376 -0.19377 -0.45160 0.46020 -0.31446 ## sit5 1.30346 0.357866 0.29887 0.76856 0.20913 -0.64145 ## sit6 0.43636 0.495037 1.21722 1.18128 -0.98242 -0.74474 ## sit7 1.07306 0.467575 -0.32245 0.03717 0.13956 -0.64972 ## sit8 0.02545 0.659714 -0.28861 -0.01424 0.47105 0.45173 ## sit9 1.42005 0.007356 -0.29000 -0.78474 0.97592 -0.80263 ## sit10 -0.50638 -0.220909 1.52981 0.26289 0.42135 0.94054 ## sit11 0.45392 0.649297 0.44573 -0.26620 -0.74522 -0.53228 ## sit12 0.18623 0.259640 0.89112 0.21096 -0.51393 2.24361 ## sit13 1.26264 0.225744 -0.96668 -0.69334 0.61990 0.43312 ## sit14 -1.48685 0.739545 -0.20926 1.09256 0.61856 -0.87999 ## sit15 -0.50622 1.108685 -2.61287 -1.00433 -1.35383 1.21964 ## sit16 -1.28653 0.898663 -0.38778 -0.47556 -0.02449 -0.29419 ## sit17 -1.72773 0.476962 -0.48878 0.71156 1.06398 -1.33473 ## sit18 -0.82844 -0.296515 1.20315 -1.49821 -0.18330 1.05231 ## sit19 -1.00247 -0.609253 0.25185 -0.85420 0.71031 0.14854 ## sit20 -0.43405 -0.338912 0.55348 -1.35776 -0.81986 -1.02468 ## sit21 -0.05083 0.122645 -0.04611 -0.56047 -0.26151 -0.98053 ## sit22 0.17891 -2.315489 -0.69084 -0.19547 0.80628 0.04291 ## sit23 -0.46443 -2.592018 -1.21615 1.56359 -0.62334 0.28748 ## sit24 0.46316 0.728185 -0.49843 1.89726 -0.80791 0.72671 La deuxiÃ¨me ligne de Importance of components, Proportion Explained, indique la proportion de la variance totale captÃ©e successivement par les axes principaux. Le premier axe principal comporte 47.68% de la variance. Le deuxiÃ¨me axe principal ajoutant une proportion de 16,51%, une reprÃ©sentation en deux axes principaux prÃ©sentent 64.19 % de la variance. prop_expl &lt;- vare_pca$CA$eig / sum(vare_pca$CA$eig) prop_expl ## PC1 PC2 PC3 PC4 PC5 ## 0.4768180610 0.1650859388 0.1408156459 0.0620101490 0.0386511040 ## PC6 PC7 PC8 PC9 PC10 ## 0.0325238535 0.0244303815 0.0196215021 0.0131238464 0.0102890284 ## PC11 PC12 PC13 PC14 ## 0.0071571089 0.0063756951 0.0028163495 0.0002813359 La dÃ©cision du nombre dâ€™axes principaux Ã  retenir est arbitraire. Elle peut dÃ©pendre dâ€™un nombre maximal de paramÃ¨tre Ã  retenir pour Ã©viter de surdimensionner un modÃ¨le (curse of dimensionality, section 11) ou dâ€™un seuil de pourcentage de variance minimal Ã  retenir, par exemple 75%. Ou bien, vous retiendrez deux composantes principales si vous dÃ©sirez prÃ©senter un seul biplot. Lâ€™approche de Kaiser-Guttmann (Borcard et al., 2011) consiste Ã  sÃ©lectionner les composantes principales dont la valeur propre est supÃ©rieure Ã  leur moyenne. plot(x = 1:length(vare_pca$CA$eig), y = vare_pca$CA$eig, type = &quot;b&quot;, xlab = &quot;Rang de la valeur propre&quot;, ylab = &quot;Valeur propre&quot;) abline(h = mean(vare_pca$CA$eig), col = &quot;red&quot;, lty = 2) Lâ€™approche du broken stick consiste Ã  couper un bÃ¢ton dâ€™une longueur de 1 en n tranches. La premiÃ¨re tranche est de longueur \\(\\frac{1}{n}\\). La tranche suivante est dâ€™une longueur de la tranche prÃ©cÃ©dente Ã  laquelle on aditionne \\(\\frac{1}{longueur~restante}\\). Puis on place les longueurs en ordre dÃ©croissant. On retient les composantes principales dont les valeurs propres cumulÃ©es sont plus grandes que le broken stick. broken_stick &lt;- function(x) { bsm &lt;- vector(&quot;numeric&quot;, length = x) bsm[1] &lt;- 1/x for (i in 2:x) { bsm[i] &lt;- bsm[i-1] + 1/(x+1-i) } bsm &lt;- rev(bsm/x) return(bsm) } Le graphique du broken stick: plot(x = 1:length(vare_pca$CA$eig), y = prop_expl, type = &quot;b&quot;, xlab = &quot;Rang de la valeur propre&quot;, ylab = &quot;Valeur propre&quot;) lines(x = 1:length(vare_pca$CA$eig), y = broken_stick(length(vare_pca$CA$eig)), col = &quot;red&quot;, lty = 2) Les approches Kaiser-Guttmann et broken stick suggÃ¨rent que les trois premiÃ¨res composantes sont suffisantes pour dÃ©crire la dispersion des donnÃ©es. Examinons les loadings (vecteurs propres) plus en particulier. Dans le langage du module vegan, les vecteurs propres sont les espÃ¨ces (species) et les scores sont les sites. vare_eigenvec &lt;- scores(vare_pca, scaling = 2, display = &quot;species&quot;, choices = 1:(ncol(vareclr)-1)) vare_eigenvec ## PC1 PC2 PC3 PC4 PC5 ## N 0.1437343 0.7606006 -0.6792046 0.1983670 0.1128526122 ## P 0.8669892 -0.3213683 -0.2949864 -0.2294036 0.1437959857 ## K 0.9122089 -0.3857245 0.2356904 0.0346904 0.2737019601 ## Ca 0.9648855 -0.3361651 -0.2147486 0.1775746 -0.2188716732 ## Mg 0.8263327 -0.2723055 0.1035276 0.5213484 -0.1495399242 ## S 0.8824519 -0.3169039 0.3538854 -0.2121562 0.1176278503 ## Al -1.0105173 -0.2441785 0.2145614 0.0267422 -0.1005559874 ## Fe -1.0337676 -0.2463987 0.1491865 0.1316173 0.1512218115 ## Mn 0.9555632 0.1041030 -0.1256178 -0.2130047 0.2565830557 ## Zn 0.7763480 -0.1030878 -0.3122919 -0.3649341 -0.5665691228 ## Mo -0.2152399 0.8717229 0.4064967 -0.3364279 -0.2134335302 ## Fv 0.2360040 0.5775863 -0.8111953 0.1273582 0.1280096553 ## Baresoil 0.5147445 0.4209983 0.4472351 0.5497950 -0.1438569673 ## Humdepth 0.7455213 0.4379436 0.5193895 0.1649306 0.0004756685 ## pH -0.5753858 -0.5863743 -0.5957495 0.2340826 -0.1517660977 ## PC6 PC7 PC8 PC9 PC10 ## N -0.050148980 -0.09111164 -0.06122008 0.315645453 0.08090232 ## P -0.042883754 0.26894062 0.34111276 0.021124287 0.08756299 ## K 0.075717162 -0.21662612 -0.01641260 0.143099440 -0.08737113 ## Ca 0.008050762 0.03630015 0.04775616 -0.073609828 -0.10601799 ## Mg -0.342213793 0.04617838 -0.12098602 -0.051599273 0.18373857 ## S -0.191386377 -0.26825994 0.15822845 0.038378858 0.05100717 ## Al -0.043569364 -0.22737412 0.10598673 0.040586196 -0.14473132 ## Fe 0.081571443 0.10553041 -0.09254655 -0.079426433 0.09908706 ## Mn 0.275275174 0.20224538 -0.19347804 -0.038859808 -0.07637994 ## Zn 0.153089144 -0.12332232 -0.14862229 0.024026151 0.02643462 ## Mo -0.167725160 0.13788948 0.17165900 0.032981311 0.01419924 ## Fv -0.109737235 -0.20911147 0.11289753 -0.281443886 -0.08391004 ## Baresoil 0.463148072 -0.02103009 0.23028292 0.004554036 0.02604286 ## Humdepth -0.273056212 0.17061078 -0.11310394 0.027515405 -0.23068827 ## pH -0.056640816 0.19890884 0.12152266 0.150118818 -0.15240317 ## PC11 PC12 PC13 PC14 ## N -0.019251478 0.045420621 0.05020956 0.002340519 ## P -0.045741546 0.145128883 -0.03337551 -0.010109130 ## K 0.183005607 0.002260341 -0.10566808 0.001169065 ## Ca 0.161460554 0.041210064 0.14341793 0.007419161 ## Mg -0.009862571 -0.063493608 -0.03782662 -0.023575986 ## S -0.138785063 -0.117144869 0.06075094 0.025874035 ## Al -0.089462074 0.058212507 0.01983102 -0.037901576 ## Fe -0.006376211 0.049837173 -0.01169516 0.036048221 ## Mn -0.083300112 -0.133353213 0.02679781 -0.021373612 ## Zn -0.064973307 0.051057277 -0.06538348 0.010896560 ## Mo 0.128814989 -0.114803631 -0.01989539 -0.001335923 ## Fv -0.012456867 -0.020157331 -0.05448619 0.005707928 ## Baresoil -0.061147847 -0.019696758 -0.01640490 0.003823725 ## Humdepth -0.102189307 0.109293684 -0.02485030 0.016559206 ## pH -0.037691048 -0.153813168 -0.04523353 0.014193061 ## attr(,&quot;const&quot;) ## [1] 4.309777 Lâ€™ordre dâ€™importance des vecteurs propres est Ã©tabli en ordre croissant des Ã©lÃ©ment des vecteurs propres associÃ©es. Un vecteur propre est une combinaison linÃ©aire des variables. Par exemple, le premier vecteur propre pointe surtout dans la direction du Fe (-1.497) et de lâ€™Al (-1.463). Le deuxiÃ¨me pointe surtout vers le Mo (2.145). Les vecteurs (loadings) dâ€™un biplot de distance prÃ©sentant les des deux premiÃ¨res composantes principales prendront les coordonnÃ©es des deux premiÃ¨res colonnes. Le vecteur Al aura la coordonnÃ©e [-1.463 ; -0.601], le vecteur de Fe sera placÃ© Ã  [-1.497 ; -0.606] et le vecteur Mo Ã  [-0.312 ; 2.145]. Il existe diffÃ©rentes fonctions dâ€™affichage des biplots. Notez que leur longueur peut Ãªtre magnifiÃ©e pour amÃ©liorer la visualisation. LanÃ§ons la fonction biplot pour crÃ©er un biplot de distance et un autre de corrÃ©lation. par(mfrow = c(1, 2)) biplot(vare_pca, scaling = 1, main = &quot;Biplot de distance&quot;) biplot(vare_pca, scaling = 2, main = &quot;Biplot de corrÃ©lation&quot;) Le biplot de distance permet de dÃ©gager les variables qui expliquent davantage la variabilitÃ© dans notre tableau: les clr du Fe et de lâ€™Al forment en grande partie le premier axe principal, alors que le clr du Mo forme en grande partie le second axe. Le biplot de corrÃ©lation montre que les clr du Fe et du Al sont corrÃ©lÃ©s dans le mÃªme sens, mais das le sens contraire du clr du Mn. Lâ€™information sur la teneur en Fe et celle de lâ€™Al est en grande partie redondante. Toutefois, le clr du Mo est presque indÃ©pendant du clr du Fe, ceux-ci Ã©tant Ã  angle presque droit (~90Â°). Ces relations peuvent Ãªtre explorÃ©es directement. par(mfrow = c(1, 2)) plot(vareclr$Al, vareclr$Fe) plot(vareclr$Mo, vareclr$Fe) Nous avons mentionnÃ© que lâ€™ACP est une rotation. Prenons un second exemple pour bien en saisir les tenants et aboutissants. Le tableau de donnÃ©es que nous chargerons provient dâ€™un infographie dâ€™un dauphin, intitullÃ©e Bottlenose Dolphin, conÃ§u par lâ€™artiste Tarnyloo. Les points correspondent Ã  la surface dâ€™un dauphin. Jâ€™ai ajoutÃ© une colonne anatomy, qui indique Ã  quelle partie anatomique le point appartient. dolphin &lt;- read_csv(&quot;data/07_dolphin.csv&quot;) ## Parsed with column specification: ## cols( ## x = col_double(), ## y = col_double(), ## z = col_double(), ## anatomy = col_character() ## ) dolphin %&gt;% sample_n(5) ## # A tibble: 5 x 4 ## x y z anatomy ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.176 0.192 0.178 Body ## 2 0.0644 -0.498 -0.165 Head ## 3 0.0619 0.439 0.307 Body ## 4 -0.0590 -0.367 -0.0389 Head ## 5 0.162 -0.359 -0.0114 Head Voici en vue isomÃ©trique ce en quoi consiste ce nuage de points. library(&quot;scatterplot3d&quot;) scatterplot3d(x = dolphin$x, y = dolphin$y, z = dolphin$z, pch = 16, cex.symbols = 0.2) Effectuons lâ€™ACP sur le dauphin. dolph_pca &lt;- rda(dolphin %&gt;% select(x, y, z), scale = FALSE) biplot(dolph_pca, scaling = 2) On nâ€™y voit pas grand chose, mais si lâ€™on extrait les scores et que lâ€™on raccourcit les vecteurs: dolph_scores &lt;- scores(dolph_pca, display = &quot;sites&quot;) dolph_loads &lt;- scores(dolph_pca, display = &quot;species&quot;) dolph_loads ## PC1 PC2 ## x -0.02990131 0.01608095 ## y -7.13731672 -1.43221776 ## z -4.56612084 2.23859843 ## attr(,&quot;const&quot;) ## [1] 9.089026 plot(dolph_scores, pch = 16, cex = 0.24, asp = 1, col = factor(dolphin$anatomy)) segments(x0 = rep(0, 3), y0 = rep(0, 3), x = dolph_loads[, 1]/50, y = dolph_loads[, 2]/50, col = &quot;chocolate&quot;, lwd = 4) La meilleure reprÃ©sentation du dauphin en 2D, selon la variance, est son profil - en effet, il est plus long et haut que large. Note. Une ACP effectue seulement une rotation des points. Les distances euclidiennes entre les points sont maintenues. Note. Lâ€™ACP a Ã©tÃ© conÃ§ue pour projetter en un nombre moindre de dimensions des observations dont les distributions sont multinormales (ce nâ€™est Ã©videmment pas le cas du dauphin). Note. Les axes principaux dâ€™une ACP sont des variables alÃ©atoires. Elles peuvent Ãªtre assujetties Ã  des tests ststistiques, des modÃ¨les, du partitionnement de donnÃ©es, etc. Excercice. Effectuez maintenant une ACP avec les donnÃ©es dâ€™iris. 8.4.1.2 Analyse de correspondance (AC) Lâ€™analyse de correspondance (AC) est particuliÃ¨rement appropriÃ©e pour traiter des donnÃ©es dâ€™abondance et dâ€™occurence. Tout comme lâ€™analyse en composantes principales, les donnÃ©es apportÃ©s vers une AC doivent Ãªtre dimensionnellement homogÃ¨nes, câ€™est-Ã -dire que chaque variable doit Ãªtre de mÃªme mÃ©trique: pour des donnÃ©es dâ€™abondance, cela signifie que les dÃ©comptes rÃ©fÃ¨rent tous au mÃªme concept: individus, colonies, surfaces occupÃ©es, etc. Alors que la distance euclidienne est prÃ©servÃ©e avec lâ€™ACP, lâ€™AC prÃ©serve la distance du \\(\\chi^2\\), qui est insensible aux double-zÃ©ros. Lâ€™AC produit \\(min(n,p)-1\\) axes principaux orthogonaux qui captent non pas le maximum de variance, mais la proportion de mesures aux carrÃ© par rapport Ã  la somme des carrÃ©s de la matrice. Le biplot obtenu peut Ãªtre prÃ©sentÃ© sous forme de biplot de site (scaling 1), oÃ¹ la distance du \\(\\chi^2\\) est prÃ©servÃ©e entre les sites ou biplot dâ€™espÃ¨ces (scaling 2), ou la distance du \\(\\chi^2\\) est prÃ©servÃ©e entre les espÃ¨ces. Lâ€™AC hÃ©rite du coup une propriÃ©tÃ© importate de la distance du \\(\\chi^2\\), qui accorde davantage de distance entre un compte de 0 et de 1 quâ€™entre 1 et 2, et davantage entre 1 et 2 quâ€™entre 2 et 3. Par exemple, sur ces trois sites, on a comptÃ© un individu A de moins que dâ€™individu B. abundance_0123 = tibble(Site = c(&quot;Site 1&quot;, &quot;Site 2&quot;, &quot;Site 3&quot;), A = c(0, 1, 9), B = c(1, 2, 10)) abundance_0123 ## # A tibble: 3 x 3 ## Site A B ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Site 1 0 1 ## 2 Site 2 1 2 ## 3 Site 3 9 10 Pourtant, la distance du \\(\\chi^2\\) est plus Ã©levÃ©e entre le site 1 et le site 2 quâ€™entre le site 2 et le site 3. dist(decostand(abundance_0123 %&gt;% select(-Site), method=&quot;chi.square&quot;)) ## 1 2 ## 2 0.6724111 ## 3 0.9555316 0.2831205 La distance du \\(\\chi^2\\) donne davantage dâ€™importance aux espÃ¨ces rares, ce dont une analyse doit tenir compte. Il pourrait Ãªtre envisageable de retirer dâ€™un tableau des espÃ¨ces rare, ou bien prÃ©transformer des donnÃ©es dâ€™abondance par une transformation de chord ou de Hellinger (tel que discutÃ© au chapitre 6), puis procÃ©der Ã  une ACP sur ces donnÃ©es (Legendre et Gallagher, 2001). 8.4.1.2.1 Application Le tableau varespec comprend des donnÃ©es de surface de couverture de 44 espÃ¨ces de plantes en lien avec les donnÃ©es environnementales du tableau varechem. Ces donnÃ©es ont Ã©tÃ© publiÃ©es dans VÃ¤re et al. (1995) et exportÃ©es du module vegan. data(&quot;varespec&quot;) varespec %&gt;%sample_n(5) ## Callvulg Empenigr Rhodtome Vaccmyrt Vaccviti Pinusylv Descflex Betupube ## 1 0.00 16.00 4 15 25.00 0.25 0.50 0.25 ## 2 0.00 6.93 0 0 10.60 0.02 0.10 0.02 ## 3 0.55 11.13 0 0 17.80 0.07 0.00 0.00 ## 4 0.00 5.30 0 0 8.20 0.00 0.05 0.00 ## 5 0.00 12.68 0 0 23.73 0.03 0.00 0.00 ## Vacculig Diphcomp Dicrsp Dicrfusc Dicrpoly Hylosple Pleuschr Polypili ## 1 0.00 0.00 0.25 0.25 3.00 0.00 2.00 0.00 ## 2 0.05 0.07 14.02 10.82 0.00 0.02 28.77 0.00 ## 3 1.60 2.07 0.00 1.62 0.00 0.00 4.67 0.02 ## 4 8.10 0.28 0.00 0.45 0.03 0.00 0.10 0.00 ## 5 0.00 0.00 0.00 3.42 0.02 0.00 19.42 0.02 ## Polyjuni Polycomm Pohlnuta Ptilcili Barbhatc Cladarbu Cladrang Cladstel ## 1 0.25 0.25 0.25 10.00 3.00 0.70 4.70 10.90 ## 2 6.98 0.13 0.00 0.22 0.00 6.00 2.25 0.00 ## 3 0.13 0.00 0.13 0.12 0.00 21.73 21.47 3.50 ## 4 0.25 0.00 0.03 0.00 0.00 35.00 42.50 0.28 ## 5 2.12 0.00 0.17 1.80 0.02 9.08 9.22 0.05 ## Cladunci Cladcocc Cladcorn Cladgrac Cladfimb Cladcris Cladchlo Cladbotr ## 1 0.25 0.00 0.05 0.25 0.25 0.25 0.25 0.25 ## 2 0.80 0.12 0.57 0.17 0.15 0.07 0.00 0.00 ## 3 0.30 0.18 0.23 0.25 0.25 0.23 0.00 0.00 ## 4 0.35 0.08 0.20 0.25 0.18 0.13 0.08 0.00 ## 5 0.73 0.08 1.42 0.50 0.17 1.78 0.05 0.05 ## Cladamau Cladsp Cetreric Cetrisla Flavniva Nepharct Stersp Peltapht ## 1 0.00 0.00 0.00 0.67 0.00 0.00 0.00 0.00 ## 2 0.00 0.02 0.03 0.02 0.00 4.87 0.10 0.07 ## 3 0.08 0.02 0.02 0.00 0.12 0.02 0.62 0.02 ## 4 0.00 0.00 0.05 0.00 0.23 0.20 0.93 0.00 ## 5 0.00 0.00 0.00 0.00 0.02 0.00 1.58 0.33 ## Icmaeric Cladcerv Claddefo Cladphyl ## 1 0.00 0.00 0.40 0 ## 2 0.00 0.02 0.05 0 ## 3 0.00 0.00 0.25 0 ## 4 0.03 0.00 0.10 0 ## 5 0.00 0.00 1.97 0 Pour effectuer lâ€™AC, nous utiliserons, comme pour lâ€™ACP, le module vegan mais cette fois-ci avec la fonction cca. Lâ€™AC en scaling 1 est effectuÃ©e sur le tableau des abondances avec les espÃ¨ces comme colonnes et les sites comme lignes. Les matrices dâ€™abondance transposÃ©es indique les sites oÃ¹ chque espÃ¨ce ont Ã©tÃ© dÃ©nombrÃ©es: pour une analyse en scaling 2, on effectue une analyse de correspondance sur la matrice dâ€™abondance (ou dâ€™occurence) transposÃ©e. Pour chacune des AC, je filtre pour mâ€™assurer que toutes les lignes contiennent au moins une observation. Ce nâ€™est pas nÃ©cessaire dans notre cas, mais je le laisse pour lâ€™exemple. vare_cca &lt;- cca(varespec %&gt;% filter(rowSums(.) &gt; 0)) summary(vare_cca, scaling = 1) ## ## Call: ## cca(X = varespec %&gt;% filter(rowSums(.) &gt; 0)) ## ## Partitioning of scaled Chi-square: ## Inertia Proportion ## Total 2.083 1 ## Unconstrained 2.083 1 ## ## Eigenvalues, and their contribution to the scaled Chi-square ## ## Importance of components: ## CA1 CA2 CA3 CA4 CA5 CA6 CA7 ## Eigenvalue 0.5249 0.3568 0.2344 0.19546 0.17762 0.12156 0.11549 ## Proportion Explained 0.2520 0.1713 0.1125 0.09383 0.08526 0.05835 0.05544 ## Cumulative Proportion 0.2520 0.4233 0.5358 0.62962 0.71489 0.77324 0.82868 ## CA8 CA9 CA10 CA11 CA12 CA13 ## Eigenvalue 0.08894 0.07318 0.05752 0.04434 0.02546 0.01710 ## Proportion Explained 0.04269 0.03513 0.02761 0.02129 0.01222 0.00821 ## Cumulative Proportion 0.87137 0.90650 0.93411 0.95539 0.96762 0.97583 ## CA14 CA15 CA16 CA17 CA18 ## Eigenvalue 0.014896 0.010160 0.007830 0.006032 0.004008 ## Proportion Explained 0.007151 0.004877 0.003759 0.002896 0.001924 ## Cumulative Proportion 0.982978 0.987855 0.991614 0.994510 0.996434 ## CA19 CA20 CA21 CA22 CA23 ## Eigenvalue 0.002865 0.0019275 0.0018074 0.0005864 0.0002434 ## Proportion Explained 0.001375 0.0009253 0.0008676 0.0002815 0.0001168 ## Cumulative Proportion 0.997809 0.9987341 0.9996017 0.9998832 1.0000000 ## ## Scaling 1 for species and site scores ## * Sites are scaled proportional to eigenvalues ## * Species are unscaled: weighted dispersion equal on all dimensions ## ## ## Species scores ## ## CA1 CA2 CA3 CA4 CA5 CA6 ## Callvulg 0.0303167 -1.597460 0.11455 -2.894569 0.1376073 2.291129 ## Empenigr 0.0751030 0.379305 0.39303 0.023675 0.8568729 -0.400964 ## Rhodtome 1.1052309 1.499299 3.04284 0.120106 3.2324306 -0.283510 ## Vaccmyrt 1.4614812 1.622935 2.72375 0.231688 0.4604556 0.712538 ## Vaccviti 0.1468014 0.313436 0.14696 0.243505 0.6868371 -0.147815 ## Pinusylv -0.4820096 0.588517 -0.36020 -0.127094 0.4064754 0.386604 ## Descflex 1.5348239 1.218806 1.87562 -0.001340 -1.3136979 -0.070731 ## Betupube 0.6694503 1.951826 3.84017 1.389423 7.5959115 -0.244478 ## Vacculig -0.0830789 -1.629259 1.05063 0.802648 -0.3058811 -1.625341 ## Diphcomp -0.5446464 -1.037570 0.52282 0.940275 0.3682126 -1.082929 ## Dicrsp 1.8120408 0.360290 -4.92082 3.088562 1.3867372 0.157815 ## Dicrfusc 1.2704743 -0.562978 -0.39718 -2.929542 0.3848272 -2.408710 ## Dicrpoly 0.7248118 1.409347 0.80341 1.915549 4.5674148 1.295447 ## Hylosple 2.0062408 1.743883 2.27549 0.928884 -3.7648428 2.254851 ## Pleuschr 1.3102086 0.583036 -0.01004 0.137298 -1.1216144 0.200422 ## Polypili -0.3805097 -1.243904 0.54593 1.477188 -0.7276341 -0.387641 ## Polyjuni 1.0133795 0.099043 -2.24697 1.510641 0.7729714 -3.062378 ## Polycomm 0.8468241 1.321773 1.13585 1.140723 2.6836594 -0.605038 ## Pohlnuta -0.0136453 0.589290 -0.35542 0.135481 0.9369707 0.397246 ## Ptilcili 0.4223631 1.598584 3.43474 1.400065 6.3209491 0.198935 ## Barbhatc 0.5018348 2.119334 4.57303 1.693188 8.1101807 0.645995 ## Cladarbu -0.1531729 -1.483884 0.20024 0.193680 0.0734141 0.358926 ## Cladrang -0.5502561 -1.084008 0.40552 0.724060 -0.3357992 -0.335924 ## Cladstel -1.4373146 1.077753 -0.44397 -0.375926 -0.2421525 0.004212 ## Cladunci 0.8151727 -1.006186 -1.82587 -1.389523 1.6046713 3.675908 ## Cladcocc -0.2133215 -0.584429 -0.21434 -0.567886 -0.0003788 -0.145303 ## Cladcorn 0.2631227 -0.177858 -0.44464 0.272422 0.3992282 -0.306738 ## Cladgrac 0.1956947 -0.311167 -0.23894 0.379013 0.4933026 0.037581 ## Cladfimb 0.0009213 -0.161418 0.18463 -0.435908 0.4831233 -0.143751 ## Cladcris 0.3373031 -0.470369 -0.05093 -0.823855 0.7182250 0.636140 ## Cladchlo -0.6200021 1.207278 0.21889 0.426447 1.9506082 0.120722 ## Cladbotr 0.5647242 1.047333 2.65330 0.907734 4.4946805 1.201655 ## Cladamau -0.6598144 -1.512880 0.83251 1.577699 -0.0407227 -1.419139 ## Cladsp -0.8209003 0.476164 -0.49752 -0.998241 -0.2393208 0.390785 ## Cetreric 0.2458192 -0.689228 -1.68427 -0.131681 0.7439412 2.374535 ## Cetrisla -0.3465221 1.362693 0.85897 0.396752 2.7526968 0.396591 ## Flavniva -1.4391907 -0.833589 -0.12919 0.007071 -1.4841375 2.956977 ## Nepharct 1.6813309 0.199484 -4.33509 2.229917 0.9561223 -5.472858 ## Stersp -0.5172793 -2.280900 0.99775 2.377013 -0.8892757 -1.441228 ## Peltapht 0.4035858 -0.043265 0.04538 0.711040 0.1824679 -0.841227 ## Icmaeric 0.0378754 -2.419595 0.72135 0.361302 -0.3736424 -2.092136 ## Cladcerv -0.9232858 -0.005233 -1.22058 0.305290 -0.8142627 0.414135 ## Claddefo 0.5190399 -0.496632 -0.15271 -0.695927 0.9042143 0.909191 ## Cladphyl -1.2836161 1.155872 -0.79912 -0.741170 -0.1608002 0.490526 ## ## ## Site scores (weighted averages of species scores) ## ## CA1 CA2 CA3 CA4 CA5 CA6 ## sit1 -0.108122 -0.53705 0.229574 0.24412 0.1405624 -0.14253 ## sit2 0.697118 -0.14441 -0.031788 -0.21743 -0.2738522 -0.08146 ## sit3 0.987603 0.15042 -1.348447 0.80472 0.3095168 0.46773 ## sit4 0.851765 0.49901 0.443559 0.12277 -0.4814871 0.07589 ## sit5 0.359881 -0.05608 0.145813 0.15087 0.2405263 -0.17770 ## sit6 0.003545 0.37017 0.027760 0.06168 -0.1158930 -0.03413 ## sit7 0.860732 -0.11504 0.110869 -1.02169 0.0772348 -0.60530 ## sit8 0.636936 -0.33250 0.001120 -0.79797 0.0130769 -0.54049 ## sit9 1.279352 0.81557 0.670053 0.23137 -0.8929976 0.41783 ## sit10 -0.195009 -0.80564 0.117686 -0.58286 -0.0007212 0.53071 ## sit11 0.528532 -0.70420 -0.517771 -0.86836 0.5713441 0.91671 ## sit12 0.382866 -0.18686 -0.004789 0.10156 0.0458125 0.21087 ## sit13 0.990715 0.11967 -1.110040 0.44929 0.1885902 -0.70694 ## sit14 -0.264704 -1.06013 0.334900 0.45973 -0.0326631 -0.19945 ## sit15 -0.428410 -1.20765 0.374344 0.74970 -0.2596294 -0.30467 ## sit16 -0.330534 -0.77498 0.130760 0.22391 0.0632686 0.09060 ## sit17 -0.899601 0.12075 -0.075742 0.03842 -0.1489585 -0.12031 ## sit18 -0.770294 -0.35351 -0.033779 -0.01795 -0.3007839 0.44303 ## sit19 -0.992193 0.50319 -0.157505 -0.07070 -0.1065172 -0.09928 ## sit20 -0.937173 0.78688 -0.258119 -0.19377 -0.0343535 -0.01259 ## sit21 -0.726413 0.49163 -0.157235 -0.08698 -0.0105774 -0.02801 ## sit22 -1.002083 0.71239 -0.236526 -0.18643 -0.0231666 -0.04928 ## sit23 -0.322647 -0.03871 -0.001297 0.09029 -0.1481448 0.06934 ## sit24 0.259527 0.80746 1.124258 0.36083 1.5437866 0.07051 varespec_eigenval &lt;- eigenvals(vare_cca, scaling = 1) prop_expl &lt;- varespec_eigenval / sum(varespec_eigenval) par(mfrow = c(1, 2)) plot(x = 1:length(varespec_eigenval), y = vare_cca$CA$eig, type = &quot;b&quot;, xlab = &quot;Rang de la valeur propre&quot;, ylab = &quot;Valeur propre&quot;) abline(h = mean(varespec_eigenval), col = &quot;red&quot;, lty = 2) plot(x = 1:length(varespec_eigenval), y = prop_expl, type = &quot;b&quot;, xlab = &quot;Rang de la valeur propre&quot;, ylab = &quot;Valeur propre&quot;) lines(x = 1:length(varespec_eigenval), y = broken_stick(length(varespec_eigenval)), col = &quot;red&quot;, lty = 2) CrÃ©ons les biplots. par(mfrow = c(1, 2)) plot(vare_cca, scaling = 1, main = &quot;Biplot des espÃ¨ces&quot;) plot(vare_cca, scaling = 2, main = &quot;Biplot des sites&quot;) Le biplot des espÃ¨ces, Ã  gauche (scaling = 1), montre la distribution des sites selon les espÃ¨ces. Les emplacements des scores (en noir) montrent les contrastes entre sites selon les espÃ¨ces qui les recouvrent. Les sites 14 et 15, par exemple, contrastent les sites 19, 20, 21 et 22 selon le 2iÃ¨me axe principal. Par ailleurs, les axes principaux sont formÃ© de plusieurs espÃ¨ces dont aucune ne domine clairement. Le biplot des sites, Ã  droite (scaling = 2), montre la distribution des recouvrements dâ€™espÃ¨ces selon les sites. Par exemple, les espÃ¨ces Betupube (Betula pubescens) et Barbhatc (Barbilophozia hatcheri ) se recouvrent en particulier le site 24. Le site 1 est difficile Ã  identifier, car il est couvert par plusieurs noms dâ€™espÃ¨ces, au bas au centre. Les sites 3 et 13 se confondent avec Dicrsp (une espÃ¨ce de Dicranum) qui le recouvre amplement. Pour les deux types de biplot, les sites oÃ¹ les espÃ¨ces situÃ©s prÃ¨s de lâ€™origine, car ils peuvent Ãªtre soit prÃ¨s de la moyenne, soit distribuÃ©s uniformÃ©ment. Le nombre de composantes Ã  retenir peut Ãªtre Ã©valuÃ© par les approches Kaiser-Guttmann et broken-stick. scaling &lt;- 1 varespec_eigenval &lt;- eigenvals(vare_cca, scaling = scaling) # peut Ãªtre effectuÃ© sur les deux types de scaling prop_expl &lt;- varespec_eigenval / sum(varespec_eigenval) par(mfrow = c(1, 2)) plot(x = 1:length(varespec_eigenval), y = vare_cca$CA$eig, type = &quot;b&quot;, xlab = &quot;Rang de la valeur propre&quot;, ylab = &quot;Valeur propre&quot;, main = paste(&quot;Eigenvalue - Kaiser-Guttmann, scaling =&quot;, scaling)) abline(h = mean(varespec_eigenval), col = &quot;red&quot;, lty = 2) plot(x = 1:length(varespec_eigenval), y = prop_expl, type = &quot;b&quot;, xlab = &quot;Rang de la valeur propre&quot;, ylab = &quot;Valeur propre&quot;, main = paste(&quot;Proportion - broken stick, scaling =&quot;, scaling)) lines(x = 1:length(varespec_eigenval), y = broken_stick(length(varespec_eigenval)), col = &quot;red&quot;, lty = 2) Pour les deux scalings, lâ€™approche Kaiser-Guttmann propose 7 axes, tandis que lâ€™approche broken-stick en propose 5. Les reprÃ©sentations biplot dâ€™analyse de correspondance peuvent prendre la forme dâ€™un boomerang, en particulier celles qui sont basÃ©es sur des donnÃ©es dâ€™occurence. Le tableau suivant initialement de Chessel et al. (1987) et est distribuÃ© dans le module ade4. library(&quot;ade4&quot;) data(&quot;doubs&quot;) fish &lt;- doubs$fish doubs_cca &lt;- cca(fish %&gt;% filter(rowSums(.) &gt; 0)) plot(doubs_cca, scaling = 2) Les numÃ©ros de sites correspondent Ã  la position dans une riviÃ¨re, 1 Ã©tant en amont et 30 en aval. Le premier axe discrimine lâ€™amont et lâ€™aval, tandis que le deuxiÃ¨me montre deux niches en amont. Bien que lâ€™on observe une discontinuitÃ© dans le cours dâ€™eau, il y a une continuitÃ© dans les abondances. Cet effet peut Ãªtre corrigÃ© en retirant la tendance de lâ€™analyse de correspondance par une detrended correspondance analysis. Pour cela, il faudra utiliser la fonction decorana, ce qui ne sera pas couvert ici. Lâ€™analyse des correspondances multiples (ACM) est utile pour lâ€™ordination des donnÃ©es catÃ©gorielles. Le module ade4 est en mesure dâ€™effectuer des AMC, mais nâ€™est pas couvert dans ce manuel. Excercice. Effectuez et analysez une AC avec les donnÃ©es de recouvrement varespec. 8.4.1.3 Positionnement multidimensionnel (PoMd) Le positionnement multidimensionnel (PoMd), ou manifold analysis, se base sur les assiciations entre les objets (mode Q) ou les variables (mode R) pour en rÃ©duire les dimensions. Alors que lâ€™analyse en composantes principales conserve la distance euclidienne et que lâ€™analyse de correspondance conserve la distance du \\(\\chi^2\\), le PoMd conserve lâ€™association que vous sÃ©lectionnerez Ã  votre convenance. Le PoMd vise Ã  reprÃ©senter en un nombre limitÃ© de dimensions (souvent 2) la distance (ou dissimilaritÃ©) quâ€™ont les objets (ou des variables) les uns par rapport aux autres dans lâ€™espace multidimensionnel. Il existe deux types dâ€™AEM. Le PoMd-mÃ©trique (metric multidimentional scaling MMDS, parfois le metric est retirÃ©, MDS, et parfois lâ€™on parle de classic MDS) vise Ã  reprÃ©senter fidÃ¨lement la distance entre les objets ou les variables. Le PoMd-mÃ©trique ne devrait Ãªtre utilisÃ©e que lorsque la mÃ©trique nâ€™est ni euclidienne, ni de \\(\\chi^2\\) et que lâ€™on dÃ©sire prÃ©server les distances entre les objets. Lâ€™PoMd-mÃ©trique aussi appelÃ©e analyse en coordonnÃ©es principales (ACoP ou de lâ€™anglais PCoA) . Le PoMd-non-mÃ©trique (nonmetric multidimentional scaling, NMDS) vise quant Ã  lui Ã  reprÃ©senter lâ€™ordre des distances entre les objets ou les variables. Câ€™est une approche par rang: le PoMd-non-mÃ©trique vise reprÃ©senter les objets sont plus proches ou plus Ã©loignÃ©es les uns des autres plutÃ´t que de reprÃ©senter leur similaritÃ© dans lâ€™espace multidimentionnelle. Lâ€™IsoMap, pour isometric feature mapping, est une extension du PoMd qui recontruit les distances selon les points retrouvÃ©s dans le voisinage. Les isomaps sont en mesure dâ€™applatir des donnÃ©es ayant des formes complexes. Nous ne traitons pour lâ€™instant que de lâ€™PoMd-mÃ©trique (fonction vegan::cmdscale) et des PoMd-non-mÃ©trique (fonction vegan::metaMDS). 8.4.1.3.1 Application Utilisons les donnÃ©es dâ€™abondance que nous avions au tout dÃ©but de ce chapitre. La matrice dâ€™association de Bray-Curtis sera utilisÃ©e. assoc_mat &lt;- vegdist(abundance, method = &quot;bray&quot;) pheatmap(assoc_mat %&gt;% as.matrix(), cluster_rows = FALSE, cluster_cols = FALSE, display_numbers = round(assoc_mat %&gt;% as.matrix(), 2)) Les sites 2 et 3 devraient Ãªtre plus prÃ¨s lâ€™un et lâ€™autre, puis les sites 3 et 4. Les autres associations sont Ã©loignÃ©s dâ€™environ la mÃªme distance. LanÃ§ons le calcul de la PoMd-mÃ©trique. pcoa &lt;- cmdscale(assoc_mat, k = nrow(abundance)-1, eig = TRUE) spec_scores &lt;- wascores(pcoa$points, abundance) ordiplot(scores(pcoa), type = &#39;t&#39;, cex = 1.5) ## species scores not available text(spec_scores, row.names(spec_scores), col = &quot;red&quot;, cex = 0.75) On observe en effet que les sites 2 et 3 sont les plus prÃ¨s. Les sites 3 et 4sont plus Ã©loignÃ©s. Les sites 1, 2 et 4 font Ã  peu prÃ¨s un triangle Ã©quilatÃ©ral, ce qui correspond Ã  ce Ã  quoi on devrait sâ€™attendre. Les wa-scores permettent de juxtaposer les espÃ¨ces sur les sites, pour rÃ©fÃ©rence. Le colibri nâ€™est prÃ©sent que sur le site 2. Le site 1 est populÃ© par des jaseurs et des mÃ©sanges, et câ€™est le seul site oÃ¹ lâ€™on a observÃ© une citelle. On a observÃ© des chardonnerets sur les sites 2 et 3. Sur le site 4, on nâ€™a observÃ© que des bruants, que lâ€™on a aussi observÃ© ailleurs, sauf au site 2. Le PoMd-non-mÃ©trique (non metric dimensional scaling, NMDS) fonctionne de la mÃªme maniÃ¨re que la PoMd-mÃ©trique, Ã  la diffÃ©rence que la distance est basÃ©e sur les rangs. Ã€ cet Ã©gard, le site 4 Ã  une distance de 0.76 du site 3, mais plutÃ´t le deuxiÃ¨me plus loin, aprÃ¨s le site 2 et avant le site 1. Utilisons la fonction metaMDS. nmds &lt;- metaMDS(assoc_mat, k = nrow(abundance)-1, eig = TRUE) ## Run 0 stress 0 ## Run 1 stress 0 ## ... Procrustes: rmse 0.1824228 max resid 0.2361895 ## Run 2 stress 0 ## ... Procrustes: rmse 0.08158121 max resid 0.1140836 ## Run 3 stress 0 ## ... Procrustes: rmse 0.09638115 max resid 0.1226334 ## Run 4 stress 7.503523e-05 ## ... Procrustes: rmse 0.09978621 max resid 0.1468914 ## Run 5 stress 0 ## ... Procrustes: rmse 0.1062878 max resid 0.1350668 ## Run 6 stress 0 ## ... Procrustes: rmse 0.1264951 max resid 0.1721956 ## Run 7 stress 0 ## ... Procrustes: rmse 0.1389736 max resid 0.1651835 ## Run 8 stress 0 ## ... Procrustes: rmse 0.1427665 max resid 0.1787661 ## Run 9 stress 0 ## ... Procrustes: rmse 0.09291298 max resid 0.1245135 ## Run 10 stress 0 ## ... Procrustes: rmse 0.1062446 max resid 0.1568264 ## Run 11 stress 0 ## ... Procrustes: rmse 0.04281796 max resid 0.05081426 ## Run 12 stress 0 ## ... Procrustes: rmse 0.1071436 max resid 0.1437232 ## Run 13 stress 0 ## ... Procrustes: rmse 0.1754142 max resid 0.2281662 ## Run 14 stress 0 ## ... Procrustes: rmse 0.1205299 max resid 0.1564748 ## Run 15 stress 0 ## ... Procrustes: rmse 0.08980122 max resid 0.114155 ## Run 16 stress 0 ## ... Procrustes: rmse 0.1241872 max resid 0.1687868 ## Run 17 stress 0 ## ... Procrustes: rmse 0.09977436 max resid 0.1400455 ## Run 18 stress 0 ## ... Procrustes: rmse 0.1125331 max resid 0.1552035 ## Run 19 stress 0 ## ... Procrustes: rmse 0.1316955 max resid 0.1574244 ## Run 20 stress 0 ## ... Procrustes: rmse 0.09172994 max resid 0.1221636 ## *** No convergence -- monoMDS stopping criteria: ## 20: stress &lt; smin ## Warning in metaMDS(assoc_mat, k = nrow(abundance) - 1, eig = TRUE): stress ## is (nearly) zero: you may have insufficient data spec_scores &lt;- wascores(nmds$points, abundance) ordiplot(scores(nmds), type = &#39;t&#39;, cex = 1.5) ## species scores not available text(spec_scores, row.names(spec_scores), col = &quot;red&quot;, cex = 0.75) Dans ce cas, entre PoMd-mÃ©trique et non-mÃ©trique, les rÃ©sultats peuvent Ãªtre interprÃ©tÃ©s de maniÃ¨re similaire. En ce qui a trait au dauphin, Pour plus de dÃ©tails, je vous invite Ã  vous rÃ©fÃ©rer Ã  Borcard et al. (2011)) ou de consulter lâ€™excellent site GUSTA ME. 8.4.1.4 Conclusion sur lâ€™ordination non contraignante Lorsque les donnÃ©es sont euclidiennes, lâ€™analyse en composantes principales (ACP) dervait Ãªtre utilisÃ©e. Lorsque la mÃ©trique est celle du \\(\\chi^2\\), on prÃ©fÃ©rera lâ€™analyse de correspondance (AC). Si la mÃ©trique est autre, le positionnement multidimensionel (PoMd) est prÃ©fÃ©rable. Dans ce dernier cas, si lâ€™on recherche une reprÃ©sentation simplifiÃ©e de la distance entre les objets ou variables, on utilisera un PoMd-mÃ©trique. Ã€ lâ€™inverse, si lâ€™on dÃ©sire une reprÃ©sentation plus fidÃ¨le au rang des distances, on prÃ©fÃ©rera lâ€™PoMd-non-mÃ©trique. 8.4.2 Ordination contraignante Alors que lâ€™ordination non contraignante vous permet de dresser un protrait de vos variables, lâ€™ordination contraignante (ou canonique) permet de tester statistiquement ainsi que de reprÃ©senter la relation entre plusieurs variables explicatives (par exemple, des conditions environnementales) et une ou plusieurs variables rÃ©ponses (par exemple, les espÃ¨ces observÃ©es). Lâ€™analyse discriminante nâ€™a fondamentalement quâ€™une seulement variable rÃ©ponse, et celle-ci doit dÃ©crire lâ€™appartenance Ã  une catÃ©gorie. Lâ€™analyse de redondance sera prÃ©fÃ©rÃ©e lorsque le nombre de variable est plus restreint (variables ionomiques et indicateurs de performance des cultures). Les dÃ©tails, ainsi que les tenants et aboutissants de ces mÃ©thodes, sont prÃ©sentÃ©s dans Numerical Ecology (Legendre et Legendre, 2012). Lâ€™analyse canonique des corrÃ©lations sera prÃ©fÃ©rÃ©e lorsque les variables sont parsemÃ©es (beaucoup de colonnes avec beaucoup de zÃ©ros, comme les variables dâ€™abondance). 8.4.2.1 Analyse discriminante Alors que lâ€™analyse en composante principale vise Ã  prÃ©senter la perspective (les axes) selon laquelle les points sont les plus Ã©clatÃ©es, lâ€™analyse discriminante, le plus souvent utilisÃ© dans sa forme linÃ©aire (ADL) et quadratique (ADQ), vise Ã  prÃ©senter la perspective selon laquelle les groupes sont les plus Ã©clatÃ©s, les groupes formant la variable contraignante. Ces groupes peuvent Ãªtre connus (e.g.Â cultivar, rÃ©gion gÃ©ographique) ou attribuÃ©s (exemple: par partitionnement). Lâ€™ADL est parfois nommÃ©e analyse canonique de la variance. Lâ€™AD vise Ã  reprÃ©senter des diffÃ©rences entre des groupes aux moyens de combinaisons linÃ©aires (ADL) ou quadratique (ADQ) de variables mesurÃ©es. Sa reprÃ©sentation sous forme de biplot permet dâ€™apprÃ©cier les diffÃ©rences entre les groupes dâ€™identifier les variables qui sont responsables de la discrimination. Biplot de distance de lâ€™analyse discriminante des ionomes dâ€™espÃ¨ces de plantes Ã  fruits cultivÃ©es sauvages et domestiquÃ©es, Source: Parent et al. (2013) Lâ€™ADL a Ã©tÃ© dÃ©veloppÃ©e par Fisher (1936), qui Ã  titre dâ€™exemple dâ€™application a utilisÃ© un jeu de donnÃ©es de dimensions dâ€™iris collectÃ©es par Edgar Anderson, du Jardin botanique du Missouri, sur 150 spÃ©cimens dâ€™iris collectÃ©s en GaspÃ©sie (Est du QuÃ©bec), ma rÃ©gion natale (suis-je assez chauvin?). Ce jeu de donnÃ©es est amplement utilisÃ© Ã  titre dâ€™exemple en analyse multivariÃ©e. Williams (1983) a prÃ©sentÃ© les tenants et aboutissants de lâ€™ADL en Ã©cologie. Tout comme les donnÃ©es passant pas une ACP doivent suivre une distribution multinormale pour Ãªtre statistiquement valide, les distributions des groupes dans une ADL doivent Ãªtre multinormales et les variances des points par groupe doivent Ãªtre homogÃ¨nesâ€¦ ce qui est rarement le cas en science. NÃ©anmoins: Heureusement, il y a des Ã©vidences dans la littÃ©rature que certaines dâ€™entre [ces rÃ¨gles] peuvent Ãªtre transgressÃ©es modÃ©rÃ©ment sans de grands changement dans les taux de classification. Cette conclusion dÃ©pends, toutefois, de la sÃ©vÃ©ritÃ© des transgressions, et de facteurs structueaux comme la position relative des moyennes des populations et de la nature des dispersions. - Williams (1983) Lâ€™ADL peut servir autant dâ€™outil dâ€™interprÃ©tation que dâ€™outil de classification, câ€™est Ã  dire de prÃ©dire une catÃ©gorie selon les variables (chapitre 12). Dans les deux cas, lorsque le nombre de variables approchent le nombre dâ€™observation, les rÃ©sultats dâ€™une ADL risque dâ€™Ãªtre difficilement interprÃ©tables. Le test appropriÃ© pour Ã©valuer lâ€™homodÃ©nÃ©itÃ© de la covariance est le M-test de Box. Ce test est peu documentÃ© dans la littÃ©rature, est rarement utilisÃ© mais a la rÃ©putation dâ€™Ãªtre particuliÃ¨rement sÃ©vÃ¨re. Il est rare que des donnÃ©es Ã©cologiques aient des dispersions (covariances) homogÃ¨nes. Contrairement Ã  lâ€™ADL, lâ€™ADQ ne demande pas Ã  ce que les dispersions (covariances) soient homogÃ¨nes. NÃ©anmoins, lâ€™ADQ ne gÃ©nÃ¨re ni de scores, ni de loadings: il sâ€™agit dâ€™un outil pour prÃ©dire des catÃ©gories (classification), non pas dâ€™un outil dâ€™ordination. 8.4.2.1.1 Application Utilisons les donnÃ©es dâ€™iris. data(&quot;iris&quot;) Testons la multinormalitÃ© par groupe. Rappelons-nous que pour considÃ©rer la distribution comme multinormale, la p-value de la distortion ainsi que la statistique de Kurtosis doivent Ãªtre Ã©gale ou plus Ã©levÃ©e que 0.05. La fonction split sÃ©pare le tableau en listes et la fonction map applique la fonction spÃ©cifiÃ©e Ã  chaque Ã©lÃ©ment de la liste. Cela permet dâ€™effectuer des tests de multinormalitÃ© sur chacune des espÃ¨ces dâ€™iris. iris %&gt;% split(.$Species) %&gt;% map(~ mvn(.x %&gt;% select(-Species), mvnTest = &quot;mardia&quot;)$multivariateNormality) ## $setosa ## Test Statistic p value Result ## 1 Mardia Skewness 25.6643445196298 0.177185884467652 YES ## 2 Mardia Kurtosis 1.29499223711605 0.195322907441935 YES ## 3 MVN &lt;NA&gt; &lt;NA&gt; YES ## ## $versicolor ## Test Statistic p value Result ## 1 Mardia Skewness 25.1850115362466 0.194444483140265 YES ## 2 Mardia Kurtosis -0.57186635893429 0.567412516528727 YES ## 3 MVN &lt;NA&gt; &lt;NA&gt; YES ## ## $virginica ## Test Statistic p value Result ## 1 Mardia Skewness 26.2705981752915 0.157059707690356 YES ## 2 Mardia Kurtosis 0.152614173978342 0.878702546726567 YES ## 3 MVN &lt;NA&gt; &lt;NA&gt; YES Le test est passÃ© pour toutes les espÃ¨ces. Voyons maintenant lâ€™homogÃ©nÃ©itÃ© de la covariance. Pour ce faire, nous aurons besoin de la fonction boxM, disponible avec le module biotools. Pour que les covariances soient considÃ©rÃ©es comme Ã©gales, la p-vaule doit Ãªtre supÃ©rieure Ã  0.05. library(&quot;heplots&quot;) boxM(iris %&gt;% select(-Species), group = iris$Species) ## ## Box&#39;s M-test for Homogeneity of Covariance Matrices ## ## data: iris %&gt;% select(-Species) ## Chi-Sq (approx.) = 140.94, df = 20, p-value &lt; 2.2e-16 On est loin dâ€™un cas oÃ¹ les distributions sont homogÃ¨nes. Nous allons nÃ©anmoins procÃ©der Ã  lâ€™analyse discriminante avec le module ade4. Nous aurons dâ€™abord besoin dâ€™effectuer une ACP avec la fonction dudi.pca de ade4 (en spÃ©cifiant une mise Ã  lâ€™Ã©chelle), que nous projeterons en ADL avec discrimin. library(&quot;ade4&quot;) iris_pca &lt;- dudi.pca(df = iris %&gt;% select(-Species), scannf = FALSE, # ne pas gÃ©nÃ©rer de graphique scale = TRUE) iris_lda &lt;- discrimin(dudi = iris_pca, fac = iris$Species, scannf = FALSE) La visualisation peut Ãªtre effectuÃ©e directement sur lâ€™objet issu de la fonction discrimin. plot(iris_lda) Il sâ€™agit toutefois dâ€™une visualisation pour le diagnostic davantage que pour la publication. Si lâ€™objectif est la pubilcation, vous pourriez utiliser la fonction plotDA que jâ€™ai conÃ§ue Ã  cet effet. Jâ€™ai aussi conÃ§u une fonction similaire qui utilise le module graphique de base de R. source(&quot;https://raw.githubusercontent.com/essicolo/AgFun/master/plotDA_gg.R&quot;) plotDA(scores = iris_lda$li, loadings = iris_lda$fa, fac = iris$Species, level=0.95, facname = &quot;Species&quot;, propLoadings = 1) ## Loading required package: ellipse ## ## Attaching package: &#39;ellipse&#39; ## The following object is masked from &#39;package:car&#39;: ## ## ellipse ## The following object is masked from &#39;package:graphics&#39;: ## ## pairs ## Loading required package: grid ## Loading required package: plyr ## ------------------------------------------------------------------------- ## You have loaded plyr after dplyr - this is likely to cause problems. ## If you need functions from both plyr and dplyr, please load plyr first, then dplyr: ## library(plyr); library(dplyr) ## ------------------------------------------------------------------------- ## ## Attaching package: &#39;plyr&#39; ## The following objects are masked from &#39;package:dplyr&#39;: ## ## arrange, count, desc, failwith, id, mutate, rename, summarise, ## summarize ## The following object is masked from &#39;package:purrr&#39;: ## ## compact Ã€ la diffÃ©rence de lâ€™ACP, lâ€™ADL maximise la sÃ©patation des groupes. Nous avions notÃ© avec lâ€™ACP que les dimensions des pÃ©tales distingaient les groupes. Puisque nous avions justement des informations sur les groupes, nous aurions pu procÃ©der directement Ã  un ADL pour obtenir des conclusions plus directes. Si la longueur des pÃ©tales permet de distinguer lâ€™espÃ¨ce setosa des deux autres, la largeur des pÃ©tales permet de distinguer virginica et versicolor, bien que les nuages de points se superposent. De maniÃ¨re bivariÃ©e, les rÃ©gions de confiance des moyennes des scores discriminants (petites ellipses) montrent des diffÃ©rence significatives au seuil 0.05. Excercice. Si lâ€™on effectuait lâ€™ADL sur notre dauphin, avec la colonne anatomy comme variable de regroupement, quâ€™obtiendrions-nous? Si lâ€™on consiÃ¨re la nageoire codale (queue) comme faisant partie du corps? Quelles sont les limitations? 8.4.2.2 Analyse de redondance (RDA) En anglais, on la nomme redundancy analysis, souvent abrÃ©gÃ©e RDA. Elle est utilisÃ©e pour rÃ©sumer les relations linÃ©aires entre des variables rÃ©ponse et des variables explicatives. La â€œredondanceâ€ se situe dans lâ€™utilisation de deux tableaux de donnÃ©es contenant de lâ€™information concordante. Lâ€™analyse de redondance est une maniÃ¨re Ã©lÃ©gante dâ€™effectuer une rÃ©gresssion linÃ©aire multiple, oÃ¹ la matrice de valeurs prÃ©dites par la rÃ©gression est assujettie Ã  une analyse en composantes principales. Il est ainsi possible de superposer les scores des variables explicatives Ã  ceux des variables rÃ©ponse. Plus prÃ©cisÃ©ment, une RDA effectue les Ã©tapes suivantes (Borcard et al. (2011)) entre une matrice de variables indÃ©pendantes (explicatives) \\(X\\) et une matrice de variables dÃ©pendantes (rÃ©ponse) \\(Y\\). 8.4.2.2.1 1. RÃ©gression entre \\(Y\\) et \\(X\\) Pour chacune des variables rÃ©ponse de \\(Y\\) (\\(y_1\\), \\(y_2\\), , \\(y_j\\)), effectuer une rÃ©gression linÃ©aire sur les variables explicatives \\(X\\). \\[\\hat{y}_j = b_j + m_{1, j} \\times x_1 + m_{2, j} \\times x_2 + ... + m_{i, j} \\times x_i\\] \\[\\hat{y}_j = y_j + y_{res, j}\\] Pour chaque observation (\\(n\\)), nous obtenons une sÃ©rie de valeurs de \\(\\hat{y}_j\\) et de \\(y_{res, j}\\). Donc chaque cellule de la matrice \\(Y\\) a ses pendant \\(\\hat{y}\\) et \\(y_{res}\\). Nous obtenons ainsi une matrice de prÃ©diction \\(\\hat{Y}\\) et une matrice des rÃ©sidus \\(Y_{res} = Y - \\hat{Y}\\). 8.4.2.2.2 2. Analyse en composantes principales Ensuite, on effectue une analyse en composantes principales (ACP) sur la matrice des prÃ©dictions \\(\\hat{Y}\\). On obtient ainsi ses valeurs et vecteurs propres. Nommons \\(U\\) ses vecteurs propres. Les fonctions de RDA mettent souvent ces veceturs Ã  lâ€™Ã©chelle avant de les retourner Ã  lâ€™utilisateur. En ordination Ã©cologique, ces vecteurs mis Ã  lâ€™Ã©chelle sont souvent appelÃ©s les scores des espÃ¨ces, bien quâ€™il ne sâ€™agisse pas nÃ©cessairement dâ€™espÃ¨ces, mais plus gÃ©nÃ©ralement des variables de la matrice dÃ©pendante \\(Y\\). Il est aussi possible dâ€™effectuer une ACP sur \\(Y_{res}\\). 8.4.2.2.3 3. Calculer les scores Les vecteurs propres \\(U\\) sont utilisÃ©s pour calculer les scores des sites, \\(Y \\times U\\), ainsi que les contraintes de site \\(\\hat{Y} \\times U\\). 8.4.2.2.4 Application Nous allons utiliser la fonction rda du module vegan. En ce qui a trait aux donnÃ©es, utilisons les donnÃ©es varespec (matrice Y) et varechem (matrice X). La fonction rda peut fonctionner avec lâ€™interface-formule de R, oÃ¹ Ã  gauche du ~ on retrouve le Y (la matrice de la communautÃ© Ã©cologique, i.e.Â les abondances dâ€™espÃ¨ces) contre le X (l), Ã  gauche, ce qui peut Ãªtre pratique pour lâ€™analyse dâ€™intÃ©ractions. Mais pour comparer deux matrices, nous pouvons dÃ©finir X et Y. Ce qui est mÃ©langeant, câ€™est que vegan, contrairement aux conventions, dÃ©fini X comme Ã©tant la matrice rÃ©ponse et Y comme Ã©tant la matrice explicative. vare_rda &lt;- rda(X = varespec, Y = vareclr, scale = FALSE) par(mfrow = c(1, 2)) ordiplot(vare_rda, scaling = 1, type = &quot;text&quot;, main = &quot;Scaling 1: triplot de distance&quot;) ordiplot(vare_rda, scaling = 2, type = &quot;text&quot;, main = &quot;Scaling 2: triplot de corrÃ©lation&quot;) La fonction ordiplot permet de crÃ©er un triplot de base. La reprÃ©sentation des wascores est rÃ©putÃ©e plus robuste (moins susceptible dâ€™Ãªtre bruitÃ©e), mais leur interprÃ©tation porte Ã  confusion (Borcard et al. (2011)). Triplot de distance (scaling 1). Les angles entre les variables explicatives reprÃ©sentent leur corrÃ©lation (non pas les variables rÃ©ponse). Triplot de corrÃ©lation (scaling 2). Les angles entre les variables reprÃ©sentent leurs corrÃ©lation, que les variables soient rÃ©ponse ou explicative, ou entre variables rÃ©ponses et variables explicatives. Les distances entre les objets sur le triplot ne sont pas des approximation de leur distance euclidienne. Les triplots montrent que les variables ont toutes un rÃ´le important sur la dispersion des sites autours des axeds principaux. Le premier axe principal est composÃ© de maniÃ¨re plus marquÃ©e par le clr de lâ€™Al et celui du Fe. Le deuxiÃ¨me axe principal est composÃ© de maniÃ¨re plus marquÃ©e par le clr du S, du P et du K. Le triplot de corrÃ©lation ne prÃ©sente pas de tendance apprÃ©ciable pour la plupart des espÃ¨ces, qui ne possÃ¨dent pas de niche particuliÃ¨re. Toutefois, lâ€™espÃ¨ce Cladstel, prÃ©sente surtout dans les sites 9 et 10, est liÃ©e Ã  de basses teneurs en N et Ã  de faibles valeurs de Baresoil (sol nu). Lâ€™espÃ¨ce Pleuschr est liÃ©e Ã  des sols oÃ¹ lâ€™on retrouve une grande Ã©paisseur dâ€™humus, ainsi que des teneurs Ã©levÃ©es en nutriment K, P, S, Ca, Mg et Zn. Elle semble apprÃ©cier les sols Ã  bas pH, mais Ã  faible teneur en Fe et Al. La teneur en N lui semble plus indiffÃ©rente (son vecteur Ã©tant presque perpendiculaire). On pourra personnaliser les graphiques en extrayant les scores. scaling &lt;- 2 sites &lt;- scores(vare_rda, display = &quot;wa&quot;, scaling = scaling) species &lt;- scores(vare_rda, display = &quot;species&quot;, scaling = scaling) env &lt;- scores(vare_rda, display = &quot;reg&quot;, scaling = scaling) plot(0, 0, type = &quot;n&quot;, xlim = c(-3, 5), ylim = c(-3, 4), asp = 1) abline(h=0, v = 0, col = &quot;grey80&quot;) text(sites/2, labels = rownames(sites), cex = 0.7, col = &quot;grey50&quot;) text(species/2, labels = rownames(species), col = &quot;green&quot;, cex = 0.7) segments(x0 = 0, y0 = 0, x = env[, 1], y = env[, 2], col = &quot;blue&quot;) text(env, labels = rownames(env), col = &quot;blue&quot;, cex = 1) On pourra effectuer une analyse de Kaiser-Guttmann ou de broken-stick de la mÃªme maniÃ¨re que prÃ©cÃ©demment. Ã‰tant une collection de rÃ©gressions, une RDA est en mesure dâ€™effectuer des tests statistiques sur les coefficients de la rÃ©gression en utilisant des permutations pour tester la signification des coefficients et des axes dâ€™une RDA. On doit nÃ©anmoins obligatoirement effectuer la RDA avec lâ€™interface formule. Lâ€™a variable de gaucheâ€™objet Ã  gauche du ~ peut Ãªtre une matrice ou un tableau, et celui de droite est dÃ©fini dans data. Le . dans lâ€™interface formule signifie â€œune combinaison linÃ©aire de toutes les variables, sans intÃ©ractionâ€. vare_rda &lt;- rda(varespec ~ ., data = vareclr, scale = FALSE) perm_test_term &lt;- anova(vare_rda, by = &quot;term&quot;) #perm_test_axis &lt;- anova(vare_rda, by = &quot;axis&quot;) La signification des axes est difficile Ã  interprÃ©ter. Toutefois, celui des variables prÃ©sente un intÃ©rÃªt. perm_test_term ## Permutation test for rda under reduced model ## Terms added sequentially (first to last) ## Permutation: free ## Number of permutations: 999 ## ## Model: rda(formula = varespec ~ N + P + K + Ca + Mg + S + Al + Fe + Mn + Zn + Mo + Fv + Baresoil + Humdepth + pH, data = vareclr, scale = FALSE) ## Df Variance F Pr(&gt;F) ## N 1 216.13 4.8470 0.010 ** ## P 1 272.71 6.1159 0.006 ** ## K 1 194.97 4.3724 0.010 ** ## Ca 1 24.92 0.5589 0.676 ## Mg 1 52.61 1.1799 0.311 ## S 1 100.07 2.2441 0.091 . ## Al 1 177.91 3.9900 0.017 * ## Fe 1 118.59 2.6595 0.062 . ## Mn 1 25.96 0.5822 0.649 ## Zn 1 35.81 0.8030 0.488 ## Mo 1 23.51 0.5273 0.686 ## Baresoil 1 98.64 2.2122 0.106 ## Humdepth 1 43.59 0.9777 0.396 ## pH 1 38.93 0.8730 0.463 ## Residual 9 401.31 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 La p-value est la probabilitÃ© que les pentes calculÃ©es pour les variables Ã©mergent de distributions dont la moyenne est nulle. Au seuil 0.05, les variables significatives sont (les clr de) lâ€™azote, le phosphore, le potassium et lâ€™aluminium. Dans le cas des matrices dâ€™abondance (ce nâ€™est pas le cas de varespec, constituÃ©e de donnÃ©es de recouvrement), il est prÃ©fÃ©rable avec les RDA de les transformer prÃ©alablement avec la transformation compositionnelle, de chord ou de Hellinger (chapitre 7). Une autre option est dâ€™effectuer une RDA sur des matrices dâ€™association en passant par une analyse en coordonnÃ©es principales (Legendre et Anderson, 1999). Enfin, les donnÃ©es dâ€™abondance Ã  lâ€™Ã©tat brutes devraient plutÃ´t passer utiliser une analyse canonique des corrÃ©lations. 8.4.2.3 Analyse canonique des correspondances (ACC) Lâ€™analyse canonique des correspondances (Canonical correspondance analysis), ACC, a Ã©tÃ© Ã  lâ€™origine conÃ§ue pour Ã©tudier les liens entre des variables environnementales et lâ€™abondance (dÃ©compte) ou lâ€™occurence (prÃ©sence-absence) dâ€™espÃ¨ces (ter Braak, 1986). Lâ€™ACC est Ã  la RDA ce que la CA est Ã  lâ€™ACP. Alors que la RDA prÃ©serve les distance euclidiennes entre variables dÃ©pendantes et indpendantes, lâ€™ACC prÃ©serve les distances du \\(\\chi^2\\). Tout comme lâ€™AC, elle hÃ©rite du coup une propriÃ©tÃ© importate de la distance du \\(\\chi^2\\): il y a davantage davantage dâ€™importance aux espÃ¨ces rares. Lâ€™analyse des correspondances canoniques est souvent utilisÃ©e dans la littÃ©rature, mais dans bien des cas une RDA sur des donnÃ©es dâ€™abondance transformÃ©es donnera des rÃ©sultats davantage intÃ©rprÃ©tables (Legendre et Gallagher, 2001). 8.4.2.3.1 Application Cet exemple dâ€™application concerne des donnÃ©es dâ€™abondance. Nous allons consÃ©quemment utiliser une CCA avec la fonction cca, toujours avec le module vegan. Les tableaux doubs_fish et doubs_env comprennent respectivement des donnÃ©es dâ€™abondance dâ€™espÃ¨ces de poissons et dans diffÃ©rents environnements de la riviÃ¨re Doubs (Europe) publiÃ©es dans Verneaux. (1973) et exportÃ©es du module ade4. data(&quot;doubs&quot;) doubs_fish &lt;- doubs$fish doubs_env &lt;- doubs$env Sur le site no 8, aucun poisson nâ€™a pas Ã©tÃ© observÃ©. Les observations ne comprenant que des zÃ©ro doivent Ãªtre prÃ©alablement retirÃ©es. tot_spec &lt;- doubs_fish %&gt;% transmute(tot_spec = apply(., 1, sum)) doubs_fish &lt;- doubs_fish %&gt;% filter(tot_spec != 0) doubs_env &lt;- doubs_env %&gt;% filter(tot_spec != 0) De la mÃªme maniÃ¨re quâ€™avec la fonction rda de vegan, nous utilisons cca pour lâ€™ACC. doubs_cca &lt;- cca(doubs_fish ~ ., data = doubs_env, scale = FALSE) Comparons les rÃ©sultats par(mfrow = c(1, 2)) ordiplot(doubs_cca, scaling = 1, type = &quot;text&quot;, main = &quot;CCA - Scaling 1 - Triplot de distance&quot;) ordiplot(doubs_cca, scaling = 2, type = &quot;text&quot;, main = &quot;CCA - Scaling 2 - Triplot de corrÃ©lation&quot;) Triplot de distance (scaling 1). La projection des variables rÃ©ponse Ã  angle droit sur les variables explicatives est une approximation de la rÃ©ponse sur lâ€™explication. (2) Un objet (site ou rÃ©ponse) situÃ© prÃ¨s dâ€™une variable explicative est plus susceptible dâ€™avoir le dÃ©compte 1. (3) Les distances entre les variables (rÃ©ponse et explicatives) approximent la distance du \\(\\chi^2\\) (traduction adaptÃ©e de Borcard et al. (2011)). Triplot de corrÃ©lation (scaling 2). La valeur optmiale de lâ€™espÃ¨ce sur une variable environnementale quantitative peut Ãªtre obtenue en projetant lâ€™espÃ¨ce Ã  angle droit sur la variable. (2) Une espÃ¨ce se trouvant prÃ¨s dâ€™une variable environnementale est susceptible de se trouver en plus grande abondance aux sites de statut 1 pour cette variable. (3) Les distances nâ€™approximent pas la distance du \\(\\chi^2\\) (traduction adaptÃ©e de Borcard et al. (2011)). "]
]
