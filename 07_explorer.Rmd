--- 
site: bookdown::bookdown_site
output: bookdown::gitbook
---

# Explorer R {#chapitre-explorer}

L'apprentissage de R peut √™tre √©tourdissant. Cette section est une petite pause fourre-tout qui vous introduira aux nombreuses possibilit√©s de R.

***
Ô∏è\ **Objectifs sp√©cifiques**:

√Ä la fin de ce chapitre, vous

- serez en mesure d'identifier les sources d'information principales sur le d√©veloppement de R et de ses modules
- comprendrez l'importance du pr√©traitement des donn√©es, en particulier dans le cadre de l'analyse de donn√©es compositionnelles, et saurez effectuer un pr√©traitement ad√©quat
- saurez comment acqu√©rir des donn√©es m√©t√©o d'Environnement Canada avec le module [weathercan](http://ropensci.github.io/weathercan/)
- saurez identifier les modules d'analyse de sols ([soiltexture](https://github.com/julienmoeys/soiltexture) et [aqp](http://ncss-tech.github.io/aqp/docs/))
- saurez comment d√©buter un projet de m√©ta-analyse et de d√©ploiement d'un logiciel sur R

***

Pour certains, le langage R est un labyrinthe. Pour d'autres, c'est une myriade de portes ouvertes. Si vous lisez ce manuel, vous vous √™tes peut-√™tre engag√© dans un labyrinthe dans l'objectif d'y trouver la cl√© qui d√©v√©rouillera une porte bien pr√©cise qui m√®ne √† un tr√©sor, un objet magique... ou un dipl√¥me. Peut-√™tre aussi prendrez-vous le go√ªt d'errer dans ce labyrinthe, explorant ses d√©bouch√©s, pour y d√©nicher au hasard des petits outils et des d√©bouch√©s.

```{r expl-zelda, out.width="50%", fig.align="center", fig.cap="S√©quence du jeu vid√©o *The legend of Zelda*, source inconnue.", echo = FALSE}
knitr::include_graphics("images/06_zelda.gif")
```

Cette section est un amalgame de plusieurs outils de R pertinents en analyse √©cologique.

## R sur le web

Dans un environnement de travail en √©volution rapide et constante, il est difficile de consid√©rer que ses comp√©tences sont abouties. Rester inform√© sur le d√©veloppement de R vous permettra de d√©nicher de r√©soudre des probl√®mes persistants de mani√®re plus efficace ou par de nouvelles avenues, et vous offrira m√™me l'occasion de d√©nicher des probl√®mes dont vous ne soup√ßonniez pas l'existance.  Plusieurs sources d'information vous permettront de vous tenir √† jour sur le d√©veloppement de R, de ses environnement de travail (RStudio, Jupyter, Atom, etc.) et des nouveaux modules qui s'y greffent. Plus largement, vous gagnerez √† vous informer sur les derni√®res tendances en calcul scientifique sur d'autres plate-forme que R (Python, Javascript, Julia, etc.). √âvidemment, nos t√¢ches quotidiennes ne nous permettent pas de tout suivre. M√™me si vous pouviez n'attrapper qu'1% du d√©filement, ce sera d√©j√† 1% de plus que rien du tout.

> √âvidemment, rester au courant aide parce que vous en apprenez davantage sur les outils et leurs applications. Mais √ßa aide aussi parce que √ßa vous permet de conna√Ætre des gens et des organisations! Il est tr√®s utile de savoir qui travaille sur quoi et o√π se d√©roulent les d√©veloppements sur un sujet donn√©, car si vous cherchez consciemment quelque chose plus tard, √ßa vous aidera √† trouver votre chemin plus facilement. - Ma√´lle Salmon, [Keeping up to date with R news](https://masalmon.eu/2019/01/25/uptodate/) (ma traduction)

Je vous propose une liste de ressources. Ne vous y tenez surtout pas: discartez ce qui ne vous convient pas, et partez √† l'aventure!

```{r expl-hobbit, out.width="70%", fig.align="center", fig.cap="Tir√© du film *The Hobbit: An Unexpected Journey*, de Peter Jackson (2012).", echo = FALSE}
knitr::include_graphics("images/06_hobbit.gif")
```

### GitHub

Nous avons vu chapitre \@ref(chapitre-git) l'importance d'utilser des outils d'archivage et de suivi de version, comme *git*, dans le d√©ploiement de la *science ouverte*. En effet, [GitHub](https://www.github.com) est une plate-forme *git* sur Internet, acquise par Microsoft, qui est devenue un r√©seau social de d√©veloppement informatique. [De nombreux modules de R y sont d√©velopp√©s](https://github.com/topics/r). Au chapitre \@ref(chapitre-git), vous avez appris √† y ouvrir un compte et √† y archiver du contenu. Vous pourrez alors **suivre** (dans le m√™me sens que sur d'autres r√©seaux sociaux) le d√©veloppement de projets et suivre les travaux des personnes qui vous semblent d'int√©r√™t.

### Twitter

Le *hashtag* `#rstats` rassemble sur [Twitter](https://twitter.com/hashtag/rstats?src=hash) ce qui se tweete sur le sujet. On y retrouve les comptes de [R-bloggers](https://twitter.com/Rbloggers), [RStudio](https://twitter.com/rstudio) et [rOpenSci](https://twitter.com/rOpenSci). Certaines communaut√©s y sont aussi actives, comme [R4DS online learning community](https://twitter.com/R4DScommunity), qui partage des nouvelles sur R, et [R-Ladies Global](https://twitter.com/RLadiesGlobal), qui vise √† amener davantage de diversit√© √† la communaut√© de R. Des comptes th√©matiques comme [Daily R Cheatsheets](https://twitter.com/daily_r_sheets), [R for the Rest of Us](https://twitter.com/rfortherest) et [One R Package a Day](https://twitter.com/RLangPackage) permettent de d√©couvrir quotidiennement de nouvelles possibilit√©s. Enfin, plusieurs personnes contribuent positivement √† la communaut√© R. [Hadley Wickham](https://twitter.com/hadleywickham) brille parmi les √©toiles de R. Les comptes de [Mara Averick](https://twitter.com/dataandme), [Claus Wilke](https://twitter.com/ClausWilke) et [David Robinson](https://twitter.com/drob) sont aussi int√©ressants. Enfin, [Thomas Lin Pedersen](https://twitter.com/thomasp85), qui travaille en visualisation chez RStudio, cr√©e des oeuvres d'art g√©n√©ratif avec R et **`ggplot2`**.

```{r expl-thomas-lin-pedersen, out.width="70%", fig.align="center", fig.cap="genesis338, une oeuvre de [Thomas Lin Pedersen](https://www.data-imaginist.com/art) cr√©√©e avec **`ggplot2`**.", echo = FALSE}
knitr::include_graphics("https://www.data-imaginist.com/art/005_genesis/genesis338.png")
```

### Nouvelles

Le site d'aggr√©gation [R-bloggers](https://www.r-bloggers.com/), mis √† jour quotidiennement, republie des articles en anglais tir√©s d'un peu partout sur la toile. On y trouve principalement des tutoriels et des annonces de nouveaux d√©veloppement. Deux fois par mois, l'organisation [rOpenSci](https://news.ropensci.org/) offre un portrait de l'univ-R (#dadjoke), ce que [R Weekely](https://rweekly.org/) offre de mani√®re hebdomadaire (l'information sera probablement redondante). Le tidyverse a quant √† lui son propre [blogue](https://www.tidyverse.org/articles/).

### Des questions?

Bien que davantage vou√©s √† la r√©solution de probl√®me qu' √† l'exploration de nouvelles opportunit√©s, [Stackoverflow](https://stackoverflow.com/questions/tagged/r) et [Cross Validated](https://stats.stackexchange.com/questions/tagged/r) sont des plate-forme pris√©es. De plus, la liste de courriels [r-sig-ecology](https://www.mail-archive.com/r-sig-ecology@r-project.org/info.html) permet des √©changes entre professionnels et novices en analyse de donn√©es √©cologiques avec R.

### Participer

R est un logiciel bas√© sur une communaut√© de d√©veloppement, d'utilisation et de vulgarisation. Des personnes offrent g√©n√©reusement du temps de support. Si vous vous sentez √† l'aise, offrez aussi le v√¥tre!

### Mise en garde

Les modules de R sont d√©velopp√©s par quiconque le veut bien: leur qualit√© n'est pas n√©cessairement audit√©e. Souvent, ils ne sont v√©rifi√©s que par une vigilance communautaire: dans ce cas, vous √™tes les cobailles. Ce qui n'est pas n√©cessairement une mauvaise chose, mais cela n√©cessite de prendre ses pr√©cautions. Dans sa conf√©rence [How to be a resilient R user](https://maelle.github.io/fluctuat_nec_mergitur), [Ma√´lle Salmon](https://twitter.com/ma_salmon) propose quelques guides pour juger de la qualit√© d'un module.

**1. Le module est-il activement d√©velopp√©?**

Bien!

![](images/06_2019-01-14-facebook-prophet.png)

Attention!

![](images/06_2019-01-14_mlammens_meteR.png)

**2. Le module est-il bien test√©?**

V√©rifiez si le module a fait l'objet d'une publication scientifique, s'il a √©t√© utilis√© avec succ√®s dans la litt√©rature ou dans des documents cr√©dibles.

**3. Le module est-il bien document√©?**

Un site internet d√©di√© est-il utilis√© pour documenter l'utilisation du module? Les fichiers d'aide sont-ils complets, et sont-ils de bonne qualit√©?

**4. Le module est-il largement utilis√©?**

Un module peu populaire n'est pas n√©cessessairement de mauvaise qualit√©: peut-√™tre est-il seulement destin√© √† des applications de niche. S'il n'est pas un indicateur √† lui seul de la solidit√© ou la validit√© d'un module, une masse critique indique que le module a pass√© sous la surveillance de plusieurs utilisateurs. Dans GitHub, ceci peut √™tre √©valu√© par le nombre d'√©toiles attribu√© au module (√©quivalent √† un J'aime).

![](images/06_peu-etoiles.png) ![](images/06_bcp-etoiles.png)

**5. Le module est-il d√©velopp√© par une personne ou une organisation cr√©dible?**

On peut affirmer sans trop se compromettre que l'√©quipe de RStudio d√©veloppe des modules de confiance. Tout comme il faudrait se m√©fier d'un module d√©velopp√© par une personne anonyme.

Le module [packagemetrics](https://github.com/ropenscilabs/packagemetrics) permet d'√©valuer ces crit√®res.

```{r expl-package_list_metrics, eval = FALSE}
# devtools::install_github("ropenscilabs/packagemetrics")
library("packagemetrics")
pm <- package_list_metrics(c("dplyr", "ggplot2", "vegan", "greta"))
metrics_table(pm)
```

### Prendre tout √ßa en note

Un logiciel de prise de notes (il en existe plein, mais je vous sugg√®re d'opter pour les options encrypt√©es) pourrait vous √™tre utile pour retrouver l'information soutir√©e de vos flux d'information. Mais certaines personnes consignent simplement leurs informations dans un carnet ou un document de traitement de texte.

## R en chaire et en os

L'Universit√© Laval (institution aupr√®s de laquelle ce manuel est d√©velopp√©) est h√¥te √† tous les 2 ans de la conf√©rence [R √† Qu√©bec](http://raquebec.ulaval.ca). La prochaine conf√©rence aura lieu en 2021.

## Quelques outils en √©cologie math√©matique avec R

### Pr√©traitement des donn√©es

Il arrive souvent ques les donn√©es brutes ne soient pas exprim√©es de mani√®re appropri√©e ou optimale pour l'analyse statistique ou la mod√©lisation. Vous devrez alors effectuer un pr√©traitement sur ces donn√©es. Lors du chapitre \@ref(chapitre-biostats), nous avons abord√© la mise √† l'√©chelle, o√π des variables num√©riques √©taient transform√©es pour avoir une moyenne de z√©ro et un √©cart-type de 1. Cette op√©ration permettait d'appr√©cier les coefficients et leur incertitude sur une m√™me √©chelle. L'encodage cat√©gorielle a quant √† lui permi d'utiliser des m√©thodes quantitatives sur des donn√©es qualitatives. Dans les deux cas, nous n'avons pas utilis√© le terme, mais il s'agissait d'un **pr√©traitement**, c'est-√†-dire une transformation des donn√©es pr√©alable √† l'analyse ou la mod√©lisation.

Un pr√©traitement peut consister simplement en une transformation logarithmique ou exponentielle. Nous verrons les transformations les plus communes comme la standardisation, la mise √† l'√©chelle sur une √©tendue et la normalisation. Puis nous verrons comment ces op√©rations de pr√©traitement sont offertes dans le module [**`recipes`**](https://tidymodels.github.io/recipes/).

**`recipes`** n'est pas en mesure d'effectuer toutes les transformations imaginables. Pour des op√©rations plus sp√©cialis√©es, si vos donn√©es forment une partie d'un tout (exprim√©es en pourcentages ou fractions), vous devriez probablement utiliser un pr√©traitement gr√¢ce aux outils de l'**analyse compositionnelle**. Avant de les aborder, nous allons traiter des transformations de base.

#### Standardisation

La standardisation consiste √† centrer vos donn√©es √† une moyenne de 0 et √† les √©chelonner √† une variance de 1, c'est-√†-dire

$$x_{standard} = \frac{x - \bar{x}}{\sigma}$$

o√π $\bar{x}$ est la moyenne du vecteur $x$ et o√π $\sigma$ est son √©cart-type.

Ce pr√©traitement des donn√©es peut s'av√©r√©r utile lorsque la mod√©lisation tient compte de l'√©chelle de vos mesures (par exemple, les param√®tres de r√©gression vus au chapitre \@ref(chapitre-biostats) ou les distances que nous verrons au chapitre \@ref(chapitre-ordination)). En effet, les pentes d'une r√©gression lin√©aire multiple ne pourront √™tre compar√©es entre elles que si elles sont une m√™me √©chelle. Par exemple, on veut mod√©liser la consommation en miles au gallon (`mpg`) de voitures en fonction de leur puissance (`hp`), le temps en secondes pour parcourir un quart de mile (`qsec`) et le nombre de cylindre.

```{r expl-scale1}
data("mtcars")
modl <- lm(mpg ~ hp + qsec + cyl, mtcars)
summary(modl)
```

Les pentes signifient que la distance parcourue par gallon d'essence diminue de 0.03552 miles au gallon pour chaque HP, de 0.89242 par seconde au quart de mile et de 2.2696 par cyclindre additionnel. L'interpr√©tation est conviviale √† cette √©chelle. Mais lequel de ces effets est le plus important? L `t value` indique que ce seraient les cylindres. Mais pour juger l'importance en terme de pente, il vaudrait mieux standardiser.

```{r expl-scale2}
library("tidyverse")
standardise <- function(x) (x-mean(x))/sd(x)
mtcars_sc <- mtcars %>%
  mutate_if(is.numeric, standardise) # ou bien scale(mtcars, center = TRUE, scale = TRUE)
modl_sc <- lm(mpg ~ hp + qsec + cyl, mtcars_sc)
summary(modl_sc)
```

Les valeurs des pentes ne peuvent plus √™tre interpr√©t√©es directement, mais peuvent maintenant √™tre compar√©es entre elles. Dans ce cas, le nombre de cilyndres a en effet une importance plus grande que la puissance et le temps pour parcourir un 1/4 de mile.

Les algorithmes bas√©s sur des distances auront, de m√™me, avantage √† √™tre standardis√©s.

#### √Ä l'√©chelle de la plage

Si vous d√©sirez pr√©server le z√©ro dans le cas de donn√©es positives ou plus g√©n√©ralement vous voulez que vos donn√©es pr√©trait√©es soient positives, vous pouvez les transformer √† l'√©chelle de la plage, c'est-√†-dire les forcer √† s'√©taler de 0 √† 1:

$$ x_{range01} = \frac{x - x_{min}}{x_{max} - x_{min}}  $$

Cette transformation est sensible aux valeurs aberrantes, et une fois le vecteur transform√© les valeurs aberrantes seront toutefois plus difficiles √† d√©tecter.

```{r expl-scale-range}
range_01 <- function(x) (x-min(x))/(max(x) - min(x))
mtcars %>%
  mutate_if(is.numeric, range_01) %>% # en fait, toutes les colonnes sont num√©riques, alors mutate_all aurait pu √™tre utilis√© au lieu de mutate_if
  sample_n(4)
```

#### Normaliser

Le terme *normaliser* est associer √† des op√©rations diff√©rentes dans la litt√©rature. Nous prendrons la nomenclature de [scikit-learn](https://scikit-learn.org/stable/modules/preprocessing.html#normalization), pour qui la normalisation consiste √† faire en sorte que la longueur du vecteur (sa norme, d'o√π *normaliser*) soit unitaire. Cette op√©ration est le plus souvent utilis√©e par observation (ligne), non pas par variable (colonne). Il existe plusieurs mani√®res de mesures la distance d'un vecteur, mais la plus commune est la distance euclidienne. La seule fois que j'ai eu √† utiliser ce pr√©traitement √©tait en analyse spectrale ([Chemometrics with R, Ron Wehrens, 2011, chapitre 3.5](https://www.springer.com/us/book/9783642178405#otherversion=9783642178412)). En R,

```{r expl-scale-norm}
library("pls")
data("gasoline")
spectro <- gasoline$NIR %>% unclass() %>% as_tibble()

normalise <- function(x) x/sqrt(sum(x^2))
spectro_norm <- spectro %>% 
  rowwise() %>% # diff√©rentes approches possibles pour les op√©rations sur les lignes
  normalise()
spectro_norm[1:4, 1:4]
```

#### Le module **`recipes`**

Nous avons vu comment standardiser avec notre propre fonction. Certaines personnes pr√©f√®rent utiliser la fonction `scale()`. Mais une nouvelle approche est en train de s'installer, avec le module **`recipes`**, un module de l'ombrelle **`tidymodels`**, un m√©ta module en d√©veloppement visant √† faire de R un outil de mod√©lisation plus convivial.

**`recipes`** fonctionne en mode *tidyverse*, c'est-√†-dire en suites d'op√©rations. [De nombreuses fonctions](https://tidymodels.github.io/recipes/reference/index.html) sont offertes, dont des fonctions d'imputation, que nous verrons au chapitre \@ref(chapitre-outliers). Nous couvrirons ici la standardisation et la mise √† l'√©chelle, juste pour l'ap√©ro üç≥.

Le module ne s'appelle pas *recette* pour rien. Il fonctionne en trois √©tapes: 

1. Monter la liste des ingr√©dients: sp√©cifier ce qu'il faut faire
1. M√©langer les ingr√©dients: transformer tout ce qu'il faut faire en une proc√©dure
1. Cuire les ingr√©dients: appliquer la proc√©dure √† un tableau.

Voici une petite application sur le tableau `lasrosas.corn`.

```{r expl-load-recipes}
library("tidymodels")
data(lasrosas.corn, package = "agridat")
lasrosas.corn %>% 
  head()
```

Disons que pour mon mod√®le statistique, ma variable de sortie est le rendement (`yield`), que je d√©sire lier √† la dose d'azote (`nitro`), √† un indicateur de la teneur en mati√®re organique du sol (`bv`) et √† la topographie (`topo`).

Mais pour rendre le mod√®le pr√©dictif (et non pas seulement descriptif), je dois l'√©valuer sur des donn√©es qui n'ont pas servies √† lisser le mod√®le (nous verrons en plus de d√©tails √ßa au chapitre \@ref(chapitre-ml)). Je vais donc s√©parer mon tableau au hasard en un tableau d'entra√Ænement comprenant 70% des observations et un autre pour tester le mod√®le comprenant le 30% restant.

```{r expl-split}
train_test_split <- lasrosas.corn %>% 
  select(yield, nitro, bv, topo) %>% 
  initial_split(prop = 0.7)
train_df <- training(train_test_split)
test_df <- testing(train_test_split)
```

Voici ma recette. Je l'expliquerai tout de suite apr√®s.

```{r expl-recette}
recette <- recipe(yield ~ ., data = train_df) %>% 
  step_zv(all_numeric()) %>% 
  step_normalize(all_numeric(), -all_outcomes()) %>% 
  step_downsample(topo) %>%
  step_dummy(topo) %>% 
  prep()
```

La recette peut se baser sur l'interface formule - souvenez-vous que le `.` signifie "toutes les autres variables du tableau". Elle se base toujours sur le jeu d'entra√Ænement. La prochaine √©tape (souvenez-vous que `%>%` signifie "puis" ou "ensuite") consiste √† retirer les variables dont la variance est non-nulle, ce qui est pratique pour √©viter que la standardisation divise par $0$. Cette √©tape est appliqu√©e √† toutes les variables num√©riques `all_numeric()`. Ensuite, je standardise avec la fonction `step_normalize()` - il y a beaucoup de confusion entre la notion de standardisation et de normalisation dans les fonctions comme dans la litt√©rature. Dans cette fonction, je sp√©cifie que la standardisation n'est applicable que sur les entr√©es num√©riques du mod√®le (`all_numeric(), -all_outcomes()`). L'√©tape `step_downsample()` retire des obsservations pour faire en sorte que les cat√©gories d'une variable apparaissent toutes en m√™me nombre. Bien que l'interface-formule de R s'en occupe automatiquement, je sp√©cifie ensuite que je d√©sire que la variable `topo` subisse un enodage cat√©goriel. Puis je m√©lange mes ingr√©dients avec la fonction `prep()`. J'obtiens un objet de type recette.

```{r expl-show-recipe}
recette
```

La recette √©tant bien m√©lang√©e, on peut en extraire le jus avec la fonction `bake()`, qui permet de g√©n√©rer le tableau transform√©.

```{r expl-bake-test}
test_proc <- bake(recette, test_df)
test_proc %>% sample_n(5)
```

La fonction `bake()` peut aussi √™tre appliqu√©e au donn√©es d'entra√Ænement, mais certaines √©tapes de recette doivent passer par des op√©rations particuli√®res, comme `step_downsample()` Il est donc pr√©f√©rable, pour les donn√©es d'entr√¢inement, d'en extraire le jus avec la fonction `juice()`.

```{r expl-bake-train}
train_proc <- bake(recette, train_df)
train_proc %>% sample_n(5)
```

Le tableau `train_proc` peut √™tre envoy√© dans un mod√®le de votre choix! Par exemple,

```{r expl-lm-baked-train}
lm(yield ~ ., train_proc) %>% 
  summary()
```

#### Analyse compositionnelle en R

En 1898, le statisticien Karl Pearson nota que des corr√©lations √©taient induites lorsque l'on effectuait des ratios par rapport √† une variable commune.

![Karl Pearson, 1897](images/06_pearson1897.png)
Source [Karl Pearson, 1897. Mathematical contributions to the theory of evolution.‚Äîon a form of spurious correlation which may arise when indices are used in the measurement of organs. Proceedings of the royal society of London](https://royalsocietypublishing.org/doi/pdf/10.1098/rspl.1896.0076)

Faisons l'exercice! Nous g√©n√©rons au hasard 1000 donn√©es (comme le proposait Pearson) pour trois dimensions: le f√©mur, le tibia et l'hum√©rus. Ces dimensions ne sont pas g√©n√©r√©es par des distributions corr√©l√©es.

```{r expl-coda-bones}
set.seed(3570536)
n <- 1000
bones <- tibble(femur = rnorm(n, 10, 3),
                tibia = rnorm(n, 8, 2),
                humerus = rnorm(n, 6, 2))
plot(bones)
cor(bones)
```

Pourtant, si j'utilise des ratios allom√©triques avec l'hum√©rus comme base,

```{r expl-bones-allo}
bones_r <- bones %>% 
  transmute(fh = femur/humerus,
            th = tibia/humerus)
plot(bones_r)
text(30, 20, paste("corr√©lation =", round(cor(bones_r$fh, bones_r$th), 2)), col = "blue")
```

Nous avons induit ce que Pearson appelait une fausse corr√©lation (*spurious correlation*). En 1960, [Chayes](https://doi.org/10.1029/JZ065i012p04185) proposa que de telles fausses corr√©lations sont induites non seulement sur des ratios de valeurs absolues, mais aussi sur des ratios d'une somme totale. Par exemple, dans une composition simple de deux types d'utilisation du territoire, si une proportion augmente, l'autre doit n√©cessairement diminuer.


```{r expl-bones-corr}
n <- 100
tibble(A = runif(n, 0, 1)) %>% 
  mutate(B = 1 - A) %>% 
  ggplot(aes(x=A, y=B)) +
  geom_point()
```

Les variables exprim√©es relativement √† une somme totale sont dites *compositionnelles*. Elles poss√®dent les caract√©ristiques suivantes.

1. **Redondance d'information**. Un syst√®me de deux proportions ne contient qu'une seule variable du fait que l'on puisse d√©duire l'une en soutrayant l'autre de la somme totale. Un vecteur compositionnel contient de l'information redondante. Pourtant, effectuer des statistiques sur l'une plut√¥t que sur l'autre donnera des r√©sultats diff√©rents.
2. **D√©pendance d'√©chelle**. Les statistiques devraient √™tre ind√©pendantes de la somme totale utilis√©e. Pourtant, elles diff√©reront sur l'on utilise par exemple, une proportion des m√¢les d'une part et des femelles d'autre part, ou la proportion de la somme des deux, de m√™me que les r√©sultats d'un test sanguin diff√©rera si l'on utilise une base s√®che ou une base humide.
3. **Distribution th√©orique des donn√©es**. √âtant donn√©e que les proportions sont confin√©es entre 0 et 1 (ou 100%, ou une somme totale quelconque), la distribution normale (qui s'√©tend de -‚àû √† +‚àû) n'est souvent pas appropri√©e. On pourra utiliser la distribution de Dirichlet ou la distribution logitique-normale, mais d'autres approches sont souvent plus pratiques.

Pour illustrer l'effet de la distribution, voyons un diagramme ternaire incluant le sable, le limon et l'argile. En utilisant des √©cart-types univari√©s, nous obtenons l'ellipse en rouge, qui non seulement repr√©sente peu l'√©talement des donn√©es, mais elle d√©passe les bornes du triangle, admettant ainsi des proportions n√©gatives. En bleu, la distribution logistique normale (issue des m√©thodes pr√©sent√©es plus loin dans cette section) convient davantage.

![](images/06_ternaire-sd.png)

Les cons√©quences d'effectuer des statistiques lin√©aires sur des donn√©es compositionnelles brutes peuvent √™tre majeures. En outre, [Pawlowksy-Glahn et Egozcue (2006)](http://dx.doi.org/10.1144/GSL.SP.2006.264.01.01), s'appuyant en outre sur Rock (1988), note les probl√®mes suivants (exprim√©s en mes mots).

1. les r√©gressions, les regroupements et les analyses en composantes principales peuvent avoir peu ou pas de signification
2. les propri√©t√©s des distributions peuvent √™tre g√©n√©r√©es par l'op√©ration de fermeture de la composition (s'assurer que le total des proportions donne 100%)
3. les r√©sultats d'analyses discriminantes lin√©aires sont propices √† √™tre illusoires
4. tous les coefficients de corr√©lation seront affect√©s √† des degr√©s inconnus
5. les r√©sultats des tests d'hypoth√®ses seront intrins√®quement fauss√©s

Pour contourner ces probl√®mes, il faut d'abord aborder les donn√©es compositionnelles pour ce qu'elles sont: des donn√©es intrins√®quement multivari√©es. Elles sont un nuage de point, et non pas une collection de variables individuelles. Ceci qui n'emp√™che pas d'effectuer des analyses consciencieusement sous des angles particuliers. 

En R, on pourra ais√©ment rapporter une composition en somme unitaire gr√¢ce √† la fonction `apply`. Mais auparavant, chargeons le module `compositions` (n'oubliez pas de l'installer au pr√©alable) pour acc√©der √† des donn√©es fictives de proportions de sable, limon et argile dans des s√©diments.

```{r expl-coda-arcticlake, message=FALSE, warning=FALSE}
library("compositions")
data("ArcticLake")
ArcticLake <- ArcticLake %>% as_tibble()
head(ArcticLake)
```

```{r expl-coda-acomp1}
comp <- ArcticLake %>%
  select(-depth) %>%
  apply(., 1, function(x) x/sum(x)) %>% 
  t()
comp[1:5, ]
```

On pourra aussi utiliser la fonction `acomp` (pour Aitchison-composition) pour fermer la composition √† une somme de 1.

```{r expl-coda-acomp2}
comp <- ArcticLake %>%
  select(-depth) %>%
  acomp(.)
comp[1:5, ]
```

Cette strat√©gie a pour avantage d'attribuer √† la variable `comp` la classe `acomp`, qui automatise les op√©rations dans l'espace compositionnel (que l'on nomme aussi le *simplex*). La repr√©sentation ternaire est souvent utilis√©e pour pr√©senter des compositions. Toutefois, il est difficile d'interpr√©ter les compositions de plus de trois parties. La classe `acomp` automatise aussi la repr√©sentation teranaire.

```{r expl-coda-tern}
plot(comp)
```

Afin de transposer cet espace cl√¥t en un espace ouvert, on pourra diviser chaque proportion par une proportion de r√©f√©rence choisie parmi n'importe quelle proportion. Du coup, on retire une dimension redondante! Dans ce ratio, on choisit d'utiliser la proportion de r√©f√©rence au d√©nominateur, ce qui est arbitraire. En utilisant le log du ratio, l'inverse du ratio ne sera qu'un changement de signe, ce qui est pratique en statistiques lin√©aries. Cette solution, propos√©e par Aitchison (1986), s'applique non seulement sur les compositions √† deux composantes, mais sur toute composition. Il s'agit alors d'utiliser une composition de r√©f√©rence pour effecteur les ratios. Pour une composition de $A$, $B$, $C$, $D$ et $E$:

$$alr_A = log \left( \frac{A}{E} \right), alr_B = log \left( \frac{B}{E} \right), alr_C = log \left( \frac{C}{E} \right), alr_D = log \left( \frac{D}{E} \right)$$

Dans R, la colonne de r√©f√©rence est par d√©faut la **derni√®re colonne de la matrice des compositions**.

```{r expl-alr}
add_lr <- alr(comp)
```

Cette derni√®re strat√©gie se nomme les **log-ratios aditifs** ($alr$ pour *additive log-ratio*). Bien que valide pour effectuer des tests statistiques, cette strat√©gie a le d√©savantage de d√©pendre de la d√©cision arbitraire de la composante √† utiliser au num√©rateur. Deuxi√®me restriction des *alr*: les axes de l'espace des *alr* n'√©tant pas orthogonaux, ils ne peuvent pas √™tre utilis√©s pour effectuer des statistiques bas√©es sur les distances (que nous couvrirons au chapitre \@ref(chapitre-ordination)).

L'autre strat√©gie propos√©e par Aitchison √©tait d'effectuer un log-ratio entre chaque composante et la moyenne g√©om√©trique de toutes les composantes. Cette transformation se nomme le **log-ratio centr√©** ($clr$, pour *centered log-ratio*)

$$clr_i = log \left( \frac{x_i}{g \left( x \right)} \right)$$

En R,

```{r expl-clr}
cen_lr <- clr(comp)
```

Avec des CLRs, les distances sont valides. Mais... nous restons avec le probl√®me de la redondance d'information. En fait, la somme de chacunes des lignes d'une matrice de clr est de 0. Pas tr√®s pratique lorsque l'on effectue des statistiques incluant une inversion de la matrice de covariance (distance de Mahalanobis, g√©ostatistiques, etc.)

```
cen_lr %>% 
  cov() %>% 
  solve()
 Error in solve.default(.) : le syst√®me est num√©riquement singulier : conditionnement de la r√©ciproque = 4.44407e-17
```

Enfin, une autre m√©thode de transformation d√©velopp√©e par Egoscue et al. (2003), les log-ratios isom√©triques (ou *isometric log-ratios, ilr*) projette les compositions comprenant D composantes dans un espace restreint de D-1 dimensions orthonorm√©es. Ces dimensions doivent doivent √™tre pr√©alablement √©tablie dans un dendrogramme de bifurcation, o√π chaque composante ou groupe de composante est successivement divis√© en deux embranchement. La mani√®re d'arranger ces balances importe peu, mais on aura avantage √† cr√©er des balances interpr√©tables.

Le diagramme de balances peut √™tre encod√© dans une partition binaire s√©quentielle (ou *sequential bianry partition, sbp*). Une *sbp* est une matrice de contraste ou chaque ligne repr√©sente une partition entre deux variables ou groupes de variables. Une composante √©tiquett√©e `+1` correspondra au groupe du num√©rateur, une composante √©tiquett√©e `-1` au d√©nominateur et une composante √©tiquett√©e `0` sera exclue de la partition ([Parent et al., 2013](http://dx.doi.org/10.3389/fpls.2013.00039)). J'ai reformul√© la fonction CoDaDendrogram pour que l'on puisse ajouter des informations int√©ressantes sur les balants horizontaux. Cette fonction est disponible sur github.

```{r expl-sbp}
source("https://raw.githubusercontent.com/essicolo/AgFun/master/codadend2.R")

sbp <- matrix(c(1, 1,-1,
                1,-1, 0),
              byrow = TRUE,
              ncol = 3)

CoDaDendrogram2(comp, V = gsi.buildilrBase(t(sbp)), ylim = c(0, 1),
                equal.height = TRUE)

```

Si la SBP est plus imposante, il pourrait √™tre plus ais√© de monter dans un chiffrier, puis de l'importer dans R via un fichier csv.

Le calcul des ILRs est effectu√© comme suit.

$$ilr_j = \sqrt{\frac{n_j^+ n_j^-}{n_j^+ + n_j^-}} log \left( \frac{g \left( c_j^+ \right)}{g \left( c_j^+ \right)} \right)$$

ou, √† la ligne $j$ de la SBP, $n_j^+$ et $n_j^-$ sont respectivement le nombre de composantes au num√©rateur et au d√©nominateur, $g \left( c_j^+ \right)$ est la moyenne g√©om√©trique des composantes au num√©rateur et $g \left( c_j^- \right)$ est la moyenne g√©om√©trique des composantes au d√©nominateur.

Les balances sont conventionnellement not√©es `[A,B | C,D]`, ou les composantes `A` et `B` au d√©nominateur sont balanc√©es avec les composantes `C` and `D` au num√©rateur. Une balance positive signifie que la moyenne g√©om√©trique des concentrations au num√©rateur est sup√©rieur √† celle au d√©nominateur, et inversement, alors qu'une balance nulle signifie que les moyennes g√©om√©triques sont √©gales (√©quilibre). Ainsi, en mod√©lisation lin√©aire, un coefficient positif sur `[A,B | C,D]` signifie que l'augmentation de l'importance de `C` et `D` comparativement √† `A` et `B` est associ√© √† une augmentation de la variable r√©ponse du mod√®le.

En R,

```{r expl-ilr}
iso_lr <- ilr(comp, V = gsi.buildilrBase(t(sbp)))
```

Notez la forme `gsi.buildilrBase(t(sbp))` est une op√©ration pour obtenir la matrice d'orthonormalit√© √† partir de la SBP.

Les ILRs sont des balances multivari√©es sur lesquelles on pourra effectuer des statistiques lin√©aries. Bien que l'interpr√©tation des r√©sultats comme collection d'interpr√©tations sur des balances univari√©es pourra √™tre affect√©e par la structure de la SBP, ni les statistiques lin√©aires multivari√©es, ni la distance entre les points ne seront affect√©s. En effet, chaque variante de la SBP est une rotation (d'un facteur de 60¬∞) par rapport √† l'origine:

```{r expl-rotation}
source("lib/ilr-rotation-sbp.R")
```

![](images/06_ilr-rotation.png)

Pour les transformations inverses, vous pourrez utiliser les fonctions `alrInv`, `clrInv` et `ilrInv`. Dans tous les cas, si vous tenez √† garder la trace de vos donn√©es dans leur format original, vous aurez avantage √† ajouter √† votre vecteur compositionnel la valeur de remplissage, constitu√© d'un amalgame des composantes non mesur√©es. Par exemple,

```{r expl-npk}
pourc <- c(N = 0.03, P = 0.001, K = 0.01)
acomp(pourc) # vous perdez la trace des proportions originales
```

```{r expl-npk-acomp}
pourc <- c(N = 0.03, P = 0.001, K = 0.01)
Fv <- 1 - sum(pourc)
comp <- acomp(c(pourc, Fv = Fv))
comp
```

```{r expl-npk-ilr}
iso_lr <- ilr(comp) # avec une sbp par d√©faut
ilrInv(iso_lr)
```

Si vos donn√©es font partie d'un tout, je vous recommande chaudement d'utiliser des m√©thodes compositionnelles autant pour l'analyse que la mod√©lisation. Pour en savoir davantage, le livre *Compositional data analysis with R*, de van den Boogart et Tolosana-Delgado, est disponible en format √©lectronique √† la biblioth√®que de l'Universit√© Laval.

Pour aller plus loin, j'ai √©cri un billet √† ce sujet (auquel √† ce jour il manque toujours un cas d'√©tude): [We should use balances and machine learning to diagnose ionomes](https://www.authorea.com/users/23640/articles/281937-we-should-use-balances-and-machine-learning-to-diagnose-ionomes).

### Acqu√©rir des donn√©es m√©t√©o

Une t√¢che commune en √©cologie est de lier des observations √† la m√©t√©o... qui sont rarement collect√©s lors d'exp√©riences. [Environnement Canada](https://meteo.gc.ca/) poss√®de sont r√©seau de stations. Les donn√©es sont disponibles sur internet en libre acc√®s. Vous pouvez chercher des stations, effectuer des requ√™tes et t√©l√©charger des fichiers csv. Pour un petit tableau, la t√¢che est plut√¥t triviale. Mais √ßa devient rapidement laborieux √† mesure que l'on doit rechercher de nombreuses donn√©es.

Le module [weathercan](http://ropensci.github.io/weathercan/), d√©velopp√© par Steffi LaZerte, permet d'effectuer des requ√™tes rapidement √† partir des coordonn√©es de votre site exp√©rimental. Par exemple, si je cherche une station m√©t√©o sfournissant des donn√©es horaires situ√© √† moins de 20 km du sommet du Mont-Bellevue, √† Sherbrooke, aux coordonn√©es [latitude 45.35, longitude -71.90],

```{r expl-weathercan-stations}
library("weathercan")
station_site <- stations_search(coords = c(45.35, -71.90), dist = 20, interval = "hour")
station_site
```

Je prends en note l'identifiant de la station d√©sir√©e (ou des stations, disons 5397 et 48371), puis je lance une requ√™te pour obtenir la m√©t√©o horaire entre les dates d√©sir√©es.

```{r expl-weathercan-data}
mont_bellevue <- weather_dl(station_ids = c(5397, 48371),
                            start = "2019-02-01",
                            end = "2019-02-07",
                            interval = "hour",
                            verbose = TRUE, tz_disp = "Etc/GMT+5")
mont_bellevue %>% head(5)
```

Et voil√†.

```{r expl-weathercan-plot}
mont_bellevue %>% 
  ggplot(aes(x = time, y = temp)) +
  geom_line(aes(colour = station_name))
```

### P√©dom√©trie avec R

*Cette section a √©t√© √©crite par [Michael Leblanc](https://www.researchgate.net/profile/Michael_Leblanc7).* Je n'y ai appliqu√© que quelques retouches esth√©tiques.

Plusieurs fonctionnalit√©s ont √©t√© d√©velopp√©es sur R afin d'aider les *p√©dom√©triciens* √† visualiser, explorer et traiter les donn√©es num√©riques en science des sols. Voici quelques exemples.

#### Texture du sol

La texture du sol est d√©finie par sa composition granulom√©trique, habituellement repr√©sent√©e par trois fractions (sable, limon, argile), laquelle peut √™tre g√©n√©ralis√©e en classe texturale. La d√©finition des classes texturales diff√®re d'un syst√®me ou d'un pays √† l'autre comme en t√©moigne l'article [Perdus dans le triangle des textures (Richer de Forges et al. 2008)](http://www.afes.fr/wp-content/uploads/2017/10/EGS_15_2_richerdeforges.pdf). La d√©finition des fractions granulom√©triques peut √©galement diff√©rer selon le domaine d'√©tude (ing√©nierie, p√©dologie) ou le pays. Par exemple, le diam√®tre du limon est de 0,002 mm √† 0,05 mm dans le syst√®me canadien, am√©ricain et fran√ßais alors qu'il est de 0,002 mm √† 0,02 mm dans le syst√®me australien et de 0,002 mm √† 0,063 mm dans le syst√®me allemand. Il est donc important de v√©rifier la m√©thodologie et le syst√®me de classification utilis√©s pour interpr√©ter les donn√©es de texture du sol. Le module `soilTexture` propose des fonctions permettant d'aborder ces multiples d√©finitions.

```{r expl-sol-soiltexture}
library("soiltexture")
```

##### Les triangles texturaux

Avec la fonction `TT.plot`, vous pouvez pr√©senter vos donn√©es granulom√©triques dans un triangle textural tel que d√©fini par les diff√©rents syst√®mes nationaux. Auparavant, cr√©ons un objet comprenant des textures al√©atoires.

```{r expl-sol-data}
set.seed(848341) # random.org
rand_text <- TT.dataset(n=100, seed.val=29)
head(rand_text)
```

Avec le module soiltexture, les tableaux de texture doivent inclure les intitull√©s exactes CLAY, SILT et SAND (notez les majuscules). Les points des textures g√©n√©r√©es peuvent √™tre port√©s dans des diagrammes ternaires texturaux de diff√©rents syst√®mes de classification, par exemple le syst√®me canadioen et le syst√®me USDA.

```{r expl-sol-tern, fig.width = 16, fig.height = 8}
par(mfrow=c(1, 2))

TT.plot(class.sys = "CA.FR.TT", 
        tri.data = rand_text,
        col = "blue")
TT.plot(class.sys = "USDA.TT", 
        tri.data = rand_text,
        col = "blue")

```

Les param√®tres de la figure (titres, polices, style de la grille, etc.) peuvent √™tre personnalis√©s avec les [arguments TT.plot](https://www.rdocumentation.org/packages/soiltexture/versions/1.5.1/topics/TT.plot).

##### Les classes texturales

La fonction `TT.points.in.classes` est utile pour d√©signer la classe texturale √† partir des donn√©es granulom√©triques, en sp√©cifiant bien le syst√®me de classification d√©sir√©.

```{r expl-sol-classes}
TT.points.in.classes(
  tri.data = rand_text[1:10, ], # 
  class.sys = "CA.FR.TT",
  PiC.type = "t"
)
```

Plusieurs autres fonctions sont propos√©es par `soiltexture` afin de visualiser, classifier et transformer les donn√©es de texture du sol : [Functions in soiltexture](https://www.rdocumentation.org/packages/soiltexture/versions/1.5.1). Julien Moeys (2018) propose √©galement le tutoriel [*The soil texture wizard: a tutorial*](https://cran.r-project.org/web/packages/soiltexture/vignettes/soiltexture_vignette.pdf).

#### Profils de sols

Le profil de sols est une entit√© d√©crite par une s√©quence de couches ou d'horizons avec diff√©rentes caract√©ristiques morphologiques. Le module AQP, pour [*Algorithms for Quantitative Pedology*](http://ncss-tech.github.io/AQP/), propose des fonctions de visualisation, d'agr√©gation et de classification permettant d'aborder la complexit√© inh√©rente aux informations p√©dologiques.

##### La visualisation de profils

Vous devez d'abord structurer vos donn√©es dans un tableau (`data.frame`) incluant minimalement ces trois colonnes :

1. Identifiant unique du profil (groupes d'horizons) (`id`)
2. Limites sup√©rieures de l'horizon (`top`)
3. Limites inf√©rieures de l'horizon (`down`)

Vos donn√©es morphologiques, physico-chimiques, etc., sont incluses dans les autres colonnes. Chargeons un fichier p√©dologique √† titre d'exemple.

```{r expl-sol-pedometrie}
profils <- read_csv("data/06_pedometric-profile.csv")
head(profils)
```

La fonction `munsell2rgb` permet de convertir le code de couleur *Munsell* en format *RGB*.

```{r expl-sol-aqp}
library("aqp")
profils$soil_color <- with(profils, munsell2rgb(hue, value, chroma))
```

Pr√©alablement √† la visualisation, le tableau est transform√© en objet `SoilProfileCollection` par la fonction `depths`. Pour ce faire, le tableau doit √™tre un pur `data.frame`, non pas un `tibble`.

```{r expl-sol-profils}
profils <- profils %>% as.data.frame()
depths(profils) <- id ~ top + bottom
```

La fonction `plot` d√©tectera le type d'objet et appellera la fonction de visualisation en cons√©quence.

```{r expl-sol-profils-plot, fig.width = 12, fig.height = 4}
par(mfrow = c(1, 3))
plot(profils, name="horizon")
title('Couleur des horizons', cex.main=1)
plot(profils, name="horizon", color='C.CNS.pc', col.label='C total (%)')
plot(profils, name="horizon", color='pH.CaCl2', col.label='pH CaCl2')
```

De multiples figures th√©matiques peuvent √™tre g√©n√©r√©es afin de repr√©senter les particuliarit√©s des profils. Pour aller plus loin, consultez les guides [*Introduction to SoilProfileCollection Objects*](http://ncss-tech.github.io/AQP/aqp/aqp-intro.html) et [*Generating Sketches from SPC Objects*](http://ncss-tech.github.io/AQP/aqp/SPC-plotting-ideas.html).

##### Les plans verticaux (depth functions)

Les plans verticaux sont des diagrammes qui permettent d'interpr√©ter les donn√©es en fonction de la profondeur. La fonction `slab` permet le calcul de statistiques descriptives par intervalles de profondeur r√©guliers, lesquelles permettent de visualiser la variabilit√© verticale des propri√©t√©s des sols.

```{r expl-sol-slab}
agg <- slab(profils, fm = ~ C.CNS.pc + pH.CaCl2)
```

La visualisation est g√©n√©r√©e par le module graphique ggplot2

```{r expl-sol-agg-ggplot}
agg %>%
  ggplot(mapping = aes(x = -top, y = p.q50)) +
  facet_grid(. ~ variable, scale = "free") +
  geom_ribbon(aes(ymin =  p.q25, ymax = p.q75), fill = "grey75", alpha = 0.5) +
  geom_path() +
  labs(x = "Profondeur (cm)",
       y = "M√©diane bord√©e des 25e and 75e percentiles") +
  coord_flip()
```

##### Le regroupement de profils

Le calcul des distances de dissimilarit√© entre les profils avec `profile_compare` permet la construction de dendrogramme et le regroupement des profils. Notez que nous survolerons au chapitre \@ref(chapitre-ordination) les concepts de dissimilarit√© et de partitionnement.

```{r expl-sol-cluster}
library("cluster")
library("mvtnorm")
library("sharpshootR") # remotes::install_github("ncss-tech/sharpshootR")
d <- profile_compare(profils, vars=c('C.CNS.pc', 'pH.CaCl2'), k=0, max_d=40)
d_diana <- diana(d)
plotProfileDendrogram(profils, name="horizon", d_diana,
                      scaling.factor = 0.3, y.offset = 5,
                      color='pH.CaCl2',  col.label='pH CaCl2')
```

##### Diagramme de relations entre les horizons

Il est possible de visualiser les transitions d'horizon les plus probables dans un groupe de profils de sols.

```{r expl-sol-relations, fig.width = 10, fig.height = 5}
tp <- hzTransitionProbabilities(profils, name="horizon")
par(mar = c(0, 0, 0, 0), mfcol = c(1, 2))
plot(profils, name="horizon")
plotSoilRelationGraph(tp, graph.mode = "directed", edge.arrow.size = 0.5, edge.scaling.factor = 2, vertex.label.cex = 0.75, 
                      vertex.label.family = "sans")
```

Consultez [AQP project](http://ncss-tech.github.io/AQP/) pour des pr√©sentations, des tutoriels et des exemples de figures qui montrent les nombreuses possibilit√©s du package `AQP`.

### M√©ta-analyses en R

Je conseille les livres [*Introduction to Meta-Analysis*](https://www.wiley.com/en-us/Introduction+to+Meta+Analysis-p-9780470057247), [*Meta-analysis with R*](https://www.springer.com/us/book/9783319214153) et [*Handbook of Meta-analysis in Ecology and Evolution*](https://press.princeton.edu/titles/10045.html) pour les m√©ta-analyses sur des √©cosyst√®mes. Le module metafor est un ioncournable pour effectuer des m√©taanalyses en R. On ne passe pas tout √† fait √† c√¥t√© si l'on utilise le module meta, lui-m√™me bas√© en partie sur metafor. Le module meta a touttefois l'avantage d'√™tre simple d'utilisation. Par exemple, pour une m√©ta-analyse d'une r√©ponse continue,

```{r expl-meta-load}
library("meta")
meta_data <- read_csv("https://portal.uni-freiburg.de/imbi/_SUPPRESS_ACCESSRULE/lehre/lehrbuecher/meta-analysis-with-r/dataset02.csv")
meta_analyse <- metacont(n.e = Ne, mean.e = Me, sd.e = Se, n.c = Nc, mean.c = Mc, sd.c = Sc, data = meta_data, sm = "SMD")
meta_analyse
```

Et pour effectuer un *forest plot*,

```{r expl-meta-plot}
forest(meta_analyse)
```

### Cr√©er des applications avec R

RStudio vous permet de d√©ployer vos r√©sultats sous forme d'applications web gr√¢ce √† son module [shiny](https://www.rstudio.com/products/shiny/). Pour ce faire, le seul pr√©alable est de savoir programmer en R. En agen√ßant une interface avec des *inputs* (listes de s√©lection, des bo√Ætes de dialogue, des s√©lecteurs, des boutons, etc.) avec des mod√®les que vous d√©veloppez, vous pourrez cr√©er des interfaces int√©ractives.

Pour cr√©er une application shiny, vous devez cr√©er une partie pour l'interface (`ui`) et une autre pour le calcul (`server`). Je n'irai pas dans les d√©tails, √©tant donn√©e qu'il s'agit d'un sujet √† part enti√®re. Pour aller plus loin, visitez le site du projet [shiny](https://www.rstudio.com/products/shiny/).

```
library("shiny")

ui <- basicPage(
  sliderInput("A", "Asymptote:", min = 0, max = 100, value = 50),
  sliderInput("E", "Environnement:", min = -10, max = 100, value = 20),
  sliderInput("R", "Taux:", min = 0, max = 0.1, value = 0.035),
  sliderInput("prix_dose", "Prix dose:", min = 0, max = 5, value = 1),
  sliderInput("prix_vente", "Prix vente:", min = 0, max = 200, value = 100),
  sliderInput("dose", "Dose:", min = 0, max = 300, value = c(0, 200)),
  plotOutput("distPlot")
)

server <- function(input, output) {
  mitsch_f <- reactive({
    input$A * (1 - exp(-input$R * (seq(input$dose[1], input$dose[2], length = 100) + input$E)))
  })
  
  mitsch_opt <- reactive({
    (log((input$A * input$R * input$prix_vente) / input$prix_dose - input$E * input$R) / input$R )
  })
  
  
  output$distPlot <- renderPlot({
    plot(seq(input$dose[1], input$dose[2], length = 100), mitsch_f(), type = "l", ylim = c(0, 100))
    abline(v = mitsch_opt() )
    text(mitsch_opt(), 2, paste("Dose optimale:", round(mitsch_opt(), 0)))
  })
}

shinyApp(ui, server)
```

Une fois l'application cr√©√©e, il est possible de la d√©ployer sur le site shninyapps.io. D'abord cr√©er une application shiny dans RStudio: File > New File > Shiny Web App. √âcrivez votre code dans le fichier app.R (dans ce cas, ce peut √™tre un copier-coller), puis cliquez sur Run App en haut √† droite de la fen√™tre d'√©dition du code. Lorsque l'application fonctionne, vous pourrez la publier via RStudio en cliquant sur le bouton Publish dans la fen√™tre Viewer (vous devez au pr√©alable avoir un comte sur shinyapp.io).

Une application sera publique et sera ouverte. https://essicolo.shinyapps.io/Mitscherlich/

Pour d√©ployer en mode priv√©, vous devrez d√©bourser pour un forfait ou installer votre propre serveur.

### Travailler en Python

Le chapitre \@ref(chapitre-biostats-bayes) a pr√©sent√© un module pour les statistiques bay√©siennes n√©cessitant un environnement Python. Il s'agissait de faire fonctionner un module en R qui, √† l'interne, effectue ses calculs en Python. Rien ne vous emp√™che d'effectuer des calculs directement en Python √† m√™me l'interface de RStudio.

Il vous faudra d'abord installer Python et les modules de calcul que vous d√©sirez. Il existe plusieurs distributions de Python. Parmi elles, Anaconda est probablement la plus intuitive √† installer. Choisissez d'abord [Anaconda](https://www.anaconda.com/download) (~500 Mo) ou [Miniconda](https://conda.io/miniconda.html) pour une installation minimale (~60 Mo) - si vous installez Miniconda, vous devrez aussi installer les modules n√©cessaires pour le calcul. Installez aussi le module [**`reticulate`**](https://rstudio.github.io/reticulate/) de R, de sortte que vous puissiez communiquer avec Python. Anaconda fonctionne avec des environnements de calcul. Chaque environnement poss√®de sa propre version de Python et ses propres modules: cela vous permet d'isoler vos environnements et de contr√¥ler la version des modules. Vous pouvez connecter R √† l'environnement de base cr√©√© lors de l'installation d'Anaconda, ou bien en cr√©er un autre. Pour en cr√©er un nouveau, incluant une liste de modules de calcul,

```
library("reticulate")
conda_create(envname = "monprojet", packages = c("python", "numpy", "scipy", "matplotlib", "pandas", "scikit-learn"))
```

Connectez-vous √† votre environnement Python, par exemple j'utilise l'environnement par d√©faut `anaconda3`.

```#{r expl-connecter-python}
library("reticulate")
conda_list()
use_condaenv("anaconda3", required = TRUE)
```

Supposons que vous travailliez en R markdown. Pour lancer un bloc de code en Python, indiquez `python` au lieu de `r` dans l'ent√™te. Il faudra que les modules **`numpy`**, **`pandas`** et **`matplotlib`** soient install√© dans votre environnement Python 

```{python expl-py-matplotlib}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
a = np.linspace(0, 30, 101)
b = np.sin(a)
# plt.plot(a, b)
# plt.title("Un graphique en Matplotlib dans RStudio")
```

Pour r√©cup√©rer une variable Python en R, pr√©c√©dez la variable de `py$`.

```{r expl-importer-var-py}
plot(py$a, py$b, type = "l", main = "Un graphique en R avec \n des variables d√©finies en Python")
```

Idem, pour r√©cup√©rer un objet R en Python, `r data("iris")`.

```{python expl-importer-var-r}
r.iris.head(6)
```

Vous aurez ainsi acc√®s aux fonctionnalit√©s de Python et R dans un m√™me flux de travail. Python n'est pas si dif√©rent de R, mais il vous faudra d√©ployer des efforts pour approvoiser le serpent. Pour d√©buter en Python, je sugg√®re [*Python Data Science Handbook*](https://jakevdp.github.io/PythonDataScienceHandbook/), de [Jake VanderPlas](https://twitter.com/jakevdp). Pour en savoir plus sur le travail en R et en Python dans RSutdio, r√©f√©rez-vous √† la [documentation du module **`reticulate`**](https://rstudio.github.io/reticulate/).

```{r, expl-rm-all, include=FALSE}
rm(list = ls())
```