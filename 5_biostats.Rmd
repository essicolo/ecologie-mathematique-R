---
title: "Biostatistiques"
output: html_notebook
---

Aux chapitres pr√©c√©dents, nous avons vu comment visualiser, organiser et manipuler des tableaux de donn√©es. La statistique est une collection de disciplines li√©es √† la collecte, l'oragnisation, l'analyse, l'interpr√©tation et la pr√©sentation de donn√©es. Les biostatistiques est l'application de ces disciplines √† la biosph√®re.

Dans [*Principles and procedures of statistics: A biometrical approach*](https://www.amazon.com/Principles-Procedures-Statistics-Biometrical-Approach/dp/0070610282), Steel, Torie et Dickey (1997) d√©finissent les statistiques ainsi:

> Les statistiques forment la science, pure et appliqu√©e, de la cr√©ation, du d√©veloppement, et de l'application de techniques par lesquelles l'incertitude de l'induction inf√©rentielle peut √™tre √©valu√©e. (ma traduction)

Alors que l'inf√©rence consiste √† g√©n√©raliser des observations sur des √©chantillons √† l'ensemble d'une population, l'induction est un type de raisonnement qui permet de g√©n√©raliser des observations en th√©ories. Les statistiques permettent d'√©valuer l'incertitude d√©coulant du processus qui permet d'abord de passer de l'√©chantillon √† la population repr√©sent√© par cet √©chantillon, puis de passer de cette repr√©sentation d'une population en lois g√©n√©rales la concernant.

La d√©finition de Whitlock et Schuluter (2015), dans [The Analysis of Biological Data](http://whitlockschluter.zoology.ubc.ca/), est plus simple, insistant sur l'inf√©rence:

> La statistique est l'√©tude des m√©thodes pour d√©crire et mesures des aspects de la nature √† partir d'√©chantillons. (ma traduction)

Les statistiques consistent √† *faire du sens* (anglicisme assum√©) avec des observations: collecter des √©chantillons, transformer les donn√©es, effectuer des tests,  analyser les r√©sultats, les interpr√©ter et les visualiser. Bien que ces t√¢ches soient complexes, en particulier en ce qui a trait aux tests statistiques, la plupart des op√©rations statistiques peuvent √™tre effectu√©es sans l'assistance de statisticien.ne.s... √† condition de comprendre suffisamment les concepts utilis√©s. Ce chapitre √† lui seul est trop court pour permettre d'int√©grer toutes les connaissances n√©cessaires √† une utilisation raisonn√©e des statistiques, mais fourni les bases pour aller plus loin. Notez que les erreurs d'interpr√©tation statistiques sont courantes et la consultation de sp√©cialistes n'est souvent pas un luxe.

Dans ce chapitre, nous verrons comment r√©pondre correctement √† une question valide et ad√©quate avec l'aide d'outils de calcul scientifique. Nous couvrirons les notions de bases des distributions et des variables al√©atoires qui nous permettront d'effectuer des tests statistiques commun avec R. Nous couvrirons aussi les erreurs commun√©ment commises en recherche acad√©mique et les moyens simples de les √©viter.

Ce chapitre est une introduction aux statistiques avec R, et ne rempalcera pas un bon cours de stats.

En plus des modules de base de R nous utiliserons

* les modules de la `tidyverse`,
* le module de don√©es agricoles `agridat`, ainsi que 
* le module `nlme` sp√©cialis√© pour la mod√©lisation mixte. 

Avant de survoler les applications statistiques avec R, je vais d'abord et rapidement pr√©senter quelques notions importantes en statistiques: populations et √©chantillons, variables, probabilit√©s et distributions. Nous allons effectuer des tests d'hypoth√®se univari√©s (notamment les tests de *t* et les analyses de variance) et d√©tailler la notion de p-value. Mais avant tout, je vais m'attarder plus longuement aux mod√®les lin√©aires g√©n√©ralis√©s, incluant en particulier des effets fixes et al√©atoires (mod√®les mixtes), qui fournissent une trousse d'analyse polyvalente en analyse multivari√©e. Je terminerai avec les perspectives multivari√©s que sont les matrices de covariance et de corr√©lation.

# Populations et √©chantillons

Le principe d'inf√©rence consiste √† g√©n√©raliser des conclusions √† l'√©chelle d'une population √† partir d'√©chantillons issus de cette population. Alors qu'une **population** contient tous les √©l√©ments √©tudi√©s, un **√©chantillon** d'une population est une observation unique. Une exp√©rience bien con√ßue fera en sorte que les √©chantillons sont repr√©sentatifs de la population qui, la plupart du temps, ne peut √™tre observ√©e enti√®rement pour des raisons pratiques.

Les principes d'exp√©rimentation servant de base √† la conception d'une bonne m√©thodologie sont pr√©sent√©s dans le cours [*Dispositifs exp√©rimentaux (BVG-7002)*](https://www.ulaval.ca/les-etudes/cours/repertoire/detailsCours/bvg-7002-dispositifs-experimentaux.html). √âgalement, je recommande le livre *Princpes d'exp√©rimentation: planification des exp√©riences et analyse de leurs r√©sultats* de Pierre Dagnelie (2012), [disponible en ligne en format PDF](http://www.dagnelie.be/docpdf/ex2012.pdf). Un bon aper√ßu des dispositifs exp√©rimentaux est aussi pr√©sent√© dans [*Introductory Statistics with R*](https://www.springer.com/us/book/9780387790534), de Peter Dalgaard (2008).

Une population est √©chantillonn√©e pour induire des **param√®tres**: un rendement typique dans des conditions m√©t√©orologiques, √©daphiques et manag√©riales donn√©es, la masse typique des faucons p√©lerins, m√¢les et femelles, le microbiome typique d'un sol agricole ou forestier, etc. Une **statistique** est une estimation d'un param√®tre calcul√©e √† partir des donn√©es, par exemple une moyenne et un √©cart-type.

Par exemple, la moyenne ($\mu$) et l'√©cart-type ($\sigma$) d'une population sont estim√©s par les moyennes ($\bar{x}$) et √©carts-types ($s$) calcul√©s sur les donn√©es issues de l'√©chantillonnage.

Chaque param√®tre est li√©e √† une perspective que l'on d√©sire cona√Ætre chez une population. Ces angles d'observations sont les **variables**.

# Les variables

Nous avons abord√© au chapitre 4 la notion de *variable* par l'interm√©diaire d'une donn√©e. Une variable est l'observation d'une caract√©ristique d√©crivant un √©chantillon et qui est susceptible de varier d'un √©chantillon √† un autre. Si les observations varient en effet d'un √©chantillon √† un autre, on parlera de variable al√©atoire. M√™me le hasard est r√©git par certaines loi: ce qui est al√©atoire dans une variable peut √™tre d√©crit par des **lois de probabilit√©**, que nous verrons plus bas.

Mais restons aux variables pour l'instant. Par convention, on peut attribuer aux variables un symbole math√©matique. Par exemple, on peut donner √† la masse volumique d'un sol (qui est le r√©sultat d'une m√©thodologie pr√©cise) le sympole $\rho$. Lorsque l'on attribue une valeur √† $\rho$, on parle d'une donn√©e. Chaque donn√©e d'une observation a un indice qui lui est propre, que l'on d√©signe souvent par $i$, que l'on place en indice $\rho_i$. Pour la premi√®re donn√©e, on a $i=1$, donc $\rho_1$. Pour un nombre $n$ d'√©chantillons, on aura $\rho_1$, $\rho_2$, $\rho_3$, ..., $\rho_n$, formant le vecteur $\rho = \left[\rho_1, \rho_2, \rho_3, ..., \rho_n \right]$.

En R, une variable est associ√©e √† un vecteur ou une colonne d'un tableau (notez que R accepte les lettres grecques).

```{r}
œÅ <- c(1.34, 1.52, 1.26, 1.43, 1.39) # matrice 1D
data <- data.frame(œÅ = œÅ) # tableau
data
```

Il existe plusieurs types de variables, qui se regroupe en deux grandes cat√©gories: les **variables quantitatives** et les **variables qualitatives**.

## Variables quantitatives

Ces variables peuvent √™tre continuent dans un espace √©chantillonal r√©el ou discr√®tes dans un espace √©chantillonnal ne consid√©rant que des valeurs fixes. Notons que la notion de nombre r√©el est toujours une approximation en sciences exp√©rimentales comme en calcul num√©rique, √©tant donn√©e que l'on est limit√© par la pr√©cision des appareils comme par le nombre d'octets √† utiliser. Bien que les valeurs fixes des disctributions discr√®tes ne soient pas toujours des valeurs enti√®res, c'est bien souvent le cas en biostatistiques comme en d√©mocgraphie, o√π les d√©comptes d'individus sont souvents pr√©sents (et o√π la notion de fraction d'individus n'est pas accept√©e).

## Variables qualitatives

On exprime parfois qu'une variable qualitative est une variable impossible √† mesurer num√©riquement: une couleur, l'appartenance √† esp√®ce ou √† une s√©rie de sol. Pourtant, dans bien des cas, les variables qualitatives peut √™tre encod√©es en variables quantitatives. Par exemple, on peut accoler des pourcentages de sable, limon et argile √† un loam sableux, qui autrement est d√©crit par la classe texturale d'un sol. Pour une couleur, on peut lui associer des pourcentages de rouge, vert et bleu, ainsi qu'un ton. En ce qui a trait aux variables ordonn√©es, il est possible de supposer un √©talement. Par exemple, une variable d'intensit√© faible-moyenne-forte peut √™tre transform√©e lin√©airement en valeurs quantitatives -1, 0 et 1. Attention toutefois, l'√©talement peut parfois √™tre quadratique ou logarithmique. Les s√©ries de sol peuvent √™tre encod√©es par la proportion de gleyfication ([Parent et al., 2017](https://www.frontiersin.org/articles/10.3389/fenvs.2017.00081/full#B4)). Quant aux cat√©gories difficilement transformables en quantit√©s, on pourra passer par l'**encodage cat√©goriel**, souvent appel√© *dummyfication*, qui nous verrons plus loin.

# Les probabilit√©s

> ¬´ Nous sommes si √©loign√©s de conna√Ætre tous les agens de la nature, et leurs divers modes d'action ; qu'il ne serait pas philosophique de nier les ph√©nom√®nes, uniquement parce qu'ils sont inexplicables dans l'√©tat actuel de nos connaissances. Seulement, nous devons les examiner avec une attention d'autant plus scrupuleuse, qu'il para√Æt plus difficile de les admettre ; et c'est ici que le calcul des probabilit√©s devient indispensable, pour d√©terminer jusqu'√† quel point il faut multiplier les observations ou les exp√©riences, afin d'obtenir en faveur des agens qu'elles indiquent, une probabilit√© sup√©rieure aux raisons que l'on peut avoir d'ailleurs, de ne pas les admettre. ¬ª ‚Äî Pierre-Simon de Laplace

Une probabilit√© est la vraissemblance qu'un √©v√©nements se r√©alise chez un √©chantillon. Les probabilit√©s forment le cadre des syst√®mes stochastiques, c'est-√†-dire des syst√®mes trop complexes pour en conna√Ætre exactement les aboutissants, auxquels ont attribue une part de hasard. Ces syst√®mes sont pr√©dominants dans les processus vivants.

On peut d√©gager deux perspectives sur les probabilit√©s: l'une passe par une interpr√©tations fr√©quentistes, l'autre bay√©siennes. L'interpr√©tation **fr√©quentiste** repr√©sente la fr√©quence des occurences apr√®s un nombre infini d'√©v√©nements. Par exemple, si vous jouez √† pile ou face un grand nombre de fois, le nombre de pile sera √©gal √† la moiti√© du nombre de lanc√©s. Il s'agit de l'interpr√©tation commun√©ment utilis√©e.

L'interpr√©tation **bay√©sienne** vise √† quantifier l'incertitude des ph√©nom√®nes. Dans cette perspective, plus l'information s'accumule, plus l'incertitude diminue. Cette approche gagne en notori√©t√© notamment parce qu'elle permet de d√©crire des ph√©nom√®nes qui, intrins√®quement, ne peuvent √™tre r√©p√©t√©s infiniments (absence d'asymptote), comme celles qui sont bien d√©finis dans le temps ou sur des populations limit√©s.

L'approche fr√©quentielle teste si les donn√©es concordent avec un mod√®le du r√©el, tandis que l'approche bay√©sienne √©value la probabilit√© que le mod√®le soit r√©el. Une erreur courrante consiste √† aborder des statistiques fr√©quentielles comme des statistiques bay√©siennes. Par exemple, si l'on d√©sire √©valuer la probabilit√© de l'existance de vie sur Mars, on devra passer par le bay√©sien, car avec les stats fr√©quentielles, l'on devra plut√¥t conclure si les donn√©es sont conforme ou non avec l'hypoth√®se de la vie sur Mars (exemple tir√©e du blogue [Dynamic Ecology](https://dynamicecology.wordpress.com/2011/10/11/frequentist-vs-bayesian-statistics-resources-to-help-you-choose/)).

Des rivalit√©s factices s'installent enter les tenants des diff√©rentes approches, dont chacune, en r√©alit√©, r√©pond √† des questions diff√©rentes dont il convient r√©fl√©chir sur les limitations. Bien que les statistiques bay√©siennes soient de plus en plus utilis√©es, nous ne couvrirons dans ce chapitre que l'approche fr√©quentielle. L'approche bay√©sienne est n√©amoins trait√©e dans le document comppl√©mentaire statistiques_bayes.ipynb (section en d√©veloppement).

# Les distributions

Une variable al√©atoire peut prendre des valeurs selon des mod√®les de distribution des probabilit√©s. Une distribution est une fonction math√©matique d√©crivant la probabilit√© d'observer une s√©rie d'√©vennements. Ces √©v√©nemements peuvent √™tre des valeurs continues, des nombres entiers, des cat√©gories, des valeurs bool√©ennes (Vrai/Faux), etc. D√©pendamment du type de valeur et des observations obtenues, on peut associer des variables √† diff√©rentes lois de probabilit√©. Toujours, l'aire sous la courbe d'une distribution de probabilit√© est √©gale √† 1.

En statistiques inf√©rentielle, les distributions sont les mod√®les, comprenant certains param√®tres comme la moyenne et la variance pour les distributions normales, √† partir desquelles les donn√©es sont g√©n√©r√©es.

Il existe deux grandes familles de distribution: **discr√®tes** et **continues**. Les distributions discr√®tes sont contraintes √† des valeurs pr√©d√©finies (finies ou infinies), alors que les distributions continues prennent n√©cessairement un nombre infinie de valeur, dont la probabilit√© ne peut pas √™tre √©valu√©e ponctuellement, mais sur un intervalle.

L'**esp√©rance** math√©matique est une fonction de tendance centrale, souvent d√©crite par un param√®tre. Il s'agit de la moyenne d'une population pour une distribution normale. La **variance**, quant √† elle, d√©crit la variabilit√© d'une population, i.e. son √©talement autour de l'esp√©rance. Pour une distribution normale, la variance d'une population est aussi appel√©e variance, souvent pr√©sent√©e par l'√©cart-type.

## Distribution binomiale

En tant que sc√©nario √† deux issues possibles, des tirages √† pile ou face suivent une loi binomiale, comme toute variable bool√©enne prenant une valeur vraie ou fausse. En biostatistiques, les cas communs sont la pr√©sence/absence d'une esp√®ce, d'une maladie, d'un trait phylog√©n√©tique, ainsi que les cat√©gories encod√©es. Lorsque l'op√©ration ne comprend qu'un seul √©chantillon (i.e. un seul tirage √† pile ou face), il s'agit d'un cas particulier d'une loi binomiale que l'on nomme une loi de *Bernouilli*.

Pour 25 tirages √† pile ou face ind√©pendants (i.e. dont l'ordre des tirages ne compte pas), on peut dessiner une courbe de distribution dont la somme des probabilit√©s est de 1. La fonction `dbinom` est une fonctions de distribution de probabilit√©s. Les fonctions de distribution de probabilit√©s discr√®tes sont appel√©es des fonctions de masse.

```{r}
library("tidyverse")
x <- 0:25
y <- dbinom(x = x, size = 25, prob = 0.5)
print(paste('La somme des probabilit√©s est de', sum(y)))
ggplot(data = data.frame(x, y), mapping = aes(x, y)) +
  geom_segment(aes(x = x, xend = x, y = 0, yend = y), color = "grey50") +
  geom_point()
```

## Distribution de Poisson

La loi de Poisson (avec un P majuscule, introduite par le math√©maticien fran√ßais Sim√©on Denis Poisson et non pas l'animal) d√©crit des distributions discr√®tes de probabilit√© d'un nombre d'√©v√©nements se produisant dans l'espace ou dans le temps. Les distributions de Poisson d√©crive ce qui tient du d√©compte. Il peut s'agir du nombre de grenouilles traversant une rue quotidiennement, du nombre de plants d'ascl√©piades se trouvant sur une terre cultiv√©e, ou du nombre d'√©v√©nements de pr√©cipitation au mois de juin, etc. La distribution de Poisson n'a qu'un seul param√®tre, $\lambda$, qui d√©crit tant la moyenne des d√©comptes.

Par exemple, en un mois de 30 jours, et une moyenne de 8 √©v√©nements de pr√©cipitation pour ce mois, on obtient la distribution suivante.

```{r}
x <- 1:30
y <- dpois(x, lambda = 8)
print(paste('La somme des probabilit√©s est de', sum(y)))
ggplot(data = data.frame(x, y), mapping = aes(x, y)) +
  geom_segment(aes(x = x, xend = x, y = 0, yend = y), color = "grey50") +
  geom_point()
```

## Distribution uniforme

La distribution la plus simple est probablement la distribution uniforme. Si la variable est discr√®te, chaque cat√©gorie est associ√© √† une probabilit√© √©gale. Si la variable est continue, la probabilit√© est directement proportionnelle √† la largeur de l'intervalle.  On utilise rarement la distribution uniforme en biostatistiques, sinon pour d√©crire des *a priori* vagues pour l'analyse bay√©sienne (ce sujet est trait√© dans le document 5.1_bayes.ipynb). Nous utilisons la fonction `dunif`. √Ä la diff√©rence des distributions discr√®tes, les fonctions de distribution de probabilit√©s continues sont appel√©es des fonctions de densit√© d'une loi de probabilit√© (*probability density function*).

```{r}
increment <- 0.01
x <- seq(-4, 4, by = increment)
y1 <- dunif(x, min = -3, max = 3)
y2 <- dunif(x, min = -2, max = 2)
y3 <- dunif(x, min = -1, max = 1)

print(paste('La somme des probabilit√©s est de', sum(y3 * increment)))

gg_unif <- data.frame(x, y1, y2, y3) %>% gather(variable, value, -x)

ggplot(data = gg_unif, mapping = aes(x = x, y = value)) +
  geom_line(aes(colour = variable))
```

## Distribution normale

La plus r√©pendue de ces lois est probablement la loi normale, parfois nomm√©e loi gaussienne et plus rarement loi laplacienne. Il s'agit de la distribution classique en forme de cloche.

La loi normale est d√©crite par une moyenne, qui d√©signe la tendance centrale, et une variance, qui d√©signe l'√©talement des probabilit√©s autours de la moyenne. La racine carr√©e de la variance est l'√©cart-type.

Les distributions de mesures exclusivement positives (comme le poids ou la taille) sont parfois avantageusement approxim√©es par une loi **log-normale**, qui est une loi normale sur le logarithme des valeurs: la moyenne d'une loi log-normale est la moyenne g√©om√©trique.

```{r}
increment <- 0.01
x <- seq(-10, 10, by = increment)
y1 <- dnorm(x, mean = 0, sd = 1)
y2 <- dnorm(x, mean = 0, sd = 2)
y3 <- dnorm(x, mean = 0, sd = 3)

print(paste('La somme des probabilit√©s est de', sum(y3 * increment)))

gg_norm <- data.frame(x, y1, y2, y3) %>% gather(variable, value, -x)

ggplot(data = gg_norm, mapping = aes(x = x, y = value)) +
  geom_line(aes(colour = variable))
```

Quelle est la probabilit√© d'obtenir le nombre 0 chez une observation continue distribu√©e normalement dont la moyenne est 0 et l'√©cart-type est de 1? R√©ponse: 0. La loi normale √©tant une distribution continue, les probabilit√©s non-nulles ne peuvent √™tre calcul√©s que sur des intervalles. Par exemple, la probabilit√© de retrouver une valeur dans l'intervalle entre -1 et 2 est calcul√©e en soustraiyant la probabilit√© cumul√©e √† -1 de la probabilit√© cumul√©e √† 2.

```{r}
increment <- 0.01
x <- seq(-5, 5, by = increment)
y <- dnorm(x, mean = 0, sd = 1)

prob_between <- c(-1, 2)

gg_norm <- data.frame(x, y)
gg_auc <- gg_norm %>%
  filter(x > prob_between[1], x < prob_between[2]) %>%
  rbind(c(prob_between[2], 0)) %>%
  rbind(c(prob_between[1], 0))

ggplot(data.frame(x, y), aes(x, y)) +
  geom_polygon(data = gg_auc, fill = '#71ad50') + # #71ad50 est un code de couleur format hexad√©cimal
  geom_line()

prob_norm_between <- pnorm(q = prob_between[2], mean = 0, sd = 1) - pnorm(q = prob_between[1], mean = 0, sd = 1)
print(paste("La probabilit√© d'obtenir un nombre entre", 
            prob_between[1], "et", 
            prob_between[2], "est d'environ", 
            round(prob_norm_between, 2) * 100, "%"))
```

La courbe normale peut √™tre utile pour √©valuer la distribution d'une population. Par exemple, on peut calculer les limites de r√©gion sur la courbe normale qui contient 95% des valeurs possibles en tranchant 2.5% de part et d'autre de la moyenne. Il s'agit ainsi de l'intervalle de confiance sur la d√©viation de la distribution.

```{r}
increment <- 0.01
x <- seq(-5, 5, by = increment)
y <- dnorm(x, mean = 0, sd = 1)

alpha <- 0.05
prob_between <- c(qnorm(p = alpha/2, mean = 0, sd = 1),
                  qnorm(p = 1 - alpha/2, mean = 0, sd = 1))

gg_norm <- data.frame(x, y)
gg_auc <- gg_norm %>%
  filter(x > prob_between[1], x < prob_between[2]) %>%
  rbind(c(prob_between[2], 0)) %>%
  rbind(c(prob_between[1], 0))

ggplot(data = data.frame(x, y), mapping = aes(x, y)) +
  geom_polygon(data = gg_auc, fill = '#71ad50') + # #71ad50 est un code de couleur format hexad√©cimal
  geom_line() +
  geom_text(data = data.frame(x = prob_between,
                              y = c(0, 0),
                              labels = round(prob_between, 2)),
            mapping = aes(label = labels))
```

On pourrait aussi √™tre int√©ress√© √† l'intervalle de confiance sur la moyenne. En effet, la moyenne suit aussi une distribution normale, dont la tendance centrale est la moyenne de la distribution, et dont l'√©cart-type est not√© *erreur standard*. On calcule cette erreur en divisant la variance par le nombre d'observation, ou en divisant l'√©cart-type par la racine carr√©e du nombre d'observations. Ainsi, pour 10 √©chantillons:

```{r}
increment <- 0.01
x <- seq(-5, 5, by = increment)
y <- dnorm(x, mean = 0, sd = 1)

alpha <- 0.05
prob_between <- c(qnorm(p = alpha/2, mean = 0, sd = 1) / sqrt(10),
                  qnorm(p = 1 - alpha/2, mean = 0, sd = 1) / sqrt(10))

gg_norm <- data.frame(x, y)
gg_auc <- gg_norm %>%
  filter(x > prob_between[1], x < prob_between[2]) %>%
  rbind(c(prob_between[2], 0)) %>%
  rbind(c(prob_between[1], 0))

ggplot(data = data.frame(x, y), mapping = aes(x, y)) +
  geom_polygon(data = gg_auc, fill = '#71ad50') + # #71ad50 est un code de couleur format hexad√©cimal
  geom_line() +
  geom_text(data = data.frame(x = prob_between,
                              y = c(0, 0),
                              labels = round(prob_between, 2)),
            mapping = aes(label = labels))
```

# Statistiques descriptives

On a vu comment g√©n√©rer des statistiques sommaires en R avec la fonction `summary()`. Reprenons les donn√©es d'iris.

```{r}
data("iris")
summary(iris)
```

Pour pr√©cis√©ment effectuer une moyenne et un √©cart-type sur un vecteur, passons par les fonctions `mean()` et `sd()`.

```{r}
mean(iris$Sepal.Length)
sd(iris$Sepal.Length)
```

Pour effectuer un sommaire de tableau pilot√© par une fonction, nous passons par la gamme de fonctions `summarise()`, de dplyr. Dans ce cas, avec `group_by()`, nous fragmentons le tableau par esp√®ce pour effectuer un sommaire sur toutes les variables.

```{r}
iris %>%
  group_by(Species) %>%
  summarise_all(mean)
```

Vous pourriez √™tre int√©ress√© par les quartiles √† 25, 50 et 75%. Mais la fonction `summarise()` n'autorise que les fonctions dont la sortie est d'un seul objet, alors faisons sorte que l'objet soit une liste - lorsque l'on imbrique une fonction `funs`, le tableau √† ins√©rer dans la fonction est indiqu√© par un `.`.

```{r}
iris %>%
  group_by(Species) %>%
  summarise_all(funs(list(quantile(.))))
```

En mode programmation classique de R, on pourra g√©n√©rer les quartiles √† la pi√®ce.

```{r}
quantile(iris$Sepal.Length[iris$Species == 'setosa'])
quantile(iris$Sepal.Length[iris$Species == 'versicolor'])
quantile(iris$Sepal.Length[iris$Species == 'virginica'])
```

La fonction `table()` permettra d'obtenir des d√©comptes par cat√©gorie, ici par plages de longueurs de s√©pales. Pour obtenir les proportions du nombre total, il s'agit d'encapsuler le tableau crois√© dans la fonction `prop.table()`.

```{r}
tableau_croise <- table(iris$Species, 
                        cut(iris$Sepal.Length, breaks = quantile(iris$Sepal.Length)))
tableau_croise
```

```{r}
prop.table(tableau_croise)
```

# Tests d'hypoth√®ses √† un et deux √©chantillons

Un test d'hypoth√®se permet de d√©cider si une hypoth√®se est confirm√©e ou rejet√©e √† un seuil de probabilit√© pr√©d√©termin√©.

Cette section est inspir√©e du chapitre 5 de [Dalgaard, 2008](https://www.springer.com/us/book/9780387790534).

---

### Information: l'hypoth√®se nulle

Les tests d'hypoth√®se √©value des *effets* statistiques (qui ne sont pas n√©cessairement des effets de causalit√©). L'effet √† √©valuer peut √™tre celui d'un traitement, d'indicateurs m√©t√©orologiques (e.g. pr√©cipitations totales, degr√©-jour, etc.), de techniques de gestion des paysages, etc. Une recherche est men√©e pour √©valuer l'hypoth√®se que l'on retrouve des diff√©rences entre des unit√©s exp√©rimentales. Par convention, l'**hypoth√®se nulle** (√©crite $H_0$) est l'hypoth√®se qu'il n'y ait pas d'effet (c'est l'hypoth√®se de l'avocat du diable üòà) √† l'√©chelle de la population (et non pas √† l'√©chelle de l'√©chantillon). √Ä l'inverse, l'**hypoth√®se alternative** (√©crite $H_1$) est l'hypoth√®se qu'il y ait un effet √† l'√©chelle de la population.

---

√Ä titre d'exercice en stats, on d√©bute souvent par en testant si deux vecteurs de valeurs continues proviennent de populations √† moyennes diff√©rentes ou si un vecteur de valeurs a √©t√© g√©n√©r√© √† partir d'une population ayant une moyenne donner. Dans cette section, nous utiliserons la fonction `t.test()` pour les tests de t et la fonction `wilcox.test()` pour les tests de Wilcoxon (aussi appel√© de Mann-Whitney).

## Test de t √† un seul √©chantillon

Nous devons assumer, pour ce test, que l'√©chantillon est recueillit d'une population dont la distribution est normale, $\mathcal{N} \sim \left( \mu, \sigma^2 \right)$, et que chaque √©chantillon est ind√©pendant l'un de l'autre. L'hypoth√®se nulle est souvent celle de l'avocat du diable, c'est-√†-dire: ici, que $\mu = \bar{x}$. L'erreur standard sur la moyenne (ESM) de l'√©chantillon, $\bar{x}$ est calcul√©e comme suit.

$$ESM = \frac{s}{\sqrt{n}}$$

o√π $s$ est l'√©cart-type de l'√©chantillon et $n$ est le nombre d'√©chantillons.

Pour tester l'intervalle de confiance de l'√©chantillon, on mutliplie l'ESM par l'aire sous la courbe de densit√© couvrant une certaine proportion de part et d'autre de l'√©chantillon. Pour un niveau de confiance de 95%, on retranche 2.5% de part et d'autre.

```{r}
set.seed(33746)
x <- rnorm(20, 16, 4)

level <-  0.95
alpha <- 1-level

x_bar <- mean(x)
s <- sd(x)
n <- length(x)

error <- qnorm(1 - alpha/2) * s / sqrt(n)
error
```

L'interval de confiance est l'erreur de par et d'autre de la moyenne.

```{r}
c(x_bar - error, x_bar + error)
```

Si la moyenne de la population est de 16, un nombre qui se situe dans l'intervalle de confiance on accepte l'hypoth√®se nulle au seuil 0.05. Si le nombre d'√©chantillon est r√©duit (g√©n√©ralement < 30), on passera plut√¥t par une distribution de t, avec $n-1$ degr√©s de libert√©.

```{r}
error <- qt(1 - alpha/2, n-1) * s / sqrt(n)
c(x_bar - error, x_bar + error)
```

Plus simplement, on pourra utiliser la fonction `t.test()` en sp√©cifiant la moyenne de la population. Nous avons g√©n√©r√© 20 donn√©es avec une moyenne de 16 et un √©cart-type de 4. Nous savons donc que la vraie moyenne de l'√©chantillon est de 16. Mais disons que nous testons l'hypoth√®se que ces donn√©es sont tir√©es d'une population dont la moyenne est 18 (et implicitement que sont √©cart-type est de 4).

```{r}
t.test(x, mu = 18)
```

La fonction retourne la valeur de t (*t-value*), le nombre de degr√©s de libert√© ($n-1 = 19$), une description de l'hypoth√®se alternative (`alternative hypothesis: true mean is not equal to 18`), ainsi que l'intervalle de confiance au niveau de 95%. Le test contient aussi la *p-value*. Bien que la *p-value* soit largement utilis√©e en science

---

### Information: la *p-value*

La *p-value*, ou valeur-p ou p-valeur, est utilis√©e pour trancher si, oui ou non, un r√©sultat est **significatif** (en langage scientifique, le mot significatif ne devrait √™tre utilis√© *que* lorsque l'on r√©f√®re √† un test d'hypoth√®se statistique). Vous retrouverez des *p-value* partout en stats. Les *p-values* indiquent la confiance que l'hypoth√®se nulle soit vraie, selon les donn√©es et le mod√®le statistique utilis√©es.

> La p-value est la probabilit√© que les donn√©es aient √©t√© g√©n√©r√©es pour obtenir un effet √©quivalent ou plus prononc√© si l'hypoth√®se nulle est vraie.

Une *p-value* √©lev√©e indique que le mod√®le appliqu√© √† vos donn√©es concordent avec la conclusion que l'hypoth√®se nulle est vraie, et inversement si la *p-value* est faible. Le seuil arbitraire utilis√©e en √©cologie et en agriculture, comme dans plusieurs domaines, est 0.05.

Les six principes de l'[American Statistical Association](https://phys.org/news/2016-03-american-statistical-association-statement-significance.html) guident l'interpr√©tation des *p-values*. [ma traduction]

0. Les *p-values* indique l'ampleur de l'incompatibiilt√© des donn√©es avec le mod√®le statistique
0. Les *p-values* nemesurent pas la probabilit√© que l'hypoth√®se √©tudi√©e soit vraie, ni la probabilit√© que les donn√©es ont √©t√© g√©n√©r√©es uniquement par la chance.
0. Les conclusions scientifiques et d√©cisions d'affaire ou politiques ne devraient pas √™tre bas√©es sur si une *p-value* atteint un seuil sp√©cifique.
0. Une inf√©rence appropri√©e demande un rapport complet et transparent.
0. Une *p-value*, ou une signification statistique, ne mesure pas l'ampleur d'un effet ou l'importance d'un r√©sultat.
0. En tant que tel, une *p-value* n'offre pas une bonne mesure des √©vidences d'un mod√®le ou d'une hypoth√®se.

Cet encadr√© est inspir√© d'un [billet de blogue de Jim Frost](https://blog.minitab.com/blog/adventures-in-statistics-2/how-to-correctly-interpret-p-values) et d'un [rapport de l'American Statistical Association](https://phys.org/news/2016-03-american-statistical-association-statement-significance.html).

---

Dans le cas pr√©c√©dent, la *p-value* √©tait de 0.01014. Pour aider notre interpr√©tation, prenons l'hypoth√®se alternative: `true mean is not equal to 18`. L'hypoth√®se nulle √©tait bien que *la vraie moyenne est √©gale √† 18*. Ins√©rons la *p-value* dans la d√©finition: la probabilit√© que les donn√©es aient √©t√© g√©n√©r√©es pour obtenir un effet √©quivalent ou plus prononc√© si l'hypoth√®se nulle est vraie est de 0.01014. Il est donc tr√®s peu probable que les donn√©es soient tir√©es d'un √©chantillon dont la moyenne est de 18. Au seuil de significativit√© de 0.05, on rejette l'hypoth√®se nulle et l'on conclu qu'√† ce seuil de confiance, l'√©chantillon ne provient pas d'une population ayant une moyenne de 18.

---

## Attention: mauvaises interpr√©tations des *p-values*

> "La p-value n'a jamais √©t√© con√ßue comme substitut au raisonnement scientifique" [Ron Wasserstein, directeur de l'American Statistical Association](https://phys.org/news/2016-03-american-statistical-association-statement-significance.html) [ma traduction]. 

**Un r√©sultat montrant une p-value plus √©lev√©e que 0.05 est-il pertinent?**

Lors d'une conf√©rence, Dr Evil ne pr√©sentent que les r√©sultats significatifs de ses essais au seuil de 0.05. Certains essais ne sont pas significatifs, mais bon, ceux-ci ne sont pas importants... En √©cartant ces r√©sultats, Dr Evil commet 3 erreurs:

1. La *p-value* n'est pas un bon indicateur de l'importance d'un test statistique. L'importance d'une variable dans un mod√®le devrait √™tre √©valu√©e par la valeur de son coefficient. Son incertitude devrait √™tre √©valu√©e par sa variance. Une mani√®re d'√©valuer plus intuitive la variance est l'√©cart-type ou l'intervalle de confiance. √Ä un certain seuil d'intervalle de confiance, la p-value traduira la probabilit√© qu'un coefficient soit r√©ellement nul ait pu g√©n√©rer des donn√©es d√©montrant un coefficient √©gal ou sup√©rieur.
1. Il est tout aussi important de savoir que le traitement fonctionne que de savoir qu'il ne fonctionne pas. Les r√©sultats d√©montrant des effets sont malheureusement davantage soumis aux journaux et davantage publi√©s que ceux ne d√©montrant pas d'effets ([Decullier et al., 2005]( https://doi.org/10.1136/bmj.38488.385995.8F )).
1. Le seuil de 0.05 est arbitraire.

---


---

### Attention au *p-hacking*

Le *p-hacking* (ou *data dredging*) consiste √† manipuler les donn√©es et les mod√®les pour faire en sorte d'obtenir des *p-values* favorables √† l'hypoth√®se test√©e et, √©ventuellement, aux conclusions recherch√©es. **√Ä √©viter dans tous les cas. Toujours. Toujours. Toujours.**

Vid√©o sugg√©r√©e (en anglais).

[![p-hacking](images/05_p-hacking.png)](https://youtu.be/0Rnq1NpHdmw)

---

## Test de Wilcoxon √† un seul √©chantillon

Le test de t suppose que la distribution des donn√©es est normale... ce qui est rarement le cas, surtout lorsque les √©chantillons sont peu nombreux. Le test de Wilcoxon ne demande aucune supposition sur la distribution: c'est un test non-param√©trique bas√© sur le tri des valeurs.

```{r}
wilcox.test(x, mu = 18)
```

Le `V` est la somme des rangs positifs. Dans ce cas, la *p-value* est semblable √† celle du test de t, et les m√™mes conclusions s'appliquent.

## Tests de t √† deux √©chantillons

Les tests √† un √©chantillon servent plut√¥t √† s'exercer: rarement en aura-t-on besoin en recherche, o√π plus souvent, on voudra comparer les moyennes de deux unit√©s exp√©rimentales. L'exp√©rience comprend donc deux s√©ries de donn√©es continues, $x_1$ et $x_2$, issus de lois de distribution normale $\mathcal{N} \left( \mu_1, \sigma_1^2 \right)$ et $\mathcal{N} \left( \mu_2, \sigma_2^2 \right)$, et nous testons l'hypoth√®se nulle que $\mu_1 = \mu_2$. La statistique t est calcul√©e comme suit.

$$t = \frac{\bar{x_1} - \bar{x_2}}{ESDM}$$

L'ESDM est l'erreur standard de la diff√©rence des moyennes:

$$ESDM = \sqrt{ESM_1^2 + ESM_2^2}$$

Si vous supposez que les variances sont identiques, l'erreur standard (s) est calcul√©e pour les √©chantillons des deux groupes, puis ins√©r√©e dans le calcul des ESM. La statistique t sera alors √©valu√©e √† $n_1 + n_2 - 2$ degr√©s de lbert√©. Si vous supposez que la variance est diff√©rente (*proc√©dure de Welch*), vous calculez les ESM avec les erreurs standards respectives, et la statistique t devient une approximation de la distribution de t avec un nombre de degr√©s de libert√© calcul√© √† partir des erreurs standards et du nombre d'√©chantillon dans les groupes: cette proc√©dure est consid√©r√©e comme plus prudente ([Dalgaard, 2008](https://www.springer.com/us/book/9780387790534), page 101).

Prenons les donn√©es d'iris pour l'exemple en excluant l'iris setosa √©tant donn√©e que les tests de t se restreignent √† deux groupes. Nous allons tester la longueur des p√©tales.

```{r}
iris_pl <- iris %>% 
    filter(Species != "setosa") %>%
    select(Species, Petal.Length)
sample_n(iris_pl, 5)
```

Dans la prochaine cellule, nous introduisons l'*interface-formule* de R, o√π l'on retrouve typiquement le `~`, entre les variables de sortie √† gauche et les variables d'entr√©e √† droite. Dans notre cas, la variable de sortie est la variable test√©e, `Petal.Length`, qui varie en fonction du groupe `Species`, qui est la variable d'entr√©e (variable explicative) - nous verrons les types de variables plus en d√©tails dans la section [Les mod√®les statistiques](#Les-mod%C3%A8les-statistiques), plus bas.

```{r}
t.test(formula = Petal.Length ~ Species,
       data = iris_pl, var.equal = FALSE)
```

Nous obtenons une sortie similaire aux pr√©c√©dentes. L'intervalle de confiance √† 95% exclu le z√©ro, ce qui est coh√©rent avec la p-value tr√®s faible, qui nous indique le rejet de l'hypoth√®se nulle au seuil 0.05. Les groupes ont donc des moyennes de longueurs de p√©tale significativement diff√©rentes.

---

### Enregistrer les r√©sultats d'un test

Il est possible d'enregistrer un test dans un objet.

```{r}
tt_pl <- t.test(formula = Petal.Length ~ Species,
                data = iris_pl, var.equal = FALSE)
summary(tt_pl)
str(tt_pl)
```

---

## Comparaison des variances

Pour comparer les variances, on a recours au test de F (F pour Fisher).

```{r}
var.test(formula = Petal.Length ~ Species,
         data = iris_pl)
```

Il semble que l'on pourrait relancer le test de t sans la proc√©dure Welch, avec `var.equal = TRUE`.

## Tests de Wilcoxon √† deux √©chantillons

Cela ressemble au test de t!

```{r}
wilcox.test(formula = Petal.Length ~ Species,
       data = iris_pl, var.equal = TRUE)
```

## Les tests pair√©s

Les tests pair√©s sont utilis√©s lorsque deux √©chantillons proviennent d'une m√™me unit√© exp√©rimentale: il s'agit en fait de tests sur la diff√©rences entre deux observations.

```{r}
set.seed(2555)

n <- 20
avant <- rnorm(n, 16, 4)
apres <- rnorm(n, 18, 3)
```

Il est important de sp√©cifier que le test est pair√©, la valeur par d√©faut de `paired` √©tant `FALSE`.

```{r}
t.test(avant, apres, paired = TRUE)
```

L'hypoth√®se nulle qu'il n'y ait pas de diff√©rence entre l'avant et l'apr√®s traitement est accept√©e au seuil 0.05.

**Exercice**. Effectuer un test de Wilcoxon pair√©.

# L'analyse de variance

L'analyse de variance consiste √† comparer des moyennes de plusieurs groupe distribu√©s normalement et de m√™me variance. Cetteb section sera √©labor√©e prochainement plus en profondeur. Consid√©rons-la pour le moment comme une r√©gression sur une variable cat√©gorielle.

```{r}
pl_aov <- aov(Petal.Length ~ Species, iris)
summary(pl_aov)
```

La prochaine section, justement, est vou√©e aux mod√®les statistiques explicatifs, qui incluent la r√©gression.

# Les mod√®les statistiques

La mod√©lisation statistique consiste √† lier de mani√®re explicite des variables de sortie $y$ (ou variables-r√©ponse ou variables d√©pendantes) √† des variables explicatives $x$ (ou variables pr√©dictives / ind√©pendantes / covariables). Les variables-r√©ponse sont mod√©ls√©es par une fonction des variables explicatives ou pr√©dictives.

Pourquoi garder les termes *explicatives* et *pr√©dictives*? Parce que les mod√®les statistiques (bas√©s sur des donn√©es et non pas sur des m√©canismes) sont de deux ordres. D'abord, les mod√®les **pr√©dictifs** sont con√ßus pour pr√©dire de mani√®re fiable une ou plusieurs variables-r√©ponse √† partir des informations contenues dans les variables qui sont, dans ce cas, pr√©dictives. Ces mod√®les sont couverts dans le chapitre 11 de ce manuel (en d√©veloppement). Lorsque l'on d√©sire tester des hypoth√®ses pour √©valuer quelles variables expliquent la r√©ponse, on parlera de mod√©lisation (et de variables) **explicatives**. En inf√©rence statistique, on √©valuera les *corr√©lations* entre les variables explicatives et les variables-r√©ponse. Un lien de corr√©lation n'est pas un lien de causalit√©. L'inf√©rence causale peut en revanche √™tre √©valu√©e par des [*mod√®les d'√©quations structurelles*](https://www.amazon.com/Cause-Correlation-Biology-Structural-Equations/dp/1107442591), sujet qui fera √©ventuellement partie de ce cours.

Cette section couvre la mod√©lisation explicative. Les variables qui contribuent √† cr√©er les mod√®les peuvent √™tre de diff√©rentes natures et distribu√©es selon diff√©rentes lois de probabilit√©. Alors que les mod√®les lin√©aires simples (*lm*) impliquent une variable-r√©ponse distribu√©e de mani√®re continue, les mod√®les lin√©aires g√©n√©ralis√©s peuvent aussi expliquer des variables de sorties discr√®tes.

Dans les deux cas, on distinguera les variables fixes et les variables al√©atoires. Les **variables fixes** sont des les variables test√©es lors de l'exp√©rience: dose du traitement, esp√®ce/cultivar, m√©t√©o, etc. Les **variables al√©atoires** sont les sources de variation qui g√©n√®rent du bruit dans le mod√®le: les unit√©s exp√©rimentales ou le temps lors de mesures r√©p√©t√©es. Les mod√®les incluant des effets fixes seulement sont des mod√®les √† effets fixes. G√©n√©ralement, les mod√®les incluant des variables al√©atoires incluent aussi des variables fixes: on parlera alors de mod√®les mixtes. Nous couvrirons ces deux types de mod√®le.

## Mod√®les √† effets fixes

Les tests de t et de Wilcoxon, explor√©s pr√©c√©demment, sont des mod√®les statistiques √† une seule variable. Nous avons vu dans l'*interface-formule* qu'une variable-r√©ponse peut √™tre li√©e √† une variable explicative avec le tilde `~`. En particulier, le test de t est r√©gression lin√©aire univari√©e (√† une seule variable explicative) dont la variable explicative comprend deux cat√©gories. De m√™me, l'anova est une r√©gression lin√©aire univari√©e dont la variable explicative comprend plusieurs cat√©gories. Or l'interface-formule peut √™tre utilis√© dans plusieurs circonstance, notamment pour ajouter plusieurs variables de diff√©rents types: on parlera de r√©gression multivari√©e.

La plupart des mod√®les statistiques peuvent √™tre approxim√©s comme une combinaison lin√©aire de variables: ce sont des mod√®les lin√©aires. Les mod√®les non-lin√©aires impliquent des strat√©gies computationnelles complexes qui rendent leur utilisation plus difficile √† manoeuvrer.

Un mod√®le lin√©aire univari√© prendra la forme $y = \beta_0 + \beta_1 x + \epsilon$, o√π $\beta_0$ est l'intercept et $\beta_1$ est la pente et $\epsilon$ est l'erreur.

Vous verrez parfois la notation $\hat{y} = \beta_0 + \beta_1 x$. La notation avec le chapeau $\hat{y}$ exprime qu'il s'agit des valeurs g√©n√©r√©es par le mod√®le. En fait, $y = \hat{y} - \epsilon$.

### Mod√®le lin√©aire univari√© avec variable continue

Prenons les donn√©es [`lasrosas.corn`](https://rdrr.io/cran/agridat/man/lasrosas.corn.html) incluses dans le module `agridat`, o√π l'on retrouve le rendement d'une production de ma√Øs √† dose d'azote variable, en Argentine.

```{r}
library("agridat")
data("lasrosas.corn")
sample_n(lasrosas.corn, 10)
```

Ces donn√©es comprennent plusieurs variables. Prenons le rendement (`yield`) comme variable de sortie et, pour le moment, ne retenons que la dose d'azote (`nitro`) comme variable explicative: il s'agit d'une r√©gression univari√©e. Les deux variables sont continuent. Explorons d'abord le nuage de points de l'une et l'autre.

```{r}
ggplot(data = lasrosas.corn, mapping = aes(x = nitro, y = yield)) +
    geom_point()
```

L'hypoth√®se nulle est que la dose d'azote n'affecte pas le rendement, c'est √† dire que le coefficient de pente et nul. Une autre hypoth√®se est que l'intercept est nul: donc qu'√† dose de 0, rendement de 0. Un mod√®le lin√©aire √† variable de sortie continue est cr√©√© avec la fonction `lm()`, pour *linear model*.

```{r}
modlin_1 <- lm(yield ~ nitro, data = lasrosas.corn)
summary(modlin_1)
```

Le diagnostic du mod√®le comprend plusieurs informations. D'abord la formule utilis√©e, affich√©e pour la tracabilit√©. Viens ensuite un aper√ßu de la distribution des r√©sidus. La m√©diane devrait s'approcher de la moyenne des r√©sidus (qui est toujours de 0). Bien que le -3.079 peut sembler important, il faut prendre en consid√©ration de l'√©chelle de y, et ce -3.079 est exprim√© en terme de rendement, ici en quintaux (i.e. 100 kg) par hectare. La distribution des r√©sidus m√©rite d'√™tre davantage investigu√©e. Nous verrons cela un peu plus tard.

Les coefficients apparaissent ensuite. Les estim√©s sont les valeurs des effets. R fournit aussi l'erreur standard associ√©e, la valeur de t ainsi que la p-value (la probabilit√© d'obtenir cet effet ou un effet plus extr√™me si en r√©alit√© il y avait absence d'effet). L'intercept est bien s√ªr plus √©lev√© que 0 (√† dose nulle, on obtient 65.8 quintaux par hectare en moyenne). La pente de la variable `nitro` est de ~0.06: pour chaque augmentation d'un kg/ha de dose, on a obtenu ~0.06 quintaux/ha de plus de ma√Øs. Donc pour 100 kg/ha de N, on a obtenu un rendement moyen de 6 quintaux de plus que l'intercept. Soulignons que l'ampleur du coefficient est tr√®s important pour guider la fertilisation: ne rapporter que la p-value, ou ne rapporter que le fait qu'elle est inf√©rieure √† 0.05 (ce qui arrive souvent dans la litt√©rature), serait tr√®s insuffisant pour l'interpr√©tation des statistiques. La p-value nous indique n√©anmoins qu'il serait tr√®s improbable qu'une telle pente ait √©t√© g√©n√©r√©e alors que celle-ci est nulle en r√©alit√©. Les √©toiles √† c√¥t√© des p-values indiquent l'ampleur selon l'√©chelle `Signif. codes` indiqu√©e en-dessous du tableau des coefficients.

Sous ce tableau, R offre d'autres statistiques. En outre, les R¬≤ et R¬≤ ajust√©s indiquent si la r√©gression passe effectivement par les points. Le R¬≤ prend un maximum de 1 lorsque la droite passe exactement sur les points.

Enfin, le test de F g√©n√®re une p-value indiquant la probabilit√© que les coefficients de pente ait √©t√© g√©n√©r√©s si les vrais coefficients √©taient nuls. Dans le cas d'une r√©gression univari√©e, cela r√©p√®te l'information sur l'unique coefficient.

On pourra √©galement obtenir les intervalles de confiance avec la fonction `confint()`.

```{r}
confint(modlin_1, level = 0.95)
```

Ou soutirer l'information de diff√©rentes mani√®res, comme avec la fonction `coefficients()`.

```{r}
coefficients(modlin_1)
```

√âgalement, on pourra ex√©cuter le mod√®le sur les donn√©es qui ont servi √† le g√©n√©rer:

```{r}
predict(modlin_1)[1:5]
```

Ou sur des donn√©es externes.

```{r}
nouvelles_donnees <- data.frame(nitro = seq(from = 0, to = 100, by = 5))
predict(modlin_1, newdata = nouvelles_donnees)[1:5]
```

### Analyse des r√©sidus

Les r√©sidus sont les erreurs du mod√®le. C'est le vecteur $\epsilon$, qui est un d√©lage entre les donn√©es et le mod√®le. Le R¬≤ est un indicateur de l'ampleur du d√©calage, mais une r√©gression lin√©aire explicative en bonne et due forme devrait √™tre acompagn√©e d'une analyse des r√©sidus. On peut les calcul√©s par $\epsilon = y - \hat{y}$, mais aussi bien utiliser la fonction `residuals()`.

```{r}
res_df <- data.frame(nitro = lasrosas.corn$nitro,
                     residus_lm = residuals(modlin_1), 
                     residus_calcul = lasrosas.corn$yield - predict(modlin_1))
sample_n(res_df, 10)
```

Dans une bonne r√©gression lin√©aire, on ne retrouvera pas de structure identifiable dans les r√©sidus, c'est-√†-dire que les r√©sidus sont bien distribu√©s de part et d'autre du mod√®le de r√©gression.

```{r}
ggplot(res_df, aes(x = nitro, y = residus_lm)) +
  geom_point() +
  labs(x = "Dose N", y = "Residus") +
  geom_hline(yintercept = 0, col = "red", size = 1)
```

Bien que le jugement soit subjectif, on peut dire confiamment qu'il n'y a pas structure particuli√®re. En revanche, on pourrait g√©n√©rer un $y$ qui varie de mani√®re quadratique avec $x$, un mod√®le lin√©aire montrera une structure √©vidente.

```{r}
set.seed(36164)
x <- 0:100
y <- 10 + x*1 + x^2 * 0.05 + rnorm(length(x), 0, 50)
modlin_2 <- lm(y ~ x)
ggplot(data.frame(x, residus = residuals(modlin_2)),
       aes(x = x, y = residus)) +
  geom_point() +
  labs(x = "x", y = "Residus") +
  geom_hline(yintercept = 0, col = "red", size = 1)
```

De m√™me, les r√©sidus ne devraient pas cro√Ætre avec $x$.

```{r}
set.seed(3984)
x <- 0:100
y <-  10 + x + x * rnorm(length(x), 0, 2)
modlin_3 <- lm(y ~ x)
ggplot(data.frame(x, residus = residuals(modlin_3)),
       aes(x = x, y = residus)) +
  geom_point() +
  labs(x = "x", y = "Residus") +
  geom_hline(yintercept = 0, col = "red", size = 1)
```

On pourra aussi inspecter les r√©sidus avec un graphique de leur distribution. Reprenons notre mod√®le de rendement du ma√Øs.

```{r}
ggplot(res_df, aes(x = residus_lm)) +
  geom_histogram(binwidth = 2, color = "white") +
  labs(x = "Residual")
```

L'histogramme devrait pr√©senter une distribution normale. Les tests de nomalit√© comme le test de Shapiro-Wilk peuvent aider, mais ils sont g√©n√©ralement tr√®s s√©v√®res.

```{r}
shapiro.test(res_df$residus_lm)
```

L'hypoth√®se nulle que la distribution est normale est rejet√©e au seuil 0.05. Dans notre cas, il est √©vident que la s√©v√©rit√© du test n'est pas en cause, car les r√©sidus semble g√©n√©rer trois ensembles. Ceci indique que les variables explicatives sont insuffisantes pour expliquer la variabilit√© de la variable-r√©ponse.

### R√©gression multiple

Comme c'est le cas pour bien des ph√©nom√®nes en √©cologie, le rendement d'une culture n'est certainement pas expliqu√© seulement par la dose d'azote.

Lorsque l'on combine plusieurs variables explicatives, on cr√©e un mod√®le de r√©gression multivari√©e, ou une r√©gression multiple. Bien que les tendances puissent sembl√©es non-lin√©aires, l'ajout de variables et le calcul des coefficients associ√©s reste un probl√®me d'alg√®bre lin√©aire.

On pourra en effet g√©n√©raliser les mod√®les lin√©aires, univari√©s et multivari√©s, de la mani√®re suivante.

$$ y = X \beta + \epsilon $$

o√π:

$X$ est la matrice du mod√®le √† $n$ observations et $p$ variables.

$$ X = \left( \begin{matrix} 
1 & x_{11} & \cdots & x_{1p}  \\ 
1 & x_{21} & \cdots & x_{2p}  \\ 
\vdots & \vdots & \ddots & \vdots  \\ 
1 & x_{n1} & \cdots & x_{np}
\end{matrix} \right) $$

$\beta$ est la matrice des $p$ coefficients, $\beta_0$ √©tant l'intercept qui multiplie la premi√®re colonne de la matrice $X$.

$$ \beta = \left( \begin{matrix} 
\beta_0  \\ 
\beta_1  \\ 
\vdots \\ 
\beta_p 
\end{matrix} \right) $$

$\epsilon$ est l'erreur de chaque observation.

$$ \epsilon = \left( \begin{matrix} 
\epsilon_0  \\ 
\epsilon_1  \\ 
\vdots \\ 
\epsilon_n
\end{matrix} \right) $$

### Mod√®les lin√©aires univari√©s avec variable cat√©gorielle **nominale**

Une variable cat√©gorielle nominale (non ordonn√©e) utilis√©e √† elle seule dans un mod√®le comme variable explicative, est un cas particulier de r√©gression multiple. En effet, l'**encodage cat√©goriel** (ou *dummyfication*) transforme une variable cat√©gorielle nominale en une matrice de mod√®le comprenant une colonne d√©signant l'intercept (une s√©rie de 1) d√©signant la cat√©gorie de r√©f√©rence, ainsi que des colonnes pour chacune des autres cat√©gories d√©signant l'appartenance (1) ou la non appartenance (0) de la cat√©gorie d√©sign√©e par la colonne.

#### L'encodage cat√©goriel

Une variable √† $C$ cat√©gories pourra √™tre d√©clin√©e en $C$ variables dont chaque colonne d√©signe par un 1 l'appartenance au groupe de la colonne et par un 0 la non-appartenance. Pour l'exemple, cr√©ons un vecteur d√©signant le cultivar de pomme de terre.

```{r}
data <- data.frame(cultivar = c('Superior', 'Superior', 'Superior', 'Russet', 'Kenebec', 'Russet'))
model.matrix(~cultivar, data)
```

Nous avons trois cat√©gories, encod√©es en trois colonnes. La premi√®re colonne est un intercept et les deux autres d√©crivent l'absence (0) ou la pr√©sence (1) des cultivars Russet et Superior. Le cultivar Kenebec est absent du tableau. En effet, en partant du principe que l'appartenance √† une cat√©gorie est mutuellement exclusive, c'est-√†-dire qu'un √©chantillon ne peut √™tre assign√© qu'√† une seule cat√©gorie, on peut d√©duire une cat√©gorie √† partir de l'information sur toutes les autres. Par exemple, si `cultivar_Russet` et `cultivar_Superior` sont toutes deux √©gales √† $0$, on concluera que `cultivar_Kenebec` est n√©cessairement √©gal √† $1$. Et si l'un d'entre `cultivar_Russet` et `cultivar_Superior` est √©gal √† $1$, `cultivar_Kenebec` est n√©cessairement √©gal √† $0$. L'information contenue dans un nombre $C$ de cat√©gorie peut √™tre encod√©e dans un nombre $C-1$ de colonnes. C'est pourquoi, dans une analyse statistique, on d√©signera une cat√©gorie comme une r√©f√©rence, que l'on d√©tecte lorsque toutes les autres cat√©gories sont encod√©es avec des $0$: cette r√©f√©rence sera incluse dans l'intercept. La cat√©gorie de r√©f√©rence par d√©faut en R est celle la premi√®re cat√©gorie dans l'ordre aphab√©tique. On pourra modifier cette r√©f√©rence avec la fonction `relevel()`.

```{r}
data$cultivar <- relevel(data$cultivar, ref = "Superior")
model.matrix(~cultivar, data)
```

Pour certains mod√®les, vous devrez vous assurer vous-m√™me de l'encodage cat√©goriel. Pour d'autre, en particulier avec l'*interface par formule* de R, ce sera fait automatiquement.

#### Exemple d'application

Prenons la topographie du terrain, qui peut prendre plusieurs niveaux.

```{r}
levels(lasrosas.corn$topo)
```

Explorons le rendement selon la topographie.

```{r}
ggplot(lasrosas.corn, aes(x = topo, y = yield)) +
    geom_boxplot()
```

Les diff√©rences sont √©videntes, et la mod√©lisation devrait montrer des effets significatifs.

L'encodage cat√©goriel peut √™tre visualis√© en g√©n√©rant la matrice de mod√®le avec la fonction `model.matrix()` et l'interface-formule - sans la variable-r√©ponse.

```{r}
model.matrix(~ topo, data = lasrosas.corn) %>% 
    tbl_df() %>% # tbl_df pour transformer la matrice en tableau
    sample_n(10) 
```

Dans le cas d'un mod√®le avec une variable cat√©gorielle nominale seule, l'intercept repr√©sente la cat√©gorie de r√©f√©rence, ici `E`. Les autres colonnes sp√©cifient l'appartenance (1) ou la non-appartenance (0) de la cat√©gorie pour chaque observation.

Cette matrice de mod√®le utilis√©e pour la r√©gression donnera un intercept, qui indiquera l'effet de la cat√©gorie de r√©f√©rence, puis les diff√©rences entre les cat√©gories subs√©quentes et la cat√©gorie de r√©f√©rence.

```{r}
modlin_4 <- lm(yield ~ topo, data = lasrosas.corn)
summary(modlin_4)
```

Le mod√®le lin√©aire est √©quivalent √† l'anova, mais les r√©sultats de `lm` sont plus √©labor√©s.

```{r}
summary(aov(yield ~ topo, data = lasrosas.corn))
```

L'analyse de r√©sidus peut √™tre effectu√©e de la m√™me mani√®re.

### Mod√®les lin√©aires univari√©s avec variable cat√©gorielle **ordinale**

Bien que j'introduise la r√©gression sur variable cat√©gorielle ordinale √† la suite de la section sur les variables nominales, nous revenons dans ce cas √† une r√©rgession simple, univari√©e. Voyons un cas √† 5 niveaux.

```{r}
statut <- c("Totalement en d√©saccort", 
            "En d√©saccord",
            "Ni en accord, ni en d√©saccord",
            "En accord",
            "Totalement en accord")
statut_o <- factor(statut, levels = statut, ordered=TRUE)
model.matrix(~statut_o) # ou bien, sans passer par model.matrix, contr.poly(5) o√π 5 est le nombre de niveaux
```

La matrice de mod√®le a 5 colonnes, soit le nombre de niveaux: un intercept, puis 4 autres d√©signant diff√©rentes valeurs que peuvent prendre les niveaux. Ces niveaux croient-ils lin√©airement? De mani√®re quadratique, cubique ou plus loin dans des distributions polynomiales?

```{r}
modmat_tidy <- data.frame(statut, model.matrix(~statut_o)[, -1]) %>%
    gather(variable, valeur, -statut)
modmat_tidy$statut <- factor(modmat_tidy$statut, 
                             levels = statut, 
                             ordered=TRUE)
ggplot(data = modmat_tidy, mapping = aes(x = statut, y = valeur)) +
    facet_wrap(. ~ variable) +
    geom_point() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

R√®gle g√©n√©rale, pour les variables ordinales, on pr√©f√©rera une distribution lin√©aire, et c'est l'option par d√©faut de la fonction `lm()`. L'utilisation d'une autre distribution peut √™tre effectu√©e √† la mitaine en utilisant dans le mod√®le la colonne d√©sir√©e de la sortie de la fonction `model.matrix()`.

### R√©gression multiple √† plusieurs variables

Reprenons le tableau de donn√©es du rendement de ma√Øs.

```{r}
head(lasrosas.corn)
```

Pour ajouter des variables au mod√®le dans l'interface-formule, on additionne les noms de colonne. La variable `lat` d√©signe la latitude, la variable `long` d√©signe la latitude et la variable `bv` (*brightness value*) d√©signe la teneur en mati√®re organique du sol (plus `bv` est √©lev√©e, plus faible est la teneur en mati√®re organique).

```{r}
modlin_5 <- lm(yield ~ lat + long + nitro + topo + bv,
               data = lasrosas.corn)
summary(modlin_5)
```

L'ampleur des coefficients est relatif √† l'√©chelle de la variable. En effet, un coefficient de 5541 sur la variable `lat` n'est pas comparable au coefficient de la variable `bv`, de -0.5089, √©tant donn√© que les variables ne sont pas exprim√©es avec la m√™me √©chelle. Pour les comparer sur une m√™me base, on peut centrer (soustraire la moyenne) et r√©durie (diviser par l'√©cart-type).

```{r}
scale_vec <- function(x) as.vector(scale(x)) # la fonction scale g√©n√®re une matrice: nous d√©sirons un vecteur

lasrosas.corn_sc <- lasrosas.corn %>%
    mutate_at(c("lat", "long", "nitro", "bv"), 
                 scale_vec)

modlin_5_sc <- lm(yield ~ lat + long + nitro + topo + bv,
               data = lasrosas.corn_sc)
summary(modlin_5_sc)
```

Typiquement, les variables cat√©gorielles, qui ne sont pas mises √† l'√©chelle, donneront des coefficients plus √©lev√©es, et devrons √™tre √©valu√©es entre elles et non comparativement aux variables mises √† l'√©chelle. Une mani√®re conviviale de repr√©senter des coefficients consiste √† cr√©er un tableau (fonction `tibble()`) incluant les coefficients ainsi que leurs intervalles de confiance, puis √† les porter graphiquement.

```{r}
intervals <- tibble(Estimate = coefficients(modlin_5_sc)[-1], # [-1] enlever l'intercept
                    LL = confint(modlin_5_sc)[-1, 1], # [-1, ] enlever la premi√®re ligne, celle de l'intercept
                    UL = confint(modlin_5_sc)[-1, 2],
                    variable = names(coefficients(modlin_5_sc)[-1])) 
intervals
```

```{r}
ggplot(data = intervals, mapping = aes(x = Estimate, y = variable)) +
    geom_vline(xintercept = 0, lty = 2) +
    geom_segment(mapping = aes(x = LL, xend = UL, 
                               y = variable, yend = variable)) +
    geom_point() +
    labs(x = "Coefficient standardis√©", y = "")
```

On y voit qu'√† l'exception de la variable `long`, tous les coefficients sont diff√©rents de 0. Le coefficient `bv` est n√©gatif, indicant que plus la valeur de `bv` est √©lev√© (donc plus le sol est pauvre en mati√®re organique), plus le rendement est faible. Plus la latitude est √©lev√©e (plus on se dirige vers le Nord de l'Argentine), plus le rendement est √©lev√©. La dose d'azote a aussi un effet statistique positif sur le rendement.

Quant aux cat√©gories topographiques, elles sont toutes diff√©rentes de la cat√©gorie `E`, ne croisant pas le z√©ro. De plus, les intervalles de confiance ne se chevauchant pas, on peut conclure en une diff√©rence significative d'une √† l'autre. Bien s√ªr, tout cela au seuil de confiance de 0.05.

On pourra retrouver des cas o√π l'effet combin√© de plusieurs variables diff√®re de l'effet des deux variables prises s√©par√©ment. Par exemple, on pourrait √©valuer l'effet de l'azote et celui de la topographie dans un m√™me mod√®le, puis y ajouter une int√©raction entre l'azote et la topographie, qui d√©finira des effets suppl√©mentaires de l'azote selon chaque cat√©gorie topographique. C'est ce que l'on appelle une int√©raction.

Dans l'interface-formule, l'int√©raction entre l'azote et la topographie est not√©e `nitro:topo`. Pour ajouter cette int√©raction, la formule deviendra `yield ~ nitro + topo + nitro:topo`. Une approche √©quivalente est d'utiliser le raccourci `yield ~ nitro*topo`.

```{r}
modlin_5_sc <- lm(yield ~ nitro*topo,
               data = lasrosas.corn_sc)
summary(modlin_5_sc)
```

Les r√©sultats montre des effets de l'azote et des cat√©gories topographiques, mais il y a davantage d'incertitude sur les int√©ractions, indiquant que l'effet statistique de l'azote est sensiblement le m√™me ind√©pendamment des niveaux topographiques.

--- 

### Attention √† ne pas surcharger le mod√®le

Il est possible d'ajouter des int√©ractions doubles, triples, quadruples, etc. Mais plus il y a d'int√©ractions, plus votre mod√®le comprendra de variables et vos tests d'hypoth√®se perdront en puissance statistique.

---

### Les mod√®les lin√©aires g√©n√©ralis√©s

Dans un mod√®le lin√©aire ordinaire, un changement constant dans les variables explicatives r√©sulte en un changement constant de la variable-r√©ponse. Cette supposition ne serait pas ad√©quate si la variable-r√©ponse √©tait un d√©compte, si elle est bool√©enne ou si, de mani√®re g√©n√©rale, la variable-r√©ponse ne suivait pas une distribution continue. Ou, de mani√®re plus sp√©cifique, il n'y a pas moyen de retrouver une distribution normale des r√©sidus? On pourra bien s√ªr transformer les variables (sujet du chapitre 6, en d√©veloppement). Mais il pourrait s'av√©rer impossible, ou tout simplement non souhaitable de transformer les variables. Le mod√®le lin√©aire g√©n√©ralis√© (MLG, ou *generalized linear model* - GLM) est une g√©n√©ralisation du mod√®le lin√©aire ordinaire chez qui la variable-r√©ponse peut √™tre caract√©ris√© par une distribution de Poisson, de Bernouilli, etc.

Prenons d'abord cas d'un d√©compte de vers fil-de-fer (`worms`) retrouv√©s dans des parcelles sous diff√©rents traitements (`trt`). Les d√©comptes sont typiquement distribu√© selon une loi de Poisson.

```{r}
cochran.wireworms %>% ggplot(aes(x = worms)) + geom_histogram()
```

Explorons les d√©comptes selon les traitements.

```{r}
cochran.wireworms %>% ggplot(aes(x = trt, y = worms)) + geom_boxplot()
```

Les traitements semble √† premi√®re vue avoir un effet comparativement au contr√¥le. Lan√ßons un MLG avec la fonction `glm()`, et sp√©cifions que la sortie est une distribution de Poisson.

```{r}
modglm_1 <- glm(worms ~ trt, cochran.wireworms, family = "poisson")
summary(modglm_1)
```

Il est tr√®s probable (p-value de ~0.66) qu'un intercept de 0.18 ayant une erreur standard de 0.4082 ait √©t√© g√©n√©r√© depuis une population dont l'intercept est nul: autrement dit, le contr√¥le n'a probablement pas eu d'effet. Quant aux autres traitements, leurs effets sont tous significatifs au seuil 0.05, mais peuvent-ils √™tre consid√©r√©s comme √©quivalents?

```{r}
intervals <- tibble(Estimate = coefficients(modglm_1), # [-1] enlever l'intercept
                    LL = confint(modglm_1)[, 1], # [-1, ] enlever la premi√®re ligne, celle de l'intercept
                    UL = confint(modglm_1)[, 2],
                    variable = names(coefficients(modglm_1))) 
intervals
```

```{r}
ggplot(data = intervals, mapping = aes(x = Estimate, y = variable)) +
    geom_vline(xintercept = 0, lty = 2) +
    geom_segment(mapping = aes(x = LL, xend = UL, 
                               y = variable, yend = variable)) +
    geom_point() +
    labs(x = "Coefficient", y = "")
```

Les intervales de confiance se superposant, on ne peut pas conclure qu'un traitement est li√© √† une r√©duction plus importante de vers qu'un autre, au seuil 0.05.

Maintenant, √† d√©faut de trouver un tableau de donn√©es plus appropri√©, prenons le tableau `mtcars`, qui rassemble des donn√©es sur des mod√®les de voitures. La colonne `vs`, pour v-shaped, inscrit 0 si les pistons sont droit et 1 s'ils sont plac√©s en V dans le moteur. Peut-on expliquer la forme des pistons selon le poids du v√©hicule (`wt`)?

```{r}
mtcars %>% sample_n(6)
```

```{r}
mtcars %>% 
    ggplot(aes(x = wt, y = vs)) + geom_point()
```

Il semble y avoir une tendance: les v√©hicules plus lourds ont plut√¥t des pistons droits (`vs = 0`). V√©rifions cela.

```{r}
modglm_2 <- glm(vs ~ wt, data = mtcars, family = binomial)
summary(modglm_2)
```

**Exercice**. Analyser les r√©sultats.

### Les mod√®les non-lin√©aires

La hauteur d'un arbre en fonction du temps n'est typiquement pas lin√©aire. Elle tend √† cro√Ætre de plus en plus lentement jusqu'√† un plateau. De m√™me, le rendement d'une culture trait√© avec des doses croissantes de fertilisants tend √† atteindre un maximum, puis √† se stabiliser.

Ces ph√©nom√®nes ne peuvent pas √™tre approxim√©s par des mod√®les lin√©aires. Examinons les donn√©es du tableau `engelstad.nitro`.

```{r}
engelstad.nitro %>% sample_n(10)
```

```{r}
engelstad.nitro %>%
    ggplot(aes(x = nitro, y = yield)) +
        facet_grid(year ~ loc) +
        geom_line() +
        geom_point()
```

Le mod√®le de Mitscherlich pourrait √™tre utilis√©.

$$ y = A \left( 1 - e^{-R \left( E + x \right)} \right) $$

o√π $y$ est le rendement, $x$ est la dose, $A$ est l'asymptote vers laquelle la courbe converge √† dose croissante, $E$ est l'√©quivalent de dose fourni par l'environnement et $R$ est le taux de r√©ponse.

Explorons la fonction.

```{r}
mitscherlich_f <- function(x, A, E, R) {
    A * (1 - exp(-R*(E + x)))
}

x <- seq(0, 350, by = 5)
y <- mitscherlich_f(x, A = 75, E = 30, R = 0.02)

ggplot(tibble(x, y), aes(x, y)) +
    geom_point(data = engelstad.nitro, aes(x = nitro, y = yield)) +
    geom_line() + ylim(c(0, 100))
```

**Exercice**. Changez les param√®tres pour visualiser comment la courbe r√©agit.

Nous pouvons d√©crire le mod√®le gr√¢ce √† l'interface formule dans la fonction `nls()`. Notez que les mod√®les non-lin√©aires demandent des strat√©gies de calcul diff√©rentes de celles des mod√®les lin√©aires. En tout temps, nous devons identifier des valeurs de d√©part raisonnables pour les param√®tres dans l'argument `start`. Vous r√©ussirez rarement √† obtenir une convergence du premier coup avec vos param√®tres de d√©part. Le d√©fi est d'en trouver qui permettront au mod√®le de converger. Parfois, le mod√®le ne convergera jamais. D'autres fois, il convergera vers des solutions diff√©rentes selon les variables de d√©part choisies.
<
```{r}
modnl_1 <-  nls(yield ~ A * (1 - exp(-R*(E + nitro))),
                data = engelstad.nitro,
                start = list(A = 50, E = 10, R = 0.2))
```

Le mod√®le ne coverge pas. Essayons les valeurs prises plus haut, lors de la cr√©ation du graphique, qui semblent bien s'ajuster.

```{r}
modnl_1 <-  nls(yield ~ A * (1 - exp(-R*(E + nitro))),
                data = engelstad.nitro,
                start = list(A = 75, E = 30, R = 0.02))
```

Bingo! Voyons maintenant le sommaire.

```{r}
summary(modnl_1)
```

Les param√®tres sont significativement diff√©rents de z√©ro au seuil 0.05, et donnent la courbe suivante.

```{r}
x <- seq(0, 350, by = 5)
y <- mitscherlich_f(x,
                    A = coefficients(modnl_1)[1],
                    E = coefficients(modnl_1)[2],
                    R = coefficients(modnl_1)[3])

ggplot(tibble(x, y), aes(x, y)) +
    geom_point(data = engelstad.nitro, aes(x = nitro, y = yield)) +
    geom_line() + ylim(c(0, 100))
```

Et les r√©sidus...

```{r}
tibble(res = residuals(modnl_1)) %>%
    ggplot(aes(x = res)) + geom_histogram(bins = 20)
```

```{r}
tibble(nitro = engelstad.nitro$nitro, res = residuals(modnl_1)) %>%
    ggplot(aes(x = nitro, y = res)) + 
        geom_point() +
        geom_hline(yintercept = 0, colour = "red")
```

Les r√©sidus ne sont pas distribu√©s normalement, mais semble bien partag√©s de part et d'autre de la courbe.

## Mod√®les √† effets mixtes

Lorsque l'on combine des variables fixes (test√©es lors de l'exp√©rience) et des variables al√©atoire (variation des unit√©s exp√©rimentales), on obtient un mod√®le mixte. Les mod√®les mixtes peuvent √™tre univari√©s, multivari√©s, lin√©aires ordinaires ou g√©n√©ralis√©s ou non lin√©aires.

√Ä la diff√©rence d'un effet fixe, un effet al√©atoire sera toujours distribu√© normalement avec une moyenne de 0 et une certaine variance. Dans un mod√®le lin√©aire o√π l'effet al√©atoire est un d√©calage d'intercept, cet effet s'additionne aux effets fixes:

$$ y = X \beta + Z b + \epsilon $$

o√π:

$Z$ est la matrice du mod√®le √† $n$ observations et $p$ variables al√©atoires. Les variables al√©atoires sont souvent des variables nominales qui subissent un encodage cat√©goriel.

$$ Z = \left( \begin{matrix} 
z_{11} & \cdots & z_{1p}  \\ 
z_{21} & \cdots & z_{2p}  \\ 
\vdots & \ddots & \vdots  \\ 
z_{n1} & \cdots & z_{np}
\end{matrix} \right) $$

$b$ est la matrice des $p$ coefficients al√©atoires.

$$ b = \left( \begin{matrix} 
b_0  \\ 
b_1  \\ 
\vdots \\ 
b_p 
\end{matrix} \right) $$

Le tableau `lasrosas.corn`, utilis√© pr√©c√©demment, contenait trois r√©p√©titions effectu√©s au cours de deux ann√©es, 1999 et 2001. √âtant donn√© que la r√©p√©tition R1 de 1999 n'a rien √† voir avec la r√©p√©tition R1 de 2001, on dit qu'elle est **embo√Æt√©e** dans l'ann√©e.

Le module `nlme` nous aidera √† monter notre mod√®le mixte.

```{r}
library("nlme")

mmodlin_1 <- lme(fixed = yield ~ lat + long + nitro + topo + bv,
                 random = ~ 1|year/rep,
                 data = lasrosas.corn)

```

√Ä ce stade vous devriez commencer √† √™tre familier avec l'interface formule et vous deviez saisir l'argument `fixed`, qui d√©signe l'effet fixe. L'effet al√©atoire, `random`, suit un tilde `~`. √Ä gauche de la barre verticale `|`, on place les variables d√©signant les effets al√©atoire sur la pente. Nous n'avons pas couvert cet aspect, alors nous le laissons √† `1`. √Ä droite, on retrouve un structure d'embo√Ætement d√©signant l'effet al√©atoire: le premier niveau est l'ann√©e, dans laquelle est embo√Æt√©e la r√©p√©tition.

```{r}
summary(mmodlin_1)
```

La sortie est semblable √† celle de la fonction `lm()`.

### Mod√®les mixtes non-lin√©aires

Le mod√®le non lin√©aire cr√©√© plus haut liait le rendement √† la dose d'azote. Toutefois, les unit√©s exp√©rimentales (le site `loc` et l'ann√©e `year`) n'√©taient pas pris en consid√©ration. Nous allons maintenant les consid√©rer. 

Nous devons d√©cider la structure de l'effet al√©atoire, et sur quelles variables il doit √™tre appliqu√© - la d√©cision appartient √† l'analyste. Il me semble plus convenable de supposer que le site et l'ann√©e affectera le rendement maximum plut√¥t que l'environnement et le taux: les effets al√©atoires seront donc affect√©s √† la variable `A`. Les effets al√©atoires n'ont pas de structure d'embo√Ætement. L'effet de l'ann√©e sur A sera celui d'une pente et l'effet de site sera celui de l'intercept. La fonction que nous utiliserons est `nlme()`.

```{r}
mm <- nlme(yield ~ A * (1 - exp(-R*(E + nitro))),
           data = engelstad.nitro, 
           start = c(A = 75, E = 30, R = 0.02), 
           fixed = list(A ~ 1, E ~ 1, R ~ 1), 
           random = A ~ year | loc)
summary(mm)
```

Les mod√®les mixtes non lin√©aires peuvent devenir tr√®s complexes lorsque les param√®tres, par exemple A, E et R, sont eux-m√™me affect√©s lin√©airement par des variables (par exemple `A ~ topo`). Pour aller plus loin, consultez [Parent et al. (2017) ](https://doi.org/10.3389/fenvs.2017.00081) ainsi que les [calculs associ√©s √† l'article](https://github.com/essicolo/site-specific-multilevel-modeling-of-potato-response-to-nitrogen-fertilization). Ou √©crivez-moi un courriel pour en discuter!

**Note**. L'interpr√©tation de p-values sur les mod√®les mixtes est controvers√©e. √Ä ce sujet, ??? Bates a √©crit une longue lettre √† la communaut√© de d√©veloppement du module `lme4`, une alternative √† `nlme`, qui remet en cause l'utilisation des p-values, [ici](https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html). De plus en plus, pour les mod√®les mixtes, on se tourne vers les statistiques bay√©siennes, couvertes dans l'annexe de cette section (`statitiques_bayes.ipynb`, en d√©veloppement). √Ä cet effet, le module [`brms`](https://github.com/paul-buerkner/brms) automatise bien des aspects de la mod√©lisation mixte bay√©sienne.

## Aller plus loin

### Statistiques g√©n√©rales:
- [The analysis of biological data](https://ariane25.bibl.ulaval.ca/ariane/wicket/detail?c=ariane&m=S&rq.ct=PE&rq.fa=false&rq.r.esc=false&rq.r.l%5B0%5D.c=TI&rq.r.l%5B0%5D.ex=false&rq.r.l%5B0%5D.op=AND&rq.r.l%5B0%5D.v=the+analysis+of+biological+data&rq.r.la=*&rq.r.loc=*&rq.r.pft=true&rq.r.ta=*&rq.r.td=*&rq.rows=5&rq.st=1)

### Statistiques avec R

- Disponibles en version √©lectronique √† la biblioth√®que de l'Universit√© Laval:
    - Introduction aux statistiques avec R: [Introductory statistics with R](https://ariane25.bibl.ulaval.ca/ariane/wicket/detail?c=ariane&m=S&rq.ct=PE&rq.fa=false&rq.r.esc=false&rq.r.l%5B0%5D.c=TI&rq.r.l%5B0%5D.ex=false&rq.r.l%5B0%5D.op=AND&rq.r.l%5B0%5D.v=Introductory+statistics+with+R&rq.r.la=*&rq.r.loc=*&rq.r.pft=true&rq.r.ta=*&rq.r.td=*&rq.rows=1&rq.st=0)
    - Approfondir les statistiques avec R: [The R Book, Second edition](https://ariane25.bibl.ulaval.ca/ariane/wicket/detail?c=ariane&m=S&rq.ct=PE&rq.fa=false&rq.r.esc=false&rq.r.l%5B0%5D.c=TI&rq.r.l%5B0%5D.ex=false&rq.r.l%5B0%5D.op=AND&rq.r.l%5B0%5D.v=the+r+book&rq.r.la=*&rq.r.loc=*&rq.r.pft=true&rq.r.ta=*&rq.r.td=*&rq.rows=15&rq.st=2)
    - Approfondir les mod√®les √† effets mixtes acec R: [Mixed Effects Models and Extensions in Ecology with R](https://ariane25.bibl.ulaval.ca/ariane/wicket/detail?c=ariane&m=S&rq.ct=PE&rq.fa=false&rq.r.esc=false&rq.r.l%5B0%5D.c=TI&rq.r.l%5B0%5D.ex=false&rq.r.l%5B0%5D.op=AND&rq.r.l%5B0%5D.v=Mixed+Effects+Models+and+Extensions+in+Ecology+with+R&rq.r.la=*&rq.r.loc=*&rq.r.pft=false&rq.r.ta=*&rq.r.td=*&rq.rows=2&rq.st=1)
- [ModernDive](https://moderndive.com/index.html), un livre en ligne offrant une approche moderne avec le package `moderndive`.