--- 
site: bookdown::bookdown_site
output: bookdown::gitbook
---

# D√©tection de valeurs aberrantes et imputation de donn√©es manquantes {#chapitre-outliers}

***
Ô∏è\ **Objectifs sp√©cifiques**:

√Ä la fin de ce chapitre, vous

- saurez comment proc√©der √† l'imputation de valeurs manquantes en mode univari√© et multivari√©
- saurez comment d√©tecter des valeurs aberrantes en mode univari√© et multivari√©

***

> **Note**. Ce chapitre a √©t√© initialement r√©dig√© par Zonlehoua Coulibali, qui a gracieusement accept√© de contribuer √† ces notes de cours. Le texte a √©t√© adapt√© au format du manuel par Serge-√âtienne Parent.

Les donn√©es √©cologiques sont g√©n√©ralement recueillies √† diff√©rentes √©chelles, concernent plusieurs sites et plusieurs variables (corr√©l√©es ou non), impliquent diff√©rents individus de diff√©rentes agences et peuvent s'√©tendre sur plusieurs ann√©es ([Alameddine et al., 2010](https://ascelibrary.org/doi/10.1061/%28ASCE%29EE.1943-7870.0000271); [Lokupitiya et al., 2006](https://doi.org/10.1002/env.773)). De ce fait, la plupart de ces bases de donn√©es contiennent des valeurs manquantes et/ou aberrantes li√©es √† diff√©rentes sources d'erreurs, pouvant parfois limiter l'utilit√© des inf√©rences statistiques ([Collins et al., 2001](https://europepmc.org/abstract/MED/11778676); [Glasson-Cicognani et Berchtold, 2010](https://hal.inria.fr/inria-00494698/document)). Il convient alors de les traiter correctement avant d'effectuer les analyses statistiques car les ignorer peut entra√Æner, outre une perte de pr√©cision, de forts biais dans les mod√®les d'analyse ([Alameddine et al., 2010](https://ascelibrary.org/doi/10.1061/%28ASCE%29EE.1943-7870.0000271); [Filzmoser et al., 2008](https://www.researchgate.net/publication/222423330_Outlier_identification_in_high_dimension); [Glasson-Cicognani et Berchtold, 2010](https://hal.inria.fr/inria-00494698/document)).

## Donn√©es manquantes: d√©finition, origine, typologie et traitement

### D√©finition

Les tableaux de donn√©es sont organis√©s en lignes et colonnes. Les lignes repr√©sentent les observations, les unit√©s, les sujets ou les cas √©tudi√©s selon le contexte, et les colonnes repr√©sentent les variables mesur√©es pour chaque observation. Les entr√©es qui sont les valeurs (ou contenus) des cellules ou encore les valeurs observ√©es, peuvent √™tre des valeurs continues, ou des valeurs cat√©goriales ([Little et Rubin, 2002](https://onlinelibrary.wiley.com/doi/book/10.1002/9781119013563)). Consid√©rant une variable al√©atoire $X$ quelconque, une donn√©e manquante $x_m$, est une donn√©e pour laquelle la valeur  de la variable $X$ est inconnue (ou absente). En d'autres termes, on ne dispose pas de la valeur de $X$ pour le sujet $i$ donn√©. C'est une donn√©e non disponible qui serait utile pour l'analyse si elle √©tait observ√©e ([Ware  et al., 2012](https://www.nejm.org/doi/full/10.1056/NEJMsm1210043)).

La litt√©rature sur les donn√©es manquantes est plus abondante dans les domaines des sciences sociales sur les donn√©es d'enqu√™tes, et des sciences m√©dicales ([Davey et al., 2001](https://www.jstor.org/stable/3069628?seq=1/subjects); [Graham, 2012](https://www.springer.com/us/book/9781461440178)). Pour repr√©senter leur r√©partition dans la table de donn√©es, une matrice indicatrice des valeurs manquantes $M = (m_{ij})$ est g√©n√©ralement utilis√©e o√π $m_{ij}$ est une variable binaire qui prend la valeur 1 si la valeur de la variable ($X$) est observ√©e et 0 si $x$ est absent ([Collins et al., 2001](https://europepmc.org/abstract/MED/11778676); [Graham, 2012](https://www.springer.com/us/book/9781461440178); [Little et Rubin, 2002](https://onlinelibrary.wiley.com/doi/book/10.1002/9781119013563)).

### Origines des donn√©es manquantes

Les donn√©es manquantes ont des origines mat√©rielles diverses. Des valeurs peuvent √™tre absentes soit parce qu'elles n'ont pas √©t√© observ√©es, ou qu'elles ont √©t√© perdues ou √©taient incoh√©rentes ([Glasson-Cicognani et Berchtold, 2010]([Glasson-Cicognani et Berchtold, 2010](https://hal.inria.fr/inria-00494698/document)). La donn√©e peut avoir √©t√©

- perdue lors de la collecte ou du processus d'enregistrement des donn√©es,
- non mesur√©e en raison du dysfonctionnement d'un √©quipement,
- non mesurable en raison de la disparition du sujet d'√©tude (mort, fugue, champ non r√©colt√©, etc.),
- √©cart√©e en raison d'une contamination,
- oubli√©e,
- non √©tudi√©e,
- etc.

### Profils des donn√©es manquantes

Les auteurs traitant des donn√©es manquantes distinguent des formes de r√©partition des donn√©es manquantes et des m√©canismes conduisant √† ces derni√®res. La r√©partition des donn√©es manquantes d√©crit les dispositions des valeurs pr√©sentes et celles qui sont manquantes dans la matrice indicatrice. Les m√©canismes √† l'origine des donn√©es manquantes d√©crivent la relation probabiliste entre les valeurs observ√©es et les valeurs manquantes de la table de donn√©es.

#### R√©partition des donn√©es manquantes

Les donn√©es manquantes se r√©partissent selon diff√©rents cas de figures ([Graham, 2012](https://www.springer.com/us/book/9781461440178); [Little et Rubin, 2002](https://onlinelibrary.wiley.com/doi/book/10.1002/9781119013563)) dont les trois principaux sont

- les valeurs manquantes univari√©es,
- les valeurs manquantes monotones et
- celles non monotones ou arbitraires.

Cette distinction est fonction de la matrice indicatrice des valeurs manquantes. Cette matrice est dite √† **valeurs manquantes univari√©es** ou de non-r√©ponse univari√©e, lorsque pour une variable donn√©e, si une observation est absente, alors toutes les observations suivantes pour cette variable sont absentes (figure \@ref(fig:mv-types)a). En exp√©rimentation agricole, ce cas de figure est qualifi√© de probl√®me de la parcelle manquante o√π, pour une raison quelconque (par exemple : une absence de germination, une destruction accidentelle d'une parcelle ou des enregistrements incorrects), un facteur √† l'√©tude est non disponible. Les **valeurs manquantes monotones** surviennent lorsque la valeur d'une variable $Y_j$ manquante pour un individu $i$ implique que toutes les variables suivantes $Y_k$ ($k > j$) sont manquantes pour cet individu (figure \@ref(fig:mv-types)b). Les **valeurs manquantes arbitraires** ou non monotones ou encore g√©n√©rales, surviennent lorsque la matrice ne dessine sp√©cifiquement aucune des formes pr√©c√©dentes (figure \@ref(fig:mv-types)c).

```{r mv-types, out.width='100%', fig.align='center', fig.cap="Exemple de profils de donn√©es manquantes", echo = FALSE}
knitr::include_graphics('images/08_mv-types.png')
```

Le module VIM permet de visualiser la structure des donn√©es manquantes.

```{r mv-vim-tidyverse, echo = FALSE}
library("VIM")
library("tidyverse")
```

Pour l'exemple, prenons le tableau `iris` puis rempla√ßons au hasard des donn√©es par des valeurs manquantes (`NA`), puis v√©rifions les proportions de donn√©es manquantes et les proportions de combinaisons de donn√©es manquantes.

```{r mv-add-na-iris}
set.seed(2868374)

data("iris")
iris_NA <- iris
n_NA <- 20
row_NA <- sample(1:nrow(iris), n_NA, replace = TRUE)
col_NA <- sample(1:ncol(iris), n_NA, replace = TRUE)
for (i in 1:n_NA) iris_NA[row_NA[i], col_NA[i]] <- NA

summary(aggr(iris_NA, sortVar = TRUE))
```

Avec la fonction `matrixplot`, il est possible de visualiser les donn√©es manquantes en rouge, tandis que les donn√©es pr√©sentes prennent un niveau de gris selon leur valeur.

```{r mv-plot-na-iris, fig.width=2, fig.height=5}
matrixplot(iris_NA)
```

#### M√©canismes conduisant aux donn√©es manquantes

Les m√©canismes conduisant aux donn√©es manquantes d√©crivent la relation entre les valeurs manquantes et celles observ√©es des variables de la table (Collins et al., 2001; [Graham, 2012](https://www.springer.com/us/book/9781461440178); [Little et Rubin, 2002](https://onlinelibrary.wiley.com/doi/book/10.1002/9781119013563)). En consid√©rant la table de donn√©e $Y = \{O,M\}$ o√π $O = \left[ o_{i, j} \right]$ repr√©sente les donn√©es observ√©es et $M = \left[ m_{i, j} \right]$ la matrice indicatrice des donn√©es manquantes, le m√©canisme √† l'origine des donn√©es manquantes est d√©fini par la distribution conditionnelle de $M$ sachant $Y$.

Lorsque la probabilit√© qu'une valeur soit manquante ne d√©pend ni des valeurs observ√©es, ni de celles manquantes, les donn√©es sont dites **manquantes compl√®tement au hasard** (* **MCAR**, missing completely at random*). La probabilit√© d'absence est donc la m√™me pour toutes les observations et elle ne d√©pend que de param√®tres ext√©rieurs ind√©pendants de cette variable (Collins et al., 2001; [Graham, 2012](https://www.springer.com/us/book/9781461440178); [Heitjan, 1997](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1380829/); [Little et Rubin, 2002](https://onlinelibrary.wiley.com/doi/book/10.1002/9781119013563); [Rubin, 1976](https://www.jstor.org/stable/2335739?seq=1#page_scan_tab_contents)). Avec de telles donn√©es (MCAR), les r√©gressions qui n'utilisent que les enregistrements complets, les moyennes des cas disponibles, les tests non-param√©triques et les m√©thodes bas√©es sur les "moments", sont toutes valides ([Heitjan, 1997](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1380829/)). Toutefois, une perte de pr√©cision est √† pr√©voir dans les r√©sultats ([Collins et al., 2001](https://europepmc.org/abstract/MED/11778676)).

Selon les m√™mes auteurs, lorsque la probabilit√© qu'une valeur soit manquante d√©pend uniquement de la composante observ√©e "O" (une ou plusieurs variables observ√©es) mais pas des valeurs manquantes elles-m√™mes, les donn√©es sont dites **manquantes au hasard** (* **MAR**: missing at random*). Dans ce cas, les m√©thodes du maximum de vraisemblance sont valides pour estimer les param√®tres du mod√®le. Les proc√©dures d'imputation multiples utilisent implicitement le m√©canisme MAR ([Collins et al., 2001](https://europepmc.org/abstract/MED/11778676); [Heitjan, 1997](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1380829/)).

Lorsque la probabilit√© qu'une valeur manque d√©pend de la valeur non observ√©e de la variable elle-m√™me ($M$), les donn√©es ne manquent pas au hasard (* **MNAR**: missing not at random*). Ce type de donn√©es ne doit pas √™tre ignor√© dans l'ajustement de mod√®les car elles induisent une perte de pr√©cision (inh√©rente √† tout cas de donn√©es manquantes) mais aussi un biais dans l'estimation des param√®tres ([Collins et al., 2001](https://europepmc.org/abstract/MED/11778676); [Heitjan, 1997](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1380829/)).

### Traitement des donn√©es manquantes

La pr√©sence de donn√©es manquantes dans une analyse peut conduire √† des estim√©s de param√®tres biais√©s, gonfler les erreurs de type I et II, baisser les performances des intervalles de confiance ([Collins et al., 2001](https://europepmc.org/abstract/MED/11778676)) et entacher la g√©n√©ralisation des r√©sultats ([Taylor et al., 2002](https://www.ncbi.nlm.nih.gov/pubmed/12370166)). Plusieurs m√©thodes existent pour calculer des estim√©s de param√®tres de mod√®les approximativement sans biais, en pr√©sence de donn√©es manquantes.

#### L'analyse des cas complets

Cette m√©thode consiste √† exclure du fichier de donn√©es tous les individus ayant au moins une donn√©e manquante ([Glasson-Cicognani et Berchtold, 2010]([Glasson-Cicognani et Berchtold, 2010](https://hal.inria.fr/inria-00494698/document)). Elle serait la plus utilis√©e pour traiter les valeurs manquantes mais n'est efficace que pour les cas de donn√©es manquant compl√®tement au hasard (MCAR) lorsque le nombre de d'observations √† √©liminer n'est pas trop important ([Davey et al., 2001](https://www.jstor.org/stable/3069628?seq=1/subjects)).

En R, de mani√®re g√©n√©rique, il est possible d'identifier une donn√©e manquante dans un tableau, une matrice ou un vecteur avec `is.na`, qui retourne un objet bool√©en (`TRUE` / `FALSE`). La fonction `any` permet d'identifier si au moins une valeur est vraie ou fausse dans un objet, alors que la fonction `all` permet d'identifier si toutes les valeurs sont vraies. On pourra v√©rifier si une ligne contient une valeur manquante avec la fonction `apply`, dans l'axe des lignes. Il faudra toutefois inverser le r√©sultat bool√©en avec un `!` pour faire en sorte que l'on √©carte les valeurs manquantes.

```{r mv-remove-na1}
row_missing <- iris_NA %>%
        filter(apply(., 1, function(x) any(is.na(x))))
row_complete <- iris_NA %>%
        filter(!apply(., 1, function(x) any(is.na(x))))
row_missing
```

Au lieu de `apply`, R fournit la fontion raccourci `complete.cases`.

```{r mv-remove-na2}
row_missing <- iris_NA %>%
        filter(complete.cases(.))
```

Le module **`tidyr`** (inclus dans tidyverse) nous facilite la vie avec la fonction `tidyr::drop_na`, qui retire toutes les lignes contenant au moins une valeur manquante.

```{r mv-remove-na3}
row_complete <- iris_NA %>%
        drop_na()
```

De m√™me, on pourra √©valuer la proportion de donn√©es manquantes.

```{r mv-remove-na-pourc}
nrow(row_complete) / nrow(iris)
```

Ou bien, √©valuer la proportion de donn√©e manquante par groupe.

```{r mv-remove-na-pourc-par-groupe}
iris_NA %>%
  group_by(Species) %>%
  summarise_each(funs(sum(is.na(.))/length(.)))
```

Pour terminer cette section, il est possible que certaines variables soient peu mesur√©es dans une √©tude. Au jugement, on pourra sacrifier une colonne contenant plusieurs donn√©es manquantes en vue de conserver des lignes.

#### L'imputation

L'imputation permet de cr√©er des bases de donn√©es compl√®tes ([Donz√©, 2001](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1012.8980&rep=rep1&type=pdf)). Elle corrige la non-r√©ponse partielle en substituant une "valeur artificielle" √† la valeur manquante. Les auteurs distinguent l'imputation unique et l'imputation multiple.

##### L'imputation unique

*L'imputation unique consiste √† remplacer chaque donn√©e manquante par une seule valeur plausible telle que la moyenne calcul√©e sur les donn√©es r√©ellement observ√©es, l'imputation par le ou les plus proche(s) voisin(s)* (la technique des plus proches voisins est couverte au chapitre \@ref(chapitre-ml)). Cette derni√®re remplace les donn√©es manquantes par des valeurs provenant d'individus similaires pour lesquels toute l'information a √©t√© observ√©e. L'imputation peut aussi se faire par r√©gression en rempla√ßant les valeurs manquantes par des valeurs pr√©dites selon un mod√®le de r√©gression ou des m√©thodes bay√©siennes plus sophistiqu√©es. L'imputation unique est valide en pr√©sence de donn√©es manquantes de type MAR ([Davey et al., 2001](https://www.jstor.org/stable/3069628?seq=1/subjects); [Donz√©, 2001](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1012.8980&rep=rep1&type=pdf); [Glasson-Cicognani et Berchtold, 2010]([Glasson-Cicognani et Berchtold, 2010](https://hal.inria.fr/inria-00494698/document)).

Selon [Heitjan (1997)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1380829/), il n'existe pas de r√®gles strictes pour d√©cider quand il faut entreprendre une imputation multiple. N√©anmoins, si la fraction des observations avec des donn√©es manquantes est inf√©rieure √† par exemple 5%, et le m√©canisme est ignorable (MCAR ou MAR), les analyses les plus simples sont satisfaisantes.

Bien que con√ßu principalement pour l'imputation multiple (on y arrive bient√¥t), le module mice permet l'imputation univari√©e. Nous allons tester l'imputation par la moyenne. Voyons par exemple la moyenne des longueurs des s√©pales.

```{r mv-imputation-mean}
mean(iris_NA$Sepal.Length[!complete.cases(iris_NA)], na.rm = TRUE)
```

Lan√ßons l'imputation par la fonction `mice`, puis la pr√©diction du tableau imput√© par la fonction `complete`.

```{r mv-imputation-mice, message=FALSE, warning=FALSE, results=FALSE}
library("mice")
iris_mice <- mice(iris_NA, method = "mean")
iris_imp <- complete(iris_mice)
```

Le tableau original peut √™tre compar√© au tableau imput√©.

```{r mv-imputation-mice-comparaison}
iris_NA[!complete.cases(iris_NA), ]
iris[!complete.cases(iris_NA), ]
iris_imp[!complete.cases(iris_NA), ]
```

Dans la colonne `Sepal.Length`, toutes les valeurs manquantes ont √©t√© remplac√©es par ~5.862.

**Exercice**. Pourquoi la pr√©diction diff√®re-t-elle de la moyenne?

----

üò± **Attention**. Lorsque les valeurs sont syst√©matiquement manquantes chez une cat√©gorie, les estimateurs seront biais√©s.

```{r mv-imputation-mice-biais}
iris_NA_biais_1 <- tibble(
  Sepal.Length = c(5.3, NA, 4.9, NA, 4.7, NA),
  Species = c("setosa", "versicolor", "setosa", "versicolor", "setosa", "versicolor")
)
mean(iris_NA_biais_1$Sepal.Length, na.rm = TRUE)

iris_NA_biais_2 <- tibble(
  Sepal.Length = c(5.3, 7.0, 4.6, 6.4, 4.8, 6.9),
  Species = c("setosa", "versicolor", "setosa", "versicolor", "setosa", "versicolor")
)
mean(iris_NA_biais_2$Sepal.Length, na.rm = TRUE)
```

Dans l'exemple pr√©c√©dent, les donn√©es sont syst√©matiquement manquantes chez l'esp√®ce *versicolor*. La moyenne de la longueur des s√©pales est donc biais√©e, et l'imputation par la moyenne de sera tout autant. L'imputation par la moyenne est jug√©e non recommandable par plusieurs statisticiens. Dans la mesure du possible, **l'imputation multiple devrait √™tre favoris√©e √† l'imputation univari√©e**.

----

##### L'imputation multiple

L'imputation multiple consiste √† imputer plusieurs fois les valeurs manquantes et √† combiner les r√©sultats pour diminuer l'erreur caus√©e par la compl√©tion ([Davey et al., 2001](https://www.jstor.org/stable/3069628?seq=1/subjects)). Les valeurs manquantes sont remplac√©es par $M$ ($M > 1$) ensembles de valeurs simul√©es donnant lieu √† $M$ versions plausibles mais diff√©rentes des donn√©es compl√®tes ([Collins et al., 2001](https://europepmc.org/abstract/MED/11778676); [Taylor et al., 2002](https://www.ncbi.nlm.nih.gov/pubmed/12370166)). En pratique, seulement $M$ allant de 5 √† 10 (imputations) est suffisant pour produire des bonnes inf√©rences ([Collins et al., 2001](https://europepmc.org/abstract/MED/11778676); [Donz√©, 2001](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1012.8980&rep=rep1&type=pdf)). Chacun des $M$ ensembles de donn√©es est analys√© de la m√™me mani√®re par des m√©thodes standards d'analyse de donn√©es compl√®tes, et les r√©sultats sont combin√©s en utilisant une arithm√©tique simple: les moyennes des param√®tres estim√©s sont calcul√©es, les erreurs standards sont combin√©es pour refleter l'incertitude des donn√©es manquantes et l'erreur d'√©chantillonnage.

L'imputation multiple est une proc√©dure bas√©e sur un mod√®le (model-based). L'utilisateur doit sp√©cifier un mod√®le de probabilit√© conjointe pour les donn√©es observ√©es et manquantes ([Collins et al., 2001](https://europepmc.org/abstract/MED/11778676); [Taylor et al., 2002](https://www.ncbi.nlm.nih.gov/pubmed/12370166)).

Le module mice donne acc√®s √† plusieurs types de mod√®les (argument `method`). Les mod√®les `cart` et `rf` tombent la la cat√©gorie de l'autoapprentissage (couvert au chapitre \@ref(chapitre-ml)). Ils ont l'avantage important d'√™tre applicables autant pour tout type de variable.

```{r mv-mult-imputation-mice, message=FALSE, warning=FALSE, results=FALSE}
iris_mice <- mice(iris_NA, method = "rf")
iris_imp <- complete(iris_mice)
```

De m√™me que pr√©c√©demment, le tableau original peut √™tre compar√© au tableau imput√©.

```{r mv-mult-imputation-mice-comparaison}
iris_NA[!complete.cases(iris_NA), ]
iris[!complete.cases(iris_NA), ]
iris_imp[!complete.cases(iris_NA), ]
```

Mieux vauit √©viter d'imputer des donn√©es compositionnelles transform√©es (alr, clr ou ilr), car l'imputation d'une dimension transform√©e aura un impact sur tout le vecteur. Dans ce cas, vous pourriez pr√©f√©rablemen utiliser la fonction `robCompositions::impCoda`.

----

Vous avez peut-√™tre remarqu√© que le mode tidyverse a √©t√© quelque peu d√©laiss√© dans cette section. Il aurait pu l'√™tre davantage, mais le mode classique (`iris[!complete.cases(iris_NA), ]` au lieu de `iris %>% drop_na()`) semblait mieux convenir pour la diversit√© de fonctions en imputation. Le module **`recipes`**, couvert rapidement au chapitre \@ref(chapitre-explorer), permet d'effectuer des op√©rations d'imputation modernes en *pipelines* (voir [*Step Functions - Imputation*](https://tidymodels.github.io/recipes/reference/index.html)). Ce module est toutefois en d√©veloppement et ne me semble pas suffisamment mature pour une utilisation professionnelle. Dans le futur, **`recipes`** deviendra probablement le module de choix pour l'imputation.

## Valeurs et √©chantillons aberrants: d√©finition, origines, m√©thodes de d√©tection et traitement

### D√©finitions

En analyse univari√©e, une valeur aberrante est une "donn√©e observ√©e" pour une variable qui semble anormale au regard des valeurs dont on dispose pour les autres observations de l'√©chantillon ([Planchon, 2005](https://www.researchgate.net/publication/26406403_Traitement_des_valeurs_aberrantes_concepts_actuels_et_tendances_generales)). En analyse multivari√©e, l'√©chantillon aberrant r√©sulte d'une erreur importante se trouvant dans un des composants du vecteur de r√©ponse, ou de petites erreurs syst√©matiques dans chacun de ses composants, et qui de ce fait, ne partage pas les relations entre les variables de la population ([Planchon, 2005](https://www.researchgate.net/publication/26406403_Traitement_des_valeurs_aberrantes_concepts_actuels_et_tendances_generales)).

La valeur ou l'observation aberrante est statistiquement discordante dans le contexte d'un mod√®le de probabilit√© suppos√© connu ([Barnett et Lewis, 1994](https://www.wiley.com/en-us/Outliers+in+Statistical+Data%2C+3rd+Edition-p-9780471930945); [Grubbs, 1969](https://www.tandfonline.com/doi/abs/10.1080/00401706.1969.10490657); [Munoz-Garcia et al., 1990](https://www.jstor.org/stable/1403805?seq=1#page_scan_tab_contents); [Pires et Santos-Pereira, 2005](https://www.researchgate.net/publication/239850370_Using_Clustering_and_Robust_Estimators_to_Detect_Outliers_in_Multivariate_Data)). Leur pr√©sence dans les donn√©es peut conduire √† des estimateurs de param√®tres biais√©s et, suite √† la r√©alisation de tests statistiques, √† une interpr√©tation des r√©sultats erron√©e ([Planchon, 2005](https://www.researchgate.net/publication/26406403_Traitement_des_valeurs_aberrantes_concepts_actuels_et_tendances_generales)).

### Origines

Dans une collecte de donn√©es, plusieurs sources de variabilit√© peuvent mener √† des donn√©es aberrantes: la variabilit√© inh√©rente mais inusit√©e ou erreur syst√©matique, l'erreur de mesure et l'erreur d'ex√©cution (figure \@ref(fig:va-origine)) ([Barnett et Lewis, 1994](https://www.wiley.com/en-us/Outliers+in+Statistical+Data%2C+3rd+Edition-p-9780471930945); [Planchon, 2005](https://www.researchgate.net/publication/26406403_Traitement_des_valeurs_aberrantes_concepts_actuels_et_tendances_generales)).

```{r mv-va-origine, out.width='100%', fig.align='center', fig.cap="Sch√©ma g√©n√©ral de traitement des valeurs aberrantes - adapt√© de Barnett et Lewis, 1994", echo = FALSE}
knitr::include_graphics('images/08_origine-va.png')
```

La variabilit√© inh√©rente est celle par laquelle les observations varient naturellement de mani√®re al√©atoire √† travers la population. L'erreur de mesure renferme les inad√©quations au niveau de la m√©thode de mesure, des instruments de mesure, l'arrondi des valeurs obtenues ou les erreurs d'enregistrement. Cette erreur est donc li√©e √† des circonstances bien d√©termin√©es. Les erreurs d'ex√©cution interviennent √©galement dans des circonstances bien d√©termin√©es. Ce sont les erreurs de manipulation, les erreurs commises dans l'assemblage des donn√©es, ou lors du traitement informatique.

L'examen des valeurs aberrantes dans une base de donn√©es a pour objectif de les identifier pour soit les supprimer, soit les conserver, ou les corriger avant d'ajuster des mod√®les non robustes ([Filzmoser et al., 2008](https://www.researchgate.net/publication/222423330_Outlier_identification_in_high_dimension); [Planchon, 2005](https://www.researchgate.net/publication/26406403_Traitement_des_valeurs_aberrantes_concepts_actuels_et_tendances_generales)). La valeur extr√™me peut √™tre li√©e √† un √©v√©nement atypique, mais n√©anmoins connu et int√©ressant √† √©tudier. Dans ce cas elle est importante √† conserver. La correction (ou accommodation) √©vite le rejet des observations aberrantes et consiste √† estimer les valeurs des param√®tres de la distribution de base de fa√ßon relativement libre sans d√©formation des r√©sultats li√©s √† leur pr√©sence ([Barnett et Lewis, 1994](https://www.wiley.com/en-us/Outliers+in+Statistical+Data%2C+3rd+Edition-p-9780471930945)).

L'approche d'identification des observations aberrantes selon [Davies et Gather (1993)](https://www.tandfonline.com/doi/abs/10.1080/01621459.1993.10476339) est de supposer qu'elles ont une distribution diff√©rente de celle du reste des observations. [Reimann et al. (2005)](https://doi.org/10.1016/j.scitotenv.2004.11.023) les distinguent ainsi des valeurs extr√™mes qui, bien qu'√©loign√©es du centre du nuage, appartiennent √† la m√™me distribution que les autres observations.

### D√©tection et traitement des valeurs aberrantes univari√©s

En analyse univari√©e, les m√©thodes graphiques telles que le diagramme de dispersion des observations class√©es en fonction de leur rang, les boxplots, les graphiques des quantiles de valeurs brutes ou des r√©sidus, permettent de signaler la pr√©sence de valeurs aberrantes ([Planchon, 2005](https://www.researchgate.net/publication/26406403_Traitement_des_valeurs_aberrantes_concepts_actuels_et_tendances_generales)).

Prenons, par exemple, les donn√©es `mtcars`.

```{r mv-load-mtcars}
data("mtcars")
mtcars %>% 
  sample_n(5)
```

Disons que nous cherchons √† d√©tecter les valeurs aberrantes de la puissance du moteur, soit la colonne `hp`. On pourrait jeter un oeil √† la colonne `hp`, mais mieux vaudrait consid√©rer qu'il ne s'agit pas de moteurs de m√™me type. De m√™me, si vous consigniez la masse des abeilles d'esp√®ces dif√©rentes collect√©es dans des trappes, vous risqueriez, en consid√©rant que les masses proviennent d'une seule distribution, d'√©carter syst√©matiquement une esp√®ce plus petite ou un autre plus imposante. Examinons donc la puissance des moteurs selon le nombre de cylindres.

```{r mv-outliers-bp}
mtcars %>% 
  ggplot(aes(x = factor(cyl), y = hp)) +
  geom_boxplot()
```

#### D√©tection selon la distance interquartile

Selon la d√©finition classique d'un boxplot, un point est affich√© comme aberrant si $x < Q_{25\%}(x) - 1.5 \times IQR_{25\%~75\%}(x)$ ou $x > Q_{75\%}(x) + 1.5 \times IQR_{25\%~75\%}(x)$, o√π $Q{a}$ est le quartile pour la probabilit√© $a$ et $IQR_{a~b}$ est la distance entre les quartiles de $a$ et $b$ ($b>a$). Les probabilit√©s des quartiles (25% et 75%), ainsi que le multiplicateur (1.5) sont arbitraires. On pourra utiliser des fonctions automatiques offertes par des modules sp√©cialis√©s. Mais pour les fonctions simples, pourquoi ne pas les concenvoir soit-m√™me!

```{r mv-iqr-function}
iqr_01 <- function(x, probs = c(.25, .75), mult = 1.5, na.rm = TRUE) {
  # x est le vecteur de valeurs
  # probs est un vecteur de deux valeurs idntifiant les quartiles recherch√©s
  # mult est le multiplicateur
  io <- rep(NA, length(x)) # cr√©er un vecteur vide qui consignera si la valeur est aberrante ou non
  limits <- quantile(x, probs = probs, na.rm = na.rm) # calculer la valeur des quartiles
  offset <- mult * (limits[2] - limits[1]) # calculer la distance limite des quartiles
  io[x > (limits[2] + offset) | x < (limits[1] - offset)] <- 0 # si en-de√ßa ou au-del√† des limites
  io[x <= (limits[2] + offset) & x >= (limits[1] - offset)] <- 1 # si √† l'int√©rieur des limites
  return(io)
}
```

En se servant des possibilit√©s de **`dplyr`**, on pourra d√©tecter les valeurs aberrantes par groupe.

```{r mv-mtcars-filter-iqr}
select <- dplyr::select # pour corriger un bug d√ª au module MASS

mtcars %>% 
  group_by(cyl) %>% # grouper par cylindre
  mutate(io = iqr_01(hp)) %>%  # d√©tecter les valeurs aberrantes
  filter(io == 1) %>% # ne conserver que les valeurs non aberrantes
  select(-io) # enlever la colonne io cr√©√©e pr√©c√©demment
```

Le nouveau tableau est de 31 lignes. La valeur enlev√©e est elle qui apparaissait pr√©c√©demment sur le boxplot.

#### D√©tection selon la cote Z

La cote Z est l'√©cart de la moyenne mesur√©e en terme de nombre d'√©cart-type. Si une valeur est situ√©e √† 3 √©carts-type de la moyenne, la cote Z est de 3. On pourra d√©tecter les valeurs aberrantes selon la distance des points en terme de cote Z, et retrancher les valeurs qui se situes au-del√† d'une certaine limite. Il n'existe pas de distance standard: √† vous de d√©cider. Mais le nombre 3 est souvent utilis√©.

```{r mv-zscore-function}
zscore_01 <- function(x, delimiter = 3, na.rm = TRUE) {
  centered <- x - mean(x, na.rm = na.rm)
  limit <- delimiter * sd(x, na.rm = na.rm)
  io <- ifelse(abs(centered) > limit, 0, 1)
  return(io)
}
```

La foncion `zscore_01` est con√ßue de la m√™me mani√®re que `iqr_01`.

```{r mv-mtcars-filter-zscore}
mtcars %>% 
  group_by(cyl) %>% # grouper par cylindre
  mutate(io = zscore_01(hp)) %>%  # d√©tecter les valeurs aberrantes
  filter(io == 1) %>% # ne conserver que les valeurs non aberrantes
  select(-io) # enlever la colonne io cr√©√©e pr√©c√©demment
```

Selon ce crit√®re, toutes les valeurs sont conserv√©es.

### D√©tection et traitement des √©chantillons aberrants multivari√©s

En analyse multivari√©e, il existe deux approches fondamentales d'identification des valeurs aberrantes: celles bas√©es sur le calcul de distances et les m√©thodes par projection ([Filzmoser et al., 2008](https://www.researchgate.net/publication/222423330_Outlier_identification_in_high_dimension); Hadi et al., 2009).

#### Approches bas√©es sur les distances

##### La distance de Mahalanobis

Les m√©thodes bas√©es sur la distance d√©tectent les valeurs aberrantes en calculant la distance, g√©n√©ralement la distance de Mahalanobis (vue au chapitre \@ref(chapitre-ordination)) entre un point particulier et le centre des donn√©es ([Filzmoser et al., 2008](https://www.researchgate.net/publication/222423330_Outlier_identification_in_high_dimension); [Pires et Santos-Pereira, 2005](https://www.researchgate.net/publication/239850370_Using_Clustering_and_Robust_Estimators_to_Detect_Outliers_in_Multivariate_Data)). Pour un √©chantillon $x$ multivari√©, la distance de Mahalanobis est calcul√©e comme:

$$ \mathscr{M} = \sqrt{(\vec{x}-\vec{\mu})^T S^{-1} (\vec{x}-\vec{\mu})}.\ $$
o√π $\vec{\mu}$ est la moyenne arithm√©tique multivari√©e (le centro√Øde) et $S$ la matrice de variance-covariances de l'√©chantillon, qui doit √™tre invers√©e.

Cette distance indique √† quel point chaque observation est √©loign√©e du centre du nuage multivari√© cr√©√© par les donn√©es ([Alameddine et al., 2010](https://ascelibrary.org/doi/10.1061/%28ASCE%29EE.1943-7870.0000271); [Davies et Gather, 1993](https://www.tandfonline.com/doi/abs/10.1080/01621459.1993.10476339)). D'apr√®s [Alameddine et al. (2010)](https://ascelibrary.org/doi/10.1061/%28ASCE%29EE.1943-7870.0000271), lorsque les donn√©es sont suppos√©es suivre une distribution normale, les carr√©s des distances $\mathscr{M}$ calcul√©es peuvent √™tre consid√©r√©s comme suivant une distribution du $\chi^2$. Par convention, tout point qui a une  d√©passant un quantile donn√© de la distribution du $\chi^2$ (par exemple, $\chi^2_{df = p ; 0.975}$, le quantile 97,5% avec $p$ (le nombre de variables) degr√©s de libert√©), est consid√©r√© comme atypique et identifi√© comme une valeur aberrante ([Filzmoser et al., 2005](https://dl.acm.org/citation.cfm?id=1650448)). Les observations aberrantes multivari√©es peuvent ainsi √™tre d√©finies comme des observations ayant une grande distance de Mahalanobis ($\mathscr{M}^2$).

L'inconv√©nient avec les m√©thodes bas√©es sur les distances r√©side dans la difficult√© d'obtenir des estim√©s robuste de la moyenne $\mu$ et de la matrice de variance-covariances $S$, puisque la distance de Mahalanobis est elle-m√™me sensible aux donn√©es extr√™mes. De plus, il serait difficile de fixer la valeur critique id√©ale de $\mathscr{M}$ permettant de s√©parer les valeurs aberrantes des points r√©guliers ([Filzmoser et al., 2005](https://dl.acm.org/citation.cfm?id=1650448); [Filzmoser et al., 2008](https://www.researchgate.net/publication/222423330_Outlier_identification_in_high_dimension)).

La fonction `sign1` du module mvoutlier d√©tecte les valeurs aberrantes selon un seuil du $\chi^2_{df = 3 ; 0.975}$ pour les transformations en log-ratio isom√©triques de Al, Fe et K dans un humus (l'inverse de la matrice de covariance des les log-ratio centr√©s est singuli√®re).

```{r mv-humus-ilr, message=FALSE}
library("mvoutlier")
library("compositions")
data("humus")
sbp <- matrix(c(1, 1,-1,-1,
                1,-1, 0, 0,
                0, 0, 1,-1), ncol = 4, byrow = TRUE)
ilr_elements <- humus %>%
        dplyr::select(Al, Fe, K, Na) %>%
        ilr(., V = gsi.buildilrBase(t(sbp))) %>%
        as_tibble(.) %>%
        dplyr::rename(AlFe_KNa = V1,
                      Al_Fe = V2,
                      K_Na = V3)
is_out <- sign1(ilr_elements, qcrit = 0.975)$wfinal01
plot(ilr_elements, col = is_out + 2)
```

La proportion de valeurs aberrantes:

```{r mv-humus-ilr-pourc-outliers}
sum(is_out == 0) / length(is_out)
```

Diff√©rentes m√©thodes *robustes* (qui s'accommodent de la pr√©sence de points extr√™mes) de d√©tection des valeurs aberrantes sont pr√©sent√©es dans la litt√©rature telles que la m√©thode du volume minimum de l'ellipso√Øde (**MVE**, *minimum volume ellipsoid*), du d√©terminant minimum de la matrice de covariance (MCD, *minimum Covariance matrix determinant*), et les estimateurs de type maximum de vraisemblance (M-estimators) ([Alameddine et al., 2010](https://ascelibrary.org/doi/10.1061/%28ASCE%29EE.1943-7870.0000271); [Filzmoser et al., 2008](https://www.researchgate.net/publication/222423330_Outlier_identification_in_high_dimension)). Ces m√©thodes calculent des distances robustes similaires aux distances de Mahalanobis, mais remplacent les matrices des moyennes et des covariances respectivement par un seuil critique multivari√© robuste (sur $\mu$) et un estimateur d'√©chelle (sur $S$) qui ne sont pas influenc√©s par les valeurs aberrantes ([Alameddine et al., 2010](https://ascelibrary.org/doi/10.1061/%28ASCE%29EE.1943-7870.0000271)).

##### La m√©thode du volume minimum de l'ellipso√Øde (MVE)

Le volume minimum de l'ellipso√Øde est le plus petit ellipso√Øde r√©gulier couvrant au moins $h$ √©l√©ments de l'ensemble des donn√©es $X = \{x_1, x_2, ..., x_n \}$ o√π l'estimateur de localisation est le centre de cet ellipso√Øde et l'estimateur de dispersion correspond √† sa matrice de covariance. $h$ est fix√© √† priori sup√©rieur ou √©gal √† $\frac{n}{2}+1$, o√π $n$ est le nombre total de points du nuage de donn√©es. Le seuil de d√©tection qui est la fraction des valeurs aberrantes qui, lorsqu'elle est d√©pass√©e entra√Æne des estim√©s totalement biais√©s est de l'ordre de 50% √† mesure que $n$ augmente ([Alameddine et al., 2010](https://ascelibrary.org/doi/10.1061/%28ASCE%29EE.1943-7870.0000271); Croux et al., 2002; [Filzmoser et al., 2005](https://dl.acm.org/citation.cfm?id=1650448); Van Aelst et Rousseeuw, 2009).

L'algorithme MVE est initi√© en choisissant au hasard un ensemble de $p+1$ points de donn√©es pour estimer le mod√®le majoritaire, o√π $p$ est le nombre de variables. Cet ensemble initial est alors augment√© pour contenir les $h$ points de donn√©es. L'algorithme passe par plusieurs it√©rations avant de converger sur l'ensemble des points les plus rapproch√©s qui auront le plus petit volume d'ellipso√Øde ([Alameddine et al., 2010](https://ascelibrary.org/doi/10.1061/%28ASCE%29EE.1943-7870.0000271)).

Le module MASS comprend la fonction `cov.mve` √† cet effet. Cette fonction demande le nombre minimal de points que l'on d√©sire conserver, en absolu. Il s'agit d'un nombre entier, alors si l'on d√©sire en utiliser une fraction (ici, 90%), il faut l'arrondir. Parmi les sorties de la fonction `cov.mve`, on retrouve les num√©ros de ligne qui se trouvent √† l'int√©rieur de l'ellipsoide.

```{r mv-humus-ilr-outliers-mve, message=FALSE}
library("MASS")
select <- dplyr::select # pour √©viter que la fonction select du module MASS remplace celle de dplyr
min_in <- round(0.9 * nrow(ilr_elements)) # le minimum de points √† garder, 90% du total
id_in <- cov.mve(ilr_elements, quantile.used = min_in)$best
is_in <- 1:nrow(ilr_elements) %in% id_in
plot(ilr_elements, col = is_in + 2)
```

La proportion de valeurs aberrantes:

```{r mv-humus-ilr-outliers-mve-pourc}
sum(!is_in) / length(is_in)
```

##### La m√©thode du d√©terminant minimum de la matrice de covariance (MCD)

La m√©thode du d√©terminant minimum de la matrice de covariance a pour objectif de trouver $h$ ($h > n$) observations de l'ensemble de donn√©es $X = \{x_1, x_2, ..., x_n \}$,  dont la matrice de covariance a le plus petit d√©terminant. Comme avec la m√©thode MVE, l'estimateur de localisation est la moyenne de ces $h$ points et celui de la dispersion est proportionnel √† la matrice de covariance ([Filzmoser et al., 2005](https://dl.acm.org/citation.cfm?id=1650448); [Hubert et al., 2018](https://onlinelibrary.wiley.com/doi/full/10.1002/wics.1421); [Rousseeuw et Van Driessen, 1999](https://www.tandfonline.com/doi/abs/10.1080/00401706.1999.10485670)).

```{r mv-humus-ilr-outliers-mcd, message=FALSE}
id_in <- cov.mcd(ilr_elements, quantile.used = min_in)$best
is_in <- 1:nrow(ilr_elements) %in% id_in
plot(ilr_elements, col = is_in + 2)
```

La proportion de valeurs aberrantes:

```{r mv-humus-ilr-outliers-mcd-pourc}
sum(!is_in) / length(is_in)
```

Mais en cas de dissym√©trie des donn√©es, ces tests (MVE, MCD) ne seraient pas applicables ([Planchon, 2005](https://www.researchgate.net/publication/26406403_Traitement_des_valeurs_aberrantes_concepts_actuels_et_tendances_generales)).

#### Les m√©thodes par projection

Ces m√©thodes de d√©tection des observations aberrantes trouvent des projections appropri√©es des donn√©es dans lesquelles les observations aberrantes sont facilement apparentes. Ces observations sont ensuite pond√©r√©s pour produire un estimateur robuste pouvant √™tre utilis√© pour identifier les observations aberrantes ([Filzmoser et al., 2008](https://www.researchgate.net/publication/222423330_Outlier_identification_in_high_dimension)). Ces m√©thodes n'assument pas une distribution particuli√®re des donn√©es mais cherchent des projections utiles. Elles ne sont donc pas affect√©es par la non-normalit√© et s'appliquent sur divers types de distributions ([Filzmoser et al., 2008](https://www.researchgate.net/publication/222423330_Outlier_identification_in_high_dimension); [Hadi et al., 2009](https://onlinelibrary.wiley.com/doi/pdf/10.1002/wics.6)). Le but de cette projection exploratoire est d'utiliser les donn√©es pour trouver des projections minimales (√† une, deux ou trois dimensions) qui fournissent les vues les plus r√©v√©latrices des donn√©es compl√®tes ([Friedman, 1987](https://amstat.tandfonline.com/doi/abs/10.1080/01621459.1987.10478427)). La m√©thode attribue un indice num√©rique √† chaque projection en fonction de la densit√© des donn√©es projet√©e pour capturer le degr√© de structure non lin√©aire pr√©sent dans la distribution projet√©e ([Friedman, 1987](https://amstat.tandfonline.com/doi/abs/10.1080/01621459.1987.10478427); [Hadi et al., 2009](https://onlinelibrary.wiley.com/doi/pdf/10.1002/wics.6)).

En R, nous revenons au module mvoutlier, mais cette fois-ci avec la fonction `sign2`, du module **`mvoutlier`**.

```{r mv-humus-ilr-outliers-sign2}
library("mvoutlier")
is_out <- sign2(ilr_elements, qcrit = 0.975)$wfinal01
plot(ilr_elements, col = is_out + 2)
```

La proportion de valeurs aberrantes:

```{r mv-humus-ilr-outliers-sign2-pourc}
sum(is_out == 0) / length(is_out)
```

```{r, include=FALSE}
rm(list = ls())
```